[
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(Kokkos::View<const Point*> &points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min:min_distance)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\tint n = points.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min:min_dist)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tmin_dist = std::min(min_dist, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min:min_dist)\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tmin_dist = std::min(min_dist, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double minDist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < minDist) {\n\t\t\t\tminDist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn minDist;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min: min_distance)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "int n = points.size();\n\tdouble min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i+1; j < n; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min:min_dist)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\tint n = points.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min:min_distance)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tmin_distance = std::min(min_distance, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\t#pragma omp parallel for reduction(min: min_distance)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_distance) {\n\t\t\t\tmin_distance = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_dist = std::numeric_limits<double>::max();\n\tint n = points.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tint n = points.size();\n\tint i, j;\n\t#pragma omp parallel for private(i, j)\n\tfor (i = 0; i < n; i++) {\n\t\tfor (j = i + 1; j < n; j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tmin_distance = std::min(min_distance, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min_distance;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\n\tstd::vector<Point> points_sorted;\n\tpoints_sorted.reserve(points.size());\n\tfor (auto const& point : points) {\n\t\tpoints_sorted.push_back(point);\n\t}\n\tstd::sort(points_sorted.begin(), points_sorted.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\n\tdouble min_distance = std::numeric_limits<double>::max();\n\tfor (auto it = points_sorted.begin(); it!= points_sorted.end(); ++it) {\n\t\tauto it2 = it + 1;\n\t\twhile (it2!= points_sorted.end() && it2->x - it->x < min_distance) {\n\t\t\tdouble distance = distance(*it, *it2);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t\t++it2;\n\t\t}\n\t}\n\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tmin_distance = std::min(min_distance, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "if (points.size() <= 3) {\n\t\tdouble min = distance(points[0], points[1]);\n\t\tfor (int i = 2; i < points.size(); i++) {\n\t\t\tdouble temp = distance(points[i], points[i-1]);\n\t\t\tif (temp < min) {\n\t\t\t\tmin = temp;\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tstd::vector<Point> left, right;\n\tPoint pivot = points[points.size()/2];\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tif (points[i].x < pivot.x) {\n\t\t\tleft.push_back(points[i]);\n\t\t} else {\n\t\t\tright.push_back(points[i]);\n\t\t}\n\t}\n\n\tdouble min = closestPair(left);\n\tdouble temp = closestPair(right);\n\tif (temp < min) {\n\t\tmin = temp;\n\t}\n\n\tfor (int i = 0; i < left.size(); i++) {\n\t\tfor (int j = 0; j < right.size(); j++) {\n\t\t\tdouble temp = distance(left[i], right[j]);\n\t\t\tif (temp < min) {\n\t\t\t\tmin = temp;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min;\n}",
            "if (points.size() < 2) {\n\t\tthrow std::invalid_argument(\"Not enough points\");\n\t}\n\n\tdouble min_distance = distance(points[0], points[1]);\n\tfor (size_t i = 0; i < points.size()-1; ++i) {\n\t\tfor (size_t j = i+1; j < points.size(); ++j) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min_distance;\n}",
            "double min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\tif (d < min) {\n\t\t\t\tmin = d;\n\t\t\t}\n\t\t}\n\t}\n\treturn min;\n}",
            "double min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min) {\n\t\t\t\tmin = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min;\n}",
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tstd::vector<Point> sortedPoints(points);\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\tdouble minDistance = distance(sortedPoints[0], sortedPoints[1]);\n\tfor (int i = 1; i < sortedPoints.size(); ++i) {\n\t\tdouble distance = distance(sortedPoints[i], sortedPoints[i-1]);\n\t\tif (distance < minDistance) {\n\t\t\tminDistance = distance;\n\t\t}\n\t}\n\treturn minDistance;\n}",
            "if (points.size() <= 3) {\n\t\tdouble min = distance(points[0], points[1]);\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < min) {\n\t\t\t\t\tmin = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\t// sort the points by x-coordinate\n\tstd::vector<Point> points_sorted(points);\n\tstd::sort(points_sorted.begin(), points_sorted.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\n\t// find the median point\n\tint median_index = points_sorted.size() / 2;\n\tPoint median = points_sorted[median_index];\n\n\t// divide the points into two groups\n\tstd::vector<Point> points_left, points_right;\n\tfor (int i = 0; i < points_sorted.size(); i++) {\n\t\tif (i == median_index) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (points_sorted[i].x < median.x) {\n\t\t\tpoints_left.push_back(points_sorted[i]);\n\t\t} else {\n\t\t\tpoints_right.push_back(points_sorted[i]);\n\t\t}\n\t}\n\n\t// find the closest pair in the left group\n\tdouble min_left = closestPair(points_left);\n\n\t// find the closest pair in the right group\n\tdouble min_right = closestPair(points_right);\n\n\t// find the closest pair in the left and right groups\n\tdouble min_lr = distance(median, points_left.back());\n\tfor (int i = 0; i < points_left.size(); i++) {\n\t\tdouble d = distance(points_left[i], points_right[0]);\n\t\tif (d < min_lr) {\n\t\t\tmin_lr = d;\n\t\t}\n\t}\n\n\t// return the minimum of the three distances\n\treturn std::min(std::min(min_left, min_right), min_lr);\n}",
            "if (points.size() <= 3) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\t\tmin = std::min(min, distance(points[i], points[j]));\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tstd::vector<Point> left, right;\n\tPoint mid = points[points.size() / 2];\n\tfor (Point const& p : points) {\n\t\tif (p.x < mid.x) {\n\t\t\tleft.push_back(p);\n\t\t} else {\n\t\t\tright.push_back(p);\n\t\t}\n\t}\n\n\tdouble min = std::numeric_limits<double>::max();\n\tfor (Point const& p : left) {\n\t\tfor (Point const& q : right) {\n\t\t\tmin = std::min(min, distance(p, q));\n\t\t}\n\t}\n\n\tdouble d1 = closestPair(left);\n\tdouble d2 = closestPair(right);\n\treturn std::min(min, std::min(d1, d2));\n}",
            "// TODO: implement\n\treturn 0.0;\n}",
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\n\tstd::vector<Point> sortedPoints(points);\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\n\tstd::vector<Point> leftPoints;\n\tstd::vector<Point> rightPoints;\n\tfor (auto const& point : sortedPoints) {\n\t\tif (point.x < sortedPoints[0].x + 1) {\n\t\t\tleftPoints.push_back(point);\n\t\t} else {\n\t\t\trightPoints.push_back(point);\n\t\t}\n\t}\n\n\tdouble minDistance = std::numeric_limits<double>::max();\n\tfor (auto const& point : leftPoints) {\n\t\tfor (auto const& otherPoint : rightPoints) {\n\t\t\tdouble distance = distance(point, otherPoint);\n\t\t\tif (distance < minDistance) {\n\t\t\t\tminDistance = distance;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn minDistance;\n}",
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\n\tstd::vector<Point> sortedPoints;\n\tsortedPoints.reserve(points.size());\n\tstd::copy(points.begin(), points.end(), std::back_inserter(sortedPoints));\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\n\tdouble minDistance = std::numeric_limits<double>::max();\n\tfor (auto it = sortedPoints.begin(); it!= sortedPoints.end(); ++it) {\n\t\tauto it2 = std::next(it);\n\t\twhile (it2!= sortedPoints.end() && it2->x - it->x < minDistance) {\n\t\t\tdouble distance = distance(*it, *it2);\n\t\t\tif (distance < minDistance) {\n\t\t\t\tminDistance = distance;\n\t\t\t}\n\t\t\t++it2;\n\t\t}\n\t}\n\n\treturn minDistance;\n}",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "// TODO: Your code here\n\tdouble min = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i+1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_distance = distance(points[0], points[1]);\n\tfor (int i = 0; i < points.size()-1; i++) {\n\t\tfor (int j = i+1; j < points.size(); j++) {\n\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}",
            "double min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min) {\n\t\t\t\tmin = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min;\n}",
            "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tif (points.size() == 2) {\n\t\treturn distance(points[0], points[1]);\n\t}\n\tstd::vector<Point> left, right;\n\tPoint const& mid = points[points.size()/2];\n\tfor (Point const& p : points) {\n\t\tif (p.x < mid.x) {\n\t\t\tleft.push_back(p);\n\t\t} else {\n\t\t\tright.push_back(p);\n\t\t}\n\t}\n\tdouble dl = closestPair(left);\n\tdouble dr = closestPair(right);\n\tdouble d = std::min(dl, dr);\n\tstd::vector<Point> close;\n\tfor (Point const& p : points) {\n\t\tif (distance(p, mid) <= d) {\n\t\t\tclose.push_back(p);\n\t\t}\n\t}\n\tfor (size_t i = 0; i < close.size(); ++i) {\n\t\tfor (size_t j = i+1; j < close.size(); ++j) {\n\t\t\td = std::min(d, distance(close[i], close[j]));\n\t\t}\n\t}\n\treturn d;\n}",
            "double min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tmin = std::min(min, distance(points[i], points[j]));\n\t\t}\n\t}\n\treturn min;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\tif (tid == 0) {\n\t\tdouble minDistance = 1e10;\n\t\tfor (int i = bid; i < numPoints; i += nb) {\n\t\t\tfor (int j = i + 1; j < numPoints; j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < minDistance) {\n\t\t\t\t\tminDistance = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t*distance = minDistance;\n\t}\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\tint i = bid * nt + tid;\n\tif (i < numPoints) {\n\t\ts_points[tid] = points[i];\n\t\ts_distance[tid] = 1e10;\n\t}\n\t__syncthreads();\n\n\tfor (int j = 0; j < nt; j++) {\n\t\tif (i < numPoints && j < numPoints) {\n\t\t\tdouble d = distance(s_points[tid], s_points[j]);\n\t\t\tif (d < s_distance[tid]) {\n\t\t\t\ts_distance[tid] = d;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\n\tfor (int stride = 1; stride < nt; stride *= 2) {\n\t\tif (tid % (2 * stride) == 0) {\n\t\t\tif (s_distance[tid] > s_distance[tid + stride]) {\n\t\t\t\ts_distance[tid] = s_distance[tid + stride];\n\t\t\t}\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tif (tid == 0) {\n\t\t*distance = s_distance[0];\n\t}\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n\t// Hint: Use the distance function defined above\n\t// Hint: Use the atomicMin function to update the distance variable\n\t// Hint: Use the __syncthreads function to synchronize threads\n\t// Hint: Use the __shfl_down function to get the minimum distance\n\t// Hint: Use the __shfl_sync function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_uint32 function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_uint64 function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_float function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the __shfl_sync_down_double function to get the minimum distance\n\t// Hint: Use the",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\n\t// TODO: Implement this function\n\t// Hint: Use the distance function defined above\n\t// Hint: You can use the min function to find the minimum distance\n\t// Hint: You can use the atomicMin function to find the minimum distance\n\t// Hint: You can use the __shfl_down function to find the minimum distance\n\t// Hint: You can use the __shfl_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_down_sync function to find the minimum distance\n\t// Hint: You can use the __shfl_sync_",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (tid >= numPoints) return;\n\tdouble minDistance = distance(points[0], points[1]);\n\tfor (int i = 1; i < numPoints; i++) {\n\t\tif (i == tid) continue;\n\t\tdouble d = distance(points[tid], points[i]);\n\t\tif (d < minDistance) minDistance = d;\n\t}\n\tif (tid == 0) *distance = minDistance;\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement this function\n\t// Hint: Use the distance function defined above\n\t// Hint: Use a parallel reduction to find the closest pair\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < numPoints && j < numPoints) {\n\t\tif (distance[0] > distance(points[i], points[j])) {\n\t\t\tdistance[0] = distance(points[i], points[j]);\n\t\t}\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\ts_points[tid] = points[bid*nt + tid];\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tdouble min_distance = distance(s_points[0], s_points[1]);\n\t\tfor (int i = 1; i < nt; i++) {\n\t\t\tdouble d = distance(s_points[0], s_points[i]);\n\t\t\tif (d < min_distance) {\n\t\t\t\tmin_distance = d;\n\t\t\t}\n\t\t}\n\t\ts_distance[0] = min_distance;\n\t}\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tdouble min_distance = s_distance[0];\n\t\tfor (int i = 1; i < nt; i++) {\n\t\t\tdouble d = s_distance[i];\n\t\t\tif (d < min_distance) {\n\t\t\t\tmin_distance = d;\n\t\t\t}\n\t\t}\n\t\t*distance = min_distance;\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\tint numBlocks = gridDim.x;\n\n\t__shared__ Point s_points[256];\n\t__shared__ double s_distance[256];\n\n\tif (tid == 0) {\n\t\ts_distance[bid] = 1e10;\n\t}\n\t__syncthreads();\n\n\tfor (int i = bid; i < numPoints; i += numBlocks) {\n\t\tfor (int j = i+1; j < numPoints; j += numBlocks) {\n\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\tif (d < s_distance[bid]) {\n\t\t\t\ts_distance[bid] = d;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tfor (int i = 1; i < numThreads; i++) {\n\t\t\tif (s_distance[i] < s_distance[0]) {\n\t\t\t\ts_distance[0] = s_distance[i];\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\t*distance = s_distance[0];\n\t}\n}",
            "// TODO: Implement\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (size == 1) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 0; i < points.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\t\tmin = std::min(min, distance(points[i], points[j]));\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tint chunk = points.size() / size;\n\tint remainder = points.size() % size;\n\n\tstd::vector<Point> myPoints;\n\tfor (int i = 0; i < chunk; ++i) {\n\t\tmyPoints.push_back(points[rank * chunk + i]);\n\t}\n\tif (rank < remainder) {\n\t\tmyPoints.push_back(points[rank * chunk + chunk + remainder]);\n\t}\n\n\tdouble min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < myPoints.size(); ++i) {\n\t\tfor (int j = i + 1; j < myPoints.size(); ++j) {\n\t\t\tmin = std::min(min, distance(myPoints[i], myPoints[j]));\n\t\t}\n\t}\n\n\tdouble min_global;\n\tMPI_Reduce(&min, &min_global, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn min_global;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size == 1) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < min) {\n\t\t\t\t\tmin = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tint chunk = points.size() / size;\n\tint remainder = points.size() % size;\n\tint start = rank * chunk;\n\tint end = start + chunk;\n\tif (rank == size - 1) {\n\t\tend += remainder;\n\t}\n\n\tdouble min = std::numeric_limits<double>::max();\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = i + 1; j < end; j++) {\n\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\tif (d < min) {\n\t\t\t\tmin = d;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble min_all;\n\tMPI_Reduce(&min, &min_all, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\treturn min_all;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size == 1) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < min) {\n\t\t\t\t\tmin = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tint chunk = points.size() / size;\n\tint remainder = points.size() % size;\n\n\tstd::vector<Point> myPoints;\n\tif (rank == 0) {\n\t\tmyPoints.insert(myPoints.end(), points.begin(), points.begin() + chunk + remainder);\n\t} else {\n\t\tmyPoints.insert(myPoints.end(), points.begin() + rank * chunk, points.begin() + (rank + 1) * chunk);\n\t}\n\n\tdouble min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < myPoints.size(); i++) {\n\t\tfor (int j = i + 1; j < myPoints.size(); j++) {\n\t\t\tdouble d = distance(myPoints[i], myPoints[j]);\n\t\t\tif (d < min) {\n\t\t\t\tmin = d;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble min_all;\n\tMPI_Reduce(&min, &min_all, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn min_all;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tdouble tmp;\n\t\t\tMPI_Recv(&tmp, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tif (tmp < min) {\n\t\t\t\tmin = tmp;\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t} else {\n\t\tint start = rank * points.size() / size;\n\t\tint end = (rank + 1) * points.size() / size;\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = start; i < end; i++) {\n\t\t\tfor (int j = i + 1; j < end; j++) {\n\t\t\t\tif (distance(points[i], points[j]) < min) {\n\t\t\t\t\tmin = distance(points[i], points[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tMPI_Send(&min, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t\treturn 0;\n\t}\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\tdouble min_dist = 0;\n\tint min_i = 0, min_j = 0;\n\tif (points.size() == 2) {\n\t\tmin_dist = distance(points[0], points[1]);\n\t\tmin_i = 0;\n\t\tmin_j = 1;\n\t}\n\telse if (points.size() > 2) {\n\t\tint mid = points.size() / 2;\n\t\tstd::vector<Point> left(points.begin(), points.begin() + mid);\n\t\tstd::vector<Point> right(points.begin() + mid, points.end());\n\n\t\tdouble left_dist, right_dist;\n\t\tint left_i, left_j, right_i, right_j;\n\t\tif (rank == 0) {\n\t\t\tleft_dist = closestPair(left);\n\t\t\tright_dist = closestPair(right);\n\t\t}\n\t\telse {\n\t\t\tMPI_Send(&left[0], left.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&right[0], right.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t\t}\n\n\t\tif (rank == 0) {\n\t\t\tMPI_Recv(&left_dist, 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tMPI_Recv(&left_i, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tMPI_Recv(&left_j, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t\t\tMPI_Recv(&right_dist, 1, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tMPI_Recv(&right_i, 1, MPI_INT, 2, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tMPI_Recv(&right_j, 1, MPI_INT, 2, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t\t\tif (left_dist < right_dist) {\n\t\t\t\tmin_dist = left_dist;\n\t\t\t\tmin_i = left_i;\n\t\t\t\tmin_j = left_j;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tmin_dist = right_dist;\n\t\t\t\tmin_i = right_i;\n\t\t\t\tmin_j = right_j;\n\t\t\t}\n\t\t}\n\t\telse if (rank == 1) {\n\t\t\tMPI_Send(&left_dist, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&left_i, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&left_j, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t\t}\n\t\telse if (rank == 2) {\n\t\t\tMPI_Send(&right_dist, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&right_i, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&right_j, 1, MPI_INT, 0, 0, MPI_COMM_WOR",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tstd::vector<Point> points_rank0(points.begin(), points.begin() + points.size() / size);\n\t\tdouble min_distance = distance(points_rank0[0], points_rank0[1]);\n\t\tfor (int i = 1; i < points_rank0.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points_rank0.size(); ++j) {\n\t\t\t\tdouble distance = distance(points_rank0[i], points_rank0[j]);\n\t\t\t\tif (distance < min_distance) {\n\t\t\t\t\tmin_distance = distance;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min_distance;\n\t} else {\n\t\tstd::vector<Point> points_rank(points.begin() + points.size() / size * rank, points.begin() + points.size() / size * (rank + 1));\n\t\tdouble min_distance = distance(points_rank[0], points_rank[1]);\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points_rank.size(); ++j) {\n\t\t\t\tdouble distance = distance(points_rank[i], points_rank[j]);\n\t\t\t\tif (distance < min_distance) {\n\t\t\t\t\tmin_distance = distance;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min_distance;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tdouble min = distance(points[0], points[1]);\n\t\tfor (int i = 1; i < points.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < min) {\n\t\t\t\t\tmin = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\telse {\n\t\t// TODO\n\t}\n}",
            "// TODO: your code here\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = points.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<Point> points_proc;\n\tif (rank < n_rem) {\n\t\tpoints_proc.insert(points_proc.end(), points.begin() + rank * (n_per_proc + 1), points.begin() + (rank + 1) * (n_per_proc + 1));\n\t} else {\n\t\tpoints_proc.insert(points_proc.end(), points.begin() + rank * n_per_proc + n_rem, points.begin() + (rank + 1) * n_per_proc + n_rem);\n\t}\n\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points_proc.size(); i++) {\n\t\tfor (int j = i + 1; j < points_proc.size(); j++) {\n\t\t\tdouble dist = distance(points_proc[i], points_proc[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble min_dist_global;\n\tMPI_Reduce(&min_dist, &min_dist_global, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn min_dist_global;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\tdouble min_dist = 0;\n\tif (rank == 0) {\n\t\tmin_dist = distance(points[0], points[1]);\n\t\tfor (int i = 2; i < points.size(); i++) {\n\t\t\tdouble dist = distance(points[0], points[i]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (size == 1) {\n\t\tdouble min = std::numeric_limits<double>::max();\n\t\tfor (int i = 0; i < points.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\t\tmin = std::min(min, distance(points[i], points[j]));\n\t\t\t}\n\t\t}\n\t\treturn min;\n\t}\n\n\tint pointsPerRank = points.size() / size;\n\tint remainder = points.size() % size;\n\tint start = rank * pointsPerRank;\n\tint end = start + pointsPerRank;\n\tif (rank == size - 1) {\n\t\tend += remainder;\n\t}\n\n\tstd::vector<Point> localPoints(points.begin() + start, points.begin() + end);\n\n\tdouble localMin = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < localPoints.size(); ++i) {\n\t\tfor (int j = i + 1; j < localPoints.size(); ++j) {\n\t\t\tlocalMin = std::min(localMin, distance(localPoints[i], localPoints[j]));\n\t\t}\n\t}\n\n\tdouble globalMin;\n\tMPI_Reduce(&localMin, &globalMin, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn globalMin;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[256];\n\t__shared__ double s_distance[256];\n\n\t// Copy the points to the shared memory\n\ts_points[tid] = points[bid*nt + tid];\n\t__syncthreads();\n\n\t// Compute the distance between the points in the shared memory\n\tif (tid < numPoints) {\n\t\ts_distance[tid] = distance(s_points[tid], s_points[tid+1]);\n\t}\n\t__syncthreads();\n\n\t// Find the minimum distance in the shared memory\n\tfor (int i = 1; i < nt; i *= 2) {\n\t\tif (tid % (2*i) == 0 && tid + i < nt) {\n\t\t\ts_distance[tid] = min(s_distance[tid], s_distance[tid + i]);\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\t// Copy the minimum distance to the global memory\n\tif (tid == 0) {\n\t\tdistance[bid] = s_distance[0];\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\tif (tid == 0) {\n\t\ts_distance[bid] = 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "// TODO: Implement this function\n\t// Hint: You can use the distance function defined above\n\t// Hint: You can use the atomicMin function to update distance\n\t// Hint: You can use the __syncthreads function to synchronize threads\n\t// Hint: You can use the threadIdx.x variable to identify the thread\n\t// Hint: You can use the blockIdx.x variable to identify the block\n\t// Hint: You can use the blockDim.x variable to identify the number of threads per block\n\t// Hint: You can use the gridDim.x variable to identify the number of blocks\n\t// Hint: You can use the __shfl_down function to share values between threads\n\t// Hint: You can use the __shfl_sync function to share values between threads\n\t// Hint: You can use the __shfl_up function to share values between threads\n\t// Hint: You can use the __shfl_xor function to share values between threads\n\t// Hint: You can use the __shfl function to share values between threads\n\t// Hint: You can use the __syncthreads_or function to synchronize threads\n\t// Hint: You can use the __syncthreads_and function to synchronize threads\n\t// Hint: You can use the __syncthreads_count function to synchronize threads\n\t// Hint: You can use the __syncthreads_ballot function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadfence_system function to synchronize threads\n\t// Hint: You can use the __threadfence_block function to synchronize threads\n\t// Hint: You can use the __threadf",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\tint start = bid * blockDim.x + tid;\n\tint end = numPoints - 1;\n\tint mid = (start + end) / 2;\n\tint left = 2 * bid * blockDim.x + tid;\n\tint right = 2 * bid * blockDim.x + tid + blockDim.x;\n\tdouble minDist = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "// TODO: Implement this function\n\t// Hint: Use the distance function defined above\n\t// Hint: Use the atomicMin function to update the distance variable\n\t// Hint: Use the __syncthreads function to synchronize threads\n\t// Hint: Use the __threadfence_block function to synchronize threads\n\t// Hint: Use the __threadfence function to synchronize threads\n\t// Hint: Use the __syncthreads_or function to synchronize threads\n\t// Hint: Use the __syncthreads_and function to synchronize threads\n\t// Hint: Use the __syncthreads_count function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads_or_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_and_sync_count_all function to synchronize threads\n\t// Hint: Use the __syncthreads_count_sync_all function to synchronize threads\n\t// Hint: Use the __syncthreads",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\tif (tid == 0) {\n\t\tdouble min = distance(points[0], points[1]);\n\t\tfor (int i = 1; i < numPoints; i++) {\n\t\t\tfor (int j = i + 1; j < numPoints; j++) {\n\t\t\t\tdouble d = distance(points[i], points[j]);\n\t\t\t\tif (d < min) {\n\t\t\t\t\tmin = d;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t*distance = min;\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\tint i = bid * blockDim.x + tid;\n\tint j = i + 1;\n\tint k = i + 2;\n\tint l = i + 3;\n\tint m = i + 4;\n\tint n = i + 5;\n\tint o = i + 6;\n\tint p = i + 7;\n\tint q = i + 8;\n\tint r = i + 9;\n\tint s = i + 10;\n\tint t = i + 11;\n\tint u = i + 12;\n\tint v = i + 13;\n\tint w = i + 14;\n\tint x = i + 15;\n\tint y = i + 16;\n\tint z = i + 17;\n\tint a = i + 18;\n\tint b = i + 19;\n\tint c = i + 20;\n\tint d = i + 21;\n\tint e = i + 22;\n\tint f = i + 23;\n\tint g = i + 24;\n\tint h = i + 25;\n\tint jj = i + 26;\n\tint kk = i + 27;\n\tint ll = i + 28;\n\tint mm = i + 29;\n\tint nn = i + 30;\n\tint oo = i + 31;\n\tint pp = i + 32;\n\tint qq = i + 33;\n\tint rr = i + 34;\n\tint ss = i + 35;\n\tint tt = i + 36;\n\tint uu = i + 37;\n\tint vv = i + 38;\n\tint ww = i + 39;\n\tint xx = i + 40;\n\tint yy = i + 41;\n\tint zz = i + 42;\n\tint aa = i + 43;\n\tint bb = i + 44;\n\tint cc = i + 45;\n\tint dd = i + 46;\n\tint ee = i + 47;\n\tint ff = i + 48;\n\tint gg = i + 49;\n\tint hh = i + 50;\n\tint jjj = i + 51;\n\tint kkk = i + 52;\n\tint lll = i + 53;\n\tint mmm = i + 54;\n\tint nnn = i + 55;\n\tint ooo = i + 56;\n\tint ppp = i + 57;\n\tint qqq = i + 58;\n\tint rrr = i + 59;\n\tint sss = i + 60;\n\tint ttt = i + 61;\n\tint uuu = i + 62;\n\tint vvv = i + 63;\n\tint www = i + 64;\n\tint xxx = i + 65;\n\tint yyy = i + 66;\n\tint zzz = i + 67;\n\tint aaa = i + 68;\n\tint bbb = i + 69;\n\tint ccc = i + 70;\n\tint ddd = i + 71;\n\tint eee = i + 72;\n\tint fff = i + 73;\n\tint ggg = i + 74;\n\tint hhh = i + 75;\n\tint jjjj = i + 76;\n\tint kkkk = i + 77;\n\tint llll = i + 78;\n\tint mmmm = i + 79;\n\tint nnnn = i + 80;\n\tint oooo = i + 81;\n\tint pppp = i + 82;\n\tint qqqq = i + 83;\n\tint rrrr = i +",
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\tint i = bid * blockDim.x + tid;\n\tint j = i + 1;\n\tdouble minDist = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nthreads = blockDim.x;\n\tint nblocks = gridDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\tint i = tid + bid * nthreads;\n\tif (i < numPoints) {\n\t\ts_points[tid] = points[i];\n\t\ts_distance[tid] = distance(s_points[tid], s_points[tid]);\n\t}\n\t__syncthreads();\n\n\tfor (int s = 1; s < nthreads; s *= 2) {\n\t\tint index = 2 * s * tid;\n\t\tif (index < nthreads) {\n\t\t\tif (index + s < nthreads) {\n\t\t\t\ts_distance[index] = min(s_distance[index], s_distance[index + s]);\n\t\t\t\ts_points[index] = s_distance[index] == s_distance[index]? s_points[index] : s_points[index + s];\n\t\t\t}\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tif (tid == 0) {\n\t\t*distance = s_distance[0];\n\t}\n}",
            "// TODO: Your code here\n\tint tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\tint numBlocks = gridDim.x;\n\tint numPointsPerBlock = numPoints / numBlocks;\n\tint numPointsThisBlock = numPointsPerBlock + (bid < numPoints % numBlocks? 1 : 0);\n\tint start = bid * numPointsPerBlock + min(bid, numPoints % numBlocks);\n\tint end = start + numPointsThisBlock;\n\tdouble minDistance = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "// TODO: Implement this function\n\t// Hint: Use the distance function above\n\t// Hint: Use the atomicMin function to update the distance variable\n\t// Hint: Use the __syncthreads function to synchronize threads\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\tif (tid == 0) {\n\t\ts_distance[bid] = 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\n\t__shared__ Point s_points[1024];\n\t__shared__ double s_distance[1024];\n\n\ts_points[tid] = points[bid * nt + tid];\n\ts_distance[tid] = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\tint i = bid * blockDim.x + tid;\n\tint j = i + 1;\n\tint n = numPoints;\n\tdouble minDist = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_closest_pair",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_left = n % size;\n\n\tstd::vector<Point> local_points;\n\tif (rank < n_left) {\n\t\tlocal_points.insert(local_points.end(), points.begin() + rank * (n_per_rank + 1), points.begin() + (rank + 1) * (n_per_rank + 1));\n\t} else {\n\t\tlocal_points.insert(local_points.end(), points.begin() + rank * n_per_rank + n_left, points.begin() + (rank + 1) * n_per_rank + n_left);\n\t}\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tlocal_min = std::min(local_min, distance(local_points[i], local_points[j]));\n\t\t}\n\t}\n\n\tdouble global_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tstd::vector<Point> points_copy(points);\n\t\tdouble min_distance = distance(points_copy[0], points_copy[1]);\n\t\tfor (int i = 1; i < points_copy.size(); i++) {\n\t\t\tfor (int j = i + 1; j < points_copy.size(); j++) {\n\t\t\t\tdouble distance = distance(points_copy[i], points_copy[j]);\n\t\t\t\tif (distance < min_distance) {\n\t\t\t\t\tmin_distance = distance;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn min_distance;\n\t} else {\n\t\t// TODO: Your code here\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_rem = n % size;\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_rem;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tdouble d = distance(local_points[i], local_points[j]);\n\t\t\tif (d < local_min) {\n\t\t\t\tlocal_min = d;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble global_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble min_dist = std::numeric_limits<double>::max();\n\n\t// TODO: Your code here\n\n\treturn min_dist;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tlocal_min = std::min(local_min, distance(local_points[i], local_points[j]));\n\t\t}\n\t}\n\n\tdouble global_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tlocal_min = std::min(local_min, distance(local_points[i], local_points[j]));\n\t\t}\n\t}\n\n\tdouble global_min = local_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\tint start_index = rank * num_points_per_rank;\n\tint end_index = start_index + num_points_per_rank;\n\tif (rank == size - 1) {\n\t\tend_index += num_points_remainder;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start_index, points.begin() + end_index);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tdouble dist = distance(local_points[i], local_points[j]);\n\t\t\tif (dist < local_min) {\n\t\t\t\tlocal_min = dist;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble global_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "// TODO: Your code here\n\tdouble min_distance = 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\n\tstd::vector<Point> points_rank;\n\tif (rank < num_points_remainder) {\n\t\tpoints_rank.resize(num_points_per_rank + 1);\n\t\tMPI_Scatter(points.data(), num_points_per_rank + 1,\n\t\t\t\t\tMPI_DOUBLE, points_rank.data(), num_points_per_rank + 1,\n\t\t\t\t\tMPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t} else {\n\t\tpoints_rank.resize(num_points_per_rank);\n\t\tMPI_Scatter(points.data(), num_points_per_rank,\n\t\t\t\t\tMPI_DOUBLE, points_rank.data(), num_points_per_rank,\n\t\t\t\t\tMPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t}\n\n\tdouble min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points_rank.size(); i++) {\n\t\tfor (int j = i + 1; j < points_rank.size(); j++) {\n\t\t\tdouble distance_ij = distance(points_rank[i], points_rank[j]);\n\t\t\tif (distance_ij < min_distance) {\n\t\t\t\tmin_distance = distance_ij;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble min_distance_global;\n\tMPI_Reduce(&min_distance, &min_distance_global, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn min_distance_global;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble min_dist = std::numeric_limits<double>::max();\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; ++i) {\n\t\t\tdouble local_min_dist;\n\t\t\tMPI_Recv(&local_min_dist, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tmin_dist = std::min(min_dist, local_min_dist);\n\t\t}\n\t} else {\n\t\tdouble local_min_dist = std::numeric_limits<double>::max();\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < points.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\t\tif (dist < local_min_dist) {\n\t\t\t\t\tlocal_min_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tMPI_Send(&local_min_dist, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\treturn min_dist;\n}",
            "// TODO: Your code here\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tint size = points.size();\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint num_threads = omp_get_max_threads();\n\tint num_points = size / num_threads;\n\tint start = rank * num_points;\n\tint end = start + num_points;\n\tif (rank == 0) {\n\t\tend = size;\n\t}\n\telse if (rank == num_threads - 1) {\n\t\tend = size;\n\t}\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = i + 1; j < end; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\tdouble min_dist_global;\n\tMPI_Reduce(&min_dist, &min_dist_global, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\treturn min_dist_global;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\n\tstd::vector<Point> points_rank;\n\tfor (int i = 0; i < num_points_per_rank; i++) {\n\t\tpoints_rank.push_back(points[rank*num_points_per_rank + i]);\n\t}\n\tif (rank < num_points_remainder) {\n\t\tpoints_rank.push_back(points[size*num_points_per_rank + rank]);\n\t}\n\n\tdouble min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points_rank.size(); i++) {\n\t\tfor (int j = i+1; j < points_rank.size(); j++) {\n\t\t\tdouble distance = distance(points_rank[i], points_rank[j]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble min_distance_global;\n\tMPI_Reduce(&min_distance, &min_distance_global, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\treturn min_distance_global;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\tint start = rank * n_per_proc + std::min(rank, n_rem);\n\tint end = (rank+1) * n_per_proc + std::min(rank+1, n_rem);\n\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; ++i) {\n\t\t\tdouble dist;\n\t\t\tMPI_Recv(&dist, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tmin_dist = std::min(min_dist, dist);\n\t\t}\n\t} else {\n\t\tdouble local_min_dist = std::numeric_limits<double>::max();\n\t\tfor (int i = start; i < end; ++i) {\n\t\t\tfor (int j = i+1; j < end; ++j) {\n\t\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\t\tif (dist < local_min_dist) {\n\t\t\t\t\tlocal_min_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tMPI_Send(&local_min_dist, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\treturn min_dist;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\tint start = rank * n_per_proc;\n\tint end = start + n_per_proc;\n\tif (rank == size - 1) {\n\t\tend += n_rem;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tlocal_min = std::min(local_min, distance(local_points[i], local_points[j]));\n\t\t}\n\t}\n\n\tdouble global_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tdouble min_distance = std::numeric_limits<double>::max();\n\t\tfor (int i = 1; i < size; ++i) {\n\t\t\tdouble distance;\n\t\t\tMPI_Recv(&distance, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tmin_distance = std::min(min_distance, distance);\n\t\t}\n\t\treturn min_distance;\n\t}\n\telse {\n\t\tdouble min_distance = std::numeric_limits<double>::max();\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < points.size(); ++i) {\n\t\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\t\tdouble distance = distance(points[i], points[j]);\n\t\t\t\tif (distance < min_distance) {\n\t\t\t\t\tmin_distance = distance;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tMPI_Send(&min_distance, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\n\tint start_index = rank * num_points_per_rank;\n\tint end_index = (rank + 1) * num_points_per_rank;\n\tif (rank == size - 1) {\n\t\tend_index += num_points_remainder;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start_index, points.begin() + end_index);\n\n\tdouble local_min = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < local_points.size(); i++) {\n\t\tfor (int j = i + 1; j < local_points.size(); j++) {\n\t\t\tdouble d = distance(local_points[i], local_points[j]);\n\t\t\tif (d < local_min) {\n\t\t\t\tlocal_min = d;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble global_min = local_min;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(Kokkos::View<const Point*> &points, Kokkos::View<Point*> &hull) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement me!\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "//...\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "//...\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "//...\n}",
            "//...\n}",
            "//...\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "//...\n}",
            "// TODO: implement this function\n}",
            "//...\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "//...\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= numPoints) return;\n\n   //...\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= numPoints) return;\n\n   // TODO: Implement the convex hull algorithm here.\n   // You may use the following helper functions:\n   //\n   // 1. __shfl_down(value, offset, width)\n   //    Returns the value of the `value` argument at the `offset`th thread in the warp.\n   //    The `width` argument specifies the size of the warp.\n   //    Example:\n   //      __shfl_down(value, 1, 32)\n   //    returns the value of the `value` argument at the thread with threadIdx.x == 1.\n   //\n   // 2. __shfl_up(value, offset, width)\n   //    Returns the value of the `value` argument at the `offset`th thread in the warp.\n   //    The `width` argument specifies the size of the warp.\n   //    Example:\n   //      __shfl_up(value, 1, 32)\n   //    returns the value of the `value` argument at the thread with threadIdx.x == 31.\n   //\n   // 3. __shfl(value, srcLane, width)\n   //    Returns the value of the `value` argument at the `srcLane`th thread in the warp.\n   //    The `width` argument specifies the size of the warp.\n   //    Example:\n   //      __shfl(value, 1, 32)\n   //    returns the value of the `value` argument at the thread with threadIdx.x == 1.\n   //\n   // 4. __syncthreads()\n   //    Synchronizes all threads in the warp.\n   //    Example:\n   //      __syncthreads()\n   //    waits until all threads in the warp have reached this point.\n   //\n   // 5. __ballot(predicate)\n   //    Returns an integer mask of the threads in the warp for which the `predicate` argument is true.\n   //    Example:\n   //      __ballot(threadIdx.x % 2 == 0)\n   //    returns a mask of the even numbered threads in the warp.\n   //\n   // 6. __any(mask)\n   //    Returns true if any of the threads in the warp have a non-zero value in the mask.\n   //    Example:\n   //      __any(__ballot(threadIdx.x % 2 == 0))\n   //    returns true if any of the even numbered threads in the warp have a non-zero value in the mask.\n   //\n   // 7. __all(mask)\n   //    Returns true if all of the threads in the warp have a non-zero value in the mask.\n   //    Example:\n   //      __all(__ballot(threadIdx.x % 2 == 0))\n   //    returns true if all of the even numbered threads in the warp have a non-zero value in the mask.\n   //\n   // 8. __popc(mask)\n   //    Returns the number of non-zero bits in the mask.\n   //    Example:\n   //      __popc(__ballot(threadIdx.x % 2 == 0))\n   //    returns the number of even numbered threads in the warp.\n   //\n   // 9. __clz(mask)\n   //    Returns the number of leading zeros in the mask.\n   //    Example:\n   //      __clz(__ballot(threadIdx.x % 2 == 0))\n   //    returns the number of leading zeros in the mask.\n   //\n   // 10. __ffs(mask)\n   //     Returns the index of the first non-zero bit in the mask.\n   //     Example:\n   //       __ffs(__ballot(threadIdx.x % 2 == 0))\n   //     returns the index of the first non-zero bit in the mask.\n   //\n   // 11. __brev(mask)\n   //     Returns the bit reversed mask.\n   //     Example:\n   //       __brev(__ballot(threadId",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement me\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "08_geometry_convex_hull",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(Kokkos::View<const Point*> &points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: Implement me!\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement me\n\treturn 0.0;\n}",
            "// TODO: implement this function\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "// TODO: Implement this function\n\treturn 0.0;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double perimeter = 0;\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = 0; j < points.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tperimeter += distance(points[i], points[j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn perimeter;\n}",
            "int n = points.size();\n\tstd::vector<Point> hull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (points[i].x <= points[k].x) {\n\t\t\tif (points[i].x < points[k].x || points[i].y < points[k].y) {\n\t\t\t\tk = i;\n\t\t\t}\n\t\t}\n\t}\n\thull[0] = points[k];\n\tk = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (points[i].x > points[k].x) {\n\t\t\tif (points[i].x > points[k].x || points[i].y > points[k].y) {\n\t\t\t\tk = i;\n\t\t\t}\n\t\t}\n\t}\n\thull[1] = points[k];\n\tint m = 2;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (i!= k) {\n\t\t\twhile (m > 1 && distance(hull[m-1], hull[m-2]) <= distance(hull[m-1], points[i])) {\n\t\t\t\tm--;\n\t\t\t}\n\t\t\thull[m] = points[i];\n\t\t\tm++;\n\t\t}\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < m; i++) {\n\t\tperimeter += distance(hull[i], hull[(i+1)%m]);\n\t}\n\treturn perimeter;\n}",
            "// TODO: your code here\n\tdouble perimeter = 0;\n\tint n = points.size();\n\tstd::vector<Point> convexHull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\twhile (k >= 2 && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = n-2, t = k+1; i >= 0; i--) {\n\t\twhile (k >= t && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tconvexHull.resize(k-1);\n\tfor (int i = 0; i < k-1; i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[i+1]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = 0; j < points.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tperimeter += distance(points[i], points[j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn perimeter;\n}",
            "// TODO: implement me\n\treturn 0;\n}",
            "double perimeter = 0;\n\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tperimeter += distance(points[i], points[j]);\n\t\t}\n\t}\n\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\n\t// TODO: compute the perimeter of the convex hull in parallel\n\n\treturn perimeter;\n}",
            "// TODO: Your code here\n\tdouble perimeter = 0;\n\tint n = points.size();\n\tstd::vector<Point> convexHull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\twhile (k >= 2 && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = n-2, t = k+1; i >= 0; i--) {\n\t\twhile (k >= t && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = 0; i < k-1; i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[i+1]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = i + 1; j < points.size(); j++) {\n\t\t\tperimeter += distance(points[i], points[j]);\n\t\t}\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\n\t// TODO: compute the perimeter of the convex hull of the points in parallel\n\n\treturn perimeter;\n}",
            "int n = points.size();\n\tstd::vector<Point> hull;\n\thull.push_back(points[0]);\n\tfor (int i = 1; i < n; i++) {\n\t\tif (points[i].x < hull[0].x) {\n\t\t\thull[0] = points[i];\n\t\t}\n\t}\n\tfor (int i = 1; i < n; i++) {\n\t\tif (points[i].x == hull[0].x) {\n\t\t\tif (points[i].y < hull[0].y) {\n\t\t\t\thull[0] = points[i];\n\t\t\t}\n\t\t}\n\t}\n\tfor (int i = 1; i < n; i++) {\n\t\tif (points[i].x > hull[0].x) {\n\t\t\thull.push_back(points[i]);\n\t\t}\n\t}\n\tfor (int i = 1; i < n; i++) {\n\t\tif (points[i].x == hull[0].x) {\n\t\t\tif (points[i].y > hull[0].y) {\n\t\t\t\thull.push_back(points[i]);\n\t\t\t}\n\t\t}\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < hull.size(); i++) {\n\t\tperimeter += distance(hull[i], hull[(i+1)%hull.size()]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\tint n = points.size();\n\tstd::vector<Point> hull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (hull[k] == points[i]) {\n\t\t\tcontinue;\n\t\t}\n\t\twhile (k >= 2 && cross(hull[k-1] - hull[k-2], hull[k-1] - points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\thull[k++] = points[i];\n\t}\n\tfor (int i = 0; i < k; i++) {\n\t\tperimeter += distance(hull[i], hull[(i+1)%k]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\tstd::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tconvexHull.push_back(points[1]);\n\tfor (int i = 2; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 && distance(convexHull[convexHull.size() - 1], convexHull[convexHull.size() - 2]) + distance(convexHull[convexHull.size() - 1], points[i]) <= distance(convexHull[convexHull.size() - 2], points[i])) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tfor (int i = 0; i < convexHull.size() - 1; i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[i + 1]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = 0; j < points.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tperimeter += distance(points[i], points[j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\tint n = points.size();\n\tstd::vector<Point> convexHull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\twhile (k >= 2 && distance(convexHull[k-2], convexHull[k-1]) + distance(convexHull[k-1], points[i]) >= distance(convexHull[k-2], points[i])) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = n-2, t = k+1; i >= 0; i--) {\n\t\twhile (k >= t && distance(convexHull[k-2], convexHull[k-1]) + distance(convexHull[k-1], points[i]) >= distance(convexHull[k-2], points[i])) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = 0; i < k-1; i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[i+1]);\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\t#pragma omp parallel for reduction(+:perimeter)\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tfor (int j = 0; j < points.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tperimeter += distance(points[i], points[j]);\n\t\t\t}\n\t\t}\n\t}\n\treturn perimeter;\n}",
            "double perimeter = 0;\n\tint n = points.size();\n\tstd::vector<Point> convexHull(n);\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\twhile (k >= 2 && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = n-2, t = k+1; i >= 0; i--) {\n\t\twhile (k >= t && cross(convexHull[k-2], convexHull[k-1], points[i]) <= 0) {\n\t\t\tk--;\n\t\t}\n\t\tconvexHull[k++] = points[i];\n\t}\n\tfor (int i = 0; i < k-1; i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[i+1]);\n\t}\n\treturn perimeter;\n}",
            "int n = points.size();\n\tdouble perimeter = 0;\n\tstd::vector<Point> hull;\n\tstd::vector<int> hull_indices;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tfor (int k = j + 1; k < n; k++) {\n\t\t\t\tif (points[i].x * (points[j].y - points[k].y) + points[j].x * (points[k].y - points[i].y) + points[k].x * (points[i].y - points[j].y) < 0) {\n\t\t\t\t\thull.push_back(Point{points[i].x, points[j].y});\n\t\t\t\t\thull.push_back(Point{points[j].x, points[k].y});\n\t\t\t\t\thull.push_back(Point{points[k].x, points[i].y});\n\t\t\t\t\thull.push_back(Point{points[i].x, points[j].y});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < hull.size(); i++) {\n\t\tfor (int j = 0; j < hull.size(); j++) {\n\t\t\tif (distance(hull[i], hull[j]) > distance(hull[i], hull[j+1])) {\n\t\t\t\tPoint temp = hull[j];\n\t\t\t\thull[j] = hull[j+1];\n\t\t\t\thull[j+1] = temp;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < hull.size(); i++) {\n\t\tperimeter += distance(hull[i], hull[i+1]);\n\t}\n\n\treturn perimeter;\n}",
            "int n = points.size();\n\tdouble perimeter = 0;\n\tstd::vector<Point> hull;\n\tstd::vector<double> angles(n);\n\tstd::vector<int> hull_indices(n);\n\tstd::vector<int> hull_indices_sorted(n);\n\tstd::vector<int> hull_indices_sorted_2(n);\n\tstd::vector<int> hull_indices_sorted_3(n);\n\tstd::vector<int> hull_indices_sorted_4(n);\n\tstd::vector<int> hull_indices_sorted_5(n);\n\tstd::vector<int> hull_indices_sorted_6(n);\n\tstd::vector<int> hull_indices_sorted_7(n);\n\tstd::vector<int> hull_indices_sorted_8(n);\n\tstd::vector<int> hull_indices_sorted_9(n);\n\tstd::vector<int> hull_indices_sorted_10(n);\n\tstd::vector<int> hull_indices_sorted_11(n);\n\tstd::vector<int> hull_indices_sorted_12(n);\n\tstd::vector<int> hull_indices_sorted_13(n);\n\tstd::vector<int> hull_indices_sorted_14(n);\n\tstd::vector<int> hull_indices_sorted_15(n);\n\tstd::vector<int> hull_indices_sorted_16(n);\n\tstd::vector<int> hull_indices_sorted_17(n);\n\tstd::vector<int> hull_indices_sorted_18(n);\n\tstd::vector<int> hull_indices_sorted_19(n);\n\tstd::vector<int> hull_indices_sorted_20(n);\n\tstd::vector<int> hull_indices_sorted_21(n);\n\tstd::vector<int> hull_indices_sorted_22(n);\n\tstd::vector<int> hull_indices_sorted_23(n);\n\tstd::vector<int> hull_indices_sorted_24(n);\n\tstd::vector<int> hull_indices_sorted_25(n);\n\tstd::vector<int> hull_indices_sorted_26(n);\n\tstd::vector<int> hull_indices_sorted_27(n);\n\tstd::vector<int> hull_indices_sorted_28(n);\n\tstd::vector<int> hull_indices_sorted_29(n);\n\tstd::vector<int> hull_indices_sorted_30(n);\n\tstd::vector<int> hull_indices_sorted_31(n);\n\tstd::vector<int> hull_indices_sorted_32(n);\n\tstd::vector<int> hull_indices_sorted_33(n);\n\tstd::vector<int> hull_indices_sorted_34(n);\n\tstd::vector<int> hull_indices_sorted_35(n);\n\tstd::vector<int> hull_indices_sorted_36(n);\n\tstd::vector<int> hull_indices_sorted_37(n);\n\tstd::vector<int> hull_indices_sorted_38(n);\n\tstd::vector<int> hull_indices_sorted_39(n);\n\tstd::vector<int> hull_indices_sorted_40(n);\n\tstd::vector<int> hull_indices_sorted_41(n);\n\tstd::vector<int> hull_indices_sorted_42(n);\n\tstd::vector<int> hull_indices_sorted_43(n);\n\tstd::vector<int> hull_indices_sorted_44(n);\n\tstd::vector<int> hull_indices_sorted_45(n);\n\tstd::vector<int> hull_indices_sorted_46(n);\n\tstd::vector<int> hull_ind",
            "// TODO: implement this function\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\tccw(convexHull[convexHull.size()-2], convexHull[convexHull.size()-1], points[i]) <= 0)\n\t\t\tconvexHull.pop_back();\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tconvexHull.push_back(points[0]);\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++)\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n\t});\n\n\tconvexHull.push_back(sortedPoints[0]);\n\tconvexHull.push_back(sortedPoints[1]);\n\tfor (int i = 2; i < sortedPoints.size(); ++i) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-2].x-convexHull[convexHull.size()-1].x)*(sortedPoints[i].y-convexHull[convexHull.size()-1].y)\n\t\t\t\t< (convexHull[convexHull.size()-2].y-convexHull[convexHull.size()-1].y)*(sortedPoints[i].x-convexHull[convexHull.size()-1].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(sortedPoints[i]);\n\t}\n\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); ++i) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t(convexHull.back().x - convexHull[convexHull.size()-2].x) * (points[i].y - convexHull.back().y) -\n\t\t\t(convexHull.back().y - convexHull[convexHull.size()-2].y) * (points[i].x - convexHull.back().x) < 0) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tconvexHull.push_back(points[0]);\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tconvexHull.push_back(points[1]);\n\tfor (int i = 2; i < points.size(); ++i) {\n\t\twhile (convexHull.size() >= 2 &&\n\t\t\t\t(convexHull[convexHull.size()-1].x-convexHull[convexHull.size()-2].x)*(points[i].y-convexHull[convexHull.size()-2].y)\n\t\t\t\t\t< (convexHull[convexHull.size()-1].y-convexHull[convexHull.size()-2].y)*(points[i].x-convexHull[convexHull.size()-2].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-1].x-convexHull[convexHull.size()-2].x)*(points[i].y-convexHull[convexHull.size()-1].y)\n\t\t\t\t< (convexHull[convexHull.size()-1].y-convexHull[convexHull.size()-2].y)*(points[i].x-convexHull[convexHull.size()-1].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "if (points.size() <= 2) {\n\t\treturn 0;\n\t}\n\n\tstd::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tconvexHull.push_back(points[1]);\n\n\tfor (int i = 2; i < points.size(); ++i) {\n\t\twhile (convexHull.size() >= 2 &&\n\t\t\t\t(convexHull[convexHull.size()-2].x-convexHull[convexHull.size()-1].x)*(points[i].y-convexHull[convexHull.size()-1].y)",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\tPoint p = points[i];\n\t\twhile (convexHull.size() > 1) {\n\t\t\tPoint p1 = convexHull[convexHull.size()-2];\n\t\t\tPoint p2 = convexHull[convexHull.size()-1];\n\t\t\tif (p1.x*p2.y + p2.x*p.y + p.x*p1.y - p1.x*p1.y - p2.x*p2.y - p.x*p2.y >= 0) {\n\t\t\t\tconvexHull.pop_back();\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tconvexHull.push_back(p);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tPoint p1 = convexHull[i];\n\t\tPoint p2 = convexHull[(i+1)%convexHull.size()];\n\t\tperimeter += distance(p1, p2);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); ++i) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\tccw(convexHull[convexHull.size()-2], convexHull[convexHull.size()-1], points[i]) <= 0) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tconvexHull.push_back(points[0]);\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size()-1; ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[i+1]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\n\tconvexHull.push_back(sortedPoints[0]);\n\tconvexHull.push_back(sortedPoints[1]);\n\n\tfor (int i = 2; i < sortedPoints.size(); i++) {\n\t\tPoint const& p = sortedPoints[i];\n\t\twhile (convexHull.size() >= 2 &&\n\t\t\t\t(convexHull[convexHull.size()-2].x-convexHull[convexHull.size()-1].x)*(p.y-convexHull[convexHull.size()-1].y) <=\n\t\t\t\t(convexHull[convexHull.size()-2].y-convexHull[convexHull.size()-1].y)*(p.x-convexHull[convexHull.size()-1].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(p);\n\t}\n\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tconvexHull.push_back(points[1]);\n\tfor (int i = 2; i < points.size(); ++i) {\n\t\twhile (convexHull.size() > 2 &&\n\t\t\t\torientation(convexHull[convexHull.size()-2], convexHull[convexHull.size()-1], points[i])!= 2) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> hull;\n\tstd::vector<Point> sortedPoints(points);\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n\t});\n\n\t// Build lower hull\n\tfor (auto it = sortedPoints.begin(); it!= sortedPoints.end(); ++it) {\n\t\twhile (hull.size() >= 2 && cross(hull[hull.size()-2], hull[hull.size()-1], *it) <= 0) {\n\t\t\thull.pop_back();\n\t\t}\n\t\thull.push_back(*it);\n\t}\n\n\t// Build upper hull\n\tint j = hull.size() - 2;\n\tfor (auto it = sortedPoints.rbegin(); it!= sortedPoints.rend(); ++it) {\n\t\twhile (hull.size() >= 2 && cross(hull[hull.size()-2], hull[hull.size()-1], *it) <= 0) {\n\t\t\thull.pop_back();\n\t\t}\n\t\thull.push_back(*it);\n\t}\n\n\t// Remove last point\n\thull.pop_back();\n\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < hull.size(); ++i) {\n\t\tperimeter += distance(hull[i], hull[(i+1) % hull.size()]);\n\t}\n\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-2] - convexHull[convexHull.size()-1]) % (points[i] - convexHull[convexHull.size()-1]) <= 0) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1) % convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-2].x-convexHull[convexHull.size()-1].x)*(points[i].y-convexHull[convexHull.size()-1].y)\n\t\t\t\t< (convexHull[convexHull.size()-2].y-convexHull[convexHull.size()-1].y)*(points[i].x-convexHull[convexHull.size()-1].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); ++i) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-2]-convexHull[convexHull.size()-1])\n\t\t\t\t\t.x*(points[i]-convexHull[convexHull.size()-1]).y\n\t\t\t\t\t-(convexHull[convexHull.size()-2]-convexHull[convexHull.size()-1])\n\t\t\t\t\t\t.y*(points[i]-convexHull[convexHull.size()-1]).x > 0) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n\t});\n\tconvexHull.push_back(sortedPoints[0]);\n\tconvexHull.push_back(sortedPoints[1]);\n\tfor (int i = 2; i < sortedPoints.size(); ++i) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull.back().x-convexHull[convexHull.size()-2].x)*(sortedPoints[i].y-convexHull.back().y)\n\t\t\t\t< (convexHull.back().y-convexHull[convexHull.size()-2].y)*(sortedPoints[i].x-convexHull.back().x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(sortedPoints[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\tconvexHull.push_back(sortedPoints[0]);\n\tconvexHull.push_back(sortedPoints[1]);\n\tfor (int i = 2; i < sortedPoints.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-1].x-convexHull[convexHull.size()-2].x)*(sortedPoints[i].y-convexHull[convexHull.size()-1].y)\n\t\t\t\t< (convexHull[convexHull.size()-1].y-convexHull[convexHull.size()-2].y)*(sortedPoints[i].x-convexHull[convexHull.size()-1].x)) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(sortedPoints[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tfor (int i = 1; i < points.size(); i++) {\n\t\twhile (convexHull.size() > 1 &&\n\t\t\t\t(convexHull[convexHull.size()-1].x - convexHull[convexHull.size()-2].x) * (points[i].y - convexHull[convexHull.size()-1].y)\n\t\t\t\t\t- (convexHull[convexHull.size()-1].y - convexHull[convexHull.size()-2].y) * (points[i].x - convexHull[convexHull.size()-1].x)\n\t\t\t\t\t<= 0) {\n\t\t\tconvexHull.pop_back();\n\t\t}\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); i++) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1) % convexHull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> hull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n\t});\n\tstd::vector<Point> upperHull;\n\tstd::vector<Point> lowerHull;\n\tfor (auto const& point : sortedPoints) {\n\t\twhile (upperHull.size() >= 2 && cross(upperHull[upperHull.size()-2], upperHull[upperHull.size()-1], point) <= 0) {\n\t\t\tupperHull.pop_back();\n\t\t}\n\t\tupperHull.push_back(point);\n\t}\n\tfor (auto const& point : sortedPoints) {\n\t\twhile (lowerHull.size() >= 2 && cross(lowerHull[lowerHull.size()-2], lowerHull[lowerHull.size()-1], point) >= 0) {\n\t\t\tlowerHull.pop_back();\n\t\t}\n\t\tlowerHull.push_back(point);\n\t}\n\tlowerHull.pop_back();\n\tupperHull.pop_back();\n\thull.insert(hull.end(), upperHull.begin(), upperHull.end());\n\thull.insert(hull.end(), lowerHull.begin(), lowerHull.end());\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < hull.size(); ++i) {\n\t\tperimeter += distance(hull[i], hull[(i+1)%hull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> hull;\n\tstd::vector<Point> sortedPoints = points;\n\tstd::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n\t\treturn p1.x < p2.x;\n\t});\n\thull.push_back(sortedPoints[0]);\n\thull.push_back(sortedPoints[1]);\n\tfor (int i = 2; i < sortedPoints.size(); i++) {\n\t\twhile (hull.size() > 1 && crossProduct(hull[hull.size()-2], hull[hull.size()-1], sortedPoints[i]) <= 0) {\n\t\t\thull.pop_back();\n\t\t}\n\t\thull.push_back(sortedPoints[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < hull.size(); i++) {\n\t\tperimeter += distance(hull[i], hull[(i+1)%hull.size()]);\n\t}\n\treturn perimeter;\n}",
            "std::vector<Point> convexHull;\n\tconvexHull.push_back(points[0]);\n\tconvexHull.push_back(points[1]);\n\tfor (int i = 2; i < points.size(); ++i) {\n\t\twhile (convexHull.size() > 2 &&\n\t\t\t\tccw(convexHull[convexHull.size()-2], convexHull[convexHull.size()-1], points[i]) <= 0)\n\t\t\tconvexHull.pop_back();\n\t\tconvexHull.push_back(points[i]);\n\t}\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convexHull.size(); ++i) {\n\t\tperimeter += distance(convexHull[i], convexHull[(i+1)%convexHull.size()]);\n\t}\n\treturn perimeter;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid >= numPoints) return;\n\n\t// TODO: implement\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\n\t// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n\t*perimeter = 0;\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nthreads = gridDim.x * blockDim.x;\n\tint nblocks = gridDim.x;\n\n\t// TODO: Compute the perimeter of the convex hull of the points.\n\t// Use the distance function to compute the distance between two points.\n\t// Use the min and max functions to find the minimum and maximum x and y coordinates of the points.\n\t// Use the atomicMin and atomicMax functions to find the minimum and maximum x and y coordinates of the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the distances between the points.\n\t// Use the atomicAdd function to sum the",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\t// You can use the distance function above to compute the distance between two points\n\t// You can use the atomicAdd function to add to the perimeter variable\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\t*perimeter = 0;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid >= numPoints) return;\n\n\t// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n\treturn 0.0;\n}",
            "// TODO: Your code here\n\tint size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tstd::vector<Point> localPoints;\n\tstd::vector<Point> globalPoints;\n\tstd::vector<Point> globalHull;\n\tdouble localPerimeter = 0;\n\tdouble globalPerimeter = 0;\n\n\tif (rank == 0) {\n\t\tlocalPoints = points;\n\t}\n\n\tMPI_Bcast(&localPoints[0], localPoints.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j]) < 0.00001) {\n\t\t\t\t\tlocalPoints.erase(localPoints.begin() + j);\n\t\t\t\t\tj--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tfor (int j = 0; j < localPoints.size(); j++) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (distance(localPoints[i], localPoints[j])",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tstd::vector<Point> localPoints;\n\tint localSize = points.size() / size;\n\tint localStart = rank * localSize;\n\tint localEnd = (rank == size - 1)? points.size() : (rank + 1) * localSize;\n\tfor (int i = localStart; i < localEnd; i++) {\n\t\tlocalPoints.push_back(points[i]);\n\t}\n\n\t// TODO: compute convex hull perimeter for localPoints\n\n\tdouble perimeter;\n\tMPI_Reduce(&perimeter, &perimeter, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\treturn perimeter;\n}",
            "// TODO\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0.0;\n}",
            "// TODO: implement this function\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: implement this function\n\treturn 0.0;\n}",
            "// TODO: implement this function\n\treturn 0.0;\n}",
            "// TODO\n\treturn 0.0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: your code here\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\tdouble perimeter = 0;\n\treturn perimeter;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\tdouble perimeter = 0;\n\tif (rank == 0) {\n\t\tstd::vector<Point> points_rank0(points.begin(), points.begin() + points.size() / size);\n\t\tperimeter = 0;\n\t\tfor (int i = 0; i < points_rank0.size(); i++) {\n\t\t\tfor (int j = 0; j < points_rank0.size(); j++) {\n\t\t\t\tif (i!= j) {\n\t\t\t\t\tperimeter += distance(points_rank0[i], points_rank0[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tstd::vector<Point> points_rank(points.begin() + points.size() / size * rank, points.begin() + points.size() / size * (rank + 1));\n\t\tperimeter = 0;\n\t\tfor (int i = 0; i < points_rank.size(); i++) {\n\t\t\tfor (int j = 0; j < points_rank.size(); j++) {\n\t\t\t\tif (i!= j) {\n\t\t\t\t\tperimeter += distance(points_rank[i], points_rank[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tdouble perimeter_all;\n\tMPI_Reduce(&perimeter, &perimeter_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\treturn perimeter_all;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point shm[1024];\n\t__shared__ int shm_size;\n\n\tif (tid == 0) {\n\t\tshm_size = numPoints;\n\t}\n\n\t__syncthreads();\n\n\tif (tid < shm_size) {\n\t\tshm[tid] = points[tid];\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tPoint *p = shm;\n\t\tPoint *q = shm;\n\t\tPoint *r = shm;\n\t\tPoint *s = shm;\n\n\t\tdouble min_x = p->x;\n\t\tdouble max_x = p->x;\n\t\tdouble min_y = p->y;\n\t\tdouble max_y = p->y;\n\n\t\tfor (int i = 1; i < shm_size; i++) {\n\t\t\tif (p->x < min_x) {\n\t\t\t\tmin_x = p->x;\n\t\t\t}\n\t\t\tif (p->x > max_x) {\n\t\t\t\tmax_x = p->x;\n\t\t\t}\n\t\t\tif (p->y < min_y) {\n\t\t\t\tmin_y = p->y;\n\t\t\t}\n\t\t\tif (p->y > max_y) {\n\t\t\t\tmax_y = p->y;\n\t\t\t}\n\t\t\tp++;\n\t\t}\n\n\t\tdouble min_distance = distance(shm[0], shm[1]);\n\t\tint min_index = 0;\n\t\tfor (int i = 1; i < shm_size; i++) {\n\t\t\tdouble distance = distance(shm[0], shm[i]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t\tmin_index = i;\n\t\t\t}\n\t\t}\n\n\t\tq = &shm[min_index];\n\t\tmin_distance = distance(shm[0], *q);\n\t\tmin_index = 0;\n\t\tfor (int i = 1; i < shm_size; i++) {\n\t\t\tdouble distance = distance(shm[0], shm[i]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t\tmin_index = i;\n\t\t\t}\n\t\t}\n\n\t\tr = &shm[min_index];\n\t\tmin_distance = distance(shm[0], *r);\n\t\tmin_index = 0;\n\t\tfor (int i = 1; i < shm_size; i++) {\n\t\t\tdouble distance = distance(shm[0], shm[i]);\n\t\t\tif (distance < min_distance) {\n\t\t\t\tmin_distance = distance;\n\t\t\t\tmin_index = i;\n\t\t\t}\n\t\t}\n\n\t\ts = &shm[min_index];\n\n\t\tdouble perimeter = distance(shm[0], *q) + distance(shm[0], *r) + distance(shm[0], *s);\n\n\t\t*perimeter = perimeter;\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t__shared__ Point s_points[MAX_POINTS];\n\t__shared__ int s_indices[MAX_POINTS];\n\n\tif (tid == 0) {\n\t\tfor (int i = 0; i < numPoints; i++) {\n\t\t\ts_points[i] = points[i];\n\t\t\ts_indices[i] = i;\n\t\t}\n\t}\n\t__syncthreads();\n\n\tint start = bid * nt;\n\tint end = min(start + nt, numPoints);\n\tint size = end - start;\n\tint mid = (start + end) / 2;\n\n\tif (size == 1) {\n\t\tif (tid == 0) {\n\t\t\t*perimeter = 0;\n\t\t}\n\t\treturn;\n\t}\n\n\tif (size == 2) {\n\t\tif (tid == 0) {\n\t\t\t*perimeter = distance(s_points[s_indices[start]], s_points[s_indices[start + 1]]);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (size == 3) {\n\t\tif (tid == 0) {\n\t\t\t*perimeter = distance(s_points[s_indices[start]], s_points[s_indices[start + 1]]) +\n\t\t\t\tdistance(s_points[s_indices[start + 1]], s_points[s_indices[start + 2]]) +\n\t\t\t\tdistance(s_points[s_indices[start + 2]], s_points[s_indices[start]]);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (tid == 0) {\n\t\t*perimeter = 0;\n\t}\n\n\t__syncthreads();\n\n\tif (tid < size) {\n\t\ts_indices[tid] = start + tid;\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < size; i++) {\n\t\tint j = (i + 1) % size;\n\t\tint k = (i + 2) % size;\n\t\tif (s_points[s_indices[i]].x > s_points[s_indices[j]].x) {\n\t\t\tswap(s_indices[i], s_indices[j]);\n\t\t}\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < size; i++) {\n\t\tint j = (i + 1) % size;\n\t\tint k = (i + 2) % size;\n\t\tif (s_points[s_indices[i]].x == s_points[s_indices[j]].x && s_points[s_indices[i]].y > s_points[s_indices[j]].y) {\n\t\t\tswap(s_indices[i], s_indices[j]);\n\t\t}\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < size; i++) {\n\t\tint j = (i + 1) % size;\n\t\tint k = (i + 2) % size;\n\t\tif (s_points[s_indices[i]].x == s_points[s_indices[j]].x && s_points[s_indices[i]].y == s_points[s_indices[j]].y) {\n\t\t\tswap(s_indices[i], s_indices[j]);\n\t\t}\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < size; i++) {\n\t\tint j = (i + 1) % size;\n\t\tint k = (i + 2) % size;\n\t\tif (s_points[s_indices[i]].x == s_points[s_indices[j]].x && s_points[s_",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid >= numPoints) return;\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// 1. Find the point with the smallest y coordinate\n\t// 2. Find the point with the smallest x coordinate that is greater than the point found in step 1\n\t// 3. Find the point with the largest x coordinate that is less than the point found in step 2\n\t// 4. Find the point with the largest y coordinate that is less than the point found in step 3\n\t// 5. Compute the perimeter of the polygon formed by the points found in steps 1-4\n\n\t// 1. Find the point with the smallest y coordinate\n\tPoint minY = points[tid];\n\tfor (int i = tid + 1; i < numPoints; i++) {\n\t\tif (points[i].y < minY.y) {\n\t\t\tminY = points[i];\n\t\t}\n\t}\n\n\t// 2. Find the point with the smallest x coordinate that is greater than the point found in step 1\n\tPoint minX = minY;\n\tfor (int i = tid + 1; i < numPoints; i++) {\n\t\tif (points[i].y == minY.y && points[i].x < minX.x) {\n\t\t\tminX = points[i];\n\t\t}\n\t}\n\n\t// 3. Find the point with the largest x coordinate that is less than the point found in step 2\n\tPoint maxX = minX;\n\tfor (int i = tid + 1; i < numPoints; i++) {\n\t\tif (points[i].y == minX.y && points[i].x > maxX.x) {\n\t\t\tmaxX = points[i];\n\t\t}\n\t}\n\n\t// 4. Find the point with the largest y coordinate that is less than the point found in step 3\n\tPoint maxY = maxX;\n\tfor (int i = tid + 1; i < numPoints; i++) {\n\t\tif (points[i].y > maxY.y) {\n\t\t\tmaxY = points[i];\n\t\t}\n\t}\n\n\t// 5. Compute the perimeter of the polygon formed by the points found in steps 1-4\n\t*perimeter = distance(minY, minX) + distance(minX, maxX) + distance(maxX, maxY) + distance(maxY, minY);\n}",
            "// TODO: Implement this function\n\tint tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint n = numPoints;\n\tint i = tid + bid * blockDim.x;\n\tint j = (tid + 1) % n;\n\tint k = (tid + 2) % n;\n\tif (i < n) {\n\t\tdouble d1 = distance(points[i], points[j]);\n\t\tdouble d2 = distance(points[i], points[k]);\n\t\tdouble d3 = distance(points[j], points[k]);\n\t\tdouble d = min(d1, min(d2, d3));\n\t\tatomicAdd(perimeter, d);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < numPoints) {\n\t\tint i = tid;\n\t\tint j = (tid + 1) % numPoints;\n\t\t*perimeter += distance(points[i], points[j]);\n\t}\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\tint tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nthreads = blockDim.x;\n\tint nblocks = gridDim.x;\n\n\t__shared__ Point shared_points[1024];\n\t__shared__ int shared_indices[1024];\n\t__shared__ int shared_size;\n\n\tif (tid == 0) {\n\t\tshared_size = numPoints;\n\t}\n\t__syncthreads();\n\n\tint start = bid * nthreads;\n\tint end = min(start + nthreads, shared_size);\n\n\tfor (int i = start + tid; i < end; i += nthreads) {\n\t\tshared_points[i] = points[i];\n\t\tshared_indices[i] = i;\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tshared_size = end - start;\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < shared_size - 1; i++) {\n\t\tfor (int j = 0; j < shared_size - i - 1; j++) {\n\t\t\tif (distance(shared_points[j], shared_points[j + 1]) > distance(shared_points[j], shared_points[j + 2])) {\n\t\t\t\tPoint temp = shared_points[j];\n\t\t\t\tshared_points[j] = shared_points[j + 1];\n\t\t\t\tshared_points[j + 1] = temp;\n\n\t\t\t\tint temp2 = shared_indices[j];\n\t\t\t\tshared_indices[j] = shared_indices[j + 1];\n\t\t\t\tshared_indices[j + 1] = temp2;\n\t\t\t}\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tshared_size = end - start;\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < shared_size - 1; i++) {\n\t\tfor (int j = 0; j < shared_size - i - 1; j++) {\n\t\t\tif (distance(shared_points[j], shared_points[j + 1]) > distance(shared_points[j], shared_points[j + 2])) {\n\t\t\t\tPoint temp = shared_points[j];\n\t\t\t\tshared_points[j] = shared_points[j + 1];\n\t\t\t\tshared_points[j + 1] = temp;\n\n\t\t\t\tint temp2 = shared_indices[j];\n\t\t\t\tshared_indices[j] = shared_indices[j + 1];\n\t\t\t\tshared_indices[j + 1] = temp2;\n\t\t\t}\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tshared_size = end - start;\n\t}\n\t__syncthreads();\n\n\tdouble perimeter_temp = 0;\n\tfor (int i = 0; i < shared_size - 1; i++) {\n\t\tperimeter_temp += distance(shared_points[i], shared_points[i + 1]);\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\t*perimeter = perimeter_temp;\n\t}\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nthreads = blockDim.x;\n\tint nblocks = gridDim.x;\n\n\t__shared__ Point shared[1024];\n\t__shared__ int shared_size;\n\n\tif (tid == 0) {\n\t\tshared_size = numPoints;\n\t}\n\n\t__syncthreads();\n\n\tif (tid < numPoints) {\n\t\tshared[tid] = points[tid];\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\t// sort the points\n\t\tfor (int i = 0; i < shared_size; i++) {\n\t\t\tfor (int j = 0; j < shared_size - 1; j++) {\n\t\t\t\tif (shared[j].x > shared[j + 1].x) {\n\t\t\t\t\tPoint temp = shared[j];\n\t\t\t\t\tshared[j] = shared[j + 1];\n\t\t\t\t\tshared[j + 1] = temp;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\t// find the convex hull\n\t\tint hull_size = 0;\n\t\tfor (int i = 0; i < shared_size; i++) {\n\t\t\tif (hull_size < 2) {\n\t\t\t\thull[hull_size++] = shared[i];\n\t\t\t}\n\t\t\telse {\n\t\t\t\twhile (hull_size > 1) {\n\t\t\t\t\tif (distance(hull[hull_size - 2], hull[hull_size - 1]) < distance(hull[hull_size - 2], shared[i])) {\n\t\t\t\t\t\thull[hull_size - 1] = shared[i];\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\thull_size--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\t// compute the perimeter\n\t\tdouble sum = 0;\n\t\tfor (int i = 0; i < hull_size - 1; i++) {\n\t\t\tsum += distance(hull[i], hull[i + 1]);\n\t\t}\n\t\tsum += distance(hull[hull_size - 1], hull[0]);\n\t\t*perimeter = sum;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i >= numPoints) return;\n\tdouble min = distance(points[i], points[0]);\n\tfor (int j = 1; j < numPoints; j++) {\n\t\tdouble d = distance(points[i], points[j]);\n\t\tif (d < min) min = d;\n\t}\n\t*perimeter += min;\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = blockDim.x;\n\tint nb = gridDim.x;\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n\t// Example:\n\n\t// input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n\t// output: 13.4477\n\n\t// TODO: Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n\t// Store the result in perimeter.\n\t// Use CUDA to compute in parallel. The",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint nt = gridDim.x * blockDim.x;\n\tint i = bid * blockDim.x + tid;\n\tint j = i + 1;\n\tint k = i + 2;\n\n\tif (i < numPoints && j < numPoints && k < numPoints) {\n\t\tdouble d1 = distance(points[i], points[j]);\n\t\tdouble d2 = distance(points[i], points[k]);\n\t\tdouble d3 = distance(points[j], points[k]);\n\t\tdouble d4 = distance(points[i], points[i+1]);\n\t\tdouble d5 = distance(points[j], points[j+1]);\n\t\tdouble d6 = distance(points[k], points[k+1]);\n\n\t\tdouble min = min(d1, min(d2, min(d3, min(d4, min(d5, d6)))));\n\t\tif (min == d1) {\n\t\t\t*perimeter += d1;\n\t\t} else if (min == d2) {\n\t\t\t*perimeter += d2;\n\t\t} else if (min == d3) {\n\t\t\t*perimeter += d3;\n\t\t} else if (min == d4) {\n\t\t\t*perimeter += d4;\n\t\t} else if (min == d5) {\n\t\t\t*perimeter += d5;\n\t\t} else {\n\t\t\t*perimeter += d6;\n\t\t}\n\t}\n}",
            "int i = threadIdx.x;\n\tint j = threadIdx.y;\n\tint k = threadIdx.z;\n\n\tif (i < numPoints && j < numPoints && k < numPoints) {\n\t\tif (i!= j && i!= k && j!= k) {\n\t\t\tdouble d1 = distance(points[i], points[j]);\n\t\t\tdouble d2 = distance(points[i], points[k]);\n\t\t\tdouble d3 = distance(points[j], points[k]);\n\t\t\tdouble d4 = distance(points[i], points[j]) + distance(points[j], points[k]) + distance(points[k], points[i]);\n\t\t\tdouble d5 = distance(points[i], points[k]) + distance(points[k], points[j]) + distance(points[j], points[i]);\n\t\t\tdouble d6 = distance(points[j], points[k]) + distance(points[k], points[i]) + distance(points[i], points[j]);\n\n\t\t\tif (d1 < d2 && d1 < d3 && d1 < d4 && d1 < d5 && d1 < d6) {\n\t\t\t\tatomicAdd(perimeter, d1);\n\t\t\t} else if (d2 < d1 && d2 < d3 && d2 < d4 && d2 < d5 && d2 < d6) {\n\t\t\t\tatomicAdd(perimeter, d2);\n\t\t\t} else if (d3 < d1 && d3 < d2 && d3 < d4 && d3 < d5 && d3 < d6) {\n\t\t\t\tatomicAdd(perimeter, d3);\n\t\t\t} else if (d4 < d1 && d4 < d2 && d4 < d3 && d4 < d5 && d4 < d6) {\n\t\t\t\tatomicAdd(perimeter, d4);\n\t\t\t} else if (d5 < d1 && d5 < d2 && d5 < d3 && d5 < d4 && d5 < d6) {\n\t\t\t\tatomicAdd(perimeter, d5);\n\t\t\t} else if (d6 < d1 && d6 < d2 && d6 < d3 && d6 < d4 && d6 < d5) {\n\t\t\t\tatomicAdd(perimeter, d6);\n\t\t\t}\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i >= numPoints) return;\n\n\tdouble min_dist = 0;\n\tfor (int j = 0; j < numPoints; j++) {\n\t\tif (i!= j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (j == 0 || dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\t*perimeter += min_dist;\n}"
        ]
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "09_geometry_convex_hull_perimeter",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n\treturn 0.0;\n}",
            "// TODO\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tstd::vector<Point> local_points;\n\tif (rank < n_extra) {\n\t\tlocal_points.resize(n_per_rank + 1);\n\t\tMPI_Scatter(points.data() + rank * (n_per_rank + 1), n_per_rank + 1,\n\t\t\t\t\tMPI_DOUBLE, local_points.data(), n_per_rank + 1,\n\t\t\t\t\tMPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t} else {\n\t\tlocal_points.resize(n_per_rank);\n\t\tMPI_Scatter(points.data() + n_extra * (n_per_rank + 1) + (rank - n_extra) * n_per_rank, n_per_rank,\n\t\t\t\t\tMPI_DOUBLE, local_points.data(), n_per_rank,\n\t\t\t\t\tMPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t}\n\n\tstd::vector<Point> hull;\n\tif (rank == 0) {\n\t\thull.resize(n);\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_points.size(); ++i) {\n\t\tPoint p = local_points[i];\n\t\tdouble min_dist = std::numeric_limits<double>::max();\n\t\tfor (int j = 0; j < hull.size(); ++j) {\n\t\t\tdouble dist = distance(p, hull[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t\tif (min_dist > 0) {\n\t\t\thull.push_back(p);\n\t\t}\n\t}\n\n\tMPI_Gather(hull.data(), hull.size(), MPI_DOUBLE,\n\t\t\t   hull.data(), hull.size(), MPI_DOUBLE,\n\t\t\t   0, MPI_COMM_WORLD);\n\n\tdouble perimeter = 0;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < hull.size(); ++i) {\n\t\t\tperimeter += distance(hull[i], hull[(i+1) % hull.size()]);\n\t\t}\n\t}\n\n\tMPI_Bcast(&perimeter, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\treturn perimeter;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Implement this function\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tstd::vector<Point> local_points;\n\tif (rank == 0) {\n\t\tlocal_points.insert(local_points.end(), points.begin(), points.begin() + n_per_rank + n_remainder);\n\t} else {\n\t\tlocal_points.insert(local_points.end(), points.begin() + rank * n_per_rank + n_remainder, points.begin() + (rank + 1) * n_per_rank + n_remainder);\n\t}\n\n\tstd::vector<Point> local_hull;\n\tstd::vector<double> local_perimeters;\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_points.size(); ++i) {\n\t\tstd::vector<Point> hull;\n\t\thull.push_back(local_points[i]);\n\t\tfor (int j = 0; j < local_points.size(); ++j) {\n\t\t\tif (i!= j) {\n\t\t\t\tif (hull.size() == 1) {\n\t\t\t\t\thull.push_back(local_points[j]);\n\t\t\t\t} else {\n\t\t\t\t\tPoint p1 = hull[hull.size() - 2];\n\t\t\t\t\tPoint p2 = hull[hull.size() - 1];\n\t\t\t\t\tPoint p3 = local_points[j];\n\t\t\t\t\tdouble cross = (p2.x - p1.x) * (p3.y - p1.y) - (p2.y - p1.y) * (p3.x - p1.x);\n\t\t\t\t\tif (cross > 0) {\n\t\t\t\t\t\thull.push_back(local_points[j]);\n\t\t\t\t\t} else if (cross == 0 && distance(p1, p3) > distance(p2, p3)) {\n\t\t\t\t\t\thull.pop_back();\n\t\t\t\t\t\thull.push_back(local_points[j]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tlocal_hull.push_back(hull[0]);\n\t\tdouble perimeter = 0;\n\t\tfor (int j = 0; j < hull.size(); ++j) {\n\t\t\tperimeter += distance(hull[j], hull[(j + 1) % hull.size()]);\n\t\t}\n\t\tlocal_perimeters.push_back(perimeter);\n\t}\n\n\tstd::vector<double> global_perimeters;\n\tMPI_Gather(&local_perimeters[0], local_perimeters.size(), MPI_DOUBLE, &global_perimeters[0], local_perimeters.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tdouble min_perimeter = global_perimeters[0];\n\t\tfor (int i = 1; i < global_perimeters.size(); ++i) {\n\t\t\tif (global_perimeters[i] < min_perimeter) {\n\t\t\t\tmin_perimeter = global_perimeters[i];\n\t\t\t}\n\t\t}\n\t\treturn min_perimeter;\n\t} else {\n\t\treturn 0;\n\t}\n}",
            "// TODO: Your code here\n\tdouble perimeter = 0;\n\tint n = points.size();\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint start = rank * n / size;\n\tint end = (rank + 1) * n / size;\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\tint local_n = local_points.size();\n\tstd::vector<Point> convex_hull;\n\tstd::vector<Point> temp;\n\tstd::vector<Point> temp2;\n\tstd::vector<Point> temp3;\n\tstd::vector<Point> temp4;\n\tstd::vector<Point> temp5;\n\tstd::vector<Point> temp6;\n\tstd::vector<Point> temp7;\n\tstd::vector<Point> temp8;\n\tstd::vector<Point> temp9;\n\tstd::vector<Point> temp10;\n\tstd::vector<Point> temp11;\n\tstd::vector<Point> temp12;\n\tstd::vector<Point> temp13;\n\tstd::vector<Point> temp14;\n\tstd::vector<Point> temp15;\n\tstd::vector<Point> temp16;\n\tstd::vector<Point> temp17;\n\tstd::vector<Point> temp18;\n\tstd::vector<Point> temp19;\n\tstd::vector<Point> temp20;\n\tstd::vector<Point> temp21;\n\tstd::vector<Point> temp22;\n\tstd::vector<Point> temp23;\n\tstd::vector<Point> temp24;\n\tstd::vector<Point> temp25;\n\tstd::vector<Point> temp26;\n\tstd::vector<Point> temp27;\n\tstd::vector<Point> temp28;\n\tstd::vector<Point> temp29;\n\tstd::vector<Point> temp30;\n\tstd::vector<Point> temp31;\n\tstd::vector<Point> temp32;\n\tstd::vector<Point> temp33;\n\tstd::vector<Point> temp34;\n\tstd::vector<Point> temp35;\n\tstd::vector<Point> temp36;\n\tstd::vector<Point> temp37;\n\tstd::vector<Point> temp38;\n\tstd::vector<Point> temp39;\n\tstd::vector<Point> temp40;\n\tstd::vector<Point> temp41;\n\tstd::vector<Point> temp42;\n\tstd::vector<Point> temp43;\n\tstd::vector<Point> temp44;\n\tstd::vector<Point> temp45;\n\tstd::vector<Point> temp46;\n\tstd::vector<Point> temp47;\n\tstd::vector<Point> temp48;\n\tstd::vector<Point> temp49;\n\tstd::vector<Point> temp50;\n\tstd::vector<Point> temp51;\n\tstd::vector<Point> temp52;\n\tstd::vector<Point> temp53;\n\tstd::vector<Point> temp54;\n\tstd::vector<Point> temp55;\n\tstd::vector<Point> temp56;\n\tstd::vector<Point> temp57;\n\tstd::vector<Point> temp58;\n\tstd::vector<Point> temp59;\n\tstd::vector<Point> temp60;\n\tstd::vector<Point> temp61;\n\tstd::vector<Point> temp62;\n\tstd::vector<Point> temp63;\n\tstd::vector<Point> temp64;\n\tstd::vector<Point> temp65;\n\tstd::vector<Point> temp66;\n\tstd::vector<Point> temp67;\n\tstd::vector<Point> temp68;\n\tstd::vector<Point> temp69;\n\tstd::vector<Point> temp70;\n\tstd::vector<Point> temp71;\n\tstd::vector<Point> temp72;\n\tstd::vector<Point> temp73;\n\tstd::",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\n\tstd::vector<Point> local_points(points.begin() + start, points.begin() + end);\n\n\tstd::vector<Point> hull;\n\tif (local_points.size() > 2) {\n\t\t// Sort the points lexicographically\n\t\tstd::sort(local_points.begin(), local_points.end(), [](Point const& p1, Point const& p2) {\n\t\t\treturn p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n\t\t});\n\n\t\t// Compute the lower hull\n\t\thull.push_back(local_points[0]);\n\t\thull.push_back(local_points[1]);\n\t\tfor (int i = 2; i < local_points.size(); ++i) {\n\t\t\twhile (hull.size() >= 2 &&\n\t\t\t\t\tdistance(hull[hull.size()-2], hull[hull.size()-1]) <= distance(hull[hull.size()-2], local_points[i])) {\n\t\t\t\thull.pop_back();\n\t\t\t}\n\t\t\thull.push_back(local_points[i]);\n\t\t}\n\n\t\t// Compute the upper hull\n\t\tstd::reverse(local_points.begin(), local_points.end());\n\t\thull.push_back(local_points[0]);\n\t\thull.push_back(local_points[1]);\n\t\tfor (int i = 2; i < local_points.size(); ++i) {\n\t\t\twhile (hull.size() >= 2 &&\n\t\t\t\t\tdistance(hull[hull.size()-2], hull[hull.size()-1]) <= distance(hull[hull.size()-2], local_points[i])) {\n\t\t\t\thull.pop_back();\n\t\t\t}\n\t\t\thull.push_back(local_points[i]);\n\t\t}\n\t\tstd::reverse(hull.begin(), hull.end());\n\t}\n\n\t// Gather the hulls from all ranks\n\tint n_hull_points = hull.size();\n\tint* n_hull_points_per_rank = new int[size];\n\tMPI_Gather(&n_hull_points, 1, MPI_INT, n_hull_points_per_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tint n_total_hull_points = 0;\n\tint* n_total_hull_points_per_rank = new int[size];\n\tMPI_Gather(&n_hull_points, 1, MPI_INT, n_total_hull_points_per_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size; ++i) {\n\t\t\tn_total_hull_points += n_total_hull_points_per_rank[i];\n\t\t}\n\t}\n\n\tPoint* hull_points = new Point[n_total_hull_points];\n\tMPI_Gatherv(hull.data(), n_hull_points, MPI_DOUBLE, hull_points, n_hull_points_per_rank, n_total_hull_points_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tdouble perimeter = 0;",
            "// TODO: Your code here\n\tint size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\n\tint start_index = rank * num_points_per_rank;\n\tint end_index = start_index + num_points_per_rank;\n\n\tif (rank == 0) {\n\t\tend_index += num_points_remainder;\n\t}\n\telse if (rank == size - 1) {\n\t\tend_index += num_points_remainder;\n\t}\n\n\tstd::vector<Point> points_rank(points.begin() + start_index, points.begin() + end_index);\n\n\tstd::vector<Point> convex_hull;\n\tconvex_hull.push_back(points_rank[0]);\n\tconvex_hull.push_back(points_rank[1]);\n\n\tfor (int i = 2; i < points_rank.size(); i++) {\n\t\twhile (convex_hull.size() >= 2 &&\n\t\t\tdistance(convex_hull[convex_hull.size() - 2], convex_hull[convex_hull.size() - 1]) <= distance(convex_hull[convex_hull.size() - 2], points_rank[i])) {\n\t\t\tconvex_hull.pop_back();\n\t\t}\n\t\tconvex_hull.push_back(points_rank[i]);\n\t}\n\n\tdouble perimeter = 0;\n\tfor (int i = 0; i < convex_hull.size() - 1; i++) {\n\t\tperimeter += distance(convex_hull[i], convex_hull[i + 1]);\n\t}\n\n\tdouble perimeter_rank;\n\tMPI_Reduce(&perimeter, &perimeter_rank, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\treturn perimeter_rank;\n\t}\n\telse {\n\t\treturn 0;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_points = points.size();\n\tint num_points_per_rank = num_points / size;\n\tint num_points_remainder = num_points % size;\n\n\tint start = rank * num_points_per_rank;\n\tint end = start + num_points_per_rank;\n\tif (rank == size - 1) {\n\t\tend += num_points_remainder;\n\t}\n\n\tstd::vector<Point> points_rank(points.begin() + start, points.begin() + end);\n\n\tstd::vector<Point> convex_hull;\n\tif (points_rank.size() > 2) {\n\t\t// sort points_rank by x-coordinate\n\t\tstd::sort(points_rank.begin(), points_rank.end(), [](Point const& p1, Point const& p2) {\n\t\t\treturn p1.x < p2.x;\n\t\t});\n\n\t\t// find the point with the smallest y-coordinate\n\t\tint min_y_index = 0;\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tif (points_rank[i].y < points_rank[min_y_index].y) {\n\t\t\t\tmin_y_index = i;\n\t\t\t}\n\t\t}\n\n\t\t// find the point with the largest y-coordinate\n\t\tint max_y_index = 0;\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tif (points_rank[i].y > points_rank[max_y_index].y) {\n\t\t\t\tmax_y_index = i;\n\t\t\t}\n\t\t}\n\n\t\t// find the point with the smallest x-coordinate\n\t\tint min_x_index = 0;\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tif (points_rank[i].x < points_rank[min_x_index].x) {\n\t\t\t\tmin_x_index = i;\n\t\t\t}\n\t\t}\n\n\t\t// find the point with the largest x-coordinate\n\t\tint max_x_index = 0;\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tif (points_rank[i].x > points_rank[max_x_index].x) {\n\t\t\t\tmax_x_index = i;\n\t\t\t}\n\t\t}\n\n\t\t// add the points to the convex hull\n\t\tconvex_hull.push_back(points_rank[min_y_index]);\n\t\tconvex_hull.push_back(points_rank[max_y_index]);\n\t\tconvex_hull.push_back(points_rank[min_x_index]);\n\t\tconvex_hull.push_back(points_rank[max_x_index]);\n\n\t\t// find the point with the smallest distance to the line between the points with the smallest and largest y-coordinate\n\t\tdouble min_distance = distance(points_rank[min_y_index], points_rank[max_y_index]);\n\t\tint min_distance_index = 0;\n\t\tfor (int i = 1; i < points_rank.size(); ++i) {\n\t\t\tif (distance(points_rank[min_y_index], points_rank[i]) < min_distance) {\n\t\t\t\tmin_distance = distance(points_rank[min_y_index], points_rank[i]);\n\t\t\t\tmin_distance_index = i;\n\t\t\t}\n\t\t}\n\t\tconvex_hull.push_back(points_rank[min_distance_index]);\n\n\t\t// find the point with the smallest distance to the line between the points with the smallest and largest x-coordinate\n\t\tmin_distance = distance(points_rank[min_x_index], points_rank[max_x_index",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Your code here\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tstd::vector<Point> points_rank(n_per_rank + (rank < n_extra));\n\tMPI_Scatter(points.data(), n_per_rank + (rank < n_extra),\n\t\t\t\tMPI_DOUBLE, points_rank.data(), n_per_rank + (rank < n_extra),\n\t\t\t\tMPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tstd::vector<Point> convex_hull;\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points_rank.size(); ++i) {\n\t\tPoint p = points_rank[i];\n\t\tif (convex_hull.empty() || distance(p, convex_hull.back()) > 1e-10) {\n\t\t\tconvex_hull.push_back(p);\n\t\t}\n\t}\n\n\tstd::vector<Point> convex_hull_global;\n\tMPI_Gather(convex_hull.data(), convex_hull.size(), MPI_DOUBLE,\n\t\t\t   convex_hull_global.data(), convex_hull.size(), MPI_DOUBLE,\n\t\t\t   0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tdouble perimeter = 0;\n\t\tfor (int i = 0; i < convex_hull_global.size(); ++i) {\n\t\t\tperimeter += distance(convex_hull_global[i], convex_hull_global[(i+1)%convex_hull_global.size()]);\n\t\t}\n\t\treturn perimeter;\n\t}\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tstd::vector<Point> localPoints;\n\tstd::vector<Point> globalPoints;\n\n\tif (rank == 0) {\n\t\tlocalPoints = points;\n\t}\n\n\tMPI_Bcast(&localPoints[0], localPoints.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < localPoints.size(); i++) {\n\t\tglobalPoints.push_back(localPoints[i]);\n\t}\n\n\tint num_threads = omp_get_max_threads();\n\tint num_points = globalPoints.size();\n\tint points_per_thread = num_points / num_threads;\n\tint remainder = num_points % num_threads;\n\n\tstd::vector<double> perimeters(num_threads);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < num_threads; i++) {\n\t\tint start = i * points_per_thread;\n\t\tint end = start + points_per_thread;\n\t\tif (i == num_threads - 1) {\n\t\t\tend += remainder;\n\t\t}\n\n\t\tstd::vector<Point> convexHull;\n\t\tconvexHull.push_back(globalPoints[start]);\n\t\tfor (int j = start + 1; j < end; j++) {\n\t\t\tconvexHull.push_back(globalPoints[j]);\n\t\t}\n\n\t\tdouble perimeter = 0;\n\t\tfor (int j = 0; j < convexHull.size(); j++) {\n\t\t\tperimeter += distance(convexHull[j], convexHull[(j + 1) % convexHull.size()]);\n\t\t}\n\n\t\tperimeters[i] = perimeter;\n\t}\n\n\tdouble totalPerimeter = 0;\n\tfor (int i = 0; i < num_threads; i++) {\n\t\ttotalPerimeter += perimeters[i];\n\t}\n\n\treturn totalPerimeter;\n}",
            "// TODO\n\treturn 0.0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: your code here\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = points.size();\n\tint n_per_rank = n / size;\n\tint n_rem = n % size;\n\tint n_local = n_per_rank + (rank < n_rem? 1 : 0);\n\n\tstd::vector<Point> local_points(n_local);\n\tMPI_Scatter(points.data(), n_local, MPI_DOUBLE, local_points.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tstd::vector<Point> hull;\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n_local; ++i) {\n\t\tPoint p = local_points[i];\n\t\tstd::vector<Point> hull_local;\n\t\tfor (int j = 0; j < n_local; ++j) {\n\t\t\tif (i!= j) {\n\t\t\t\tPoint q = local_points[j];\n\t\t\t\tif (distance(p, q) < distance(p, hull_local.back())) {\n\t\t\t\t\thull_local.push_back(q);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\thull_local.push_back(p);\n\t\thull.insert(hull.end(), hull_local.begin(), hull_local.end());\n\t}\n\n\tstd::vector<double> perimeters(size);\n\tMPI_Gather(&hull.size(), 1, MPI_DOUBLE, perimeters.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tdouble perimeter = 0;\n\t\tfor (int i = 0; i < size; ++i) {\n\t\t\tperimeter += perimeters[i];\n\t\t}\n\t\treturn perimeter;\n\t}\n\treturn 0;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// TODO: Implement this function\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(Kokkos::View<const int**> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Your code here\n\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Fill in the body of this function\n\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max_size = 0;\n\tint max_size_component = 0;\n\tint size = 0;\n\tint component = 0;\n\tstd::vector<int> visited(N, 0);\n\n\t#pragma omp parallel for private(size, component)\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tsize = 0;\n\t\t\tcomponent = 0;\n\t\t\tdfs(A, N, i, visited, size, component);\n\t\t\tif (size > max_size) {\n\t\t\t\tmax_size = size;\n\t\t\t\tmax_size_component = component;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn max_size_component;\n}",
            "std::vector<int> visited(N, 0);\n    std::vector<int> component_size(N, 0);\n    int max_component_size = 0;\n    int max_component_index = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        if (visited[i] == 0) {\n            int size = 0;\n            std::vector<int> stack;\n            stack.push_back(i);\n            while (!stack.empty()) {\n                int v = stack.back();\n                stack.pop_back();\n                if (visited[v] == 0) {\n                    visited[v] = 1;\n                    size++;\n                    for (int j = 0; j < N; j++) {\n                        if (A[v * N + j] == 1 && visited[j] == 0) {\n                            stack.push_back(j);\n                        }\n                    }\n                }\n            }\n            component_size[i] = size;\n            if (size > max_component_size) {\n                max_component_size = size;\n                max_component_index = i;\n            }\n        }\n    }\n\n    return component_size[max_component_index];\n}",
            "std::vector<int> visited(N, 0);\n\tint largest = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tint count = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint curr = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tcount++;\n\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\tif (A[curr * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (count > largest) {\n\t\t\t\tlargest = count;\n\t\t\t}\n\t\t}\n\t}\n\treturn largest;\n}",
            "// TODO: implement\n    return 0;\n}",
            "std::vector<int> visited(N, 0);\n\tint max_component_size = 0;\n\tint component_size = 0;\n\n\t#pragma omp parallel for shared(visited) private(component_size)\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcomponent_size = 0;\n\t\t\tdfs(A, i, visited, component_size);\n\t\t\tif (component_size > max_component_size) {\n\t\t\t\tmax_component_size = component_size;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn max_component_size;\n}",
            "// TODO: Implement\n\treturn 0;\n}",
            "std::vector<int> visited(N, 0);\n    int max_component = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (visited[i] == 0) {\n            int component = 0;\n            std::vector<int> stack;\n            stack.push_back(i);\n            visited[i] = 1;\n            while (stack.size() > 0) {\n                int v = stack.back();\n                stack.pop_back();\n                component++;\n                for (int j = 0; j < N; ++j) {\n                    if (A[v * N + j] == 1 && visited[j] == 0) {\n                        stack.push_back(j);\n                        visited[j] = 1;\n                    }\n                }\n            }\n            if (component > max_component) {\n                max_component = component;\n            }\n        }\n    }\n    return max_component;\n}",
            "std::vector<int> visited(N, 0);\n\tstd::vector<int> component(N, 0);\n\tint largest = 0;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tint current = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\tcomponent[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t\tcomponent[j] = 1;\n\t\t\t\t\t\tcurrent++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (current > largest) {\n\t\t\t\tlargest = current;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn largest;\n}",
            "std::vector<int> visited(N, 0);\n\tint max_component = 0;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint component_size = 0;\n\t\t\tstd::vector<int> stack;\n\t\t\tstack.push_back(i);\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tif (visited[v] == 0) {\n\t\t\t\t\tvisited[v] = 1;\n\t\t\t\t\tcomponent_size++;\n\t\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\t\tif (A[N * v + j] == 1) {\n\t\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (component_size > max_component) {\n\t\t\t\tmax_component = component_size;\n\t\t\t}\n\t\t}\n\t}\n\treturn max_component;\n}",
            "// TODO: implement\n    return 0;\n}",
            "std::vector<int> visited(N, 0);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  // TODO: Implement this function\n\n  return max_component;\n}",
            "int max_component = 0;\n\tstd::vector<int> visited(N, 0);\n\tstd::vector<int> component_size(N, 0);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tint size = 0;\n\t\t\tstd::vector<int> stack;\n\t\t\tstack.push_back(i);\n\t\t\twhile (stack.size() > 0) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tif (visited[v] == 0) {\n\t\t\t\t\tvisited[v] = 1;\n\t\t\t\t\tsize++;\n\t\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcomponent_size[i] = size;\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (component_size[i] > max_component) {\n\t\t\tmax_component = component_size[i];\n\t\t}\n\t}\n\n\treturn max_component;\n}",
            "std::vector<int> visited(N, 0);\n\tstd::vector<int> component(N, 0);\n\tint max_component = 0;\n\tint component_size = 0;\n\tint component_id = 0;\n\tint current_vertex = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcomponent_size = 0;\n\t\t\tcomponent_id = 0;\n\t\t\tcurrent_vertex = i;\n\t\t\twhile (current_vertex!= -1) {\n\t\t\t\tvisited[current_vertex] = 1;\n\t\t\t\tcomponent[current_vertex] = component_id;\n\t\t\t\tcomponent_size++;\n\t\t\t\tcurrent_vertex = -1;\n\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\tif (A[i*N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tcurrent_vertex = j;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (component_size > max_component) {\n\t\t\t\tmax_component = component_size;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn max_component;\n}",
            "int max_component = 0;\n\tint component = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tstd::vector<bool> visited(N, false);\n\t\tvisited[i] = true;\n\t\tcomponent = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (visited[j]) {\n\t\t\t\tfor (int k = 0; k < N; k++) {\n\t\t\t\t\tif (A[j * N + k] == 1 &&!visited[k]) {\n\t\t\t\t\t\tvisited[k] = true;\n\t\t\t\t\t\tcomponent++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t#pragma omp critical\n\t\t{\n\t\t\tif (component > max_component) {\n\t\t\t\tmax_component = component;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn max_component;\n}",
            "// TODO: implement this function\n\n    return 0;\n}",
            "int num_threads = 0;\n\tint largest_component = 0;\n\n\t#pragma omp parallel\n\t{\n\t\tint id = omp_get_thread_num();\n\t\tint num_threads = omp_get_num_threads();\n\t\tint start = id * (N / num_threads);\n\t\tint end = (id + 1) * (N / num_threads);\n\t\tint component_size = 0;\n\n\t\tfor (int i = start; i < end; i++) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t\tcomponent_size++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t#pragma omp critical\n\t\t{\n\t\t\tif (component_size > largest_component) {\n\t\t\t\tlargest_component = component_size;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn largest_component;\n}",
            "int max = 0;\n\tint count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (int i = 0; i < N; i++) {\n\t\tif (!visited[i]) {\n\t\t\tcount = 0;\n\t\t\tdfs(A, i, visited, count);\n\t\t\tif (count > max) {\n\t\t\t\tmax = count;\n\t\t\t}\n\t\t}\n\t}\n\treturn max;\n}",
            "// TODO: Fill this in\n\tint max_component = 0;\n\tint component = 0;\n\tint visited[N];\n\tint visited_count = 0;\n\tint i, j;\n\n\tfor (i = 0; i < N; i++) {\n\t\tvisited[i] = 0;\n\t}\n\n\tfor (i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tvisited[i] = 1;\n\t\t\tvisited_count++;\n\t\t\tcomponent = 1;\n\t\t\tfor (j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\tvisited_count++;\n\t\t\t\t\tcomponent++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (component > max_component) {\n\t\t\t\tmax_component = component;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn max_component;\n}",
            "std::vector<int> visited(N, 0);\n    int max_size = 0;\n    int max_size_component = 0;\n    int size = 0;\n    int component = 0;\n    int i, j;\n\n    #pragma omp parallel for private(i, j, size, component)\n    for (i = 0; i < N; i++) {\n        if (visited[i] == 0) {\n            size = 0;\n            component = 0;\n            for (j = 0; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    size++;\n                    component = j;\n                }\n            }\n            if (size > max_size) {\n                max_size = size;\n                max_size_component = component;\n            }\n            visited[i] = 1;\n            visited[component] = 1;\n        }\n    }\n\n    return max_size;\n}",
            "int max_size = 0;\n\tstd::vector<int> visited(N, 0);\n\tstd::vector<int> component_size(N, 0);\n\tstd::vector<int> component_id(N, 0);\n\tint component_count = 0;\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcomponent_size[i] = 1;\n\t\t\tcomponent_id[i] = component_count;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tcomponent_size[i] += 1;\n\t\t\t\t\tcomponent_id[j] = component_count;\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcomponent_count++;\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tmax_size = std::max(max_size, component_size[i]);\n\t}\n\n\treturn max_size;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "serial",
        "prompt": "/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<bool> visited(N, false);\n\tstd::vector<int> components(N, 0);\n\tint max_component = 0;\n\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i]) continue;\n\t\tint component_size = 0;\n\t\tstd::queue<int> q;\n\t\tq.push(i);\n\t\tvisited[i] = true;\n\t\twhile (!q.empty()) {\n\t\t\tint curr = q.front();\n\t\t\tq.pop();\n\t\t\tcomponent_size++;\n\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\tif (A[curr * N + j] == 1 &&!visited[j]) {\n\t\t\t\t\tq.push(j);\n\t\t\t\t\tvisited[j] = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcomponents[i] = component_size;\n\t\tmax_component = std::max(max_component, component_size);\n\t}\n\n\treturn max_component;\n}",
            "std::vector<int> visited(N, 0);\n\tint largest = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint count = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint curr = q.front();\n\t\t\t\tq.pop();\n\t\t\t\t++count;\n\t\t\t\tvisited[curr] = 1;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[curr * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tlargest = std::max(largest, count);\n\t\t}\n\t}\n\treturn largest;\n}",
            "std::vector<int> visited(N, 0);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited[i] == 0) {\n            dfs(A, i, visited);\n            ++count;\n        }\n    }\n\n    return count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint largest = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint count = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tcount++;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (count > largest) {\n\t\t\t\tlargest = count;\n\t\t\t}\n\t\t}\n\t}\n\treturn largest;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i);\n\t\t\t++count;\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tstd::vector<int> component(N, 0);\n\tint largest = 0;\n\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tint current = 0;\n\t\t\tdfs(A, visited, component, i, current);\n\t\t\tlargest = std::max(largest, current);\n\t\t}\n\t}\n\n\treturn largest;\n}",
            "std::vector<int> visited(N, 0);\n  int largest = 0;\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      int count = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = 1;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        ++count;\n        for (size_t j = 0; j < N; ++j) {\n          if (A[v * N + j] == 1 && visited[j] == 0) {\n            q.push(j);\n            visited[j] = 1;\n          }\n        }\n      }\n      largest = std::max(largest, count);\n    }\n  }\n  return largest;\n}",
            "std::vector<int> visited(N, 0);\n\tint max_component = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint component = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\t++component;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tmax_component = std::max(max_component, component);\n\t\t}\n\t}\n\treturn max_component;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i);\n\t\t\t++count;\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<bool> visited(N, false);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, i, visited, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tstd::vector<int> component(N, 0);\n\tint largest = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint comp = 0;\n\t\t\tdfs(A, visited, component, comp, i);\n\t\t\tlargest = std::max(largest, comp);\n\t\t}\n\t}\n\treturn largest;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint max_component = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tint component = 0;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint node = q.front();\n\t\t\t\tq.pop();\n\t\t\t\t++component;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[node * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tmax_component = std::max(max_component, component);\n\t\t}\n\t}\n\treturn max_component;\n}",
            "std::vector<int> visited(N, 0);\n    int largest = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited[i] == 0) {\n            largest = std::max(largest, dfs(A, visited, i, N));\n        }\n    }\n    return largest;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int largest = 0;\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      int count = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = 1;\n      while (!q.empty()) {\n        int curr = q.front();\n        q.pop();\n        ++count;\n        for (size_t j = 0; j < N; ++j) {\n          if (A[curr * N + j] == 1 && visited[j] == 0) {\n            q.push(j);\n            visited[j] = 1;\n          }\n        }\n      }\n      largest = std::max(largest, count);\n    }\n  }\n  return largest;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "hip",
        "prompt": "/* Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int row = blockIdx.x;\n\tint col = blockIdx.y;\n\tint tid = threadIdx.x;\n\tint numThreads = blockDim.x;\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the result in largestComponentSize.\n\t// A is an NxN adjacency matrix stored in row-major.\n\t// Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n\t// Example:\n\n\t// input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n\t// output: 2\n\n\t// TODO: Implement this function\n\n\t// Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n\t// Store the",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) return;\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint componentSize = 0;\n\t\tint i = tid;\n\t\twhile (A[i * N + i] == 0) {\n\t\t\ti++;\n\t\t}\n\t\tcomponentSize++;\n\t\twhile (i < N) {\n\t\t\tif (A[i * N + i] == 1) {\n\t\t\t\ti++;\n\t\t\t\tcomponentSize++;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tatomicMax(largestComponentSize, componentSize);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tif (count > *largestComponentSize) {\n\t\t\t*largestComponentSize = count;\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) return;\n\tif (A[i * N + j] == 1) atomicAdd(largestComponentSize, 1);\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(largestComponentSize, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(largestComponentSize, 1);\n\t\t}\n\t}\n}",
            "// TODO: Your code here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(largestComponentSize, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) return;\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N && i!= j) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicMax(largestComponentSize, 1);\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint componentSize = 0;\n\t\tint i = tid;\n\t\twhile (A[i * N + i] == 1) {\n\t\t\tcomponentSize++;\n\t\t\ti = A[i * N + i] - 1;\n\t\t}\n\t\tatomicMax(largestComponentSize, componentSize);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint nthreads = blockDim.x * gridDim.x;\n\tint componentSize = 0;\n\tint componentId = 0;\n\tint componentCount = 0;\n\tint componentLargest = 0;\n\tint componentLargestId = 0;\n\n\tfor (int i = tid; i < N; i += nthreads) {\n\t\tif (A[i * N + i] == 1) {\n\t\t\tcomponentSize = 0;\n\t\t\tcomponentId = i;\n\t\t\tcomponentCount = 0;\n\t\t\tcomponentLargest = 0;\n\t\t\tcomponentLargestId = 0;\n\t\t\twhile (componentId!= -1) {\n\t\t\t\tcomponentSize++;\n\t\t\t\tcomponentCount++;\n\t\t\t\tif (componentSize > componentLargest) {\n\t\t\t\t\tcomponentLargest = componentSize;\n\t\t\t\t\tcomponentLargestId = componentId;\n\t\t\t\t}\n\t\t\t\tcomponentId = A[componentId * N + componentId];\n\t\t\t}\n\t\t\tif (componentCount > 1) {\n\t\t\t\tatomicAdd(largestComponentSize, componentLargest);\n\t\t\t}\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = blockIdx.z * blockDim.z + threadIdx.z;\n\n\tif (i >= N || j >= N || k >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1 && A[j * N + k] == 1 && A[i * N + k] == 0) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "// TODO: implement\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\n\treturn 0;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint* A_local = new int[N*N];\n\tint* A_global = new int[N*N];\n\tint* A_global_temp = new int[N*N];\n\tint* A_global_temp2 = new int[N*N];\n\tint* A_global_temp3 = new int[N*N];\n\tint* A_global_temp4 = new int[N*N];\n\tint* A_global_temp5 = new int[N*N];\n\tint* A_global_temp6 = new int[N*N];\n\tint* A_global_temp7 = new int[N*N];\n\tint* A_global_temp8 = new int[N*N];\n\tint* A_global_temp9 = new int[N*N];\n\tint* A_global_temp10 = new int[N*N];\n\tint* A_global_temp11 = new int[N*N];\n\tint* A_global_temp12 = new int[N*N];\n\tint* A_global_temp13 = new int[N*N];\n\tint* A_global_temp14 = new int[N*N];\n\tint* A_global_temp15 = new int[N*N];\n\tint* A_global_temp16 = new int[N*N];\n\tint* A_global_temp17 = new int[N*N];\n\tint* A_global_temp18 = new int[N*N];\n\tint* A_global_temp19 = new int[N*N];\n\tint* A_global_temp20 = new int[N*N];\n\tint* A_global_temp21 = new int[N*N];\n\tint* A_global_temp22 = new int[N*N];\n\tint* A_global_temp23 = new int[N*N];\n\tint* A_global_temp24 = new int[N*N];\n\tint* A_global_temp25 = new int[N*N];\n\tint* A_global_temp26 = new int[N*N];\n\tint* A_global_temp27 = new int[N*N];\n\tint* A_global_temp28 = new int[N*N];\n\tint* A_global_temp29 = new int[N*N];\n\tint* A_global_temp30 = new int[N*N];\n\tint* A_global_temp31 = new int[N*N];\n\tint* A_global_temp32 = new int[N*N];\n\tint* A_global_temp33 = new int[N*N];\n\tint* A_global_temp34 = new int[N*N];\n\tint* A_global_temp35 = new int[N*N];\n\tint* A_global_temp36 = new int[N*N];\n\tint* A_global_temp37 = new int[N*N];\n\tint* A_global_temp38 = new int[N*N];\n\tint* A_global_temp39 = new int[N*N];\n\tint* A_global_temp40 = new int[N*N];\n\tint* A_global_temp41 = new int[N*N];\n\tint* A_global_temp42 = new int[N*N];\n\tint* A_global_temp43 = new int[N*N];\n\tint* A_global_temp44 = new int[N*N];\n\tint* A_global_temp45 = new int[N*N];\n\tint* A_global_temp46 = new int[N*N];\n\tint* A_global_temp47 = new int[N*N];\n\tint* A_global_temp48 = new int[N*N];\n\tint* A_global_temp49 = new int[N*N];\n\tint* A_global_temp50 = new int[N",
            "// TODO: Your code here\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N && A[i * N + j] == 1) {\n        atomicAdd(largestComponentSize, 1);\n    }\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Fill in your code here\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint count = 0;\n\tif (i < N && j < N && A[i * N + j] == 1) {\n\t\tcount++;\n\t}\n\tif (count > *largestComponentSize) {\n\t\t*largestComponentSize = count;\n\t}\n}",
            "// TODO: Your code here\n\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint row = tid / N;\n\tint col = tid % N;\n\tif (row == col) {\n\t\tif (A[tid] == 1) {\n\t\t\tatomicAdd(largestComponentSize, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x;\n\tint j = blockIdx.y;\n\tint k = threadIdx.x;\n\n\tint size = 0;\n\tint visited[N];\n\tint queue[N];\n\tint front = 0;\n\tint rear = 0;\n\n\tif (i == j) {\n\t\tif (A[i * N + k] == 1) {\n\t\t\tvisited[k] = 1;\n\t\t\tqueue[rear++] = k;\n\t\t\twhile (front!= rear) {\n\t\t\t\tint u = queue[front++];\n\t\t\t\tfor (int v = 0; v < N; v++) {\n\t\t\t\t\tif (A[u * N + v] == 1 && visited[v] == 0) {\n\t\t\t\t\t\tvisited[v] = 1;\n\t\t\t\t\t\tqueue[rear++] = v;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsize = size + 1;\n\t\t}\n\t}\n\tif (size > *largestComponentSize) {\n\t\t*largestComponentSize = size;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(largestComponentSize, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint count = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tcount++;\n\t\t}\n\t}\n\t*largestComponentSize = count;\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(largestComponentSize, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i >= N || j >= N) return;\n\n    if (A[i * N + j] == 1) {\n        atomicAdd(largestComponentSize, 1);\n    }\n}",
            "// TODO: implement\n\n}",
            "// TODO: Your code here\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint size = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tsize = 1;\n\t\t\tfor (int k = 0; k < N; k++) {\n\t\t\t\tif (A[i * N + k] == 1) {\n\t\t\t\t\tsize++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (size > *largestComponentSize) {\n\t\t*largestComponentSize = size;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint size = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tsize++;\n\t\t}\n\t}\n\tatomicAdd(largestComponentSize, size);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (A[i * N + j] == 1) {\n            atomicAdd(largestComponentSize, 1);\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(largestComponentSize, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (A[i * N + j] == 1) {\n            atomicAdd(largestComponentSize, 1);\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "12_graph_largest_component",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n\tint max_component = 0;\n\tint component = 0;\n\tint rank = 0;\n\tint size = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint *component_array = new int[size];\n\tint *max_component_array = new int[size];\n\tint *max_component_array_temp = new int[size];\n\tint *component_array_temp = new int[size];\n\tint *component_array_temp2 = new int[size];\n\tint *component_array_temp3 = new int[size];\n\tint *component_array_temp4 = new int[size];\n\tint *component_array_temp5 = new int[size];\n\tint *component_array_temp6 = new int[size];\n\tint *component_array_temp7 = new int[size];\n\tint *component_array_temp8 = new int[size];\n\tint *component_array_temp9 = new int[size];\n\tint *component_array_temp10 = new int[size];\n\tint *component_array_temp11 = new int[size];\n\tint *component_array_temp12 = new int[size];\n\tint *component_array_temp13 = new int[size];\n\tint *component_array_temp14 = new int[size];\n\tint *component_array_temp15 = new int[size];\n\tint *component_array_temp16 = new int[size];\n\tint *component_array_temp17 = new int[size];\n\tint *component_array_temp18 = new int[size];\n\tint *component_array_temp19 = new int[size];\n\tint *component_array_temp20 = new int[size];\n\tint *component_array_temp21 = new int[size];\n\tint *component_array_temp22 = new int[size];\n\tint *component_array_temp23 = new int[size];\n\tint *component_array_temp24 = new int[size];\n\tint *component_array_temp25 = new int[size];\n\tint *component_array_temp26 = new int[size];\n\tint *component_array_temp27 = new int[size];\n\tint *component_array_temp28 = new int[size];\n\tint *component_array_temp29 = new int[size];\n\tint *component_array_temp30 = new int[size];\n\tint *component_array_temp31 = new int[size];\n\tint *component_array_temp32 = new int[size];\n\tint *component_array_temp33 = new int[size];\n\tint *component_array_temp34 = new int[size];\n\tint *component_array_temp35 = new int[size];\n\tint *component_array_temp36 = new int[size];\n\tint *component_array_temp37 = new int[size];\n\tint *component_array_temp38 = new int[size];\n\tint *component_array_temp39 = new int[size];\n\tint *component_array_temp40 = new int[size];\n\tint *component_array_temp41 = new int[size];\n\tint *component_array_temp42 = new int[size];\n\tint *component_array_temp43 = new int[size];\n\tint *component_array_temp44 = new int[size];\n\tint *component_array_temp45 = new int[size];\n\tint *component_array_temp46 = new int[size];\n\tint *component_array_temp47 = new int[size];\n\tint *component_array_temp48 = new int[size];\n\tint *component_array_temp49 = new int[size];\n\tint *component_array_temp50 = new int[size];\n\tint *component_array_temp51 = new int[size];\n\tint *component_array_temp52 = new int[size];\n\tint *component_array_temp53 = new int[size];\n\tint *",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement this function\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int num_threads = omp_get_max_threads();\n\tint num_procs = omp_get_num_procs();\n\tint my_rank = omp_get_thread_num();\n\tint num_vertices = N;\n\tint num_edges = 0;\n\tint *adj_matrix = new int[num_vertices * num_vertices];\n\tint *visited = new int[num_vertices];\n\tint *component_sizes = new int[num_procs];\n\tint *component_sizes_sum = new int[num_procs];\n\tint *component_sizes_sum_temp = new int[num_procs];\n\tint *component_sizes_sum_final = new int[num_procs];\n\tint *component_sizes_sum_final_temp = new int[num_procs];\n\tint *component_sizes_sum_final_temp2 = new int[num_procs];\n\tint *component_sizes_sum_final_temp3 = new int[num_procs];\n\tint *component_sizes_sum_final_temp4 = new int[num_procs];\n\tint *component_sizes_sum_final_temp5 = new int[num_procs];\n\tint *component_sizes_sum_final_temp6 = new int[num_procs];\n\tint *component_sizes_sum_final_temp7 = new int[num_procs];\n\tint *component_sizes_sum_final_temp8 = new int[num_procs];\n\tint *component_sizes_sum_final_temp9 = new int[num_procs];\n\tint *component_sizes_sum_final_temp10 = new int[num_procs];\n\tint *component_sizes_sum_final_temp11 = new int[num_procs];\n\tint *component_sizes_sum_final_temp12 = new int[num_procs];\n\tint *component_sizes_sum_final_temp13 = new int[num_procs];\n\tint *component_sizes_sum_final_temp14 = new int[num_procs];\n\tint *component_sizes_sum_final_temp15 = new int[num_procs];\n\tint *component_sizes_sum_final_temp16 = new int[num_procs];\n\tint *component_sizes_sum_final_temp17 = new int[num_procs];\n\tint *component_sizes_sum_final_temp18 = new int[num_procs];\n\tint *component_sizes_sum_final_temp19 = new int[num_procs];\n\tint *component_sizes_sum_final_temp20 = new int[num_procs];\n\tint *component_sizes_sum_final_temp21 = new int[num_procs];\n\tint *component_sizes_sum_final_temp22 = new int[num_procs];\n\tint *component_sizes_sum_final_temp23 = new int[num_procs];\n\tint *component_sizes_sum_final_temp24 = new int[num_procs];\n\tint *component_sizes_sum_final_temp25 = new int[num_procs];\n\tint *component_sizes_sum_final_temp26 = new int[num_procs];\n\tint *component_sizes_sum_final_temp27 = new int[num_procs];\n\tint *component_sizes_sum_final_temp28 = new int[num_procs];\n\tint *component_sizes_sum_final_temp29 = new int[num_procs];\n\tint *component_sizes_sum_final_temp30 = new int[num_procs];\n\tint *component_sizes_sum_final_temp31 = new int[num_procs];\n\tint *component_sizes_sum_final_temp32 = new int[num_procs];\n\tint *component_sizes_sum_final_temp33 = new int[num_procs];\n\tint *component_sizes_sum_final_temp34 = new int[num_procs",
            "// TODO: implement this function\n\tint num_threads = omp_get_max_threads();\n\tint num_procs = omp_get_num_procs();\n\tint rank = omp_get_thread_num();\n\tint size = omp_get_num_threads();\n\tint num_vertices = N;\n\tint num_edges = num_vertices * (num_vertices - 1) / 2;\n\tint num_components = 0;\n\tint num_vertices_in_component = 0;\n\tint num_vertices_in_component_max = 0;\n\tint num_vertices_in_component_max_rank = 0;\n\tint num_vertices_in_component_rank = 0;\n\tint num_vertices_in_component_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max_rank_max = 0;\n\tint num_vertices_in_component_rank_max_rank_max_rank",
            "// TODO: implement\n  return 0;\n}",
            "// TODO\n}",
            "// TODO: implement\n  return 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint *A_local = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local[i] = A[i];\n\t}\n\n\tint *A_local_copy = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_2 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_2[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_3 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_3[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_4 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_4[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_5 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_5[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_6 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_6[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_7 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_7[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_8 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_8[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_9 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_9[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_10 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_10[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_11 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_11[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_12 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_12[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_13 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_13[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_14 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_14[i] = A_local[i];\n\t}\n\n\tint *A_local_copy_15 = new int[N*N];\n\tfor (int i = 0; i < N*N; i++) {\n\t\tA_local_copy_",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: Your code here\n\tint max_size = 0;\n\tint max_size_index = 0;\n\tint size = 0;\n\tint size_index = 0;\n\tint *visited = new int[N];\n\tint *visited_size = new int[N];\n\tint *visited_index = new int[N];\n\tint *visited_size_index = new int[N];\n\tint *visited_index_size = new int[N];\n\tint *visited_index_size_index = new int[N];\n\tint *visited_size_index_index = new int[N];\n\tint *visited_index_size_index_index = new int[N];\n\tint *visited_size_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index = new int[N];\n\tint *visited_index_size_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index_index",
            "int result = 0;\n\tint *visited = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tvisited[i] = 0;\n\t}\n\tint *component = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent[i] = 0;\n\t}\n\tint *component_count = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_count[i] = 0;\n\t}\n\tint *component_size = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size[i] = 0;\n\t}\n\tint *component_size_temp = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp[i] = 0;\n\t}\n\tint *component_size_temp2 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp2[i] = 0;\n\t}\n\tint *component_size_temp3 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp3[i] = 0;\n\t}\n\tint *component_size_temp4 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp4[i] = 0;\n\t}\n\tint *component_size_temp5 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp5[i] = 0;\n\t}\n\tint *component_size_temp6 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp6[i] = 0;\n\t}\n\tint *component_size_temp7 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp7[i] = 0;\n\t}\n\tint *component_size_temp8 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp8[i] = 0;\n\t}\n\tint *component_size_temp9 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp9[i] = 0;\n\t}\n\tint *component_size_temp10 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp10[i] = 0;\n\t}\n\tint *component_size_temp11 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp11[i] = 0;\n\t}\n\tint *component_size_temp12 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp12[i] = 0;\n\t}\n\tint *component_size_temp13 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp13[i] = 0;\n\t}\n\tint *component_size_temp14 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp14[i] = 0;\n\t}\n\tint *component_size_temp15 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp15[i] = 0;\n\t}\n\tint *component_size_temp16 = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tcomponent_size_temp16[i] ="
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int maxDegree = 0;\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n                          KOKKOS_LAMBDA(const int i, int &localMaxDegree) {\n                            int degree = 0;\n                            for (int j = 0; j < N; ++j) {\n                              if (A(i, j) == 1) {\n                                degree++;\n                              }\n                            }\n                            localMaxDegree = std::max(localMaxDegree, degree);\n                          },\n                          Kokkos::Max<int>(maxDegree));\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::OpenMP>(0, N), KOKKOS_LAMBDA(const int i, int &localMaxDegree) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j)!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tlocalMaxDegree = std::max(localMaxDegree, degree);\n\t}, Kokkos::Max<int>(maxDegree));\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n\t\t\t\t\t\t\t[&](const int i, int &maxDegree) {\n\t\t\t\t\t\t\t\tint degree = 0;\n\t\t\t\t\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\t\t\t\t\tif (A(i, j)!= 0) {\n\t\t\t\t\t\t\t\t\t\tdegree++;\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tif (degree > maxDegree) {\n\t\t\t\t\t\t\t\t\tmaxDegree = degree;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tmaxDegree);\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::Cuda>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &localMaxDegree) {\n\t\t\tint degree = 0;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tdegree++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tlocalMaxDegree = std::max(localMaxDegree, degree);\n\t\t},\n\t\tKokkos::Max<int>(maxDegree)\n\t);\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "Kokkos::View<int*> maxDegree(\"maxDegree\", 1);\n  Kokkos::parallel_for(\"maxDegree\", N, KOKKOS_LAMBDA(const int i) {\n    int max = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        max++;\n      }\n    }\n    if (max > maxDegree(0)) {\n      maxDegree(0) = max;\n    }\n  });\n  Kokkos::fence();\n  return maxDegree(0);\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n\t\t\t\t\t\t\t[&](const int i, int &maxDegree) {\n\t\t\t\t\t\t\t\tint degree = 0;\n\t\t\t\t\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\t\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\t\t\t\t\t\tdegree++;\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tif (degree > maxDegree) {\n\t\t\t\t\t\t\t\t\tmaxDegree = degree;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tmaxDegree);\n\treturn maxDegree;\n}",
            "// TODO: Implement this function\n\tint maxDegree = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "int maxDegree = 0;\n\n\t// TODO: Compute the maximum degree of the graph using Kokkos\n\n\treturn maxDegree;\n}",
            "// Create a Kokkos view to hold the maximum degree\n\tKokkos::View<int*, Kokkos::HostSpace> maxDegree(\"maxDegree\", 1);\n\n\t// Create a Kokkos view to hold the degree of each node\n\tKokkos::View<int*, Kokkos::HostSpace> degree(\"degree\", N);\n\n\t// Initialize the degree view to 0\n\tKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n\t\tdegree(i) = 0;\n\t});\n\n\t// Compute the degree of each node\n\tKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tdegree(i)++;\n\t\t\t}\n\t\t}\n\t});\n\n\t// Find the maximum degree\n\tKokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& lmax) {\n\t\tif (degree(i) > lmax) {\n\t\t\tlmax = degree(i);\n\t\t}\n\t}, Kokkos::Max<int>(maxDegree(0)));\n\n\t// Copy the maximum degree back to the host\n\tint maxDegree_host;\n\tKokkos::deep_copy(maxDegree_host, maxDegree);\n\n\treturn maxDegree_host;\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::Cuda>(0, N), KOKKOS_LAMBDA(const int i, int &maxDegree) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j)!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}, maxDegree);\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Fill in the code\n\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "int maxDegree = 0;\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n    KOKKOS_LAMBDA(const int i, int &localMaxDegree) {\n      int degree = 0;\n      for (int j = 0; j < N; j++) {\n        degree += A(i, j);\n      }\n      localMaxDegree = std::max(localMaxDegree, degree);\n    },\n    Kokkos::Max<int>(maxDegree));\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::Cuda>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &localMaxDegree) {\n\t\t\tint degree = 0;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j)!= 0) {\n\t\t\t\t\tdegree++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tlocalMaxDegree = std::max(localMaxDegree, degree);\n\t\t},\n\t\tKokkos::Max<int>(maxDegree)\n\t);\n\treturn maxDegree;\n}",
            "// TODO: Fill in the code here\n\tint maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max_degree = 0;\n\n  #pragma omp parallel for reduction(max: max_degree)\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        degree++;\n      }\n    }\n    max_degree = std::max(max_degree, degree);\n  }\n\n  return max_degree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int max = 0;\n\t#pragma omp parallel for reduction(max:max)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint sum = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tsum += A[i*N + j];\n\t\t}\n\t\tif (sum > max) {\n\t\t\tmax = sum;\n\t\t}\n\t}\n\treturn max;\n}",
            "int max_degree = 0;\n\n\t#pragma omp parallel for reduction(max:max_degree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > max_degree) {\n\t\t\tmax_degree = degree;\n\t\t}\n\t}\n\n\treturn max_degree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        degree++;\n      }\n    }\n    if (degree > maxDegree) {\n      maxDegree = degree;\n    }\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\n  #pragma omp parallel for reduction(max: maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i*N + j] == 1) {\n        degree++;\n      }\n    }\n    if (degree > maxDegree) {\n      maxDegree = degree;\n    }\n  }\n\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max: maxDegree)\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\t#pragma omp critical\n\t\t{\n\t\t\tif (degree > maxDegree) {\n\t\t\t\tmaxDegree = degree;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j]!= 0) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max: maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int max_degree = 0;\n\n\t#pragma omp parallel for reduction(max:max_degree)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max_degree) {\n\t\t\tmax_degree = degree;\n\t\t}\n\t}\n\n\treturn max_degree;\n}",
            "int max = 0;\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\t#pragma omp critical\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        degree++;\n      }\n    }\n    #pragma omp critical\n    {\n      if (degree > maxDegree) {\n        maxDegree = degree;\n      }\n    }\n  }\n\n  return maxDegree;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "serial",
        "prompt": "/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A[i * N + j];\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int max = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int max = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmax = std::max(max, degree);\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int max = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++degree;\n            }\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}",
            "int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A[i * N + j];\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "hip",
        "prompt": "/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. Store the result in maxDegree.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) atomicMax(maxDegree, __ldg(&A[i * N + j]));\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tint j = blockDim.y * blockIdx.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tmax = A[i * N + j];\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int max = 0;\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j]!= 0) {\n        max++;\n      }\n    }\n    atomicMax(maxDegree, max);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tmax = A[i * N + j];\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int max = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i]!= 0) {\n        max++;\n      }\n    }\n    atomicMax(maxDegree, max);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int degree = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        degree++;\n      }\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int max = 0;\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint sum = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tsum += A[i * N + j];\n\t\t}\n\t\tif (sum > max) {\n\t\t\tmax = sum;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j]!= 0) {\n\t\t\tmax = 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j]!= 0) {\n\t\t\tatomicMax(maxDegree, A[i * N + j]);\n\t\t}\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint max = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] > max) {\n\t\t\t\tmax = A[tid * N + i];\n\t\t\t}\n\t\t}\n\t\tmaxDegree[tid] = max;\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint degree = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] > 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int max = 0;\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int degree = 0;\n    for (int i = 0; i < N; i++) {\n      degree += A[tid * N + i];\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint degree = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int maxDegree = 0;\n    for (size_t i = 0; i < N; i++) {\n        int degree = 0;\n        for (size_t j = 0; j < N; j++) {\n            degree += A[i*N + j];\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j]!= 0) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i*N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i*N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tif (degree > maxDegree) {\n\t\t\tmaxDegree = degree;\n\t\t}\n\t}\n\treturn maxDegree;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "// TODO: implement this function\n\tint max = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tint count = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tif (count > max) {\n\t\t\tmax = count;\n\t\t}\n\t}\n\treturn max;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. Store the result in maxDegree.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tmax = 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint max = 0;\n\tif (i < N) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tmax++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, max);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int max = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                max++;\n            }\n        }\n        atomicMax(maxDegree, max);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint max = 0;\n\tif (i < N) {\n\t\tint sum = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tsum += A[i * N + j];\n\t\t}\n\t\tif (sum > max) {\n\t\t\tmax = sum;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint max = 0;\n\tif (tid < N) {\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tmax++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, max);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tmax = max + 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tmax = 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tint sum = 0;\n\tif (i < N && j < N) {\n\t\tfor (int k = 0; k < N; k++) {\n\t\t\tsum += A[i * N + k] * A[j * N + k];\n\t\t}\n\t\tif (sum > max) {\n\t\t\tmax = sum;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int max = 0;\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint sum = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tsum += A[i * N + j];\n\t\t}\n\t\tif (sum > max) {\n\t\t\tmax = sum;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint max = 0;\n\tif (i < N) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tmax++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, max);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tmax++;\n\t\t}\n\t}\n\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint max = 0;\n\tif (i < N) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tmax++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, max);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j]!= 0) {\n        degree++;\n      }\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] > 0) {\n\t\t\tmax = 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        degree++;\n      }\n    }\n    atomicMax(maxDegree, degree);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tmax = A[i * N + j];\n\t}\n\tatomicMax(maxDegree, max);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tatomicMax(maxDegree, degree);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint max = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tmax = 1;\n\t\t}\n\t}\n\tatomicMax(maxDegree, max);\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "13_graph_highest_degree",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the highest node degree in the graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A[i * N + j];\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        degree++;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j]!= 0) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max: maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "// TODO: Your code here\n  int maxDegree = 0;\n  int localMaxDegree = 0;\n  int localMaxDegree_temp = 0;\n  int localMaxDegree_temp2 = 0;\n  int localMaxDegree_temp3 = 0;\n  int localMaxDegree_temp4 = 0;\n  int localMaxDegree_temp5 = 0;\n  int localMaxDegree_temp6 = 0;\n  int localMaxDegree_temp7 = 0;\n  int localMaxDegree_temp8 = 0;\n  int localMaxDegree_temp9 = 0;\n  int localMaxDegree_temp10 = 0;\n  int localMaxDegree_temp11 = 0;\n  int localMaxDegree_temp12 = 0;\n  int localMaxDegree_temp13 = 0;\n  int localMaxDegree_temp14 = 0;\n  int localMaxDegree_temp15 = 0;\n  int localMaxDegree_temp16 = 0;\n  int localMaxDegree_temp17 = 0;\n  int localMaxDegree_temp18 = 0;\n  int localMaxDegree_temp19 = 0;\n  int localMaxDegree_temp20 = 0;\n  int localMaxDegree_temp21 = 0;\n  int localMaxDegree_temp22 = 0;\n  int localMaxDegree_temp23 = 0;\n  int localMaxDegree_temp24 = 0;\n  int localMaxDegree_temp25 = 0;\n  int localMaxDegree_temp26 = 0;\n  int localMaxDegree_temp27 = 0;\n  int localMaxDegree_temp28 = 0;\n  int localMaxDegree_temp29 = 0;\n  int localMaxDegree_temp30 = 0;\n  int localMaxDegree_temp31 = 0;\n  int localMaxDegree_temp32 = 0;\n  int localMaxDegree_temp33 = 0;\n  int localMaxDegree_temp34 = 0;\n  int localMaxDegree_temp35 = 0;\n  int localMaxDegree_temp36 = 0;\n  int localMaxDegree_temp37 = 0;\n  int localMaxDegree_temp38 = 0;\n  int localMaxDegree_temp39 = 0;\n  int localMaxDegree_temp40 = 0;\n  int localMaxDegree_temp41 = 0;\n  int localMaxDegree_temp42 = 0;\n  int localMaxDegree_temp43 = 0;\n  int localMaxDegree_temp44 = 0;\n  int localMaxDegree_temp45 = 0;\n  int localMaxDegree_temp46 = 0;\n  int localMaxDegree_temp47 = 0;\n  int localMaxDegree_temp48 = 0;\n  int localMaxDegree_temp49 = 0;\n  int localMaxDegree_temp50 = 0;\n  int localMaxDegree_temp51 = 0;\n  int localMaxDegree_temp52 = 0;\n  int localMaxDegree_temp53 = 0;\n  int localMaxDegree_temp54 = 0;\n  int localMaxDegree_temp55 = 0;\n  int localMaxDegree_temp56 = 0;\n  int localMaxDegree_temp57 = 0;\n  int localMaxDegree_temp58 = 0;\n  int localMaxDegree_temp59 = 0;\n  int localMaxDegree_temp60 = 0;\n  int localMaxDegree_temp61 = 0;\n  int localMaxDegree_temp62 = 0;\n  int localMaxDegree_temp",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i*N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; i++) {\n    int degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max: maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max:maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n  int localMaxDegree = 0;\n  #pragma omp parallel for reduction(max:localMaxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    localMaxDegree = std::max(localMaxDegree, degree);\n  }\n  int globalMaxDegree = 0;\n  MPI_Reduce(&localMaxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return globalMaxDegree;\n}",
            "int maxDegree = 0;\n    #pragma omp parallel for reduction(max: maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++degree;\n            }\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}",
            "int maxDegree = 0;\n  #pragma omp parallel for reduction(max: maxDegree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    maxDegree = std::max(maxDegree, degree);\n  }\n  return maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max: maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n\t#pragma omp parallel for reduction(max:maxDegree)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tint degree = 0;\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++degree;\n\t\t\t}\n\t\t}\n\t\tmaxDegree = std::max(maxDegree, degree);\n\t}\n\treturn maxDegree;\n}",
            "int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++degree;\n            }\n        }\n        maxDegree = std::max(maxDegree, degree);\n    }\n    return maxDegree;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "Kokkos::View<int*, Kokkos::HostSpace> count(\"count\", N);\n  Kokkos::parallel_for(\n    \"edgeCount\",\n    Kokkos::RangePolicy<Kokkos::HostSpace>(0, N),\n    KOKKOS_LAMBDA(const int i) {\n      int sum = 0;\n      for (int j = 0; j < N; j++) {\n        sum += A(i, j);\n      }\n      count(i) = sum;\n    }\n  );\n  Kokkos::fence();\n\n  int total = 0;\n  for (int i = 0; i < N; i++) {\n    total += count(i);\n  }\n  return total / 2;\n}",
            "// TODO: Implement this function\n  int edgeCount = 0;\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        edgeCount++;\n      }\n    }\n  }\n  return edgeCount;\n}",
            "int sum = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, N),\n    KOKKOS_LAMBDA(const int i, int &local_sum) {\n      for (int j = 0; j < N; j++) {\n        if (A(i, j) == 1) {\n          local_sum++;\n        }\n      }\n    },\n    sum);\n  return sum;\n}",
            "int num_edges = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::OpenMP>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &local_num_edges) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocal_num_edges++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tnum_edges\n\t);\n\treturn num_edges;\n}",
            "int numEdges = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::Cuda>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &local_numEdges) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocal_numEdges++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tnumEdges\n\t);\n\treturn numEdges;\n}",
            "// TODO: Implement this function\n  // Hint: You can use Kokkos::parallel_reduce to compute the sum of the elements of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a submatrix of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kokkos::View\n  // Hint: You can use Kokkos::subview to get a subvector of a Kok",
            "Kokkos::View<int*> counts(\"counts\", N);\n\tKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n\t\tint sum = 0;\n\t\tfor (int j = 0; j < N; ++j) {\n\t\t\tsum += A(i, j);\n\t\t}\n\t\tcounts(i) = sum;\n\t});\n\tKokkos::fence();\n\n\tint total = 0;\n\tfor (int i = 0; i < N; ++i) {\n\t\ttotal += counts(i);\n\t}\n\treturn total;\n}",
            "// TODO: implement me\n  return 0;\n}",
            "int numEdges = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &local_numEdges) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocal_numEdges++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tnumEdges\n\t);\n\treturn numEdges;\n}",
            "// Create a Kokkos view for the output\n\tKokkos::View<int*> out(\"edgeCount\", 1);\n\n\t// Create a Kokkos parallel_for to count the number of edges\n\tKokkos::parallel_for(\n\t\t\"edgeCount\",\n\t\tKokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<int> > >(0, N),\n\t\tKOKKOS_LAMBDA(const int i) {\n\t\t\tint count = 0;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tout(0) += count;\n\t\t}\n\t);\n\n\t// Wait for the parallel_for to finish\n\tKokkos::fence();\n\n\t// Copy the output from the device to the host\n\tint out_host;\n\tKokkos::deep_copy(out_host, out);\n\n\treturn out_host;\n}",
            "// TODO: Implement me\n  return 0;\n}",
            "// TODO: Fill in the body of this function\n\n\treturn 0;\n}",
            "// TODO: Fill in the body of this function\n  // You may need to use Kokkos::parallel_reduce\n  // You may need to use Kokkos::atomic_add\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview\n  //",
            "int numEdges = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &local_numEdges) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocal_numEdges++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tnumEdges);\n\treturn numEdges;\n}",
            "// Create a Kokkos view to hold the number of edges in each row of A\n\tKokkos::View<int*> rowCount(\"rowCount\", N);\n\n\t// Create a Kokkos parallel_for to compute the number of edges in each row\n\tKokkos::parallel_for(\n\t\t\"rowCount\",\n\t\tKokkos::RangePolicy<Kokkos::OpenMP>(0, N),\n\t\tKOKKOS_LAMBDA(const int i) {\n\t\t\tint count = 0;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\trowCount(i) = count;\n\t\t}\n\t);\n\n\t// Create a Kokkos parallel_reduce to compute the total number of edges\n\tint totalCount = Kokkos::parallel_reduce(\n\t\t\"totalCount\",\n\t\tKokkos::RangePolicy<Kokkos::OpenMP>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int& total) {\n\t\t\ttotal += rowCount(i);\n\t\t},\n\t\t0\n\t);\n\n\treturn totalCount;\n}",
            "int sum = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::OpenMP>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &local_sum) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocal_sum++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tsum);\n\treturn sum;\n}",
            "// TODO: Implement me!\n\treturn 0;\n}",
            "int sum = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n    KOKKOS_LAMBDA(const int i, int &local_sum) {\n      for (int j = 0; j < N; ++j) {\n        if (A(i, j) == 1) {\n          local_sum++;\n        }\n      }\n    },\n    sum);\n  return sum;\n}",
            "int numEdges = 0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n\t\tKOKKOS_LAMBDA(const int i, int &localNumEdges) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A(i, j) == 1) {\n\t\t\t\t\tlocalNumEdges++;\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\tnumEdges);\n\treturn numEdges;\n}",
            "int numEdges = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N), KOKKOS_LAMBDA(const int i, int &local_numEdges) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A(i, j) == 1) {\n\t\t\t\tlocal_numEdges++;\n\t\t\t}\n\t\t}\n\t}, numEdges);\n\treturn numEdges;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of edges in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (i!= j && A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i*N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (i!= j && A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (i!= j && A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i*N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (i!= j && A[i * N + j]) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        count += 1;\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of edges in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j]) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = i + 1; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j]) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j]) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of edges in the graph defined by the adjacency matrix A.\n   Store the result in numEdges.\n   A is an NxN adjacency matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint sum = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tsum += A[tid * N + i];\n\t\t}\n\t\tatomicAdd(numEdges, sum);\n\t}\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint i = bid * blockDim.x + tid;\n\tint j = i + 1;\n\tint k = i + 2;\n\tint l = i + 3;\n\n\tif (i < N && j < N && k < N && l < N) {\n\t\tif (A[i * N + j] + A[i * N + k] + A[i * N + l] + A[j * N + k] + A[j * N + l] + A[k * N + l] > 0) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tatomicAdd(numEdges, 1);\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = tid; i < N; i += stride) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        atomicAdd(numEdges, 1);\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = tid; i < N; i += stride) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        atomicAdd(numEdges, 1);\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint count = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = tid; i < N; i += stride) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        atomicAdd(numEdges, 1);\n      }\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    int count = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        count++;\n      }\n    }\n    atomicAdd(numEdges, count);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\tint sum = 0;\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tsum += A[i * N + j];\n\t\t}\n\t}\n\tatomicAdd(numEdges, sum);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    int count = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        count++;\n      }\n    }\n    atomicAdd(numEdges, count);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numEdges, 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tatomicAdd(numEdges, 1);\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int sum = 0;\n  for (int i = tid; i < N; i += stride) {\n    for (int j = 0; j < N; j++) {\n      sum += A[i * N + j];\n    }\n  }\n  atomicAdd(numEdges, sum);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint sum = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tsum += A[tid * N + i];\n\t\t}\n\t\tatomicAdd(numEdges, sum);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int count = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        count++;\n      }\n    }\n    numEdges[tid] = count;\n  }\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int sum = 0;\n    MPI_Reduce(&count, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_edge_count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_edge_count;\n      }\n    }\n  }\n\n  int global_edge_count = 0;\n  MPI_Reduce(&local_edge_count, &global_edge_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return global_edge_count;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  int count = 0;\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n\n  int sum = 0;\n  MPI_Reduce(&count, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\n\tint totalCount = 0;\n\tMPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn totalCount;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint totalCount = 0;\n\tMPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn totalCount;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int count = 0;\n  for (int i = 0; i < N; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n  int result = 0;\n  MPI_Reduce(&count, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n\n  int totalCount = 0;\n  MPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return totalCount;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n\n  int totalCount = 0;\n  MPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return totalCount;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (i!= j && A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement\n  return 0;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (i!= j && A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint localCount = count;\n\tint globalCount = 0;\n\tMPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn globalCount;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\tint count = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tif (i!= j && A[i * N + j] == 1) {\n\t\t\t\t++count;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n\n    int totalCount = 0;\n    MPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return totalCount;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n\n    int totalCount = 0;\n    MPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return totalCount;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of edges in the graph defined by the adjacency matrix A.\n   Store the result in numEdges.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N && i!= j && A[i * N + j] == 1) {\n\t\tatomicAdd(numEdges, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numEdges, 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tatomicAdd(numEdges, 1);\n\t\t\t}\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint idx = i * N + j;\n\tif (i < N && j < N && A[idx] == 1) {\n\t\tatomicAdd(numEdges, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint sum = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tsum += A[i * N + j];\n\t\t}\n\t\tatomicAdd(numEdges, sum);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint count = 0;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tatomicAdd(numEdges, count);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N && i!= j) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint count = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = blockIdx.z * blockDim.z + threadIdx.z;\n\tif (i < N && j < N && k < N) {\n\t\tif (A[i * N * N + j * N + k] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint index = i * N + j;\n\tif (i < N && j < N && A[index] == 1) {\n\t\tatomicAdd(numEdges, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint stride = blockDim.x * gridDim.x;\n\tint count = 0;\n\tfor (int k = i; k < N; k += stride) {\n\t\tif (A[k * N + j] == 1) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tatomicAdd(numEdges, count);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\tint count = 0;\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\tatomicAdd(numEdges, count);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int k = i; k < N; k += stride) {\n\t\tif (A[k * N + j] == 1) {\n\t\t\tatomicAdd(numEdges, 1);\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint count = 0;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[tid * N + i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint count = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tnumEdges[i] = count;\n\t}\n}",
            "int tid = threadIdx.x;\n\tint i = tid;\n\tint j = tid;\n\tint count = 0;\n\twhile (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tcount++;\n\t\t}\n\t\ti += blockDim.x;\n\t\tj += blockDim.x;\n\t}\n\tatomicAdd(numEdges, count);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\tint count = 0;\n\tfor (int i = tid; i < N; i += stride) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\tatomicAdd(numEdges, count);\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "11_graph_edge_count",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of edges in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint localCount = 0;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\t\tlocalCount++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tint recvCount;\n\tMPI_Reduce(&localCount, &recvCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\treturn recvCount;\n\t}\n\telse {\n\t\treturn 0;\n\t}\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int edgeCount = 0;\n  if (rank == 0) {\n    for (size_t i = 0; i < N; i++) {\n      for (size_t j = 0; j < N; j++) {\n        if (A[i * N + j] == 1) {\n          edgeCount++;\n        }\n      }\n    }\n  }\n\n  return edgeCount;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  int local_count = 0;\n\n  #pragma omp parallel for reduction(+:local_count)\n  for (int i = 0; i < N; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        local_count++;\n      }\n    }\n  }\n\n  int global_count = 0;\n  MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return global_count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint localCount = 0;\n\tint start = rank * (N / size);\n\tint end = (rank + 1) * (N / size);\n\tif (rank == size - 1) {\n\t\tend = N;\n\t}\n\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tlocalCount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tMPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn count;\n}",
            "int num_threads = omp_get_max_threads();\n  int num_procs = omp_get_num_procs();\n  int rank = omp_get_thread_num();\n  int num_edges = 0;\n\n  #pragma omp parallel for num_threads(num_threads) reduction(+:num_edges)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        num_edges++;\n      }\n    }\n  }\n\n  return num_edges;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint start = rank * N / size;\n\tint end = (rank + 1) * N / size;\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint result;\n\tMPI_Reduce(&count, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn result;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint localCount = 0;\n\tint start = rank * N / size;\n\tint end = (rank + 1) * N / size;\n\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tlocalCount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tMPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn count;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = 0;\n    if (rank == 0) {\n        for (size_t i = 0; i < N; i++) {\n            for (size_t j = 0; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    count++;\n                }\n            }\n        }\n    } else {\n        for (size_t i = 0; i < N; i++) {\n            for (size_t j = 0; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    int result = 0;\n    MPI_Reduce(&count, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint num_edges = 0;\n\tint num_rows = N / size;\n\tint num_rows_extra = N % size;\n\tint start_row = rank * num_rows;\n\tint end_row = start_row + num_rows;\n\tif (rank == size - 1) {\n\t\tend_row += num_rows_extra;\n\t}\n\n\t#pragma omp parallel for reduction(+:num_edges)\n\tfor (int i = start_row; i < end_row; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tnum_edges++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint total_edges = 0;\n\tMPI_Reduce(&num_edges, &total_edges, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn total_edges;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_edges = 0;\n    int num_edges_per_rank = 0;\n\n    // TODO: Your code here\n\n    // TODO: Use MPI to sum up the number of edges across all ranks\n\n    return num_edges;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_edge_count = 0;\n    int global_edge_count = 0;\n\n    // TODO: Implement this function\n\n    MPI_Reduce(&local_edge_count, &global_edge_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return global_edge_count;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  if (rank == 0) {\n    for (size_t i = 0; i < N; i++) {\n      for (size_t j = 0; j < N; j++) {\n        if (A[i * N + j] == 1) {\n          count++;\n        }\n      }\n    }\n  }\n\n  int local_count = 0;\n  if (rank!= 0) {\n    for (size_t i = rank; i < N; i += size) {\n      for (size_t j = 0; j < N; j++) {\n        if (A[i * N + j] == 1) {\n          local_count++;\n        }\n      }\n    }\n  }\n\n  int global_count = 0;\n  MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return global_count;\n}",
            "int result = 0;\n  #pragma omp parallel for reduction(+:result)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (i!= j && A[i * N + j]) {\n        ++result;\n      }\n    }\n  }\n  return result;\n}",
            "int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_edges = 0;\n    if (rank == 0) {\n        // Count edges in the first half of the matrix\n        for (size_t i = 0; i < N / 2; i++) {\n            for (size_t j = 0; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    num_edges++;\n                }\n            }\n        }\n    } else {\n        // Count edges in the second half of the matrix\n        for (size_t i = N / 2; i < N; i++) {\n            for (size_t j = 0; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    num_edges++;\n                }\n            }\n        }\n    }\n\n    // Reduce the number of edges on rank 0\n    int num_edges_total = 0;\n    MPI_Reduce(&num_edges, &num_edges_total, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return num_edges_total;\n}",
            "int rank = 0;\n\tint size = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint edgeCount = 0;\n\tint start = rank * N / size;\n\tint end = (rank + 1) * N / size;\n\tif (rank == size - 1) {\n\t\tend = N;\n\t}\n\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tedgeCount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint result = 0;\n\tMPI_Reduce(&edgeCount, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn result;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint numEdges = 0;\n\tint numRows = N / size;\n\tint numExtraRows = N % size;\n\tint startRow = rank * numRows + std::min(rank, numExtraRows);\n\tint endRow = startRow + numRows + (rank < numExtraRows? 1 : 0);\n\n\tfor (int i = startRow; i < endRow; ++i) {\n\t\tfor (int j = 0; j < N; ++j) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tnumEdges++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint totalNumEdges = 0;\n\tMPI_Reduce(&numEdges, &totalNumEdges, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn totalNumEdges;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint local_count = 0;\n\tint start = rank * N / size;\n\tint end = (rank + 1) * N / size;\n\n\t#pragma omp parallel for reduction(+:local_count)\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tlocal_count++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint global_count;\n\tMPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\treturn global_count;\n\t} else {\n\t\treturn 0;\n\t}\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n  int start = rank * N / size;\n  int end = (rank + 1) * N / size;\n  for (int i = start; i < end; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n\n  int total = 0;\n  MPI_Reduce(&count, &total, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return total;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint local_count = 0;\n\n\t#pragma omp parallel for reduction(+:local_count)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tlocal_count++;\n\t\t\t}\n\t\t}\n\t}\n\n\tint global_count = 0;\n\tMPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tcount = global_count;\n\t}\n\n\treturn count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint local_count = 0;\n\tint local_size = N / size;\n\tint local_start = rank * local_size;\n\tint local_end = local_start + local_size;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (i < j && A[i * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = local_start; i < local_end; i++) {\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (i < j && A[i * N + j] == 1) {\n\t\t\t\t\tlocal_count++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tint sum = 0;\n\tMPI_Reduce(&local_count, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn sum;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix.\n\t Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(Kokkos::View<const int**> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: Implement me!\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Implement me!\n\treturn 0;\n}",
            "// TODO: Your code here\n\n  return 0;\n}",
            "// TODO: Your code here\n\n\treturn 0;\n}",
            "// TODO: Your code here\n\n\treturn 0;\n}",
            "// TODO: Implement me!\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Implement this function\n\n\treturn 0;\n}",
            "// TODO: Fill in the body of this function.\n\t// Hint: You may want to use Kokkos::parallel_for.\n\n\treturn 0;\n}",
            "// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcount++;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::vector<int> stack;\n\t\t\tstack.push_back(i);\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tvisited[v] = true;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int count = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      #pragma omp critical\n      count++;\n      dfs(A, visited, i);\n    }\n  }\n\n  return count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t#pragma omp critical\n\t\t\tcount++;\n\t\t\tdfs(A, i, visited);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\t#pragma omp critical\n\t\t\tcount++;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int count = 0;\n\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    if (visited[i] == 0) {\n      #pragma omp critical\n      count++;\n      dfs(i, A, visited);\n    }\n  }\n\n  return count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\t#pragma omp critical\n\t\t\tcount++;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (!visited[i]) {\n\t\t\t#pragma omp critical\n\t\t\tcount++;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\tfor (int i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\t#pragma omp critical\n\t\t\tcount++;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i*N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\t++count;\n\t\t\t}\n\t\t\tstd::vector<bool> stack(N, false);\n\t\t\tstack[i] = true;\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tsize_t j = stack.size() - 1;\n\t\t\t\tstack.pop_back();\n\t\t\t\tif (!visited[j]) {\n\t\t\t\t\tvisited[j] = true;\n\t\t\t\t\tfor (size_t k = 0; k < N; ++k) {\n\t\t\t\t\t\tif (A[j * N + k] &&!visited[k]) {\n\t\t\t\t\t\t\tstack.push_back(k);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::vector<int> stack;\n\t\t\tstack.push_back(i);\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tvisited[v] = true;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  std::vector<bool> visited(N, false);\n  for (size_t i = 0; i < N; ++i) {\n    if (!visited[i]) {\n      ++count;\n      std::vector<int> stack;\n      stack.push_back(i);\n      while (!stack.empty()) {\n        int node = stack.back();\n        stack.pop_back();\n        visited[node] = true;\n        for (size_t j = 0; j < N; ++j) {\n          if (A[node * N + j] == 1 &&!visited[j]) {\n            stack.push_back(j);\n          }\n        }\n      }\n    }\n  }\n  return count;\n}",
            "int count = 0;\n\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n  }\n\n  return count;\n}",
            "int count = 0;\n  std::vector<int> visited(N, 0);\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    if (visited[i] == 0) {\n      #pragma omp critical\n      count++;\n      dfs(A, visited, i);\n    }\n  }\n  return count;\n}",
            "// TODO: Implement this function\n  int count = 0;\n  std::vector<int> visited(N, 0);\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      count++;\n      dfs(A, i, visited);\n    }\n  }\n  return count;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcount++;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint node = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tvisited[node] = 1;\n\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\tif (A[node * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n  std::vector<bool> visited(N, false);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    if (!visited[i]) {\n      #pragma omp critical\n      ++count;\n      std::vector<bool> stack(N, false);\n      stack[i] = true;\n      while (!stack.empty()) {\n        size_t j = stack.size() - 1;\n        stack.pop_back();\n        if (!visited[j]) {\n          visited[j] = true;\n          for (size_t k = 0; k < N; ++k) {\n            if (A[j * N + k] &&!visited[k]) {\n              stack.push_back(k);\n            }\n          }\n        }\n      }\n    }\n  }\n  return count;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i, N);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, N);\n\t\t\t++count;\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<bool> visited(N, false);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\tdfs(A, i, visited);\n\t\t\t++count;\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i, N);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      dfs(A, visited, i, count);\n    }\n  }\n  return count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      dfs(A, visited, i, count);\n    }\n  }\n  return count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i, N);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, i, visited);\n\t\t}\n\t}\n\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tdfs(A, visited, i, count);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tdfs(A, i, visited);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<bool> visited(N, false);\n\tint count = 0;\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tdfs(A, visited, i);\n\t\t}\n\t}\n\treturn count;\n}",
            "std::vector<int> visited(N, 0);\n  int count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      dfs(A, i, visited, count);\n    }\n  }\n  return count;\n}",
            "std::vector<int> visited(N, 0);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited[i] == 0) {\n            ++count;\n            dfs(A, visited, i);\n        }\n    }\n\n    return count;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n\t Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int count = 0;\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        count++;\n      }\n    }\n    atomicAdd(numComponents, count);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "// TODO: implement\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int count = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        count++;\n      }\n    }\n    atomicAdd(numComponents, count);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int num = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        num++;\n      }\n    }\n    atomicAdd(numComponents, num);\n  }\n}",
            "// Get the global thread index\n\tconst int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n\t// Check if the thread is within the bounds of the array\n\tif (tid < N) {\n\n\t\t// Initialize the visited array\n\t\tbool visited[N];\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tvisited[i] = false;\n\t\t}\n\n\t\t// Initialize the number of components\n\t\tint num = 0;\n\n\t\t// Iterate through the rows of the matrix\n\t\tfor (int i = 0; i < N; i++) {\n\n\t\t\t// Check if the row has already been visited\n\t\t\tif (!visited[i]) {\n\n\t\t\t\t// Mark the row as visited\n\t\t\t\tvisited[i] = true;\n\n\t\t\t\t// Iterate through the columns of the matrix\n\t\t\t\tfor (int j = 0; j < N; j++) {\n\n\t\t\t\t\t// Check if the current element is connected to the row\n\t\t\t\t\tif (A[i * N + j] &&!visited[j]) {\n\n\t\t\t\t\t\t// Mark the column as visited\n\t\t\t\t\t\tvisited[j] = true;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Increment the number of components\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\n\t\t// Write the number of components to the global memory\n\t\tnumComponents[tid] = num;\n\t}\n}",
            "// Get the global thread index\n\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n\t// Each thread is responsible for counting the number of connected components\n\t// in the subgraph of A defined by the vertices [tid, N)\n\tint count = 0;\n\tfor (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n\t\t// Check if the current vertex is connected to any other vertices\n\t\tbool connected = false;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tconnected = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (connected) {\n\t\t\tcount++;\n\t\t}\n\t}\n\n\t// Add the count to the global count\n\tatomicAdd(numComponents, count);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int num = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        num++;\n      }\n    }\n    atomicAdd(numComponents, num);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint numThreads = blockDim.x * gridDim.x;\n\tint component = 0;\n\tint numComponents = 0;\n\tfor (int i = tid; i < N; i += numThreads) {\n\t\tif (A[i * N + i] == 1) {\n\t\t\tcomponent++;\n\t\t\tnumComponents++;\n\t\t}\n\t}\n\tatomicAdd(numComponents, numComponents);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int num = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        num++;\n      }\n    }\n    atomicAdd(numComponents, num);\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = blockIdx.z * blockDim.z + threadIdx.z;\n\n\tif (i >= N || j >= N || k >= N) return;\n\n\tif (A[i * N + j] == 1 && A[j * N + k] == 1 && A[i * N + k] == 0) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int numConnected = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        numConnected++;\n      }\n    }\n    if (numConnected == 0) {\n      atomicAdd(numComponents, 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint numThreads = blockDim.x * gridDim.x;\n\tint numComponentsLocal = 0;\n\tfor (int i = tid; i < N; i += numThreads) {\n\t\tif (A[i * N + i] == 1) {\n\t\t\tnumComponentsLocal++;\n\t\t}\n\t}\n\tatomicAdd(numComponents, numComponentsLocal);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int num = 0;\n    for (int i = 0; i < N; i++) {\n      if (A[tid * N + i] == 1) {\n        num++;\n      }\n    }\n    atomicAdd(numComponents, num);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint numThreads = blockDim.x * gridDim.x;\n\tint numComponentsLocal = 0;\n\n\tfor (int i = tid; i < N; i += numThreads) {\n\t\tif (A[i * N + i] == 1) {\n\t\t\tnumComponentsLocal++;\n\t\t}\n\t}\n\n\tatomicAdd(numComponents, numComponentsLocal);\n}",
            "// Get the global thread index\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check if the thread is within the bounds of the matrix\n  if (i < N) {\n    // Initialize the visited array to false\n    bool visited[N];\n    for (int j = 0; j < N; j++) {\n      visited[j] = false;\n    }\n\n    // Initialize the number of components to 0\n    int num = 0;\n\n    // Traverse the graph starting at vertex i\n    dfs(A, N, i, visited, &num);\n\n    // Add the number of components to the global count\n    atomicAdd(numComponents, num);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (A[i * N + j] == 1) {\n      atomicAdd(numComponents, 1);\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO\n\treturn 0;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (!visited[i]) {\n\t\t\tcount++;\n\t\t\tstd::vector<int> stack = {i};\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tvisited[v] = true;\n\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tstd::vector<int> visited(N, 0);\n\tstd::vector<int> queue;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tqueue.push_back(i);\n\t\t\twhile (queue.size() > 0) {\n\t\t\t\tint v = queue.back();\n\t\t\t\tqueue.pop_back();\n\t\t\t\tif (visited[v] == 0) {\n\t\t\t\t\tcount++;\n\t\t\t\t\tvisited[v] = 1;\n\t\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\t\tqueue.push_back(j);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tint totalCount = 0;\n\tMPI_Reduce(&count, &totalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\treturn totalCount;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO\n}",
            "int count = 0;\n  std::vector<bool> visited(N, false);\n  for (size_t i = 0; i < N; ++i) {\n    if (!visited[i]) {\n      ++count;\n      std::queue<int> q;\n      q.push(i);\n      while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        visited[node] = true;\n        for (size_t j = 0; j < N; ++j) {\n          if (A[node * N + j] &&!visited[j]) {\n            q.push(j);\n          }\n        }\n      }\n    }\n  }\n  return count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Implement this function\n\treturn 0;\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::queue<size_t> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tsize_t v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tvisited[v] = true;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "// TODO: implement this function\n\treturn 0;\n}",
            "// TODO: implement this function\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n\t Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N && A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i >= N || j >= N) {\n    return;\n  }\n\n  if (A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint id = i * N + j;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[id] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = i * N + j;\n  if (i < N && j < N && A[index] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tatomicAdd(numComponents, 1);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N)\n\t\treturn;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) return;\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (A[i * N + j] == 1) {\n    atomicAdd(numComponents, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int row = blockIdx.y * blockDim.y + threadIdx.y;\n\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif (row >= N || col >= N) {\n\t\treturn;\n\t}\n\n\tif (A[row * N + col] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint index = i * N + j;\n\tif (i < N && j < N && A[index] == 1) {\n\t\tint count = 0;\n\t\tfor (int k = 0; k < N; k++) {\n\t\t\tif (A[i * N + k] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numComponents, count);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1) {\n\t\tatomicAdd(numComponents, 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i >= N || j >= N) return;\n\tif (A[i * N + j] == 1) {\n\t\tint count = 0;\n\t\tfor (int k = 0; k < N; k++) {\n\t\t\tif (A[i * N + k] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(numComponents, count);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (A[i * N + j] == 1) {\n      atomicAdd(numComponents, 1);\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (A[i * N + j] == 1) {\n      atomicAdd(numComponents, 1);\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "14_graph_count_components",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of connected components in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n\t Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\n\t// TODO: implement\n\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\tfor (int i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\tvisited[i] = 1;\n\t\t\tcount++;\n\t\t\tfor (int j = 0; j < N; ++j) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (!visited[i]) {\n\t\t\tcount++;\n\t\t\tvisited[i] = true;\n\t\t\tstd::vector<int> stack = {i};\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tvisited[j] = true;\n\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint* visited = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tvisited[i] = 0;\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tdfs(A, i, visited, N);\n\t\t}\n\t}\n\n\tdelete[] visited;\n\n\tint* counts = new int[size];\n\tcounts[rank] = count;\n\tMPI_Gather(MPI_IN_PLACE, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tint total = 0;\n\tfor (int i = 0; i < size; i++) {\n\t\ttotal += counts[i];\n\t}\n\n\tdelete[] counts;\n\n\treturn total;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::vector<bool> stack(N, false);\n\t\t\tstack[i] = true;\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tsize_t j = stack.size() - 1;\n\t\t\t\tstack.pop_back();\n\t\t\t\tif (visited[j]) continue;\n\t\t\t\tvisited[j] = true;\n\t\t\t\tfor (size_t k = 0; k < N; ++k) {\n\t\t\t\t\tif (A[j * N + k] == 1) {\n\t\t\t\t\t\tstack.push_back(k);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tstd::vector<int> visited(N, 0);\n\tstd::vector<int> toVisit;\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\ttoVisit.push_back(i);\n\t\t\tvisited[i] = 1;\n\t\t}\n\t}\n\n\twhile (toVisit.size() > 0) {\n\t\tint current = toVisit.back();\n\t\ttoVisit.pop_back();\n\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (A[current * N + i] == 1 && visited[i] == 0) {\n\t\t\t\ttoVisit.push_back(i);\n\t\t\t\tvisited[i] = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tint count_local = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 1) {\n\t\t\tcount_local++;\n\t\t}\n\t}\n\n\tint count_global;\n\tMPI_Reduce(&count_local, &count_global, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\treturn count_global;\n\t}\n\telse {\n\t\treturn 0;\n\t}\n}",
            "// TODO\n\treturn 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tstd::vector<int> visited(N, 0);\n\tstd::vector<int> queue;\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tqueue.push_back(i);\n\t\t\twhile (!queue.empty()) {\n\t\t\t\tint v = queue.back();\n\t\t\t\tqueue.pop_back();\n\t\t\t\tvisited[v] = 1;\n\t\t\t\tcount++;\n\t\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tqueue.push_back(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tint total = 0;\n\tMPI_Reduce(&count, &total, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn total;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = 0;\n\n  // TODO: implement this function\n\n  return count;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\tint count = 0;\n\tint* visited = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tvisited[i] = 0;\n\t}\n\tfor (int i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcount++;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (int j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tdelete[] visited;\n\tint result = 0;\n\tMPI_Reduce(&count, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\treturn result;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint count = 0;\n\tint* counts = new int[size];\n\tint* displs = new int[size];\n\tint* recvcounts = new int[size];\n\n\tstd::vector<int> local_A(A.begin() + rank * N, A.begin() + (rank + 1) * N);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (local_A[i] == 1 && local_A[j] == 1) {\n\t\t\t\tlocal_A[i] = local_A[j] = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (local_A[i] == 1) {\n\t\t\tcount++;\n\t\t}\n\t}\n\n\tMPI_Gather(&count, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\tdispls[i] = i * N;\n\t\t\trecvcounts[i] = N;\n\t\t}\n\n\t\tint* recvbuf = new int[size * N];\n\t\tMPI_Gatherv(counts, N, MPI_INT, recvbuf, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\tfor (int i = 0; i < size * N; i++) {\n\t\t\tif (recvbuf[i] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\n\t\tdelete[] recvbuf;\n\t}\n\n\tdelete[] counts;\n\tdelete[] displs;\n\tdelete[] recvcounts;\n\n\treturn count;\n}",
            "int num_components = 0;\n\tstd::vector<int> visited(N, 0);\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\t#pragma omp critical\n\t\t\tnum_components++;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn num_components;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement\n  return 0;\n}",
            "int count = 0;\n  std::vector<int> visited(N, 0);\n  for (int i = 0; i < N; ++i) {\n    if (visited[i] == 0) {\n      count++;\n      dfs(A, visited, i);\n    }\n  }\n  return count;\n}",
            "int numComponents = 0;\n\tstd::vector<int> visited(N, 0);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++numComponents;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint curr = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[curr * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn numComponents;\n}",
            "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (visited[i] == 0) {\n\t\t\tcount++;\n\t\t\tvisited[i] = 1;\n\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\tif (A[i * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tvisited[v] = true;\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint *A_local = new int[N*N];\n\tint *A_local_copy = new int[N*N];\n\tint *A_global = new int[N*N];\n\tint *A_global_copy = new int[N*N];\n\tint *A_global_copy_2 = new int[N*N];\n\tint *A_global_copy_3 = new int[N*N];\n\tint *A_global_copy_4 = new int[N*N];\n\tint *A_global_copy_5 = new int[N*N];\n\tint *A_global_copy_6 = new int[N*N];\n\tint *A_global_copy_7 = new int[N*N];\n\tint *A_global_copy_8 = new int[N*N];\n\tint *A_global_copy_9 = new int[N*N];\n\tint *A_global_copy_10 = new int[N*N];\n\tint *A_global_copy_11 = new int[N*N];\n\tint *A_global_copy_12 = new int[N*N];\n\tint *A_global_copy_13 = new int[N*N];\n\tint *A_global_copy_14 = new int[N*N];\n\tint *A_global_copy_15 = new int[N*N];\n\tint *A_global_copy_16 = new int[N*N];\n\tint *A_global_copy_17 = new int[N*N];\n\tint *A_global_copy_18 = new int[N*N];\n\tint *A_global_copy_19 = new int[N*N];\n\tint *A_global_copy_20 = new int[N*N];\n\tint *A_global_copy_21 = new int[N*N];\n\tint *A_global_copy_22 = new int[N*N];\n\tint *A_global_copy_23 = new int[N*N];\n\tint *A_global_copy_24 = new int[N*N];\n\tint *A_global_copy_25 = new int[N*N];\n\tint *A_global_copy_26 = new int[N*N];\n\tint *A_global_copy_27 = new int[N*N];\n\tint *A_global_copy_28 = new int[N*N];\n\tint *A_global_copy_29 = new int[N*N];\n\tint *A_global_copy_30 = new int[N*N];\n\tint *A_global_copy_31 = new int[N*N];\n\tint *A_global_copy_32 = new int[N*N];\n\tint *A_global_copy_33 = new int[N*N];\n\tint *A_global_copy_34 = new int[N*N];\n\tint *A_global_copy_35 = new int[N*N];\n\tint *A_global_copy_36 = new int[N*N];\n\tint *A_global_copy_37 = new int[N*N];\n\tint *A_global_copy_38 = new int[N*N];\n\tint *A_global_copy_39 = new int[N*N];\n\tint *A_global_copy_40 = new int[N*N];\n\tint *A_global_copy_41 = new int[N*N];\n\tint *A_global_copy_42 = new int[N*N];\n\tint *A_global_copy_43 = new int[N*N];\n\tint *A_global_copy_44 = new int[N*N];\n\tint *A_global_copy_45 = new int[N*N];\n\tint *A_global_copy_46 = new int[N*N];\n\tint *A_global_copy",
            "int numComponents = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++numComponents;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint u = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tvisited[u] = true;\n\t\t\t\tfor (size_t v = 0; v < N; ++v) {\n\t\t\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\t\t\tq.push(v);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn numComponents;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Implement me\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Fill in the body of this function.\n\n\treturn 0;\n}",
            "// TODO: implement me\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Fill in the body of this function\n\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Fill in the body of this function\n  // Hint: Use Kokkos::parallel_for to iterate over the rows of A\n  // Hint: Use Kokkos::single to iterate over the columns of A\n  // Hint: Use Kokkos::atomic_fetch_min to update the minimum distance\n  // Hint: Use Kokkos::atomic_compare_exchange to update the predecessor\n  // Hint: Use Kokkos::deep_copy to copy the contents of a Kokkos view to the host\n\n  return 0;\n}",
            "// TODO: Implement me\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Implement me\n\treturn 0;\n}",
            "// TODO: Your code here\n  //\n  // You can use the following variables:\n  //   N: the number of vertices in the graph\n  //   A: the adjacency matrix\n  //   source: the source vertex\n  //   dest: the destination vertex\n  //\n  // You can use the following Kokkos functions:\n  //   Kokkos::parallel_for\n  //   Kokkos::parallel_reduce\n  //   Kokkos::View\n  //   Kokkos::deep_copy\n  //   Kokkos::subview\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep_copy\n  //   Kokkos::deep",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill this in\n  return 0;\n}",
            "// TODO: Your code here\n    int *dist = new int[N];\n    int *visited = new int[N];\n    for (int i = 0; i < N; i++) {\n        dist[i] = INT_MAX;\n        visited[i] = 0;\n    }\n    dist[source] = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 1 && dist[i]!= INT_MAX && dist[i] + 1 < dist[j]) {\n                dist[j] = dist[i] + 1;\n            }\n        }\n    }\n    return dist[dest];\n}",
            "// TODO: implement\n  return 0;\n}",
            "int *dist = new int[N];\n\tint *visited = new int[N];\n\tint *parent = new int[N];\n\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tvisited[i] = 0;\n\t\tparent[i] = -1;\n\t}\n\n\tdist[source] = 0;\n\tparent[source] = -1;\n\n\tbool found = false;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1 && dist[i]!= INT_MAX && dist[i] + 1 < dist[j]) {\n\t\t\t\tdist[j] = dist[i] + 1;\n\t\t\t\tparent[j] = i;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tif (dist[i]!= INT_MAX) {\n\t\t\tif (i == dest) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (found) {\n\t\tint pathLength = 0;\n\t\tint current = dest;\n\t\twhile (current!= -1) {\n\t\t\tpathLength++;\n\t\t\tcurrent = parent[current];\n\t\t}\n\t\treturn pathLength;\n\t}\n\telse {\n\t\treturn -1;\n\t}\n}",
            "int* dist = new int[N];\n\tbool* visited = new bool[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tvisited[i] = false;\n\t}\n\tdist[source] = 0;\n\tstd::vector<int> queue;\n\tqueue.push_back(source);\n\twhile (!queue.empty()) {\n\t\tint u = queue.back();\n\t\tqueue.pop_back();\n\t\tif (u == dest) {\n\t\t\tbreak;\n\t\t}\n\t\tif (visited[u]) {\n\t\t\tcontinue;\n\t\t}\n\t\tvisited[u] = true;\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\tif (dist[u] + 1 < dist[v]) {\n\t\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\t\tqueue.push_back(v);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tint ret = dist[dest];\n\tdelete[] dist;\n\tdelete[] visited;\n\treturn ret;\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::vector<int> parent(N, -1);\n\tstd::vector<bool> visited(N, false);\n\tstd::vector<int> Q;\n\tQ.push_back(source);\n\twhile (Q.size() > 0) {\n\t\tint u = Q.back();\n\t\tQ.pop_back();\n\t\tvisited[u] = true;\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] == 1 &&!visited[v]) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tparent[v] = u;\n\t\t\t\tQ.push_back(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == -1) {\n\t\treturn -1;\n\t}\n\telse {\n\t\treturn dist[dest];\n\t}\n}",
            "int *dist = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t}\n\tdist[source] = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1) {\n\t\t\t\tif (dist[i]!= INT_MAX) {\n\t\t\t\t\tdist[j] = std::min(dist[j], dist[i] + 1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tint result = dist[dest];\n\tdelete[] dist;\n\treturn result;\n}",
            "// TODO: Fill this in\n  int *dist = new int[N];\n  int *prev = new int[N];\n  int *visited = new int[N];\n  int *q = new int[N];\n\n  for (int i = 0; i < N; i++) {\n    dist[i] = INT_MAX;\n    prev[i] = -1;\n    visited[i] = 0;\n  }\n\n  dist[source] = 0;\n  int front = 0;\n  int rear = 0;\n  q[rear++] = source;\n\n  while (front!= rear) {\n    int u = q[front++];\n    for (int v = 0; v < N; v++) {\n      if (A[u*N + v] == 1 && visited[v] == 0) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        q[rear++] = v;\n      }\n    }\n    visited[u] = 1;\n  }\n\n  int shortestPathLength = dist[dest];\n\n  delete[] dist;\n  delete[] prev;\n  delete[] visited;\n  delete[] q;\n\n  return shortestPathLength;\n}",
            "// TODO: Fill this in\n  int length = 0;\n  std::vector<int> dist(N, -1);\n  std::vector<int> prev(N, -1);\n  std::vector<bool> visited(N, false);\n  std::queue<int> q;\n  q.push(source);\n  dist[source] = 0;\n  visited[source] = true;\n  while (!q.empty()) {\n    int u = q.front();\n    q.pop();\n    for (int v = 0; v < N; v++) {\n      if (A[u * N + v] == 1 &&!visited[v]) {\n        q.push(v);\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        visited[v] = true;\n      }\n    }\n  }\n  if (dist[dest] == -1) {\n    return -1;\n  }\n  int cur = dest;\n  while (cur!= source) {\n    length++;\n    cur = prev[cur];\n  }\n  return length;\n}",
            "int *dist = new int[N];\n\tint *prev = new int[N];\n\tint *visited = new int[N];\n\tint *queue = new int[N];\n\tint qsize = 0;\n\tint qfront = 0;\n\tint qrear = 0;\n\tint qtemp;\n\tint qtemp2;\n\tint qtemp3;\n\tint qtemp4;\n\tint qtemp5;\n\tint qtemp6;\n\tint qtemp7;\n\tint qtemp8;\n\tint qtemp9;\n\tint qtemp10;\n\tint qtemp11;\n\tint qtemp12;\n\tint qtemp13;\n\tint qtemp14;\n\tint qtemp15;\n\tint qtemp16;\n\tint qtemp17;\n\tint qtemp18;\n\tint qtemp19;\n\tint qtemp20;\n\tint qtemp21;\n\tint qtemp22;\n\tint qtemp23;\n\tint qtemp24;\n\tint qtemp25;\n\tint qtemp26;\n\tint qtemp27;\n\tint qtemp28;\n\tint qtemp29;\n\tint qtemp30;\n\tint qtemp31;\n\tint qtemp32;\n\tint qtemp33;\n\tint qtemp34;\n\tint qtemp35;\n\tint qtemp36;\n\tint qtemp37;\n\tint qtemp38;\n\tint qtemp39;\n\tint qtemp40;\n\tint qtemp41;\n\tint qtemp42;\n\tint qtemp43;\n\tint qtemp44;\n\tint qtemp45;\n\tint qtemp46;\n\tint qtemp47;\n\tint qtemp48;\n\tint qtemp49;\n\tint qtemp50;\n\tint qtemp51;\n\tint qtemp52;\n\tint qtemp53;\n\tint qtemp54;\n\tint qtemp55;\n\tint qtemp56;\n\tint qtemp57;\n\tint qtemp58;\n\tint qtemp59;\n\tint qtemp60;\n\tint qtemp61;\n\tint qtemp62;\n\tint qtemp63;\n\tint qtemp64;\n\tint qtemp65;\n\tint qtemp66;\n\tint qtemp67;\n\tint qtemp68;\n\tint qtemp69;\n\tint qtemp70;\n\tint qtemp71;\n\tint qtemp72;\n\tint qtemp73;\n\tint qtemp74;\n\tint qtemp75;\n\tint qtemp76;\n\tint qtemp77;\n\tint qtemp78;\n\tint qtemp79;\n\tint qtemp80;\n\tint qtemp81;\n\tint qtemp82;\n\tint qtemp83;\n\tint qtemp84;\n\tint qtemp85;\n\tint qtemp86;\n\tint qtemp87;\n\tint qtemp88;\n\tint qtemp89;\n\tint qtemp90;\n\tint qtemp91;\n\tint qtemp92;\n\tint qtemp93;\n\tint qtemp94;\n\tint qtemp95;\n\tint qtemp96;\n\tint qtemp97;\n\tint qtemp98;\n\tint qtemp99;\n\tint qtemp100;\n\tint qtemp101;\n\tint qtemp102;\n\tint qtemp103;\n\tint qtemp104;\n\tint qtemp105;\n\tint qtemp106;\n\tint qtemp107;\n\tint qtemp108;\n\tint qtemp109;\n\tint qtemp110;\n\tint qtemp111;\n\tint qtemp112;\n\tint qtemp113;\n\tint qtemp114;\n\tint qtemp115;\n\tint qtemp116;\n\tint qtemp117;\n\tint qtemp11",
            "int *dist = new int[N];\n\tint *visited = new int[N];\n\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tvisited[i] = 0;\n\t}\n\n\tdist[source] = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tif (dist[i]!= INT_MAX && dist[i] + 1 < dist[j]) {\n\t\t\t\t\tdist[j] = dist[i] + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tint result = dist[dest];\n\n\tdelete[] dist;\n\tdelete[] visited;\n\n\treturn result;\n}",
            "int *dist = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = -1;\n\t}\n\tdist[source] = 0;\n\tbool *visited = new bool[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tvisited[i] = false;\n\t}\n\tvisited[source] = true;\n\tint *parent = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tparent[i] = -1;\n\t}\n\tparent[source] = -1;\n\tint *Q = new int[N];\n\tint front = 0;\n\tint rear = 0;\n\tQ[rear] = source;\n\trear++;\n\twhile (front!= rear) {\n\t\tint u = Q[front];\n\t\tfront++;\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] == 1 &&!visited[v]) {\n\t\t\t\tvisited[v] = true;\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tparent[v] = u;\n\t\t\t\tQ[rear] = v;\n\t\t\t\trear++;\n\t\t\t}\n\t\t}\n\t}\n\tint length = dist[dest];\n\tdelete[] dist;\n\tdelete[] visited;\n\tdelete[] parent;\n\tdelete[] Q;\n\treturn length;\n}",
            "int* dist = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t}\n\tdist[source] = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i*N + j] == 1 && dist[i]!= INT_MAX && dist[i] + 1 < dist[j]) {\n\t\t\t\tdist[j] = dist[i] + 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tint result = dist[dest];\n\tdelete[] dist;\n\treturn result;\n}",
            "int *dist = new int[N];\n  int *prev = new int[N];\n  int *visited = new int[N];\n  int *queue = new int[N];\n  int *queue_end = queue;\n  int *queue_begin = queue;\n  int *queue_tmp = queue;\n  int *queue_tmp_end = queue;\n\n  for (int i = 0; i < N; i++) {\n    dist[i] = INT_MAX;\n    prev[i] = -1;\n    visited[i] = 0;\n  }\n\n  dist[source] = 0;\n  *queue_end = source;\n  queue_end++;\n\n  while (queue_begin!= queue_end) {\n    int u = *queue_begin;\n    queue_begin++;\n\n    for (int v = 0; v < N; v++) {\n      if (A[u * N + v] == 1 && visited[v] == 0) {\n        if (dist[v] > dist[u] + 1) {\n          dist[v] = dist[u] + 1;\n          prev[v] = u;\n\n          if (v == dest) {\n            return dist[dest];\n          }\n\n          *queue_tmp_end = v;\n          queue_tmp_end++;\n        }\n      }\n    }\n\n    queue_tmp = queue_begin;\n    queue_begin = queue_end;\n    queue_end = queue_tmp_end;\n\n    for (int i = 0; i < N; i++) {\n      visited[*queue_begin] = 1;\n      queue_begin++;\n    }\n  }\n\n  return -1;\n}",
            "int *dist = new int[N];\n  int *visited = new int[N];\n  int *parent = new int[N];\n  for (int i = 0; i < N; i++) {\n    dist[i] = INT_MAX;\n    visited[i] = 0;\n    parent[i] = -1;\n  }\n  dist[source] = 0;\n  bool found = false;\n  int min = INT_MAX;\n  int min_index = -1;\n  int count = 0;\n  while (!found) {\n    int min = INT_MAX;\n    int min_index = -1;\n    for (int i = 0; i < N; i++) {\n      if (visited[i] == 0 && dist[i] < min) {\n        min = dist[i];\n        min_index = i;\n      }\n    }\n    if (min_index == -1) {\n      break;\n    }\n    visited[min_index] = 1;\n    for (int i = 0; i < N; i++) {\n      if (A[min_index * N + i] == 1 && visited[i] == 0) {\n        if (dist[i] > dist[min_index] + 1) {\n          dist[i] = dist[min_index] + 1;\n          parent[i] = min_index;\n        }\n      }\n    }\n    count++;\n    if (min_index == dest) {\n      found = true;\n      break;\n    }\n  }\n  int path_length = dist[dest];\n  delete[] dist;\n  delete[] visited;\n  delete[] parent;\n  return path_length;\n}",
            "// TODO: implement me\n  int* dist = new int[N];\n  int* prev = new int[N];\n  int* visited = new int[N];\n  int* queue = new int[N];\n  int* next = new int[N];\n  int* next_visited = new int[N];\n  int* next_queue = new int[N];\n\n  for (int i = 0; i < N; i++) {\n    dist[i] = INT_MAX;\n    prev[i] = -1;\n    visited[i] = 0;\n    queue[i] = 0;\n    next[i] = 0;\n    next_visited[i] = 0;\n    next_queue[i] = 0;\n  }\n\n  dist[source] = 0;\n  queue[0] = source;\n  int queue_size = 1;\n  int next_queue_size = 0;\n  int visited_size = 0;\n  int next_visited_size = 0;\n\n  while (queue_size > 0) {\n    for (int i = 0; i < queue_size; i++) {\n      int u = queue[i];\n      for (int v = 0; v < N; v++) {\n        if (A[u * N + v] == 1 && dist[v] > dist[u] + 1) {\n          dist[v] = dist[u] + 1;\n          prev[v] = u;\n          if (visited[v] == 0) {\n            visited[v] = 1;\n            visited_size++;\n            next[next_visited_size] = v;\n            next_visited_size++;\n          }\n        }\n      }\n    }\n    queue_size = 0;\n    for (int i = 0; i < visited_size; i++) {\n      queue[queue_size] = next[i];\n      queue_size++;\n    }\n    visited_size = 0;\n    next_queue_size = 0;\n    for (int i = 0; i < next_visited_size; i++) {\n      next_queue[next_queue_size] = next[i];\n      next_queue_size++;\n    }\n    next_visited_size = 0;\n  }\n\n  int result = dist[dest];\n\n  delete[] dist;\n  delete[] prev;\n  delete[] visited;\n  delete[] queue;\n  delete[] next;\n  delete[] next_visited;\n  delete[] next_queue;\n\n  return result;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int length = 0;\n\tint* dist = new int[N];\n\tbool* visited = new bool[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = -1;\n\t\tvisited[i] = false;\n\t}\n\tdist[source] = 0;\n\tvisited[source] = true;\n\tint* queue = new int[N];\n\tint head = 0;\n\tint tail = 0;\n\tqueue[tail++] = source;\n\twhile (head!= tail) {\n\t\tint u = queue[head++];\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] == 1 &&!visited[v]) {\n\t\t\t\tvisited[v] = true;\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tqueue[tail++] = v;\n\t\t\t}\n\t\t}\n\t}\n\tlength = dist[dest];\n\tdelete[] dist;\n\tdelete[] visited;\n\tdelete[] queue;\n\treturn length;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n\tint *dist = new int[N];\n\tint *prev = new int[N];\n\tint *visited = new int[N];\n\tint *q = new int[N];\n\tint *q_end = q;\n\tint *q_start = q;\n\tint *q_last = q + N;\n\tint *q_first = q;\n\tint *q_next = q;\n\tint *q_last_visited = q;\n\tint *q_first_visited = q;\n\tint *q_next_visited = q;\n\tint *q_last_unvisited = q;\n\tint *q_first_unvisited = q;\n\tint *q_next_unvisited = q;\n\tint *q_last_unvisited_prev = q;\n\tint *q_first_unvisited_prev = q;\n\tint *q_next_unvisited_prev = q;\n\tint *q_last_unvisited_next = q;\n\tint *q_first_unvisited_next = q;\n\tint *q_next_unvisited_next = q;\n\tint *q_last_unvisited_next_prev = q;\n\tint *q_first_unvisited_next_prev = q;\n\tint *q_next_unvisited_next_prev = q;\n\tint *q_last_unvisited_next_next = q;\n\tint *q_first_unvisited_next_next = q;\n\tint *q_next_unvisited_next_next = q;\n\tint *q_last_unvisited_next_next_prev = q;\n\tint *q_first_unvisited_next_next_prev = q;\n\tint *q_next_unvisited_next_next_prev = q;\n\tint *q_last_unvisited_next_next_next = q;\n\tint *q_first_unvisited_next_next_next = q;\n\tint *q_next_unvisited_next_next_next = q;\n\tint *q_last_unvisited_next_next_next_prev = q;\n\tint *q_first_unvisited_next_next_next_prev = q;\n\tint *q_next_unvisited_next_next_next_prev = q;\n\tint *q_last_unvisited_next_next_next_next = q;\n\tint *q_first_unvisited_next_next_next_next = q;\n\tint *q_next_unvisited_next_next_next_next = q;\n\tint *q_last_unvisited_next_next_next_next_prev = q;\n\tint *q_first_unvisited_next_next_next_next_prev = q;\n\tint *q_next_unvisited_next_next_next_next_prev = q;\n\tint *q_last_unvisited_next_next_next_next_next = q;\n\tint *q_first_unvisited_next_next_next_next_next = q;\n\tint *q_next_unvisited_next_next_next_next_next = q;\n\tint *q_last_unvisited_next_next_next_next_next_prev = q;\n\tint *q_first_unvisited_next_next_next_next_next_prev = q;\n\tint *q_next_unvisited_next_next_next_next_next_prev = q;\n\tint *q_last_unvisited_next_next_next_next_next_next = q;\n\tint *q_first_unvisited_next_next_next_next_next_next = q;\n\tint *q_next_unvisited_next_next_next_next_next_next = q;\n\tint *q_last_unvisited_next_next_next_next_next_next_prev = q;\n\tint *q_first_unvisited_next_next_next_next_next_next_prev = q;\n\tint *q_next_unvisited_next_next_next_next_next_next_prev"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "serial",
        "prompt": "/* Return the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<int> dist(N, INT_MAX);\n\tdist[source] = 0;\n\tstd::vector<bool> visited(N, false);\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tif (u == dest)\n\t\t\tbreak;\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t\tvisited[v] = true;\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "// TODO: Your code here\n\tstd::vector<int> dist(N, INT_MAX);\n\tdist[source] = 0;\n\tstd::vector<bool> visited(N, false);\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tvisited[u] = true;\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\tdist[v] = std::min(dist[v], dist[u] + 1);\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "// TODO: write your code here\n\tstd::vector<int> dist(N, INT_MAX);\n\tdist[source] = 0;\n\tstd::vector<bool> visited(N, false);\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tvisited[u] = true;\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\tdist[v] = std::min(dist[v], dist[u] + 1);\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\n\tstd::queue<int> q;\n\tq.push(source);\n\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == -1) return -1;\n\tint u = dest;\n\tstd::vector<int> path;\n\twhile (u!= -1) {\n\t\tpath.push_back(u);\n\t\tu = prev[u];\n\t}\n\treturn path.size() - 1;\n}",
            "std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n    return dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, INT_MAX);\n\tdist[source] = 0;\n\tstd::vector<bool> visited(N, false);\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tif (u == dest)\n\t\t\tbreak;\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\tvisited[v] = true;\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}",
            "std::vector<int> dist(N, -1);\n\tdist[source] = 0;\n\tstd::queue<int> q;\n\tq.push(source);\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == -1) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\treturn dist[dest];\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "hip",
        "prompt": "/* Compute the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Store the result in pathLength.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1 && i!= j) {\n\t\t\tif (i == source) {\n\t\t\t\tpathLength[j] = 1;\n\t\t\t} else if (j == source) {\n\t\t\t\tpathLength[i] = 1;\n\t\t\t} else {\n\t\t\t\tpathLength[j] = pathLength[i] + 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: implement\n\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint nthreads = gridDim.x * blockDim.x;\n\tint i;\n\n\tfor (i = tid; i < N; i += nthreads) {\n\t\tint j;\n\t\tint min = INT_MAX;\n\t\tfor (j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] < min) {\n\t\t\t\tmin = A[i * N + j];\n\t\t\t}\n\t\t}\n\t\tA[i * N + i] = 0;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = blockIdx.z * blockDim.z + threadIdx.z;\n\tif (i >= N || j >= N || k >= N) return;\n\tif (i == source && j == dest) {\n\t\tpathLength[k] = 1;\n\t} else if (i == source && j!= dest) {\n\t\tpathLength[k] = 0;\n\t} else if (i!= source && j == dest) {\n\t\tpathLength[k] = 0;\n\t} else {\n\t\tpathLength[k] = A[i * N + j];\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i == j) {\n\t\tif (i == source) {\n\t\t\tpathLength[i] = 0;\n\t\t} else {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t} else {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tpathLength[j] = min(pathLength[i] + 1, pathLength[j]);\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tif (i == source && j == dest) {\n\t\t\t\tpathLength[0] = 0;\n\t\t\t} else if (i == dest && j == source) {\n\t\t\t\tpathLength[0] = 0;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = tid; i < N; i += stride) {\n        int pathLength_i = 0;\n        int j = i;\n        while (j!= source) {\n            j = A[j * N + i];\n            pathLength_i++;\n        }\n        if (i == dest) {\n            *pathLength = pathLength_i;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int distance = A[source * N + tid];\n        if (distance == 0) {\n            pathLength[tid] = -1;\n        } else {\n            pathLength[tid] = distance;\n        }\n    }\n}",
            "// TODO: Replace this with your code\n\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint i = tid / N;\n\tint j = tid % N;\n\tif (i == j) {\n\t\tif (i == source) {\n\t\t\tpathLength[i] = 0;\n\t\t} else {\n\t\t\tpathLength[i] = -1;\n\t\t}\n\t} else if (A[i * N + j] == 1) {\n\t\tpathLength[j] = pathLength[i] + 1;\n\t}\n}",
            "// TODO: Fill this in\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tif (i == source && j == dest) {\n\t\t\t\tpathLength[0] = 0;\n\t\t\t} else if (i == source) {\n\t\t\t\tpathLength[j] = 1;\n\t\t\t} else if (j == dest) {\n\t\t\t\tpathLength[i] = 1;\n\t\t\t} else {\n\t\t\t\tpathLength[i] = pathLength[i] + pathLength[j];\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int i = tid; i < N; i += stride) {\n\t\tif (A[source * N + i] == 1) {\n\t\t\tint length = 1;\n\t\t\tint j = i;\n\t\t\twhile (A[j * N + dest] == 0) {\n\t\t\t\tj = A[j * N + source];\n\t\t\t\tlength++;\n\t\t\t}\n\t\t\tif (length < *pathLength) {\n\t\t\t\t*pathLength = length;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Fill this in\n\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int dist = 0;\n    int v = source;\n    while (v!= dest) {\n      v = A[v * N + tid];\n      dist++;\n    }\n    pathLength[tid] = dist;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint dist = A[source * N + tid];\n\t\tif (dist!= 0) {\n\t\t\tint next = A[tid * N + dest];\n\t\t\tif (next!= 0) {\n\t\t\t\t*pathLength = dist + next;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int path = 0;\n    int current = tid;\n    while (current!= source) {\n      current = A[current * N + tid];\n      path++;\n    }\n    pathLength[tid] = path;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (i == source && A[i * N + j] == 1) {\n\t\t*pathLength = 1;\n\t} else if (i == dest && A[i * N + j] == 1) {\n\t\t*pathLength = 1;\n\t} else if (i == j) {\n\t\t*pathLength = 0;\n\t} else if (A[i * N + j] == 1) {\n\t\t*pathLength = 1 + min(pathLength[i], pathLength[j]);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int distance = A[source * N + tid];\n    if (distance!= 0) {\n      int next = tid;\n      while (next!= dest) {\n        next = A[next * N + dest];\n        distance++;\n      }\n      if (distance < *pathLength) {\n        *pathLength = distance;\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid == source) {\n\t\tpathLength[tid] = 0;\n\t} else {\n\t\tpathLength[tid] = INT_MAX;\n\t}\n\t__syncthreads();\n\n\tfor (int i = 0; i < N; i++) {\n\t\tint u = tid;\n\t\tint v = -1;\n\t\tint minDistance = INT_MAX;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[u * N + j] && pathLength[j] < minDistance) {\n\t\t\t\tv = j;\n\t\t\t\tminDistance = pathLength[j];\n\t\t\t}\n\t\t}\n\t\tif (v >= 0) {\n\t\t\tpathLength[v] = minDistance + 1;\n\t\t}\n\t\t__syncthreads();\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int i = 0; i < N; i++) {\n        if (tid == source) {\n            pathLength[dest] = 1;\n        }\n        __syncthreads();\n\n        if (tid!= source && tid!= dest) {\n            if (A[tid * N + i] == 1) {\n                pathLength[tid] = min(pathLength[tid], pathLength[i] + 1);\n            }\n        }\n        __syncthreads();\n    }\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\tint length = 0;\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tif (rank == 0) {\n\t\tstd::vector<int> dist(N, INT_MAX);\n\t\tdist[source] = 0;\n\t\tstd::vector<bool> visited(N, false);\n\t\tstd::vector<int> parent(N, -1);\n\t\tstd::queue<int> q;\n\t\tq.push(source);\n\t\twhile (!q.empty()) {\n\t\t\tint u = q.front();\n\t\t\tq.pop();\n\t\t\tvisited[u] = true;\n\t\t\tfor (int v = 0; v < N; v++) {\n\t\t\t\tif (A[u * N + v] &&!visited[v]) {\n\t\t\t\t\tq.push(v);\n\t\t\t\t\tparent[v] = u;\n\t\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tlength = dist[dest];\n\t}\n\tint length_global;\n\tMPI_Reduce(&length, &length_global, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\treturn length_global;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n\tint length = 0;\n\tint *dist = new int[N];\n\tint *prev = new int[N];\n\tint *visited = new int[N];\n\tint *q = new int[N];\n\tint front = 0;\n\tint rear = 0;\n\tint count = 0;\n\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tprev[i] = -1;\n\t\tvisited[i] = 0;\n\t}\n\n\tdist[source] = 0;\n\tq[rear++] = source;\n\tvisited[source] = 1;\n\n\twhile (front!= rear) {\n\t\tint u = q[front++];\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v] == 1 && visited[v] == 0) {\n\t\t\t\tq[rear++] = v;\n\t\t\t\tprev[v] = u;\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tvisited[v] = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (dist[dest] == INT_MAX) {\n\t\tlength = -1;\n\t}\n\telse {\n\t\tlength = dist[dest];\n\t}\n\n\tdelete[] dist;\n\tdelete[] prev;\n\tdelete[] visited;\n\tdelete[] q;\n\n\treturn length;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: Your code here\n  int length = 0;\n  int* dist = new int[N];\n  int* prev = new int[N];\n  for (int i = 0; i < N; i++) {\n    dist[i] = INT_MAX;\n    prev[i] = -1;\n  }\n  dist[source] = 0;\n  bool* visited = new bool[N];\n  for (int i = 0; i < N; i++) {\n    visited[i] = false;\n  }\n  for (int i = 0; i < N; i++) {\n    int min_dist = INT_MAX;\n    int min_index = -1;\n    for (int j = 0; j < N; j++) {\n      if (!visited[j] && dist[j] < min_dist) {\n        min_dist = dist[j];\n        min_index = j;\n      }\n    }\n    if (min_index == -1) {\n      break;\n    }\n    visited[min_index] = true;\n    for (int j = 0; j < N; j++) {\n      if (A[min_index * N + j] &&!visited[j] && dist[min_index] + 1 < dist[j]) {\n        dist[j] = dist[min_index] + 1;\n        prev[j] = min_index;\n      }\n    }\n  }\n  int cur = dest;\n  while (cur!= -1) {\n    length++;\n    cur = prev[cur];\n  }\n  delete[] dist;\n  delete[] prev;\n  delete[] visited;\n  return length;\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Store the result in pathLength.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N && A[i * N + j] == 1) {\n\t\tif (i == source) {\n\t\t\tpathLength[j] = 1;\n\t\t} else if (j == source) {\n\t\t\tpathLength[i] = 1;\n\t\t} else if (i == dest) {\n\t\t\tpathLength[j] = 1;\n\t\t} else if (j == dest) {\n\t\t\tpathLength[i] = 1;\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (A[i * N + j] == 1 && i == source && j == dest) {\n      *pathLength = 0;\n    } else if (A[i * N + j] == 1 && i == source) {\n      *pathLength = 1;\n    } else if (A[i * N + j] == 1 && j == dest) {\n      *pathLength = 1;\n    } else if (A[i * N + j] == 1) {\n      *pathLength = 2;\n    }\n  }\n}",
            "// TODO: implement this function\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N)\n\t\treturn;\n\n\tif (A[i * N + j] == 1 && i == source && j == dest) {\n\t\t*pathLength = 0;\n\t}\n\n\tif (A[i * N + j] == 1 && i == source && j!= dest) {\n\t\t*pathLength = 1;\n\t}\n\n\tif (A[i * N + j] == 1 && i!= source && j == dest) {\n\t\t*pathLength = 1;\n\t}\n\n\tif (A[i * N + j] == 1 && i!= source && j!= dest) {\n\t\t*pathLength = 2;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1 && i!= j) {\n\t\t\tif (i == source) {\n\t\t\t\tpathLength[j] = 1;\n\t\t\t} else if (j == source) {\n\t\t\t\tpathLength[i] = 1;\n\t\t\t} else if (pathLength[i]!= 0 && pathLength[j]!= 0) {\n\t\t\t\tpathLength[i] = pathLength[i] + pathLength[j];\n\t\t\t}\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) return;\n\n\tif (A[i * N + j] == 1 && i == source && j == dest) {\n\t\t*pathLength = 0;\n\t}\n\n\tif (A[i * N + j] == 1 && i == source && j!= dest) {\n\t\t*pathLength = 1;\n\t}\n\n\tif (A[i * N + j] == 1 && i!= source && j == dest) {\n\t\t*pathLength = 1;\n\t}\n\n\tif (A[i * N + j] == 1 && i!= source && j!= dest) {\n\t\t*pathLength = 2;\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x;\n\tint j = blockIdx.y;\n\tint k = threadIdx.x;\n\tint l = threadIdx.y;\n\tint m = threadIdx.z;\n\tint n = blockDim.x;\n\tint o = blockDim.y;\n\tint p = blockDim.z;\n\tint q = gridDim.x;\n\tint r = gridDim.y;\n\tint s = gridDim.z;\n\tint t = blockDim.x * blockDim.y * blockDim.z;\n\n\tif (i == j && k == l && m == 0) {\n\t\tif (i == source) {\n\t\t\tpathLength[i] = 0;\n\t\t} else {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k == source) {\n\t\t\tpathLength[i] = 0;\n\t\t}\n\t\tif (i == k && j == k && k == dest) {\n\t\t\tpathLength[i] = 0;\n\t\t}\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {\n\t\tif (i == k && j == k && k!= source && k!= dest) {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\tfor (int k = 0; k < N; k++) {",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1) {\n\t\t\tif (i == source && j == dest) {\n\t\t\t\tpathLength[0] = 0;\n\t\t\t}\n\t\t\telse if (i == source) {\n\t\t\t\tpathLength[j] = 1;\n\t\t\t}\n\t\t\telse if (j == dest) {\n\t\t\t\tpathLength[i] = 1;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tpathLength[i] = pathLength[i] + pathLength[j];\n\t\t\t}\n\t\t}\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid == source) {\n    pathLength[tid] = 0;\n  } else {\n    pathLength[tid] = INT_MAX;\n  }\n\n  __syncthreads();\n\n  for (int i = 0; i < N; i++) {\n    if (A[tid * N + i] == 1) {\n      atomicMin(&pathLength[i], pathLength[tid] + 1);\n    }\n  }\n\n  __syncthreads();\n\n  if (tid == dest) {\n    *pathLength = pathLength[tid];\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint nthreads = blockDim.x * gridDim.x;\n\tint i;\n\tfor (i = tid; i < N; i += nthreads) {\n\t\tif (A[source * N + i] == 1) {\n\t\t\t*pathLength = 1 + shortestPathLength(A, N, i, dest, pathLength);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1 && i == source) {\n\t\t\t*pathLength = 1;\n\t\t}\n\t\telse if (A[i * N + j] == 1 && i!= source && j!= source) {\n\t\t\t*pathLength = *pathLength + 1;\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (A[i * N + j] == 1) {\n            if (i == source && j == dest) {\n                pathLength[0] = 0;\n            } else if (i == source) {\n                pathLength[j] = 1;\n            } else if (j == dest) {\n                pathLength[i] = 1;\n            } else {\n                pathLength[i] = 1 + min(pathLength[i], pathLength[j]);\n            }\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (i == source && j == dest) {\n\t\tpathLength[0] = 0;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tpathLength[0] = min(pathLength[0], 1 + pathLength[j]);\n\t}\n}",
            "int i = blockIdx.x;\n\tint j = blockIdx.y;\n\tint k = threadIdx.x;\n\tint l = threadIdx.y;\n\tint index = i*N*N + j*N + k*N + l;\n\tif (i == j && k == l && A[index] == 0) {\n\t\tpathLength[index] = -1;\n\t} else if (i == j && k == l && A[index] == 1) {\n\t\tpathLength[index] = 1;\n\t} else if (i == j && k == l && A[index] == 2) {\n\t\tpathLength[index] = 2;\n\t} else if (i == j && k == l && A[index] == 3) {\n\t\tpathLength[index] = 3;\n\t} else if (i == j && k == l && A[index] == 4) {\n\t\tpathLength[index] = 4;\n\t} else if (i == j && k == l && A[index] == 5) {\n\t\tpathLength[index] = 5;\n\t} else if (i == j && k == l && A[index] == 6) {\n\t\tpathLength[index] = 6;\n\t} else if (i == j && k == l && A[index] == 7) {\n\t\tpathLength[index] = 7;\n\t} else if (i == j && k == l && A[index] == 8) {\n\t\tpathLength[index] = 8;\n\t} else if (i == j && k == l && A[index] == 9) {\n\t\tpathLength[index] = 9;\n\t} else if (i == j && k == l && A[index] == 10) {\n\t\tpathLength[index] = 10;\n\t} else if (i == j && k == l && A[index] == 11) {\n\t\tpathLength[index] = 11;\n\t} else if (i == j && k == l && A[index] == 12) {\n\t\tpathLength[index] = 12;\n\t} else if (i == j && k == l && A[index] == 13) {\n\t\tpathLength[index] = 13;\n\t} else if (i == j && k == l && A[index] == 14) {\n\t\tpathLength[index] = 14;\n\t} else if (i == j && k == l && A[index] == 15) {\n\t\tpathLength[index] = 15;\n\t} else if (i == j && k == l && A[index] == 16) {\n\t\tpathLength[index] = 16;\n\t} else if (i == j && k == l && A[index] == 17) {\n\t\tpathLength[index] = 17;\n\t} else if (i == j && k == l && A[index] == 18) {\n\t\tpathLength[index] = 18;\n\t} else if (i == j && k == l && A[index] == 19) {\n\t\tpathLength[index] = 19;\n\t} else if (i == j && k == l && A[index] == 20) {\n\t\tpathLength[index] = 20;\n\t} else if (i == j && k == l && A[index] == 21) {\n\t\tpathLength[index] = 21;\n\t} else if (i == j && k == l && A[index] == 22) {\n\t\tpathLength[index] = 22;\n\t} else if (i == j && k == l && A[index] == 23) {\n\t\tpathLength[index] = 23;\n\t} else if (i == j && k == l && A[index] == 24) {\n\t\tpathLength[index] = 24;\n\t} else if (i == j && k == l && A[index] == 25) {\n\t\tpathLength[index] = 25;\n\t} else if (i == j && k == l && A[index] ==",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (A[i * N + j] == 1) {\n            if (i == source && j == dest) {\n                pathLength[0] = 0;\n            } else if (i == dest && j == source) {\n                pathLength[0] = 0;\n            } else if (i == source || j == dest) {\n                pathLength[0] = -1;\n            } else {\n                pathLength[0] = -1;\n            }\n        }\n    }\n}",
            "// TODO: Fill this in\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i >= N || j >= N) {\n\t\treturn;\n\t}\n\n\tif (i == j) {\n\t\tA[i * N + j] = 0;\n\t}\n\n\tif (A[i * N + j] == 1) {\n\t\tA[i * N + j] = A[source * N + i] + A[i * N + j] + A[j * N + dest];\n\t}\n\n\tif (i == dest && j == source) {\n\t\t*pathLength = A[i * N + j];\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (i < N && j < N) {\n\t\tif (A[i * N + j] == 1 && i!= j) {\n\t\t\tpathLength[i] = 1 + pathLength[j];\n\t\t}\n\t}\n}"
        ]
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_shortest_path",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint* localA = new int[N*N];\n\tint* localA_copy = new int[N*N];\n\tint* localA_copy2 = new int[N*N];\n\tint* localA_copy3 = new int[N*N];\n\tint* localA_copy4 = new int[N*N];\n\tint* localA_copy5 = new int[N*N];\n\tint* localA_copy6 = new int[N*N];\n\tint* localA_copy7 = new int[N*N];\n\tint* localA_copy8 = new int[N*N];\n\tint* localA_copy9 = new int[N*N];\n\tint* localA_copy10 = new int[N*N];\n\tint* localA_copy11 = new int[N*N];\n\tint* localA_copy12 = new int[N*N];\n\tint* localA_copy13 = new int[N*N];\n\tint* localA_copy14 = new int[N*N];\n\tint* localA_copy15 = new int[N*N];\n\tint* localA_copy16 = new int[N*N];\n\tint* localA_copy17 = new int[N*N];\n\tint* localA_copy18 = new int[N*N];\n\tint* localA_copy19 = new int[N*N];\n\tint* localA_copy20 = new int[N*N];\n\tint* localA_copy21 = new int[N*N];\n\tint* localA_copy22 = new int[N*N];\n\tint* localA_copy23 = new int[N*N];\n\tint* localA_copy24 = new int[N*N];\n\tint* localA_copy25 = new int[N*N];\n\tint* localA_copy26 = new int[N*N];\n\tint* localA_copy27 = new int[N*N];\n\tint* localA_copy28 = new int[N*N];\n\tint* localA_copy29 = new int[N*N];\n\tint* localA_copy30 = new int[N*N];\n\tint* localA_copy31 = new int[N*N];\n\tint* localA_copy32 = new int[N*N];\n\tint* localA_copy33 = new int[N*N];\n\tint* localA_copy34 = new int[N*N];\n\tint* localA_copy35 = new int[N*N];\n\tint* localA_copy36 = new int[N*N];\n\tint* localA_copy37 = new int[N*N];\n\tint* localA_copy38 = new int[N*N];\n\tint* localA_copy39 = new int[N*N];\n\tint* localA_copy40 = new int[N*N];\n\tint* localA_copy41 = new int[N*N];\n\tint* localA_copy42 = new int[N*N];\n\tint* localA_copy43 = new int[N*N];\n\tint* localA_copy44 = new int[N*N];\n\tint* localA_copy45 = new int[N*N];\n\tint* localA_copy46 = new int[N*N];\n\tint* localA_copy47 = new int[N*N];\n\tint* localA_copy48 = new int[N*N];\n\tint* localA_copy49 = new int[N*N];\n\tint* localA_copy50 = new int[N*N];\n\tint* localA_copy51 = new int[N*N];\n\tint* localA_copy52 = new int[N*N];\n\tint* localA_copy53 = new int[N*N];\n\tint* localA_copy54 = new int[N",
            "// TODO\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// TODO: Your code here\n\treturn 0;\n}",
            "// TODO: implement\n\treturn 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_source = source / size;\n    int local_dest = dest / size;\n\n    int local_N = N / size;\n\n    std::vector<int> local_A(local_N * local_N);\n    for (int i = 0; i < local_N; i++) {\n        for (int j = 0; j < local_N; j++) {\n            local_A[i * local_N + j] = A[i * N + j];\n        }\n    }\n\n    int local_shortest_path_length = -1;\n\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n\n        int local_shortest_path_length_thread = -1;\n\n        #pragma omp for\n        for (int i = 0; i < local_N; i++) {\n            for (int j = 0; j < local_N; j++) {\n                if (local_A[i * local_N + j] == 1) {\n                    int local_shortest_path_length_thread_temp = -1;\n                    if (i == local_source && j == local_dest) {\n                        local_shortest_path_length_thread_temp = 0;\n                    } else if (i == local_source) {\n                        local_shortest_path_length_thread_temp = 1;\n                    } else if (j == local_dest) {\n                        local_shortest_path_length_thread_temp = 1;\n                    } else {\n                        local_shortest_path_length_thread_temp = -1;\n                    }\n\n                    if (local_shortest_path_length_thread_temp!= -1) {\n                        if (local_shortest_path_length_thread == -1) {\n                            local_shortest_path_length_thread = local_shortest_path_length_thread_temp;\n                        } else {\n                            local_shortest_path_length_thread = std::min(local_shortest_path_length_thread, local_shortest_path_length_thread_temp);\n                        }\n                    }\n                }\n            }\n        }\n\n        if (local_shortest_path_length_thread!= -1) {\n            #pragma omp critical\n            {\n                if (local_shortest_path_length == -1) {\n                    local_shortest_path_length = local_shortest_path_length_thread;\n                } else {\n                    local_shortest_path_length = std::min(local_shortest_path_length, local_shortest_path_length_thread);\n                }\n            }\n        }\n    }\n\n    int global_shortest_path_length = -1;\n    MPI_Reduce(&local_shortest_path_length, &global_shortest_path_length, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    return global_shortest_path_length;\n}",
            "// TODO: Your code here\n    int rank = 0, size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int *local_A = new int[N*N];\n    int *local_A_copy = new int[N*N];\n    int *local_A_copy_2 = new int[N*N];\n    int *local_A_copy_3 = new int[N*N];\n    int *local_A_copy_4 = new int[N*N];\n    int *local_A_copy_5 = new int[N*N];\n    int *local_A_copy_6 = new int[N*N];\n    int *local_A_copy_7 = new int[N*N];\n    int *local_A_copy_8 = new int[N*N];\n    int *local_A_copy_9 = new int[N*N];\n    int *local_A_copy_10 = new int[N*N];\n    int *local_A_copy_11 = new int[N*N];\n    int *local_A_copy_12 = new int[N*N];\n    int *local_A_copy_13 = new int[N*N];\n    int *local_A_copy_14 = new int[N*N];\n    int *local_A_copy_15 = new int[N*N];\n    int *local_A_copy_16 = new int[N*N];\n    int *local_A_copy_17 = new int[N*N];\n    int *local_A_copy_18 = new int[N*N];\n    int *local_A_copy_19 = new int[N*N];\n    int *local_A_copy_20 = new int[N*N];\n    int *local_A_copy_21 = new int[N*N];\n    int *local_A_copy_22 = new int[N*N];\n    int *local_A_copy_23 = new int[N*N];\n    int *local_A_copy_24 = new int[N*N];\n    int *local_A_copy_25 = new int[N*N];\n    int *local_A_copy_26 = new int[N*N];\n    int *local_A_copy_27 = new int[N*N];\n    int *local_A_copy_28 = new int[N*N];\n    int *local_A_copy_29 = new int[N*N];\n    int *local_A_copy_30 = new int[N*N];\n    int *local_A_copy_31 = new int[N*N];\n    int *local_A_copy_32 = new int[N*N];\n    int *local_A_copy_33 = new int[N*N];\n    int *local_A_copy_34 = new int[N*N];\n    int *local_A_copy_35 = new int[N*N];\n    int *local_A_copy_36 = new int[N*N];\n    int *local_A_copy_37 = new int[N*N];\n    int *local_A_copy_38 = new int[N*N];\n    int *local_A_copy_39 = new int[N*N];\n    int *local_A_copy_40 = new int[N*N];\n    int *local_A_copy_41 = new int[N*N];\n    int *local_A_copy_42 = new int[N*N];\n    int *local_A_copy_43 = new int[N*N];\n    int *local_A_copy_44 = new int[N*N];\n    int *local_A_copy_45 = new int[N*N];\n    int *local_A_copy_46 = new int[N*N];\n    int *local_A_copy_47 = new int[N*N];\n    int *local_A_copy_48 = new int[N",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  return 0;\n}",
            "int length = 0;\n\t// TODO: implement\n\treturn length;\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\tint length = 0;\n\tint *dist = new int[N];\n\tint *prev = new int[N];\n\tint *visited = new int[N];\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tprev[i] = -1;\n\t\tvisited[i] = 0;\n\t}\n\tdist[source] = 0;\n\tint count = 0;\n\twhile (count < N) {\n\t\tint min = INT_MAX;\n\t\tint u = -1;\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tif (visited[i] == 0 && dist[i] < min) {\n\t\t\t\tmin = dist[i];\n\t\t\t\tu = i;\n\t\t\t}\n\t\t}\n\t\tvisited[u] = 1;\n\t\tcount++;\n\t\tfor (int v = 0; v < N; v++) {\n\t\t\tif (A[u * N + v]!= 0 && dist[v] > dist[u] + A[u * N + v]) {\n\t\t\t\tdist[v] = dist[u] + A[u * N + v];\n\t\t\t\tprev[v] = u;\n\t\t\t}\n\t\t}\n\t}\n\tint curr = dest;\n\twhile (curr!= -1) {\n\t\tlength++;\n\t\tcurr = prev[curr];\n\t}\n\tdelete[] dist;\n\tdelete[] prev;\n\tdelete[] visited;\n\treturn length;\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint* dist = new int[N];\n\tint* dist_temp = new int[N];\n\tint* dist_temp_temp = new int[N];\n\tint* dist_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp",
            "// TODO: implement this\n  return 0;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint* A_local = new int[N*N];\n\tint* dist = new int[N];\n\tint* dist_local = new int[N];\n\tint* dist_local_temp = new int[N];\n\tint* dist_temp = new int[N];\n\n\tfor (int i = 0; i < N; i++) {\n\t\tdist[i] = INT_MAX;\n\t\tdist_local[i] = INT_MAX;\n\t\tdist_local_temp[i] = INT_MAX;\n\t\tdist_temp[i] = INT_MAX;\n\t}\n\n\tdist[source] = 0;\n\tdist_local[source] = 0;\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tA_local[i*N + j] = A[i*N + j];\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_local[j] = 1 + dist_local[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist[j] = 1 + dist[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_local_temp[j] = 1 + dist_local_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_temp[j] = 1 + dist_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tint* dist_local_temp_temp = new int[N];\n\tint* dist_temp_temp = new int[N];\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_local_temp_temp[j] = 1 + dist_local_temp_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_temp_temp[j] = 1 + dist_temp_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tint* dist_local_temp_temp_temp = new int[N];\n\tint* dist_temp_temp_temp = new int[N];\n\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A_local[i*N + j] == 1) {\n\t\t\t\tdist_local_temp_temp_temp[j] = 1 + dist_local_temp_temp_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < N; i",
            "// TODO: Your code here\n\treturn 0;\n}",
            "int num_threads = omp_get_max_threads();\n\tint num_ranks = 0;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\tint rank = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint num_rows_per_rank = N / num_ranks;\n\tint num_rows_remainder = N % num_ranks;\n\tint num_rows_rank = num_rows_per_rank;\n\tif (rank < num_rows_remainder) {\n\t\tnum_rows_rank++;\n\t}\n\tint num_rows_rank_start = rank * num_rows_per_rank;\n\tif (rank < num_rows_remainder) {\n\t\tnum_rows_rank_start += rank;\n\t}\n\tint num_rows_rank_end = num_rows_rank_start + num_rows_rank;\n\n\tint num_cols_per_rank = N / num_ranks;\n\tint num_cols_remainder = N % num_ranks;\n\tint num_cols_rank = num_cols_per_rank;\n\tif (rank < num_cols_remainder) {\n\t\tnum_cols_rank++;\n\t}\n\tint num_cols_rank_start = rank * num_cols_per_rank;\n\tif (rank < num_cols_remainder) {\n\t\tnum_cols_rank_start += rank;\n\t}\n\tint num_cols_rank_end = num_cols_rank_start + num_cols_rank;\n\n\tint num_rows_rank_local = num_rows_rank_end - num_rows_rank_start;\n\tint num_cols_rank_local = num_cols_rank_end - num_cols_rank_start;\n\n\tstd::vector<int> A_rank(num_rows_rank_local * num_cols_rank_local);\n\tfor (int i = 0; i < num_rows_rank_local; i++) {\n\t\tfor (int j = 0; j < num_cols_rank_local; j++) {\n\t\t\tA_rank[i * num_cols_rank_local + j] = A[num_rows_rank_start + i][num_cols_rank_start + j];\n\t\t}\n\t}\n\n\tint shortest_path_length = 0;\n\tif (rank == 0) {\n\t\tstd::vector<int> dist(N, -1);\n\t\tdist[source] = 0;\n\t\tstd::vector<bool> visited(N, false);\n\t\tstd::vector<int> queue;\n\t\tqueue.push_back(source);\n\t\tvisited[source] = true;\n\t\twhile (!queue.empty()) {\n\t\t\tint u = queue.front();\n\t\t\tqueue.erase(queue.begin());\n\t\t\tfor (int v = 0; v < N; v++) {\n\t\t\t\tif (A[u][v] == 1 &&!visited[v]) {\n\t\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\t\tqueue.push_back(v);\n\t\t\t\t\tvisited[v] = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tshortest_path_length = dist[dest];\n\t}\n\n\tint shortest_path_length_local = 0;\n\t#pragma omp parallel for num_threads(num_threads)\n\tfor (int i = 0; i < num_rows_rank_local; i++) {\n\t\tfor (int j = 0; j < num_cols_rank_local; j++) {\n\t\t\tif (A_rank[i * num_cols_rank_local + j] == 1) {\n\t\t\t\tshortest_path_length_local = std::max(shortest_path_length_local, i + j);\n\t\t\t}\n\t\t}\n\t}\n\n\tint shortest_path_length_global = 0;\n\tMPI_Reduce(&shortest_path_length_local, &shortest_path_length_global,",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n  return 0;\n}",
            "// TODO: Your code here\n\tint rank = 0;\n\tint size = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint *dist = new int[N];\n\tint *dist_temp = new int[N];\n\tint *dist_temp2 = new int[N];\n\tint *dist_temp3 = new int[N];\n\tint *dist_temp4 = new int[N];\n\tint *dist_temp5 = new int[N];\n\tint *dist_temp6 = new int[N];\n\tint *dist_temp7 = new int[N];\n\tint *dist_temp8 = new int[N];\n\tint *dist_temp9 = new int[N];\n\tint *dist_temp10 = new int[N];\n\tint *dist_temp11 = new int[N];\n\tint *dist_temp12 = new int[N];\n\tint *dist_temp13 = new int[N];\n\tint *dist_temp14 = new int[N];\n\tint *dist_temp15 = new int[N];\n\tint *dist_temp16 = new int[N];\n\tint *dist_temp17 = new int[N];\n\tint *dist_temp18 = new int[N];\n\tint *dist_temp19 = new int[N];\n\tint *dist_temp20 = new int[N];\n\tint *dist_temp21 = new int[N];\n\tint *dist_temp22 = new int[N];\n\tint *dist_temp23 = new int[N];\n\tint *dist_temp24 = new int[N];\n\tint *dist_temp25 = new int[N];\n\tint *dist_temp26 = new int[N];\n\tint *dist_temp27 = new int[N];\n\tint *dist_temp28 = new int[N];\n\tint *dist_temp29 = new int[N];\n\tint *dist_temp30 = new int[N];\n\tint *dist_temp31 = new int[N];\n\tint *dist_temp32 = new int[N];\n\tint *dist_temp33 = new int[N];\n\tint *dist_temp34 = new int[N];\n\tint *dist_temp35 = new int[N];\n\tint *dist_temp36 = new int[N];\n\tint *dist_temp37 = new int[N];\n\tint *dist_temp38 = new int[N];\n\tint *dist_temp39 = new int[N];\n\tint *dist_temp40 = new int[N];\n\tint *dist_temp41 = new int[N];\n\tint *dist_temp42 = new int[N];\n\tint *dist_temp43 = new int[N];\n\tint *dist_temp44 = new int[N];\n\tint *dist_temp45 = new int[N];\n\tint *dist_temp46 = new int[N];\n\tint *dist_temp47 = new int[N];\n\tint *dist_temp48 = new int[N];\n\tint *dist_temp49 = new int[N];\n\tint *dist_temp50 = new int[N];\n\tint *dist_temp51 = new int[N];\n\tint *dist_temp52 = new int[N];\n\tint *dist_temp53 = new int[N];\n\tint *dist_temp54 = new int[N];\n\tint *dist_temp55 = new int[N];\n\tint *dist_temp56 = new int[N];\n\tint *dist_temp57 = new int[N];\n\tint *dist_temp58 = new int[N];\n\tint *dist_temp59 = new int[N];\n\tint *dist_temp60 = new int[N];\n\tint *dist_temp61 = new int[N];\n\tint *dist_temp62 = new int[N];\n\tint *dist_temp63 = new int[N];\n\tint *dist_temp64 = new",
            "// TODO: implement\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use Kokkos to sum in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(Kokkos::View<const double*> const& x, Kokkos::View<const double*> const& y) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_reduce\n  // Hint: Use Kokkos::Min\n  // Hint: Use Kokkos::subview\n\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a Kokkos view for the sum of the minimum elements.\n  Kokkos::View<double> sum(\"sum\", 1);\n\n  // Create a Kokkos parallel_reduce to sum the minimum elements.\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += std::min(x(i), y(i));\n    },\n    sum);\n\n  // Copy the sum back to the host.\n  double sum_host = 0.0;\n  Kokkos::deep_copy(sum_host, sum);\n\n  return sum_host;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  double sum = 0;\n  return sum;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO\n  return 0.0;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0.0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); ++i) {\n    sum += std::min(x[i], y[i]);\n  }\n\n  return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "int n = x.size();\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "// TODO: Fill in your code here\n    double sum = 0;\n    int n = x.size();\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "serial",
        "prompt": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
            "double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "hip",
        "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use AMD HIP to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(sum, min(x[i], y[i]));\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        atomicAdd(sum, min(x[tid], y[tid]));\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(sum, min(x[idx], y[idx]));\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "// TODO: Implement this function\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(sum, min(x[idx], y[idx]));\n    }\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, fmin(x[i], y[i]));\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(sum, min(x[tid], y[tid]));\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int n_proc = n / n_per_proc;\n\n  std::vector<double> x_proc(n_per_proc);\n  std::vector<double> y_proc(n_per_proc);\n  std::vector<double> x_proc_min(n_per_proc);\n  std::vector<double> y_proc_min(n_per_proc);\n\n  MPI_Scatter(x.data(), n_per_proc, MPI_DOUBLE, x_proc.data(), n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), n_per_proc, MPI_DOUBLE, y_proc.data(), n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_proc; i++) {\n    x_proc_min[i] = std::min(x_proc[i], y_proc[i]);\n    y_proc_min[i] = std::min(x_proc[i], y_proc[i]);\n  }\n\n  std::vector<double> x_proc_min_sum(n_per_proc);\n  std::vector<double> y_proc_min_sum(n_per_proc);\n\n  MPI_Reduce(x_proc_min.data(), x_proc_min_sum.data(), n_per_proc, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(y_proc_min.data(), y_proc_min_sum.data(), n_per_proc, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  double sum = 0;\n  for (int i = 0; i < n_per_proc; i++) {\n    sum += x_proc_min_sum[i] + y_proc_min_sum[i];\n  }\n\n  return sum;\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    double sum = 0;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_proc + std::min(rank, n_rem);\n    int end = (rank + 1) * n_per_proc + std::min(rank + 1, n_rem);\n    for (int i = start; i < end; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    double sum = 0;\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n    if (rank == size - 1) {\n        end += n_rem;\n    }\n    for (int i = start; i < end; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return sum_all;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> x_local(n_per_proc + (rank < n_rem? 1 : 0));\n  std::vector<double> y_local(n_per_proc + (rank < n_rem? 1 : 0));\n\n  MPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, x_local.data(),\n              n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, y_local.data(),\n              n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double sum = 0;\n  for (int i = 0; i < n_per_proc + (rank < n_rem? 1 : 0); ++i) {\n    sum += std::min(x_local[i], y_local[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum_all;\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  double sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n\n  return sum;\n}",
            "// TODO: replace this with your code\n  return 0.0;\n}",
            "// TODO: implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  double sum = 0;\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[i * n_per_proc], n_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      MPI_Send(&y[i * n_per_proc], n_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  std::vector<double> x_proc(n_per_proc + n_rem);\n  std::vector<double> y_proc(n_per_proc + n_rem);\n\n  if (rank == 0) {\n    x_proc = x;\n    y_proc = y;\n  } else {\n    MPI_Recv(&x_proc[0], n_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&y_proc[0], n_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  for (int i = 0; i < n_per_proc + n_rem; i++) {\n    sum += std::min(x_proc[i], y_proc[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement this function\n\n    return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n    double sum = 0;\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n    if (rank == size - 1) {\n        end += n_remainder;\n    }\n    for (int i = start; i < end; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return sum_all;\n}",
            "// TODO: implement this function\n  return 0.0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    double sum = 0.0;\n    for (int i = 0; i < n_per_proc; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    if (rank < n_rem) {\n        sum += std::min(x[n_per_proc + rank], y[n_per_proc + rank]);\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum_all;\n}",
            "// TODO: replace this with your code\n  return 0.0;\n}",
            "// TODO: replace this line with your code\n  return 0.0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: your code here\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    double sum = 0;\n    if (x.size()!= y.size()) {\n        throw std::invalid_argument(\"Vectors must be of equal size\");\n    }\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            sum += std::min(x[i], y[i]);\n        }\n    }\n    MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    return sum;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use CUDA to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "// TODO: Implement this function\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int idx = threadIdx.x;\n    int stride = blockDim.x;\n    double min = 0;\n    while (idx < N) {\n        min = min(x[idx], y[idx]);\n        atomicAdd(sum, min);\n        idx += stride;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(sum, min(x[idx], y[idx]));\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}",
            "// TODO: Implement this function\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "// TODO: Implement this function\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(sum, min(x[idx], y[idx]));\n    }\n}",
            "// TODO: Implement this function\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        double min = fmin(x[index], y[index]);\n        atomicAdd(sum, min);\n    }\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    __shared__ double s[BLOCK_SIZE];\n    double min = 0;\n    if (i < N) {\n        min = min(x[i], y[i]);\n    }\n    s[tid] = min;\n    __syncthreads();\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s[tid] = min(s[tid], s[tid + s]);\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        atomicAdd(sum, s[0]);\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double min = min(x[index], y[index]);\n        atomicAdd(sum, min);\n    }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        atomicAdd(sum, min(x[index], y[index]));\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(sum, min(x[i], y[i]));\n    }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_sum_of_min_of_pairs",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI and OpenMP to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  double sum = 0;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  double localSum = 0;\n  for (int i = start; i < end; i++) {\n    localSum += std::min(x[i], y[i]);\n  }\n  double globalSum;\n  MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  return globalSum;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  std::vector<double> x_local(n_per_rank + (rank < n_extra? 1 : 0));\n  std::vector<double> y_local(n_per_rank + (rank < n_extra? 1 : 0));\n\n  MPI_Scatter(x.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE,\n              x_local.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE,\n              0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE,\n              y_local.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE,\n              0, MPI_COMM_WORLD);\n\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n_per_rank + (rank < n_extra? 1 : 0); ++i) {\n    sum += std::min(x_local[i], y_local[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum_all;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = omp_get_max_threads();\n    int num_elements = x.size();\n    int num_elements_per_thread = num_elements / num_threads;\n    int num_elements_remainder = num_elements % num_threads;\n\n    double sum = 0;\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        int thread_id = omp_get_thread_num();\n        int start_index = thread_id * num_elements_per_thread;\n        int end_index = start_index + num_elements_per_thread;\n        if (thread_id == num_threads - 1) {\n            end_index += num_elements_remainder;\n        }\n\n        double thread_sum = 0;\n        for (int i = start_index; i < end_index; i++) {\n            thread_sum += std::min(x[i], y[i]);\n        }\n\n        #pragma omp critical\n        {\n            sum += thread_sum;\n        }\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum_all;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n\n  std::vector<double> x_local(n_local);\n  std::vector<double> y_local(n_local);\n  std::vector<double> z_local(n_local);\n\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), n_local, MPI_DOUBLE, y_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < n_local; i++) {\n    z_local[i] = std::min(x_local[i], y_local[i]);\n  }\n\n  double sum = 0;\n  MPI_Reduce(z_local.data(), &sum, n_local, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n    int n_per_rank_p1 = n_per_rank + 1;\n\n    double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n_per_rank_p1; i++) {\n        int start = i * n_per_rank + std::min(i, n_rem);\n        int end = (i + 1) * n_per_rank + std::min(i + 1, n_rem);\n        double min = std::min(x[start], y[start]);\n        for (int j = start + 1; j < end; j++) {\n            min = std::min(min, std::min(x[j], y[j]));\n        }\n        sum += min;\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum_all;\n}",
            "// TODO: Your code here\n    double sum = 0;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    double local_sum = 0;\n    for (int i = start; i < end; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n    double global_sum = 0;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_sum;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: replace this line with your code\n  return 0.0;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  double sum = 0;\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[i * chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      MPI_Send(&y[i * chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n  if (rank!= 0) {\n    std::vector<double> x_rank(chunk + remainder);\n    std::vector<double> y_rank(chunk + remainder);\n    MPI_Recv(&x_rank[0], chunk + remainder, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&y_rank[0], chunk + remainder, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for (int i = 0; i < chunk + remainder; i++) {\n      sum += std::min(x_rank[i], y_rank[i]);\n    }\n  } else {\n    for (int i = 0; i < chunk + remainder; i++) {\n      sum += std::min(x[i], y[i]);\n    }\n  }\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  return sum_all;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int x_size = x.size();\n  int y_size = y.size();\n  int x_min_size = x_size / size;\n  int y_min_size = y_size / size;\n  int x_rem = x_size % size;\n  int y_rem = y_size % size;\n\n  std::vector<double> x_min(x_min_size);\n  std::vector<double> y_min(y_min_size);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[i * x_min_size], x_min_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      MPI_Send(&y[i * y_min_size], y_min_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < x_min_size; i++) {\n      x_min[i] = x[i];\n    }\n    for (int i = 0; i < y_min_size; i++) {\n      y_min[i] = y[i];\n    }\n  } else {\n    MPI_Recv(&x_min[0], x_min_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&y_min[0], y_min_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x_min[i * x_min_size], x_min_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&y_min[i * y_min_size], y_min_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < x_rem; i++) {\n      x_min[i + x_min_size * (size - 1)] = x[i + x_min_size * (size - 1)];\n    }\n    for (int i = 0; i < y_rem; i++) {\n      y_min[i + y_min_size * (size - 1)] = y[i + y_min_size * (size - 1)];\n    }\n  }\n\n  double sum = 0;\n  for (int i = 0; i < x_min_size; i++) {\n    sum += std::min(x_min[i], y_min[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum_all;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_extra;\n  }\n\n  double sum = 0;\n  for (int i = start; i < end; i++) {\n    sum += std::min(x[i], y[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum_all;\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  std::vector<double> x_local(n_local), y_local(n_local);\n  if (rank < n_remainder) {\n    x_local = std::vector<double>(x.begin() + rank * (n_per_rank + 1),\n                                  x.begin() + (rank + 1) * (n_per_rank + 1));\n    y_local = std::vector<double>(y.begin() + rank * (n_per_rank + 1),\n                                  y.begin() + (rank + 1) * (n_per_rank + 1));\n  } else {\n    x_local = std::vector<double>(x.begin() + rank * n_per_rank + n_remainder,\n                                  x.begin() + (rank + 1) * n_per_rank + n_remainder);\n    y_local = std::vector<double>(y.begin() + rank * n_per_rank + n_remainder,\n                                  y.begin() + (rank + 1) * n_per_rank + n_remainder);\n  }\n\n  double sum = 0;\n  for (int i = 0; i < n_local; i++) {\n    sum += std::min(x_local[i], y_local[i]);\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  return sum_all;\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  for (int i = start; i < end; i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  return sum_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<double> local_min(chunk + (rank < remainder? 1 : 0));\n\n    #pragma omp parallel for\n    for (int i = 0; i < chunk + (rank < remainder? 1 : 0); i++) {\n        local_min[i] = std::min(x[i * size + rank], y[i * size + rank]);\n    }\n\n    std::vector<double> global_min(n);\n    MPI_Allreduce(local_min.data(), global_min.data(), n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return std::accumulate(global_min.begin(), global_min.end(), 0.0);\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n\treturn 0.0;\n}",
            "// Create a Kokkos parallel reduction to compute the sum of the vector\n  double sum = 0.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    sum);\n\n  // Divide by the number of elements to get the average\n  return sum / x.extent(0);\n}",
            "double sum = 0.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    sum);\n  return sum / x.extent(0);\n}",
            "// TODO: Your code here\n  double sum = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n    lsum += x(i);\n  }, sum);\n  return sum / x.extent(0);\n}",
            "double sum = 0.0;\n\tKokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n\t\tKOKKOS_LAMBDA(const int i, double& lsum) {\n\t\t\tlsum += x(i);\n\t\t},\n\t\tsum\n\t);\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tKokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n\t\tlsum += x(i);\n\t}, sum);\n\treturn sum / x.extent(0);\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "double sum = 0.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    Kokkos::Sum<double>(sum)\n  );\n  return sum / x.extent(0);\n}",
            "double sum = 0;\n\tKokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n\t\tlsum += x(i);\n\t}, sum);\n\treturn sum / x.extent(0);\n}",
            "double sum = 0.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    sum);\n  return sum / x.extent(0);\n}",
            "double sum = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    sum);\n  return sum / x.extent(0);\n}",
            "// TODO: Your code here\n\tdouble sum = 0;\n\tKokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n\t\tlsum += x(i);\n\t}, sum);\n\treturn sum / x.extent(0);\n}",
            "// TODO: Your code here\n  double sum = 0.0;\n  for (int i = 0; i < x.extent(0); i++) {\n    sum += x(i);\n  }\n  return sum / x.extent(0);\n}",
            "// TODO: Implement this function\n\tdouble sum = 0.0;\n\tKokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n\t\tlsum += x(i);\n\t}, sum);\n\treturn sum / x.extent(0);\n}",
            "double sum = 0.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      lsum += x(i);\n    },\n    sum);\n  return sum / x.extent(0);\n}",
            "double sum = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n    lsum += x(i);\n  }, sum);\n  return sum / x.extent(0);\n}",
            "// Create a Kokkos parallel reduction to sum the elements of x.\n\t// The lambda function is the \"operator\" for the reduction.\n\t// The initial value of the reduction is 0.\n\tdouble sum = Kokkos::parallel_reduce(\n\t\tKokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n\t\tKOKKOS_LAMBDA(const int i, double& lsum) {\n\t\t\tlsum += x(i);\n\t\t},\n\t\t0.0\n\t);\n\n\t// Compute the average of the elements of x.\n\tdouble average = sum / x.extent(0);\n\n\treturn average;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: implement me\n\treturn 0.0;\n}",
            "// Your code here\n\tdouble sum = 0;\n\tKokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n\t\tlsum += x(i);\n\t}, Kokkos::Sum<double>(sum));\n\treturn sum / x.extent(0);\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n  int n = x.size();\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n  }\n  return sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; ++i) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\n\treturn sum / n;\n}",
            "double sum = 0;\n    int n = x.size();\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n    }\n\n    return sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0.0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; ++i) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "int n = x.size();\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n    }\n    return sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0.0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; ++i) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\n\treturn sum / n;\n}",
            "double sum = 0;\n  int n = x.size();\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n  }\n  return sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n    int n = x.size();\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n    }\n\n    return sum / n;\n}",
            "int n = x.size();\n\tdouble sum = 0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}",
            "double sum = 0;\n\tint n = x.size();\n\tint i;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "serial",
        "prompt": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n  for (double i : x) {\n    sum += i;\n  }\n  return sum / x.size();\n}",
            "double sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n    for (auto i : x) {\n        sum += i;\n    }\n    return sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto& i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum / x.size();\n}",
            "double sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n    for (auto i : x) {\n        sum += i;\n    }\n    return sum / x.size();\n}",
            "double sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n  for (auto i : x) {\n    sum += i;\n  }\n  return sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
            "double sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "hip",
        "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(average, x[i]);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\tint idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tatomicAdd(average, x[idx]);\n\t}\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  double sum = 0.0;\n  if (tid < N) {\n    sum += x[tid];\n  }\n  __syncthreads();\n  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      sum += x[tid + s];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    *average = sum / N;\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\tdouble sum = 0;\n\tfor (int i = tid; i < N; i += stride) {\n\t\tsum += x[i];\n\t}\n\tatomicAdd(average, sum);\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tatomicAdd(average, x[tid]);\n\t}\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(average, x[tid]);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(average, x[idx]);\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(average, x[i]);\n  }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: compute the average of x on rank 0\n  // TODO: broadcast the result to all ranks\n\n  return 0.0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double sum = 0;\n  for (auto i : x) {\n    sum += i;\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  double average;\n  if (rank == 0) {\n    average = sum_all / x.size();\n  }\n\n  MPI_Bcast(&average, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  return average;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int count = x.size();\n    int chunk = count / size;\n    int remainder = count % size;\n\n    std::vector<double> local_sum(chunk + (rank < remainder));\n    std::vector<double> global_sum(chunk + (rank < remainder));\n\n    for (int i = 0; i < chunk + (rank < remainder); i++) {\n        local_sum[i] = x[i * size + rank];\n    }\n\n    MPI_Reduce(&local_sum[0], &global_sum[0], chunk + (rank < remainder), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    double sum = 0;\n    for (int i = 0; i < chunk + (rank < remainder); i++) {\n        sum += global_sum[i];\n    }\n\n    return sum / count;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  std::vector<double> local_sum(n_per_rank + 1);\n  std::vector<double> global_sum(n_per_rank + 1);\n\n  for (int i = 0; i < n_per_rank; i++) {\n    local_sum[i] = x[rank * n_per_rank + i];\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_remainder; i++) {\n      local_sum[n_per_rank + i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_remainder; i++) {\n      local_sum[n_per_rank + i] = x[rank * n_per_rank + i];\n    }\n  }\n\n  MPI_Reduce(&local_sum[0], &global_sum[0], n_per_rank + 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    double sum = 0;\n    for (int i = 0; i < n_per_rank + 1; i++) {\n      sum += global_sum[i];\n    }\n    return sum / n;\n  } else {\n    return 0;\n  }\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double sum = 0;\n  for (auto i : x) {\n    sum += i;\n  }\n\n  double sum_all;\n  MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  double avg;\n  if (rank == 0) {\n    avg = sum_all / size;\n  }\n\n  MPI_Bcast(&avg, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  return avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tstd::vector<double> local_sum(count_per_rank);\n\tstd::vector<double> local_sum_remainder(count_remainder);\n\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tlocal_sum[i] = x[rank * count_per_rank + i];\n\t}\n\n\tfor (int i = 0; i < count_remainder; i++) {\n\t\tlocal_sum_remainder[i] = x[rank * count_per_rank + count_per_rank + i];\n\t}\n\n\tdouble sum = 0;\n\tdouble sum_remainder = 0;\n\n\tMPI_Reduce(&local_sum[0], &sum, count_per_rank, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\tMPI_Reduce(&local_sum_remainder[0], &sum_remainder, count_remainder, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble avg = 0;\n\tif (rank == 0) {\n\t\tavg = (sum + sum_remainder) / count;\n\t}\n\n\treturn avg;\n}",
            "int size;\n\tint rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tdouble sum = 0;\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tsum += x[rank * count_per_rank + i];\n\t}\n\tif (rank < count_remainder) {\n\t\tsum += x[rank * count_per_rank + count_per_rank];\n\t}\n\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble avg = sum_all / count;\n\n\treturn avg;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n\n    double sum_all = 0;\n    MPI_Allreduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return sum_all / x.size();\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  double sum = 0;\n  for (auto i : x) {\n    sum += i;\n  }\n  double avg = sum / x.size();\n  double avg_all;\n  MPI_Reduce(&avg, &avg_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    avg_all = avg_all / size;\n  }\n  return avg_all;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tdouble sum = 0;\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tsum += x[rank * count_per_rank + i];\n\t}\n\tif (rank < count_remainder) {\n\t\tsum += x[rank * count_per_rank + count_per_rank];\n\t}\n\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble average = sum_all / count;\n\n\treturn average;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n\n    double sum_all;\n    MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    double avg = sum_all / (size * x.size());\n\n    return avg;\n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n  }\n  double avg = sum / x.size();\n  double avg_all;\n  MPI_Reduce(&avg, &avg_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    avg_all /= size;\n  }\n  return avg_all;\n}",
            "int size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint remainder = count % size;\n\n\tdouble sum = 0;\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tsum += x[rank * count_per_rank + i];\n\t}\n\n\tif (rank < remainder) {\n\t\tsum += x[rank * count_per_rank + count_per_rank];\n\t}\n\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble average;\n\tif (rank == 0) {\n\t\taverage = sum_all / count;\n\t}\n\n\tMPI_Bcast(&average, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\treturn average;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    for (auto i : x) {\n        sum += i;\n    }\n\n    double avg = 0;\n    if (rank == 0) {\n        avg = sum / x.size();\n    }\n\n    MPI_Bcast(&avg, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    return avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tdouble sum = 0;\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tsum += x[rank * count_per_rank + i];\n\t}\n\tif (rank < count_remainder) {\n\t\tsum += x[rank * count_per_rank + count_per_rank];\n\t}\n\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble average = sum_all / count;\n\treturn average;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\n\tdouble avg = 0;\n\tif (rank == 0) {\n\t\tavg = sum / x.size();\n\t}\n\n\tMPI_Bcast(&avg, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\treturn avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble sum = 0;\n\tfor (auto i : x) {\n\t\tsum += i;\n\t}\n\n\tdouble average = sum / x.size();\n\n\tdouble average_all;\n\tMPI_Reduce(&average, &average_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\taverage_all /= size;\n\t}\n\n\treturn average_all;\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n  }\n  int sum_all;\n  MPI_Allreduce(&sum, &sum_all, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  return sum_all / (double)x.size() / size;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tdouble sum = 0;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsum += x[i];\n\t}\n\tdouble avg = sum / x.size();\n\tdouble avg_all;\n\tMPI_Reduce(&avg, &avg_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tavg_all = avg_all / size;\n\t}\n\treturn avg_all;\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n  }\n  double avg = sum / x.size();\n  double avg_all;\n  MPI_Reduce(&avg, &avg_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    avg_all = avg_all / size;\n  }\n  return avg_all;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(average, x[i]);\n    }\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n\tint i = threadIdx.x;\n\tdouble sum = 0;\n\twhile (i < N) {\n\t\tsum += x[i];\n\t\ti += blockDim.x;\n\t}\n\t*average = sum / N;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n    int i = threadIdx.x;\n    int sum = 0;\n    int count = 0;\n    while (i < N) {\n        sum += x[i];\n        count++;\n        i += blockDim.x;\n    }\n    *average = (double) sum / count;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "// TODO: Implement this function\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd function to compute the average\n  // Use the atomicAdd",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(average, x[i]);\n\t}\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "23_reduce_average",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank + n_remainder; i++) {\n            sum += x[i];\n        }\n    } else {\n        MPI_Status status;\n        MPI_Recv(&x[0], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n        for (int i = 0; i < n_per_rank; i++) {\n            sum += x[i];\n        }\n    }\n\n    double average = sum / (n_per_rank * size + n_remainder);\n\n    return average;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\tdouble sum = 0;\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Send(&x[i * chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < chunk + remainder; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t}\n\telse {\n\t\tMPI_Recv(&x[0], chunk, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t}\n\tdouble average = sum / (chunk + remainder);\n\tMPI_Bcast(&average, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\treturn average;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_left = n % size;\n\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\tfor (int i = 0; i < n_per_rank; i++) {\n\t\tsum_local += x[i];\n\t}\n\n\tif (rank < n_left) {\n\t\tsum_local += x[n_per_rank + rank];\n\t}\n\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn sum / n;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble sum = 0;\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Send(&x[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t} else {\n\t\tMPI_Recv(&x[0], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(&x[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tfor (int j = 0; j < n_per_rank; j++) {\n\t\t\t\tsum += x[i * n_per_rank + j];\n\t\t\t}\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tsum += x[i + size * n_per_rank];\n\t\t}\n\t}\n\n\tdouble avg = sum / (n_per_rank * size + n_remainder);\n\treturn avg;\n}",
            "int size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\tint n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n\n\tdouble sum = 0.0;\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n_local; i++) {\n\t\tsum += x[i];\n\t}\n\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble avg = sum_all / n;\n\treturn avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tsum_local += x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = remainder; i < x.size(); i++) {\n\t\t\tsum_local += x[i];\n\t\t}\n\t}\n\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn sum / x.size();\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\tint n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\tdouble avg = 0;\n\tint start = rank * n_per_rank + std::min(rank, n_remainder);\n\tint end = start + n_local;\n\tfor (int i = start; i < end; i++) {\n\t\tsum_local += x[i];\n\t}\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tavg = sum / n;\n\t}\n\treturn avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tdouble sum = 0.0;\n\tdouble sum_all = 0.0;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i + n_per_rank * (rank - 1)];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(&sum_all, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t}\n\telse {\n\t\tMPI_Send(&sum, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_extra; i++) {\n\t\t\tsum_all += x[i + n_per_rank * (size - 1)];\n\t\t}\n\t}\n\n\treturn sum_all / (n + n_extra);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tdouble sum = 0;\n\tdouble sum_all = 0;\n\tdouble sum_local = 0;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\n\tfor (int i = start; i < end; i++) {\n\t\tsum_local += x[i];\n\t}\n\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tsum_all = sum / n;\n\t}\n\n\treturn sum_all;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\n\t#pragma omp parallel for reduction(+:sum_local)\n\tfor (int i = 0; i < n_per_rank; i++) {\n\t\tsum_local += x[i];\n\t}\n\n\tif (rank < n_remainder) {\n\t\tsum_local += x[n_per_rank + rank];\n\t}\n\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\treturn sum / n;\n\t}\n\telse {\n\t\treturn 0;\n\t}\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n\n  double local_sum = 0;\n  for (int i = 0; i < n_local; ++i) {\n    local_sum += x[rank * n_per_rank + i];\n  }\n\n  double sum = 0;\n  MPI_Reduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  double avg = sum / n;\n\n  return avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_left = n % size;\n\n\tdouble sum = 0;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum += x[i + rank * n_per_rank];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tdouble tmp;\n\t\t\tMPI_Recv(&tmp, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tsum += tmp;\n\t\t}\n\t}\n\telse {\n\t\tMPI_Send(&sum, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0) {\n\t\tsum += n_left * x[n_per_rank + rank * n_per_rank];\n\t\treturn sum / n;\n\t}\n\telse {\n\t\treturn sum / n;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\tdouble sum = 0;\n\tfor (int i = start; i < end; i++) {\n\t\tsum += x[i];\n\t}\n\tdouble sum_all;\n\tMPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\tdouble avg = sum_all / n;\n\treturn avg;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  double sum = 0;\n  double sum_local = 0;\n  for (int i = 0; i < n_per_rank; i++) {\n    sum_local += x[i];\n  }\n  if (rank < n_remainder) {\n    sum_local += x[n_per_rank + rank];\n  }\n  MPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum / n;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> x_local(n_per_rank);\n  std::vector<double> x_local_sum(n_per_rank);\n  std::vector<double> x_local_sum_all(n_per_rank);\n  std::vector<double> x_local_sum_all_reduced(n_per_rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i + n_per_rank * (rank - 1)];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sum[i] = x_local[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sum[i] = 0;\n    }\n  }\n\n  for (int i = 0; i < n_per_rank; i++) {\n    x_local_sum[i] += x_local[i];\n  }\n\n  MPI_Reduce(x_local_sum.data(), x_local_sum_all.data(), n_per_rank, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sum_all_reduced[i] = x_local_sum_all[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sum_all_reduced[i] = 0;\n    }\n  }\n\n  double sum = 0;\n  for (int i = 0; i < n_per_rank; i++) {\n    sum += x_local_sum_all_reduced[i];\n  }\n\n  double average = sum / n;\n\n  return average;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint local_size = x.size() / size;\n\tint local_rank = rank;\n\n\tdouble local_sum = 0;\n\tfor (int i = 0; i < local_size; i++) {\n\t\tlocal_sum += x[local_rank * local_size + i];\n\t}\n\n\tdouble global_sum = 0;\n\tMPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tdouble global_average = global_sum / x.size();\n\treturn global_average;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_threads = omp_get_max_threads();\n\tint num_elements = x.size();\n\tint num_elements_per_thread = num_elements / num_threads;\n\tint num_elements_remaining = num_elements % num_threads;\n\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\n\t#pragma omp parallel\n\t{\n\t\tint thread_id = omp_get_thread_num();\n\t\tint num_elements_local = num_elements_per_thread;\n\t\tif (thread_id < num_elements_remaining) {\n\t\t\tnum_elements_local++;\n\t\t}\n\t\tint start_index = thread_id * num_elements_per_thread;\n\t\tif (thread_id < num_elements_remaining) {\n\t\t\tstart_index += thread_id;\n\t\t}\n\t\tint end_index = start_index + num_elements_local;\n\n\t\tfor (int i = start_index; i < end_index; i++) {\n\t\t\tsum_local += x[i];\n\t\t}\n\t}\n\n\tMPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tsum /= num_elements;\n\t}\n\n\treturn sum;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_threads = omp_get_max_threads();\n  int num_per_thread = x.size() / num_threads;\n  int num_extra = x.size() % num_threads;\n\n  double sum = 0;\n  #pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    int start = thread_id * num_per_thread;\n    int end = start + num_per_thread;\n    if (thread_id == num_threads - 1) {\n      end += num_extra;\n    }\n    double sum_local = 0;\n    for (int i = start; i < end; i++) {\n      sum_local += x[i];\n    }\n    #pragma omp critical\n    {\n      sum += sum_local;\n    }\n  }\n  double avg = sum / x.size();\n  return avg;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_left = n % size;\n\n\tdouble sum = 0;\n\tdouble sum_local = 0;\n\tdouble sum_global = 0;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank + n_left; i++) {\n\t\t\tsum_local += x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tsum_local += x[i];\n\t\t}\n\t}\n\n\tMPI_Reduce(&sum_local, &sum_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tsum = sum_global / n;\n\t}\n\n\treturn sum;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tdouble sum = 0;\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_extra;\n\t}\n\n\tdouble sum_local = 0;\n\tfor (int i = start; i < end; i++) {\n\t\tsum_local += x[i];\n\t}\n\n\tdouble sum_global;\n\tMPI_Reduce(&sum_local, &sum_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tsum = sum_global / n;\n\t}\n\n\treturn sum;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use Kokkos to compute product in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(Kokkos::View<const double*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a Kokkos parallel reduction to compute the product\n  Kokkos::View<double*> product(\"product\", 1);\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n      if (i % 2 == 0) {\n        lsum *= x(i);\n      } else {\n        lsum *= 1.0 / x(i);\n      }\n    },\n    product);\n  // Copy the result back to the host\n  double result;\n  Kokkos::deep_copy(result, product);\n  return result;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO\n  return 0.0;\n}",
            "// TODO: Implement me!\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a parallel_reduce functor to compute the product\n  class ParallelReduce {\n  public:\n    ParallelReduce(Kokkos::View<const double*> const& x) : x_(x) {}\n\n    KOKKOS_INLINE_FUNCTION\n    void operator()(const int i, double& lsum) const {\n      // Compute the product of x_i with every odd indexed element inverted\n      lsum *= x_(i) * (i % 2? 1.0 / x_(i) : 1.0);\n    }\n\n    KOKKOS_INLINE_FUNCTION\n    void join(double& lsum, const double& rsum) const { lsum *= rsum; }\n\n  private:\n    Kokkos::View<const double*> x_;\n  };\n\n  // Create a parallel_reduce functor to compute the product\n  ParallelReduce functor(x);\n\n  // Initialize the result to 1.0\n  double result = 1.0;\n\n  // Compute the product in parallel\n  Kokkos::parallel_reduce(x.extent(0), functor, result);\n\n  // Return the result\n  return result;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "double result = 1.0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, double& lsum) {\n      if (i % 2 == 0) {\n        lsum *= x(i);\n      } else {\n        lsum *= 1.0 / x(i);\n      }\n    },\n    result);\n  return result;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO\n  return 0.0;\n}",
            "Kokkos::View<double*> y(\"y\", x.extent(0));\n  Kokkos::parallel_for(\n      \"productWithInverses\",\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        if (i % 2 == 0) {\n          y(i) = x(i);\n        } else {\n          y(i) = 1.0 / x(i);\n        }\n      });\n  Kokkos::fence();\n  double result = 1.0;\n  Kokkos::parallel_reduce(\n      \"productWithInverses\",\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, double& lsum) { lsum *= y(i); },\n      result);\n  Kokkos::fence();\n  return result;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a Kokkos parallel reduction to compute the product of the vector x\n  // with every odd indexed element inverted.\n  //\n  // The reduction is a \"parallel_reduce\" which means that the reduction is\n  // performed in parallel.\n  //\n  // The reduction is a \"reducer\" which means that the reduction is performed\n  // using a reducer.\n  //\n  // The reducer is a \"Kokkos::Sum\" which means that the reduction is performed\n  // using the \"Sum\" reducer.\n  //\n  // The reducer is a \"Kokkos::Sum<double>\" which means that the reduction is\n  // performed using the \"Sum\" reducer with a value type of \"double\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace>\" which means that\n  // the reduction is performed using the \"Sum\" reducer with a value type of\n  // \"double\" and a memory space of \"Kokkos::HostSpace\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"Kokkos::HostSpace\", and a memory\n  // traits of \"Kokkos::MemoryTraits<Kokkos::Unmanaged>\".\n  //\n  // The reducer is a \"Kokkos::Sum<double, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\"\n  // which means that the reduction is performed using the \"Sum\" reducer with a\n  // value type of \"double\", a memory space of \"",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double product = 1;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1/x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1.0 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1.0 / x[i];\n    }\n  }\n  return result;\n}",
            "double product = 1.0;\n\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n\n    return product;\n}",
            "double product = 1;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(product:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(product:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1/x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double result = 1.0;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1.0 / x[i];\n    }\n  }\n  return result;\n}",
            "double product = 1;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1.0;\n  #pragma omp parallel for reduction(product:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 1) {\n      product *= 1.0 / x[i];\n    } else {\n      product *= x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1.0 / x[i];\n    }\n  }\n  return product;\n}",
            "double result = 1;\n  #pragma omp parallel for reduction(product: result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
            "double result = 1;\n  #pragma omp parallel for reduction(product: result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1.0 / x[i];\n    }\n  }\n  return result;\n}",
            "double result = 1.0;\n\n    #pragma omp parallel for reduction(*:result)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            result *= x[i];\n        } else {\n            result *= 1.0 / x[i];\n        }\n    }\n\n    return result;\n}",
            "double product = 1;\n    #pragma omp parallel for reduction(product:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n    #pragma omp parallel for reduction(prod: product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1/x[i];\n        }\n    }\n    return product;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "serial",
        "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double product = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double result = 1;\n  for (size_t i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n  for (size_t i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1.0;\n  for (size_t i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1.0 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1.0;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1.0 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "double product = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n  for (size_t i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1.0 / x[i];\n    }\n  }\n  return product;\n}",
            "double product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "hip",
        "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use AMD HIP to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      product[i] = x[i];\n    } else {\n      product[i] = 1.0 / x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1.0 / x[i];\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prod = 1.0;\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 0) {\n                prod *= x[i];\n            } else {\n                prod *= 1.0 / x[i];\n            }\n        }\n        product[tid] = prod;\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        double prod = 1.0;\n        for (int j = 0; j < N; j++) {\n            if (j % 2 == 0) {\n                prod *= x[j];\n            } else {\n                prod *= 1.0 / x[j];\n            }\n        }\n        product[i] = prod;\n    }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      product[i] = x[i];\n    } else {\n      product[i] = 1.0 / x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      product[i] = x[i];\n    } else {\n      product[i] = 1.0 / x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double value = x[i];\n        if (i % 2 == 1) {\n            value = 1.0 / value;\n        }\n        atomicAdd(product, value);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double prod = 1.0;\n    for (int j = 0; j < N; j++) {\n      if (j % 2 == 0) {\n        prod *= x[j];\n      } else {\n        prod *= 1.0 / x[j];\n      }\n    }\n    product[i] = prod;\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    double value = x[idx];\n    if (idx % 2 == 1) value = 1.0 / value;\n    atomicAdd(product, value);\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    double prod = 1.0;\n    for (int i = tid; i < N; i += stride) {\n        if (i % 2 == 0) {\n            prod *= x[i];\n        } else {\n            prod *= 1.0 / x[i];\n        }\n    }\n    atomicAdd(product, prod);\n}",
            "// TODO: Implement this function\n  //\n  // 1. Use the gridDim and blockIdx to compute the index of the current thread\n  // 2. Use the index to compute the index of the element in x\n  // 3. Use the index to compute the index of the element in product\n  // 4. Use the index to compute the index of the element in x that should be inverted\n  // 5. Use the index to compute the index of the element in product that should be inverted\n  // 6. Use the index to compute the index of the element in x that should be multiplied\n  // 7. Use the index to compute the index of the element in product that should be multiplied\n  // 8. Use the index to compute the index of the element in x that should be multiplied\n  // 9. Use the index to compute the index of the element in product that should be multiplied\n  // 10. Use the index to compute the index of the element in x that should be multiplied\n  // 11. Use the index to compute the index of the element in product that should be multiplied\n  // 12. Use the index to compute the index of the element in x that should be multiplied\n  // 13. Use the index to compute the index of the element in product that should be multiplied\n  // 14. Use the index to compute the index of the element in x that should be multiplied\n  // 15. Use the index to compute the index of the element in product that should be multiplied\n  // 16. Use the index to compute the index of the element in x that should be multiplied\n  // 17. Use the index to compute the index of the element in product that should be multiplied\n  // 18. Use the index to compute the index of the element in x that should be multiplied\n  // 19. Use the index to compute the index of the element in product that should be multiplied\n  // 20. Use the index to compute the index of the element in x that should be multiplied\n  // 21. Use the index to compute the index of the element in product that should be multiplied\n  // 22. Use the index to compute the index of the element in x that should be multiplied\n  // 23. Use the index to compute the index of the element in product that should be multiplied\n  // 24. Use the index to compute the index of the element in x that should be multiplied\n  // 25. Use the index to compute the index of the element in product that should be multiplied\n  // 26. Use the index to compute the index of the element in x that should be multiplied\n  // 27. Use the index to compute the index of the element in product that should be multiplied\n  // 28. Use the index to compute the index of the element in x that should be multiplied\n  // 29. Use the index to compute the index of the element in product that should be multiplied\n  // 30. Use the index to compute the index of the element in x that should be multiplied\n  // 31. Use the index to compute the index of the element in product that should be multiplied\n  // 32. Use the index to compute the index of the element in x that should be multiplied\n  // 33. Use the index to compute the index of the element in product that should be multiplied\n  // 34. Use the index to compute the index of the element in x that should be multiplied\n  // 35. Use the index to compute the index of the element in product that should be multiplied\n  // 36. Use the index to compute the index of the element in x that should be multiplied\n  // 37. Use the index to compute the index of the element in product that should be multiplied\n  // 38. Use the index to compute the index of the element in x that should be multiplied\n  // 39. Use the index to compute the index of the element in product that should be multiplied\n  // 40. Use the index to compute the index of the element in x that should be multiplied\n  // 41. Use the index to compute the index of the element in product that should be multiplied\n  // 42. Use the index to compute the index of the element in x that should be multiplied\n  // 43. Use the index to compute the index of the element in product that should be multip",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double value = x[i];\n        if (i % 2 == 1) {\n            value = 1.0 / value;\n        }\n        atomicAdd(product, value);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    double prod = 1;\n    for (int i = 0; i < N; i++) {\n        if (i % 2 == 0) {\n            prod *= x[i];\n        } else {\n            prod *= 1.0 / x[i];\n        }\n    }\n    product[idx] = prod;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double prod = x[i];\n    for (int j = i + 1; j < N; j += 2) {\n      prod *= 1.0 / x[j];\n    }\n    product[i] = prod;\n  }\n}",
            "// TODO\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    if (index % 2 == 0) {\n      product[index] = x[index];\n    } else {\n      product[index] = 1.0 / x[index];\n    }\n  }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  std::vector<double> x_local(n_per_rank + (rank < n_extra));\n  MPI_Scatter(x.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE,\n              x_local.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE, 0,\n              MPI_COMM_WORLD);\n\n  double result = 1;\n  for (int i = 0; i < x_local.size(); ++i) {\n    if (i % 2 == 0) {\n      result *= x_local[i];\n    } else {\n      result *= 1 / x_local[i];\n    }\n  }\n\n  double result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_DOUBLE, MPI_PROD, 0,\n             MPI_COMM_WORLD);\n  return result_global;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    double product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n\n    double product_all;\n    MPI_Reduce(&product, &product_all, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return product_all;\n}",
            "double product = 1;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n    return product;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_left = n % size;\n  int n_proc = rank < n_left? rank * (n_per_proc + 1) : rank * n_per_proc + n_left;\n  int n_proc_next = rank < n_left? (rank + 1) * (n_per_proc + 1) : (rank + 1) * n_per_proc + n_left;\n  double prod = 1;\n  for (int i = n_proc; i < n_proc_next; i++) {\n    if (i % 2 == 0) {\n      prod *= x[i];\n    } else {\n      prod *= 1 / x[i];\n    }\n  }\n  double prod_all;\n  MPI_Reduce(&prod, &prod_all, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n  return prod_all;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0.0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_left = n % size;\n    int n_local = n_per_rank + (rank < n_left? 1 : 0);\n\n    std::vector<double> x_local(n_local);\n    MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double product = 1.0;\n    for (int i = 0; i < n_local; i++) {\n        if (i % 2 == 0) {\n            product *= x_local[i];\n        } else {\n            product *= 1.0 / x_local[i];\n        }\n    }\n\n    double product_global;\n    MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return product_global;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int n_proc = rank < n_rem? rank * (n_per_proc + 1) : rank * n_per_proc + n_rem;\n\n  double local_prod = 1;\n  for (int i = 0; i < n_per_proc; i++) {\n    local_prod *= x[n_proc + i];\n  }\n  if (rank < n_rem) {\n    local_prod *= 1 / x[n_proc + n_per_proc];\n  }\n\n  double global_prod;\n  MPI_Reduce(&local_prod, &global_prod, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return global_prod;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result /= x[i];\n    }\n  }\n\n  // TODO: Your code here\n  double result_all;\n  MPI_Reduce(&result, &result_all, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n  return result_all;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> x_local(n_per_rank);\n  std::copy(x.begin() + rank * n_per_rank, x.begin() + (rank + 1) * n_per_rank, x_local.begin());\n  if (rank < n_rem) {\n    x_local.push_back(x[rank * n_per_rank + n_per_rank]);\n  }\n\n  double prod = 1;\n  for (int i = 0; i < x_local.size(); i++) {\n    if (i % 2 == 0) {\n      prod *= x_local[i];\n    } else {\n      prod *= 1 / x_local[i];\n    }\n  }\n\n  double prod_global;\n  MPI_Reduce(&prod, &prod_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return prod_global;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_proc + std::min(rank, n_rem);\n  int end = (rank + 1) * n_per_proc + std::min(rank + 1, n_rem);\n\n  double local_prod = 1.0;\n  for (int i = start; i < end; i += 2) {\n    local_prod *= x[i];\n  }\n\n  double global_prod = 1.0;\n  MPI_Reduce(&local_prod, &global_prod, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return global_prod;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n\n    std::vector<double> x_local(n_local);\n    MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double prod = 1.0;\n    for (int i = 0; i < n_local; i++) {\n        if (i % 2 == 0) {\n            prod *= x_local[i];\n        } else {\n            prod *= 1.0 / x_local[i];\n        }\n    }\n\n    double prod_global;\n    MPI_Reduce(&prod, &prod_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return prod_global;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  double prod = 1.0;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      prod *= x[i];\n    } else {\n      prod *= 1.0 / x[i];\n    }\n  }\n  return prod;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int n_local = n_per_rank + (rank < n_extra? 1 : 0);\n\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double prod = 1.0;\n  for (int i = 0; i < n_local; ++i) {\n    if (i % 2 == 0) {\n      prod *= x_local[i];\n    } else {\n      prod *= 1.0 / x_local[i];\n    }\n  }\n\n  double prod_global;\n  MPI_Reduce(&prod, &prod_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return prod_global;\n}",
            "// TODO: Implement this function\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_extra = n % size;\n\n  std::vector<double> x_local(n_per_proc + (rank < n_extra));\n  MPI_Scatter(x.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE,\n              x_local.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE, 0,\n              MPI_COMM_WORLD);\n\n  double product = 1;\n  for (int i = 0; i < n_per_proc + (rank < n_extra); ++i) {\n    if (i % 2 == 0) {\n      product *= x_local[i];\n    } else {\n      product *= 1 / x_local[i];\n    }\n  }\n\n  double product_global;\n  MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0,\n             MPI_COMM_WORLD);\n\n  return product_global;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> x_local(n_per_proc + (rank < n_rem? 1 : 0));\n  MPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE,\n              x_local.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE,\n              0, MPI_COMM_WORLD);\n\n  double prod = 1.0;\n  for (int i = 0; i < x_local.size(); ++i) {\n    if (i % 2 == 0) {\n      prod *= x_local[i];\n    } else {\n      prod *= 1.0 / x_local[i];\n    }\n  }\n\n  double prod_global;\n  MPI_Reduce(&prod, &prod_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return prod_global;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n    if (rank == size - 1) {\n        end += n_extra;\n    }\n\n    double product = 1;\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n\n    double product_all;\n    MPI_Reduce(&product, &product_all, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return product_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_product = 1;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            local_product *= x[i];\n        } else {\n            local_product *= 1 / x[i];\n        }\n    }\n\n    double global_product;\n    MPI_Reduce(&local_product, &global_product, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return global_product;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> x_local(n_per_proc + (rank < n_rem));\n  MPI_Scatter(x.data(), n_per_proc + (rank < n_rem), MPI_DOUBLE,\n              x_local.data(), n_per_proc + (rank < n_rem), MPI_DOUBLE, 0,\n              MPI_COMM_WORLD);\n\n  double product = 1;\n  for (int i = 0; i < n_per_proc + (rank < n_rem); ++i) {\n    if (i % 2 == 0) {\n      product *= x_local[i];\n    } else {\n      product *= 1 / x_local[i];\n    }\n  }\n\n  double product_global;\n  MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0,\n             MPI_COMM_WORLD);\n\n  return product_global;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use CUDA to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double prod = 1;\n        for (int j = 0; j < N; j++) {\n            if (j % 2 == 0) {\n                prod *= x[j];\n            } else {\n                prod *= 1 / x[j];\n            }\n        }\n        product[i] = prod;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (index % 2 == 0) {\n            product[0] *= x[index];\n        } else {\n            product[0] *= 1 / x[index];\n        }\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = x[i] / x[i - 1];\n        }\n    }\n}",
            "// TODO: Implement this function\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    if (index % 2 == 0) {\n      product[index] = x[index];\n    } else {\n      product[index] = 1.0 / x[index];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double prod = 1.0;\n        for (int j = 0; j < N; j++) {\n            if (j % 2 == 0) {\n                prod *= x[j];\n            } else {\n                prod *= 1.0 / x[j];\n            }\n        }\n        product[i] = prod;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (idx % 2 == 0) {\n            product[idx] = x[idx];\n        } else {\n            product[idx] = x[idx] / x[idx - 1];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1 / x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1 / x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i] * x[i + 1];\n        } else {\n            product[i] = x[i] * 1 / x[i - 1];\n        }\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double value = x[index];\n        if (index % 2 == 1) {\n            value = 1 / value;\n        }\n        atomicAdd(product, value);\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1.0 / x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1 / x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double prod = 1;\n        for (int j = 0; j < N; j++) {\n            if (j % 2 == 0) {\n                prod *= x[j];\n            } else {\n                prod *= 1 / x[j];\n            }\n        }\n        product[i] = prod;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            product[i] = x[i];\n        } else {\n            product[i] = 1.0 / x[i];\n        }\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double product = 1;\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 0) {\n                product *= x[i];\n            } else {\n                product *= 1 / x[i];\n            }\n        }\n        product[index] = product;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double prod = 1;\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 0) {\n                prod *= x[i];\n            } else {\n                prod *= 1 / x[i];\n            }\n        }\n        product[0] = prod;\n    }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "22_reduce_product_of_inverses",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI and OpenMP to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_remainder = num_elements % size;\n  int num_elements_rank = num_elements_per_rank;\n  if (rank < num_elements_remainder) {\n    num_elements_rank++;\n  }\n\n  std::vector<double> x_rank(num_elements_rank);\n  MPI_Scatter(x.data(), num_elements_rank, MPI_DOUBLE, x_rank.data(), num_elements_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double product = 1.0;\n  #pragma omp parallel for reduction(mul:product)\n  for (int i = 0; i < num_elements_rank; i++) {\n    if (i % 2 == 0) {\n      product *= x_rank[i];\n    } else {\n      product *= 1.0 / x_rank[i];\n    }\n  }\n\n  double product_global;\n  MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return product_global;\n}",
            "// TODO\n    return 0.0;\n}",
            "double result = 1;\n    #pragma omp parallel for reduction(mul: result)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            result *= x[i];\n        } else {\n            result *= 1.0 / x[i];\n        }\n    }\n    return result;\n}",
            "// TODO: Your code here\n\n  return 0.0;\n}",
            "// TODO: Implement this function\n  double product = 1;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n  if (rank == size - 1) {\n    end = x.size();\n  }\n  for (int i = start; i < end; i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  double result;\n  MPI_Reduce(&product, &result, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double product = 1;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1 / x[i];\n        }\n    }\n\n    double product_all;\n    MPI_Reduce(&product, &product_all, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return product_all;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk_size = n / size;\n    int remainder = n % size;\n    int start = rank * chunk_size;\n    int end = start + chunk_size;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    double local_prod = 1;\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            local_prod *= x[i];\n        } else {\n            local_prod *= 1 / x[i];\n        }\n    }\n\n    double global_prod;\n    MPI_Reduce(&local_prod, &global_prod, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n    return global_prod;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  double product = 1;\n  int chunk_size = n / size;\n  int start = rank * chunk_size;\n  int end = start + chunk_size;\n  if (rank == size - 1) {\n    end = n;\n  }\n  for (int i = start; i < end; i++) {\n    if (i % 2 == 0) {\n      product *= x[i];\n    } else {\n      product *= 1 / x[i];\n    }\n  }\n  double result;\n  MPI_Reduce(&product, &result, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int n_local = n_per_rank + (rank < n_extra? 1 : 0);\n\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double product = 1.0;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < n_local; ++i) {\n    if (i % 2 == 0) {\n      product *= x_local[i];\n    } else {\n      product *= 1.0 / x_local[i];\n    }\n  }\n\n  double product_global;\n  MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return product_global;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double prod = 1;\n\n    #pragma omp parallel for reduction(prod:prod)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            prod *= x[i];\n        } else {\n            prod *= 1 / x[i];\n        }\n    }\n\n    double result;\n    MPI_Reduce(&prod, &result, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    double result = 1;\n    int chunk_size = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * chunk_size;\n    int end = start + chunk_size;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            result *= x[i];\n        } else {\n            result *= 1 / x[i];\n        }\n    }\n\n    double total_result;\n    MPI_Reduce(&result, &total_result, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n    return total_result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n\n  double local_product = 1.0;\n  for (int i = start; i < end; i++) {\n    if (i % 2 == 0) {\n      local_product *= x[i];\n    } else {\n      local_product *= 1.0 / x[i];\n    }\n  }\n\n  double global_product = 1.0;\n  MPI_Reduce(&local_product, &global_product, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return global_product;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_remainder = num_elements % size;\n  int num_elements_rank = num_elements_per_rank + (rank < num_elements_remainder);\n\n  std::vector<double> x_rank(num_elements_rank);\n  MPI_Scatter(x.data(), num_elements_rank, MPI_DOUBLE, x_rank.data(), num_elements_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double product = 1;\n  #pragma omp parallel for reduction(*:product)\n  for (int i = 0; i < num_elements_rank; i++) {\n    if (i % 2 == 0) {\n      product *= x_rank[i];\n    } else {\n      product *= 1 / x_rank[i];\n    }\n  }\n\n  double product_global;\n  MPI_Reduce(&product, &product_global, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return product_global;\n}",
            "// TODO\n    return 0.0;\n}",
            "// TODO: Implement this function\n    double product = 1;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunkSize = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<double> myX;\n    if (rank == 0) {\n        myX.resize(chunkSize + remainder);\n        std::copy(x.begin(), x.begin() + chunkSize + remainder, myX.begin());\n    } else {\n        myX.resize(chunkSize);\n        MPI_Recv(&myX[0], chunkSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    int start = rank * chunkSize;\n    int end = start + chunkSize;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    double myProduct = 1;\n    for (int i = start; i < end; i += 2) {\n        myProduct *= myX[i];\n        if (i + 1 < end) {\n            myProduct /= myX[i + 1];\n        }\n    }\n\n    double globalProduct;\n    MPI_Reduce(&myProduct, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return globalProduct;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n    double result = 1.0;\n    int num_threads = omp_get_max_threads();\n    int num_elements = x.size();\n    int elements_per_thread = num_elements / num_threads;\n    int remainder = num_elements % num_threads;\n    int start = rank * elements_per_thread;\n    int end = start + elements_per_thread;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    #pragma omp parallel for reduction(*:result)\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            result *= x[i];\n        } else {\n            result *= 1.0 / x[i];\n        }\n    }\n\n    double global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n    return global_result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double product = 1.0;\n\n    // TODO: Compute product in parallel\n\n    return product;\n}",
            "// TODO: Implement this function\n  double product = 1;\n  int size = x.size();\n  int rank = 0;\n  int num_threads = 0;\n  int num_ranks = 0;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  omp_set_num_threads(num_ranks);\n  #pragma omp parallel\n  {\n    num_threads = omp_get_num_threads();\n  }\n\n  // TODO: Implement this function\n  double local_product = 1;\n  int local_size = size / num_ranks;\n  int local_rank = rank;\n  int local_start = local_rank * local_size;\n  int local_end = local_start + local_size;\n  if (local_end > size)\n    local_end = size;\n  for (int i = local_start; i < local_end; i++) {\n    if (i % 2 == 0)\n      local_product *= x[i];\n    else\n      local_product *= 1 / x[i];\n  }\n\n  double global_product = 1;\n  MPI_Reduce(&local_product, &global_product, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\n  return global_product;\n}",
            "// TODO: Implement this function\n    return 0.0;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use Kokkos to reduce in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(Kokkos::View<const bool*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a Kokkos reduction variable\n  Kokkos::View<bool, Kokkos::LayoutLeft, Kokkos::HostSpace> result(\"result\");\n  Kokkos::deep_copy(result, false);\n\n  // Create a Kokkos parallel_reduce lambda\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, x.size()),\n      KOKKOS_LAMBDA(const int i, bool& lhs) {\n        lhs = lhs ^ x[i];\n      },\n      result);\n\n  // Copy the result back to the host\n  bool result_host = false;\n  Kokkos::deep_copy(result_host, result);\n  return result_host;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Reducer = Kokkos::Experimental::MinMaxScalar<bool>;\n  using ReducerView = Kokkos::View<Reducer, ExecutionSpace>;\n\n  // Create a reducer view to hold the result\n  ReducerView reducer(\"reducer\", 1);\n\n  // Initialize the reducer to the identity value\n  Kokkos::parallel_for(\n      \"init_reducer\",\n      Policy(0, 1),\n      KOKKOS_LAMBDA(int) {\n        reducer() = true;\n      });\n\n  // Reduce the vector\n  Kokkos::parallel_reduce(\n      \"reduce_vector\",\n      Policy(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i, Reducer& r) {\n        r() = r() ^ x(i);\n      },\n      reducer);\n\n  // Return the result\n  return reducer() ^ true;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using FunctorType = Kokkos::Experimental::ReduceFunctor<ReducerType, PolicyType>;\n\n  // Create a reducer to hold the result of the reduction.\n  ReducerType reducer(true, false);\n\n  // Create a policy to run the reduction.\n  PolicyType policy(0, x.extent(0));\n\n  // Create a functor to perform the reduction.\n  FunctorType functor(reducer, policy, x);\n\n  // Run the reduction.\n  Kokkos::parallel_reduce(functor);\n\n  // Return the result of the reduction.\n  return reducer.get();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Reducer = Kokkos::Experimental::MinMaxScalar<bool>;\n  using ReducerType = typename Reducer::value_type;\n  Reducer reducer(true, false);\n  ReducerType result = Kokkos::parallel_reduce(\n      Policy(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, ReducerType& lhs) {\n        lhs.update(x(i));\n      },\n      reducer);\n  return result.min_val!= result.max_val;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Reducer = Kokkos::reduction_identity<bool, Kokkos::reduction_xor<bool>>;\n  Reducer reducer;\n  return Kokkos::parallel_reduce(Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n                                 [&](int i, bool& lval) { lval ^= x(i); },\n                                 reducer);\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n\n  ReducerType reducer(false, false);\n  Kokkos::parallel_reduce(\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, ReducerType& lsum) {\n        lsum.update(x[i]);\n      },\n      reducer);\n  return reducer.min() ^ reducer.max();\n}",
            "Kokkos::View<bool*> result(\"result\", 1);\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, bool& l) { l ^= x[i]; },\n      result);\n  return result[0];\n}",
            "Kokkos::View<bool*> x_copy(\"x_copy\", x.extent(0));\n  Kokkos::deep_copy(x_copy, x);\n  Kokkos::View<bool*> x_reduced(\"x_reduced\", 1);\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, bool& update) { update ^= x_copy(i); },\n      Kokkos::ExclusiveSum<bool>(x_reduced));\n  return x_reduced(0);\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n\n  ReducerType reducer(false, true);\n  Kokkos::parallel_reduce(\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i, ReducerType& lsum) {\n        lsum.update(x[i]);\n      },\n      reducer);\n  return reducer.min();\n}",
            "Kokkos::View<bool*> x_copy(\"x_copy\", x.extent(0));\n  Kokkos::deep_copy(x_copy, x);\n  Kokkos::parallel_for(\n      \"reduceLogicalXOR\",\n      Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<bool> > >(\n          x.extent(0)),\n      [=](int i, bool& lval) {\n        lval = lval ^ x_copy(i);\n      });\n  Kokkos::fence();\n  return x_copy(0);\n}",
            "// Create a Kokkos reduction variable to hold the result\n  Kokkos::View<bool, Kokkos::LayoutLeft, Kokkos::HostSpace> result(\"result\");\n\n  // Create a Kokkos parallel_reduce to compute the reduction\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, bool& lhs) {\n        // The lambda is called once for each element of x.\n        // The first argument is the index of the element.\n        // The second argument is the reduction variable.\n        // The lambda should update the reduction variable.\n        lhs ^= x(i);\n      },\n      result);\n\n  // Copy the result back to the host\n  bool result_host = false;\n  Kokkos::deep_copy(result_host, result);\n\n  return result_host;\n}",
            "// Create a reduction variable to hold the result.\n  Kokkos::View<bool, Kokkos::LayoutLeft, Kokkos::HostSpace> result(\"result\", 1);\n\n  // Create a parallel_reduce to perform the reduction.\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, bool& lhs) {\n        // The lambda function is executed in parallel.\n        // lhs is the reduction variable.\n        // x is the input vector.\n        lhs ^= x(i);\n      },\n      result);\n\n  // Copy the result back to the host.\n  bool result_host = false;\n  Kokkos::deep_copy(result_host, result);\n  return result_host;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n\n  ReducerType reducer(false, true);\n  Kokkos::parallel_reduce(\n      \"reduceLogicalXOR\",\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, ReducerType& lsum) {\n        lsum.update(x[i]);\n      },\n      reducer);\n  return reducer.min();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::reduction_identity::Xor<bool>;\n  using Reducer = Kokkos::reduction_identity::Xor<bool>;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n\n  ReducerType reducer = Reducer();\n  bool result = Kokkos::parallel_reduce(Policy(0, x.extent(0)), reducer, x);\n  Kokkos::fence();\n  return result;\n}",
            "Kokkos::View<bool*> y(\"y\", 1);\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, bool& lval) {\n      lval ^= x(i);\n    },\n    Kokkos::ExclusiveSum<bool>(y));\n  Kokkos::fence();\n  return y(0);\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Reducer = Kokkos::Experimental::MinMaxScalar<bool>;\n\n  // Create a reducer\n  Reducer reducer(true, false);\n\n  // Execute the reduction\n  Kokkos::parallel_reduce(\n      Policy(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i, Reducer& lsum) {\n        lsum.update(x[i]);\n      },\n      reducer);\n\n  // Return the result\n  return reducer.min();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using ReducerFunctorType = Kokkos::Experimental::MinMax<bool>;\n\n  // Create a reducer to hold the result\n  ReducerType reducer(true, false);\n\n  // Create a functor to perform the reduction\n  ReducerFunctorType functor(reducer);\n\n  // Run the reduction\n  Kokkos::parallel_reduce(PolicyType(0, x.extent(0)), functor, reducer);\n\n  // Return the result\n  return reducer.min();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Reducer = Kokkos::reducer_logical_xor<bool>;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  Reducer reducer(false);\n  Kokkos::parallel_reduce(Policy(0, x.extent(0)), reducer, x);\n  return reducer.reference();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using ReducerType = Kokkos::Experimental::MinMaxScalar<bool>;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using FunctorType = Kokkos::Experimental::ReduceFunctor<ReducerType, PolicyType>;\n\n  ReducerType reducer(false, true);\n  PolicyType policy(0, x.extent(0));\n  FunctorType functor(reducer, policy, x);\n  Kokkos::parallel_reduce(functor);\n  return reducer.value();\n}",
            "Kokkos::View<bool*> result(\"result\", 1);\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, bool& l) { l ^= x[i]; },\n      result);\n  return result[0];\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n\n    return result;\n}",
            "int n = x.size();\n    std::vector<bool> y(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        y[i] = x[i];\n    }\n    for (int i = 1; i < n; i *= 2) {\n        #pragma omp parallel for\n        for (int j = 0; j < n; j += 2 * i) {\n            y[j] = y[j] ^ y[j + i];\n        }\n    }\n    return y[0];\n}",
            "int n = x.size();\n  std::vector<bool> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    y[i] = x[i];\n  }\n  for (int i = 1; i < n; i *= 2) {\n    #pragma omp parallel for\n    for (int j = 0; j < n; j += 2 * i) {\n      y[j] = y[j] ^ y[j + i];\n    }\n  }\n  return y[0];\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (size_t i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); i++) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (size_t i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (size_t i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (size_t i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); i++) {\n    result ^= x[i];\n  }\n  return result;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "serial",
        "prompt": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "bool result = false;\n  for (bool b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto b : x) {\n    result ^= b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result ^= b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (bool b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "return std::reduce(x.begin(), x.end(), false, std::logical_xor<bool>());\n}",
            "bool result = false;\n    for (auto i : x) {\n        result = result ^ i;\n    }\n    return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result ^= b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (bool b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n    for (bool b : x) {\n        result ^= b;\n    }\n    return result;\n}",
            "bool result = false;\n  for (bool b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& e : x) {\n    result = result ^ e;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto const& b : x) {\n    result = result ^ b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (bool b : x) {\n    result ^= b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (auto b : x) {\n    result ^= b;\n  }\n  return result;\n}",
            "bool result = false;\n  for (bool b : x) {\n    result ^= b;\n  }\n  return result;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "hip",
        "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use AMD HIP to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, double *output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  //\n  // Hint: Use a for loop to compute the logical XOR reduction of the vector of bools x.\n  //       Use the atomicXor function to update the output value.\n  //       Use the __syncthreads function to synchronize the threads in the block.\n  //       Use the __ballot_sync function to compute the logical XOR reduction of the vector of bools x.\n  //       Use the __popc function to count the number of set bits in the result of the logical XOR reduction.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads in the block.\n  //       Use the __shfl_sync function to broadcast the result of the logical XOR reduction to all threads",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n  bool local_result = false;\n  for (int i = tid; i < N; i += stride) {\n    local_result = local_result ^ x[i];\n  }\n  __shared__ bool sdata[1024];\n  int lane = tid % warpSize;\n  int wid = tid / warpSize;\n  local_result = warpReduceLogicalXOR(local_result);\n  if (lane == 0)\n    sdata[wid] = local_result;\n  __syncthreads();\n  if (tid < blockDim.x / warpSize)\n    local_result = warpReduceLogicalXOR(sdata[tid]);\n  if (tid == 0)\n    atomicExch(output, local_result);\n}",
            "// TODO: Implement this function\n    // Use AMD HIP to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n    // The result is stored in output.\n    //\n    // Hint: Use the __ballot() intrinsic to compute the logical AND reduction of the vector of bools x.\n    // Hint: Use the __popc() intrinsic to compute the number of set bits in a 32-bit integer.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical AND reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical AND reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical OR reduction of the vector of bools x.\n    // Hint: Use the __shfl_down() intrinsic to compute the logical XOR reduction of the vector of bools x.",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = x[i];\n  while (i < N) {\n    i += gridSize;\n    sdata[tid] = sdata[tid] ^ x[i];\n  }\n  __syncthreads();\n  if (blockDim.x >= 512) {\n    if (tid < 256) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 256];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 256) {\n    if (tid < 128) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 128];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 128) {\n    if (tid < 64) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 64];\n    }\n    __syncthreads();\n  }\n  if (tid < 32) {\n    warpReduce(sdata, tid);\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n  // Use the __syncthreads() intrinsic to synchronize threads in the block.\n  // Use the atomicXor() intrinsic to atomically update the value pointed to by output.\n  // Use the __ballot() intrinsic to compute the logical XOR reduction of the vector of bools x.\n  // Use the __popc() intrinsic to count the number of bits set in the result of the logical XOR reduction.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result of the logical XOR reduction to all threads in the block.\n  // Use the __shfl_sync() intrinsic to broadcast the result",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n  __shared__ double sdata[1024];\n  double temp = 0;\n  for (int i = tid; i < N; i += stride) {\n    temp ^= x[i];\n  }\n  sdata[tid] = temp;\n  __syncthreads();\n  int i = blockDim.x / 2;\n  while (i!= 0) {\n    if (tid < i) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + i];\n    }\n    __syncthreads();\n    i /= 2;\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int gridSize = blockDim.x * gridDim.x;\n  bool myResult = false;\n\n  while (i < N) {\n    myResult = myResult ^ x[i];\n    i += gridSize;\n  }\n  sdata[tid] = myResult;\n  __syncthreads();\n\n  if (blockDim.x >= 512) {\n    if (tid < 256) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 256];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 256) {\n    if (tid < 128) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 128];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 128) {\n    if (tid < 64) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 64];\n    }\n    __syncthreads();\n  }\n  if (tid < 32) {\n    warpReduceXOR(sdata, tid);\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  bool result = false;\n  for (int i = tid; i < N; i += stride) {\n    result = result ^ x[i];\n  }\n  atomicXor(output, result);\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction of the vector x.\n  // Use the atomicOr function to compute the logical XOR reduction",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n  bool result = false;\n  for (int i = tid; i < N; i += stride) {\n    result = result ^ x[i];\n  }\n  atomicXor(output, result);\n}",
            "// TODO: Implement this function\n}",
            "__shared__ bool sdata[BLOCK_SIZE];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n\n  bool myResult = false;\n  while (i < N) {\n    myResult = myResult ^ x[i];\n    i += gridSize;\n  }\n\n  sdata[tid] = myResult;\n  __syncthreads();\n\n  if (BLOCK_SIZE >= 512) {\n    if (tid < 256) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 256];\n    }\n    __syncthreads();\n  }\n  if (BLOCK_SIZE >= 256) {\n    if (tid < 128) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 128];\n    }\n    __syncthreads();\n  }\n  if (BLOCK_SIZE >= 128) {\n    if (tid < 64) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 64];\n    }\n    __syncthreads();\n  }\n\n  if (tid < 32) {\n    warpReduceLogicalXOR(sdata, tid);\n  }\n\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  bool result = false;\n  for (int i = 0; i < x.size(); i++) {\n    result = result ^ x[i];\n  }\n\n  return result;\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  int xor_result = 0;\n  for (int i = 0; i < x.size(); i++) {\n    xor_result = xor_result ^ x[i];\n  }\n\n  int result;\n  MPI_Reduce(&xor_result, &result, 1, MPI_INT, MPI_BXOR, 0, MPI_COMM_WORLD);\n\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n    return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  // Hint: Use MPI_Reduce\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  // Hint: Use MPI_Comm_size\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_BOTTOM\n  // Hint: Use MPI_LOR\n  // Hint: Use MPI_Comm_rank\n  //",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  bool result = false;\n  for (int i = 0; i < x.size(); i++) {\n    result ^= x[i];\n  }\n  MPI_Allreduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n    return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  bool result = false;\n  return result;\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  bool result = false;\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    bool result = false;\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            result = result ^ x[i];\n        }\n    }\n    MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  bool result = false;\n  for (int i = 0; i < x.size(); i++) {\n    result = result ^ x[i];\n  }\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement\n    return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement\n  bool result = false;\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    bool result = false;\n    int count = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i]) {\n            count++;\n        }\n    }\n    int recv_count;\n    MPI_Reduce(&count, &recv_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        if (recv_count % 2 == 0) {\n            result = false;\n        } else {\n            result = true;\n        }\n    }\n    return result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n\n  return false;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    bool result = false;\n    int count = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i]) {\n            count++;\n        }\n    }\n    int recv_count;\n    MPI_Reduce(&count, &recv_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        result = (recv_count % 2 == 1);\n    }\n    return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  bool result = false;\n  MPI_Reduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: your code here\n    bool result = false;\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            result = result ^ x[i];\n        }\n    }\n    MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    bool result = false;\n    MPI_Allreduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n    return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  bool result = false;\n  MPI_Allreduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use CUDA to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, double *output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = false;\n  while (i < N) {\n    sdata[tid] = sdata[tid] ^ x[i];\n    i += gridSize;\n  }\n  __syncthreads();\n  if (blockDim.x >= 512) {\n    if (tid < 256) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 256];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 256) {\n    if (tid < 128) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 128];\n    }\n    __syncthreads();\n  }\n  if (blockDim.x >= 128) {\n    if (tid < 64) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + 64];\n    }\n    __syncthreads();\n  }\n  if (tid < 32) {\n    warpReduceLogicalXOR(sdata, tid);\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nthreads = blockDim.x;\n    int nblocks = gridDim.x;\n\n    __shared__ bool sdata[1024];\n\n    int i = tid + bid * nthreads;\n    bool my_xor = false;\n    while (i < N) {\n        my_xor = my_xor ^ x[i];\n        i += nthreads * nblocks;\n    }\n    sdata[tid] = my_xor;\n    __syncthreads();\n\n    int j = nthreads / 2;\n    while (j > 0) {\n        if (tid < j) {\n            sdata[tid] = sdata[tid] ^ sdata[tid + j];\n        }\n        __syncthreads();\n        j /= 2;\n    }\n\n    if (tid == 0) {\n        output[bid] = sdata[0];\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = x[i];\n  while (i < N) {\n    if (i + gridSize < N) {\n      sdata[tid] = sdata[tid] ^ x[i + gridSize];\n    }\n    __syncthreads();\n    gridSize /= 2;\n    if (tid < gridSize) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + gridSize];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    *output = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n    // Use the __syncthreads() intrinsic to synchronize threads in a block\n    // Use the atomicXor() intrinsic to perform an atomic logical XOR operation on a double\n    // Use the atomicAdd() intrinsic to perform an atomic addition operation on a double\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform an addition reduction across threads in a warp\n    // Use the __shfl_down_sync() intrinsic to perform a logical XOR reduction across threads in a warp\n    // Use the __sh",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int blockSize = blockDim.x;\n    int gridSize = gridDim.x;\n\n    __shared__ bool sdata[1024];\n    int tid_in_block = tid % blockSize;\n    int bid_in_grid = bid % gridSize;\n    int index = bid_in_grid * blockSize + tid_in_block;\n\n    bool my_data = false;\n    if (index < N) {\n        my_data = x[index];\n    }\n\n    sdata[tid_in_block] = my_data;\n    __syncthreads();\n\n    for (unsigned int s = blockSize / 2; s > 0; s >>= 1) {\n        if (tid_in_block < s) {\n            sdata[tid_in_block] = sdata[tid_in_block] ^ sdata[tid_in_block + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid_in_block == 0) {\n        output[bid_in_grid] = sdata[0];\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Use an atomic operation to update the output value\n    // Use a for loop to iterate over the values in x\n    // Use a shared memory array to store intermediate results\n    // Use a __syncthreads() to synchronize threads\n    // Use a __ballot_sync() to compute intermediate results\n    // Use an __atomic_fetch_or() to update the output value\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = x[i];\n  __syncthreads();\n  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + s];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n  __shared__ bool shared[1024];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int i = bid * blockDim.x + tid;\n  int lane = i % warpSize;\n  int wid = i / warpSize;\n  int numThreads = blockDim.x * gridDim.x;\n  int numWarps = numThreads / warpSize;\n  bool result = false;\n\n  for (int j = i; j < N; j += numThreads) {\n    bool temp = x[j];\n    result = result ^ temp;\n  }\n\n  shared[tid] = result;\n  __syncthreads();\n\n  if (wid == 0) {\n    for (int j = 1; j < numWarps; j++) {\n      result = result ^ shared[j * warpSize];\n    }\n    output[0] = result;\n  }\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int gsize = gridDim.x;\n    __shared__ bool sdata[1024];\n    int i = tid + bid * bsize;\n    bool myData = false;\n    while (i < N) {\n        myData = myData ^ x[i];\n        i += bsize * gsize;\n    }\n    sdata[tid] = myData;\n    __syncthreads();\n    if (bsize >= 512) {\n        if (tid < 256) {\n            sdata[tid] = sdata[tid] ^ sdata[tid + 256];\n        }\n        __syncthreads();\n    }\n    if (bsize >= 256) {\n        if (tid < 128) {\n            sdata[tid] = sdata[tid] ^ sdata[tid + 128];\n        }\n        __syncthreads();\n    }\n    if (bsize >= 128) {\n        if (tid < 64) {\n            sdata[tid] = sdata[tid] ^ sdata[tid + 64];\n        }\n        __syncthreads();\n    }\n    if (tid < 32) {\n        warpReduceLogicalXOR(sdata, tid);\n    }\n    if (tid == 0) {\n        output[bid] = sdata[0];\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  __shared__ bool sdata[BLOCK_SIZE];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = x[i];\n  while (i < N) {\n    if (i + gridSize < N) {\n      sdata[tid] = sdata[tid] ^ x[i + gridSize];\n    }\n    __syncthreads();\n    gridSize /= 2;\n    if (tid < gridSize) {\n      sdata[tid] = sdata[tid] ^ sdata[tid + gridSize];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    *output = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Use an atomic operation to update the output value\n    // Use a for loop to compute the logical XOR reduction of the vector x\n    // Use a shared memory array to store intermediate values\n    // Use a block-wide synchronization to make sure all threads in the block have finished\n    // Use a grid-wide synchronization to make sure all blocks have finished\n    // Use atomicAdd to update the output value\n    // Use a grid-wide synchronization to make sure all blocks have finished\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "21_reduce_xor",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI and OpenMP to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk + std::min(rank, remainder);\n  int end = (rank + 1) * chunk + std::min(rank + 1, remainder);\n  bool result = false;\n  for (int i = start; i < end; ++i) {\n    result ^= x[i];\n  }\n  std::vector<bool> results(size);\n  MPI_Allgather(&result, 1, MPI_C_BOOL, results.data(), 1, MPI_C_BOOL, MPI_COMM_WORLD);\n  result = false;\n  for (auto r : results) {\n    result ^= r;\n  }\n  return result;\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  bool result = false;\n  for (int i = start; i < end; i++) {\n    result = result ^ x[i];\n  }\n  MPI_Reduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int n = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  // TODO: Implement this function\n\n  return false;\n}",
            "// TODO\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  bool result = false;\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  for (int i = start; i < end; i++) {\n    result ^= x[i];\n  }\n\n  // TODO: Your code here\n  MPI_Allreduce(&result, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}",
            "int n = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  // TODO: Your code here\n  bool result = false;\n  int chunk = n / nranks;\n  int remainder = n % nranks;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == nranks - 1) {\n    end += remainder;\n  }\n  std::vector<bool> local_result(end - start);\n  for (int i = start; i < end; i++) {\n    local_result[i - start] = x[i];\n  }\n  std::vector<bool> global_result(n);\n  MPI_Allreduce(local_result.data(), global_result.data(), n, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return global_result[0];\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n\n  return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  bool result = false;\n  int i;\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  for (i = start; i < end; i++) {\n    result ^= x[i];\n  }\n  std::vector<bool> result_vec(1, result);\n  MPI_Reduce(MPI_IN_PLACE, &result_vec[0], 1, MPI_C_BOOL, MPI_LXOR, 0,\n             MPI_COMM_WORLD);\n  return result_vec[0];\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  std::vector<bool> local_result(chunk + (rank < remainder));\n  #pragma omp parallel for\n  for (int i = 0; i < chunk + (rank < remainder); i++) {\n    local_result[i] = x[i * size + rank];\n  }\n  std::vector<bool> global_result(chunk + (rank < remainder));\n  MPI_Reduce(local_result.data(), global_result.data(), chunk + (rank < remainder), MPI_C_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      for (int j = 0; j < chunk + (i < remainder); j++) {\n        global_result[j] = global_result[j] ^ global_result[chunk + (i < remainder) + j];\n      }\n    }\n  }\n  return global_result[0];\n}",
            "int n = x.size();\n    int n_threads = omp_get_max_threads();\n    int n_ranks = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    // TODO: Implement this function\n    bool result = false;\n    return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  bool result = false;\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_extra;\n  }\n  std::vector<bool> x_local(x.begin() + start, x.begin() + end);\n  result = std::accumulate(x_local.begin(), x_local.end(), result, std::logical_xor<bool>());\n\n  MPI_Allreduce(MPI_IN_PLACE, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return result;\n}",
            "int n = x.size();\n  int nthreads = omp_get_max_threads();\n  int nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  // TODO: Your code here\n\n  return false;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  bool result = false;\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<bool> local_result(end - start);\n  for (int i = start; i < end; i++) {\n    local_result[i - start] = x[i];\n  }\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Status status;\n      MPI_Recv(&local_result[0], chunk, MPI_CXX_BOOL, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Send(&local_result[0], chunk, MPI_CXX_BOOL, 0, 0, MPI_COMM_WORLD);\n  }\n  if (rank == 0) {\n    for (int i = 0; i < local_result.size(); i++) {\n      result ^= local_result[i];\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n  std::vector<bool> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_C_BOOL, x_local.data(), n_local, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < n_local; ++i) {\n    result ^= x_local[i];\n  }\n  MPI_Allreduce(MPI_IN_PLACE, &result, 1, MPI_C_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO\n  return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_threads = omp_get_max_threads();\n  int chunk = x.size() / num_threads;\n  int remainder = x.size() % num_threads;\n\n  std::vector<bool> local_result(chunk + 1);\n  std::vector<bool> global_result(chunk + 1);\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int tid = omp_get_thread_num();\n    int start = tid * chunk;\n    int end = start + chunk;\n    if (tid == num_threads - 1) {\n      end += remainder;\n    }\n    for (int i = start; i < end; i++) {\n      local_result[i] = x[i];\n    }\n    for (int i = start; i < end; i++) {\n      local_result[i] = local_result[i] ^ local_result[i + 1];\n    }\n    local_result[end] = local_result[start];\n\n    MPI_Gather(&local_result[start], chunk + 1, MPI_C_BOOL, &global_result[0], chunk + 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      global_result[0] = global_result[0] ^ global_result[i];\n    }\n  }\n\n  return global_result[0];\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n  return 0;\n}",
            "int result = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i, int& lresult) {\n      if (x(i) % 2 == 1 && x(i) < lresult) {\n        lresult = x(i);\n      }\n    },\n    Kokkos::Min<int>(result)\n  );\n  return result;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // Create a parallel_reduce to find the smallest odd number in the vector.\n  // The lambda function is called for each element of the vector.\n  // The lambda function is called with two arguments:\n  //   - the index of the element in the vector\n  //   - a reference to the smallest odd number found so far\n  // The lambda function must return the smallest odd number found so far.\n  int smallestOdd = 0;\n  Kokkos::parallel_reduce(\n    Policy(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, int& smallestOdd) {\n      // TODO: fill in the body of the lambda function\n    },\n    smallestOdd);\n\n  // Return the smallest odd number found\n  return smallestOdd;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "int result = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lresult) {\n      if (x(i) % 2!= 0 && x(i) < lresult) {\n        lresult = x(i);\n      }\n    },\n    Kokkos::Min<int>(result)\n  );\n  return result;\n}",
            "int result = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lresult) {\n      if (x(i) % 2 == 1 && x(i) < lresult) {\n        lresult = x(i);\n      }\n    },\n    Kokkos::Min<int>(result)\n  );\n  return result;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Member = Kokkos::MemberType<ExecutionSpace>;\n\n  // Create a Kokkos view to hold the result.\n  Kokkos::View<int, Kokkos::HostSpace> result(\"result\");\n\n  // Set the initial value of the result to a large number.\n  Kokkos::deep_copy(result, std::numeric_limits<int>::max());\n\n  // Create a Kokkos parallel_for loop to find the smallest odd number.\n  Kokkos::parallel_for(\n      \"smallestOdd\",\n      Policy(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, Member) {\n        // Check if the current value is odd.\n        if (x(i) % 2 == 1) {\n          // Atomically compare and swap the result with the current value.\n          // If the current value is smaller than the result, then the result\n          // is updated with the current value.\n          Kokkos::atomic_compare_exchange_strong(result, i, x(i));\n        }\n      });\n\n  // Wait for the parallel_for loop to finish.\n  Kokkos::fence();\n\n  // Copy the result to the host.\n  int result_host = 0;\n  Kokkos::deep_copy(result_host, result);\n\n  // Return the result.\n  return result_host;\n}",
            "int result = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lresult) {\n      if (x(i) % 2 == 1 && x(i) < lresult) {\n        lresult = x(i);\n      }\n    },\n    Kokkos::Min<int>(result)\n  );\n  return result;\n}",
            "int result;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lmin) {\n      if (x(i) % 2!= 0) {\n        lmin = std::min(lmin, x(i));\n      }\n    },\n    Kokkos::Min<int>(result));\n  return result;\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*, Kokkos::HostSpace> y(\"y\", n);\n  Kokkos::parallel_for(\n      \"smallestOdd\", n, KOKKOS_LAMBDA(int i) { y(i) = x(i) % 2; });\n  Kokkos::fence();\n  int smallestOdd = 0;\n  for (int i = 0; i < n; i++) {\n    if (y(i) == 1) {\n      smallestOdd = x(i);\n      break;\n    }\n  }\n  return smallestOdd;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Member = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // Create a Kokkos view to hold the result.\n  Kokkos::View<int, Kokkos::HostSpace> result(\"result\");\n\n  // Set the initial value of the result to the largest possible value.\n  Kokkos::deep_copy(result, std::numeric_limits<int>::max());\n\n  // Launch a parallel Kokkos kernel to compute the smallest odd number.\n  Kokkos::parallel_for(\n      \"smallestOdd\",\n      Policy(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        // Check if the current value is odd and smaller than the current result.\n        if (x(i) % 2 == 1 && x(i) < result()) {\n          // Set the result to the current value.\n          result() = x(i);\n        }\n      });\n\n  // Wait for the kernel to finish.\n  Kokkos::fence();\n\n  // Copy the result back to the host.\n  int result_host;\n  Kokkos::deep_copy(result_host, result);\n\n  return result_host;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int min = 0;\n    int min_index = 0;\n    int n = x.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n            min_index = i;\n        }\n    }\n\n    return min;\n}",
            "int result = 0;\n  int min = 1000000000;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n    }\n  }\n  return min;\n}",
            "int min = 0;\n    #pragma omp parallel for reduction(min:min)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 1) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int min = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 1 && x[i] < min) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int result = 0;\n  int min = 1000000000;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      if (x[i] < min) {\n        min = x[i];\n      }\n    }\n  }\n  return min;\n}",
            "int min = x[0];\n    int i;\n    #pragma omp parallel for private(i)\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int result = 0;\n  #pragma omp parallel for reduction(min:result)\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 1) {\n      result = x[i];\n    }\n  }\n  return result;\n}",
            "int result = 0;\n  #pragma omp parallel for reduction(min:result)\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] % 2!= 0) {\n      result = std::min(result, x[i]);\n    }\n  }\n  return result;\n}",
            "int min = 0;\n    #pragma omp parallel for reduction(min:min)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int minOdd = 0;\n    #pragma omp parallel for reduction(min:minOdd)\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 1) {\n            minOdd = std::min(minOdd, x[i]);\n        }\n    }\n    return minOdd;\n}",
            "int result = 0;\n  #pragma omp parallel for reduction(min:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0) {\n      result = x[i];\n    }\n  }\n  return result;\n}",
            "int min = x[0];\n  int min_index = 0;\n  int n = x.size();\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n      min_index = i;\n    }\n  }\n\n  return min;\n}",
            "int min = 0;\n    int min_index = 0;\n    int n = x.size();\n    int i;\n\n    #pragma omp parallel for private(i)\n    for (i = 0; i < n; i++) {\n        if (x[i] % 2 == 1) {\n            #pragma omp critical\n            {\n                if (x[i] < min) {\n                    min = x[i];\n                    min_index = i;\n                }\n            }\n        }\n    }\n\n    return min;\n}",
            "int smallest = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 1 && x[i] < smallest) {\n            smallest = x[i];\n        }\n    }\n    return smallest;\n}",
            "int smallestOdd = 0;\n  #pragma omp parallel for reduction(min:smallestOdd)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      smallestOdd = x[i];\n    }\n  }\n  return smallestOdd;\n}",
            "int min = 0;\n  #pragma omp parallel for reduction(min:min)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n    }\n  }\n  return min;\n}",
            "int minOdd = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < minOdd) {\n            minOdd = x[i];\n        }\n    }\n    return minOdd;\n}",
            "int minOdd = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < minOdd) {\n            minOdd = x[i];\n        }\n    }\n    return minOdd;\n}",
            "int min = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 1 && x[i] < min) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int minOdd = 0;\n  #pragma omp parallel for reduction(min:minOdd)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0) {\n      minOdd = x[i];\n    }\n  }\n  return minOdd;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallestOdd == 0) {\n                smallestOdd = x[i];\n            } else if (x[i] < smallestOdd) {\n                smallestOdd = x[i];\n            }\n        }\n    }\n    return smallestOdd;\n}",
            "int minOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (minOdd == 0) {\n                minOdd = x[i];\n            } else if (x[i] < minOdd) {\n                minOdd = x[i];\n            }\n        }\n    }\n    return minOdd;\n}",
            "int smallest = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallest == 0) {\n                smallest = x[i];\n            } else if (x[i] < smallest) {\n                smallest = x[i];\n            }\n        }\n    }\n    return smallest;\n}",
            "int smallest = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallest == 0) {\n                smallest = x[i];\n            } else if (x[i] < smallest) {\n                smallest = x[i];\n            }\n        }\n    }\n    return smallest;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallestOdd == 0) {\n                smallestOdd = x[i];\n            } else if (x[i] < smallestOdd) {\n                smallestOdd = x[i];\n            }\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallestOdd == 0) {\n                smallestOdd = x[i];\n            } else if (x[i] < smallestOdd) {\n                smallestOdd = x[i];\n            }\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0) {\n      if (smallestOdd == 0) {\n        smallestOdd = x[i];\n      } else if (x[i] < smallestOdd) {\n        smallestOdd = x[i];\n      }\n    }\n  }\n  return smallestOdd;\n}",
            "int min = INT_MAX;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n    }\n  }\n  return min;\n}",
            "int smallestOdd = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0) {\n      if (smallestOdd == 0) {\n        smallestOdd = x[i];\n      } else if (x[i] < smallestOdd) {\n        smallestOdd = x[i];\n      }\n    }\n  }\n  return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallestOdd == 0) {\n                smallestOdd = x[i];\n            } else if (x[i] < smallestOdd) {\n                smallestOdd = x[i];\n            }\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallestOdd == 0) {\n                smallestOdd = x[i];\n            } else if (x[i] < smallestOdd) {\n                smallestOdd = x[i];\n            }\n        }\n    }\n    return smallestOdd;\n}",
            "int min = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n        }\n    }\n    return min;\n}",
            "int smallestOdd = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n      smallestOdd = x[i];\n    }\n  }\n  return smallestOdd;\n}",
            "int smallestOdd = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    return smallestOdd;\n}",
            "int smallest = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (smallest == 0) {\n                smallest = x[i];\n            } else if (x[i] < smallest) {\n                smallest = x[i];\n            }\n        }\n    }\n    return smallest;\n}",
            "int smallest = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (i == 0) {\n                smallest = x[i];\n            } else if (x[i] < smallest) {\n                smallest = x[i];\n            }\n        }\n    }\n    return smallest;\n}",
            "int smallest = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0) {\n            if (i == 0) {\n                smallest = x[i];\n            } else if (x[i] < smallest) {\n                smallest = x[i];\n            }\n        }\n    }\n    return smallest;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "hip",
        "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use AMD HIP to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] % 2 == 1 && x[tid] < *smallest) {\n      *smallest = x[tid];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2!= 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] % 2 == 1) {\n      atomicMin(smallest, x[tid]);\n    }\n  }\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] % 2!= 0 && x[tid] < *smallest) {\n      *smallest = x[tid];\n    }\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] % 2!= 0 && x[tid] < *smallest) {\n      *smallest = x[tid];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2!= 0 && x[tid] < *smallest) {\n            *smallest = x[tid];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2!= 0 && x[tid] < *smallest) {\n            *smallest = x[tid];\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (x[tid] % 2!= 0) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int smallest_odd = 0;\n  for (int i = idx; i < N; i += stride) {\n    if (x[i] % 2!= 0 && x[i] < smallest_odd) {\n      smallest_odd = x[i];\n    }\n  }\n  atomicMin(smallest, smallest_odd);\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int temp = 0;\n  if (tid < N) {\n    if (x[tid] % 2!= 0) {\n      temp = x[tid];\n    }\n  }\n  __syncthreads();\n  atomicMin(smallest, temp);\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N && x[tid] % 2!= 0) {\n    atomicMin(smallest, x[tid]);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int min = INT_MAX;\n  for (int i = tid; i < N; i += stride) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n    }\n  }\n  atomicMin(smallest, min);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2!= 0) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] % 2!= 0) {\n      atomicMin(smallest, x[tid]);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2!= 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] % 2!= 0) {\n      atomicMin(smallest, x[tid]);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    int smallest_odd = 0;\n\n    for (int i = tid; i < N; i += stride) {\n        if (x[i] % 2!= 0) {\n            smallest_odd = x[i];\n            break;\n        }\n    }\n\n    atomicMin(smallest, smallest_odd);\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int min = 0;\n  int min_rank = 0;\n  int local_min = 0;\n  int local_min_rank = 0;\n  int local_size = x.size() / size;\n  int local_start = rank * local_size;\n  int local_end = (rank + 1) * local_size;\n  if (rank == size - 1) {\n    local_end = x.size();\n  }\n  for (int i = local_start; i < local_end; i++) {\n    if (x[i] % 2!= 0 && x[i] < local_min) {\n      local_min = x[i];\n      local_min_rank = rank;\n    }\n  }\n  MPI_Allreduce(&local_min, &min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_rank, &min_rank, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  if (rank == min_rank) {\n    return min;\n  } else {\n    return 0;\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int min = 0;\n  int min_rank = 0;\n  int min_val = 0;\n\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] % 2!= 0) {\n        min = i;\n        min_val = x[i];\n        break;\n      }\n    }\n  }\n\n  MPI_Bcast(&min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&min_val, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  return min_val;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int min = INT_MAX;\n  int min_rank = -1;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n      min_rank = rank;\n    }\n  }\n\n  int result;\n  MPI_Reduce(&min, &result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_min = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2!= 0 && x[i] < local_min) {\n            local_min = x[i];\n        }\n    }\n\n    int global_min = local_min;\n    MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_min = INT_MAX;\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 1 && x[i] < local_min) {\n            local_min = x[i];\n        }\n    }\n\n    int global_min = INT_MAX;\n    MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min;\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_min = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 1 && x[i] < local_min) {\n            local_min = x[i];\n        }\n    }\n\n    int global_min = local_min;\n    MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int smallestOdd = 0;\n  int smallestOdd_rank = 0;\n  int smallestOdd_size = 0;\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2!= 0) {\n      if (smallestOdd == 0) {\n        smallestOdd = x[i];\n        smallestOdd_rank = rank;\n        smallestOdd_size = 1;\n      } else if (smallestOdd > x[i]) {\n        smallestOdd = x[i];\n        smallestOdd_rank = rank;\n        smallestOdd_size = 1;\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> smallestOdd_all(size);\n    MPI_Gather(&smallestOdd, 1, MPI_INT, smallestOdd_all.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n    smallestOdd = smallestOdd_all[0];\n    for (int i = 1; i < size; i++) {\n      if (smallestOdd > smallestOdd_all[i]) {\n        smallestOdd = smallestOdd_all[i];\n      }\n    }\n  } else {\n    MPI_Gather(&smallestOdd, 1, MPI_INT, NULL, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n\n  return smallestOdd;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement\n\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    return 0;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: your code here\n    int smallestOdd = 0;\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] % 2!= 0) {\n                smallestOdd = x[i];\n                break;\n            }\n        }\n    }\n\n    MPI_Bcast(&smallestOdd, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    return smallestOdd;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int smallestOdd = 0;\n    int smallestOddRank = 0;\n\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] % 2!= 0 && x[i] < smallestOdd) {\n                smallestOdd = x[i];\n                smallestOddRank = 0;\n            }\n        }\n    }\n\n    MPI_Bcast(&smallestOdd, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&smallestOddRank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    return smallestOdd;\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "cuda",
        "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 1 && x[tid] < *smallest) {\n            *smallest = x[tid];\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2!= 0) {\n        *smallest = x[tid];\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N && x[tid] % 2!= 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 1 && x[tid] < *smallest) {\n            *smallest = x[tid];\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 1 && x[idx] < *smallest) {\n        *smallest = x[idx];\n    }\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2!= 0 && x[idx] < *smallest) {\n            *smallest = x[idx];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int mySmallest = x[tid];\n    for (int i = 0; i < N; i++) {\n        if (x[i] % 2!= 0 && x[i] < mySmallest) {\n            mySmallest = x[i];\n        }\n    }\n    *smallest = mySmallest;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] % 2 == 1 && x[i] < *smallest) {\n            *smallest = x[i];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int i = tid;\n    int min = x[0];\n    while (i < N) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n        }\n        i += blockDim.x;\n    }\n    *smallest = min;\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    int smallest_odd = 0;\n\n    if (i < N) {\n        if (x[i] % 2!= 0) {\n            if (i == 0) {\n                smallest_odd = x[i];\n            } else {\n                if (x[i] < smallest_odd) {\n                    smallest_odd = x[i];\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        *smallest = smallest_odd;\n    }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] % 2!= 0 && x[i] < *smallest) {\n            *smallest = x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N && x[i] % 2!= 0) {\n        atomicMin(smallest, x[i]);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2!= 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] % 2!= 0 && x[i] < *smallest) {\n            *smallest = x[i];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] % 2!= 0 && x[i] < *smallest) {\n            *smallest = x[i];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    int temp = 0;\n    if (i < N) {\n        if (x[i] % 2!= 0) {\n            temp = x[i];\n        }\n    }\n    atomicMin(smallest, temp);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N && x[tid] % 2 == 1) {\n        atomicMin(smallest, x[tid]);\n    }\n}",
            "int tid = threadIdx.x;\n    int i = tid;\n    int smallest_odd = 0;\n    while (i < N) {\n        if (x[i] % 2!= 0 && x[i] < smallest_odd) {\n            smallest_odd = x[i];\n        }\n        i += blockDim.x;\n    }\n    atomicMin(smallest, smallest_odd);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2!= 0 && x[tid] < *smallest) {\n            *smallest = x[tid];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "24_reduce_smallest_odd_number",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = x.size() / num_threads;\n    int num_remain = x.size() % num_threads;\n    int num_per_rank = num_per_thread + (rank < num_remain? 1 : 0);\n    int start = rank * num_per_thread + std::min(rank, num_remain);\n    int end = start + num_per_rank;\n\n    int min_odd = INT_MAX;\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2!= 0 && x[i] < min_odd) {\n            min_odd = x[i];\n        }\n    }\n\n    int min_odd_all;\n    MPI_Allreduce(&min_odd, &min_odd_all, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return min_odd_all;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    int my_min = std::numeric_limits<int>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank; i++) {\n        if (x[i] % 2 == 1 && x[i] < my_min) {\n            my_min = x[i];\n        }\n    }\n\n    if (rank < n_remainder) {\n        if (x[n_per_rank + rank] % 2 == 1 && x[n_per_rank + rank] < my_min) {\n            my_min = x[n_per_rank + rank];\n        }\n    }\n\n    int min = my_min;\n    MPI_Allreduce(&my_min, &min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return min;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int local_n = n / size;\n  int local_start = rank * local_n;\n  int local_end = local_start + local_n;\n  int local_smallest = x[local_start];\n  for (int i = local_start + 1; i < local_end; i++) {\n    if (x[i] < local_smallest && x[i] % 2!= 0) {\n      local_smallest = x[i];\n    }\n  }\n\n  int global_smallest;\n  MPI_Reduce(&local_smallest, &global_smallest, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return global_smallest;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n  std::vector<int> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_INT, x_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n  int min = 0;\n  if (rank == 0) {\n    min = x_local[0];\n  }\n  for (int i = 1; i < n_local; ++i) {\n    if (x_local[i] % 2 == 1 && x_local[i] < min) {\n      min = x_local[i];\n    }\n  }\n  MPI_Reduce(&min, &min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return min;\n}",
            "int n = x.size();\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = n / num_threads;\n    int num_extra = n % num_threads;\n\n    int my_min = std::numeric_limits<int>::max();\n    int my_min_index = 0;\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        int thread_id = omp_get_thread_num();\n        int start = thread_id * num_per_thread;\n        int end = start + num_per_thread;\n        if (thread_id == num_threads - 1) {\n            end += num_extra;\n        }\n\n        for (int i = start; i < end; i++) {\n            if (x[i] % 2 == 1 && x[i] < my_min) {\n                my_min = x[i];\n                my_min_index = i;\n            }\n        }\n    }\n\n    int min_index = my_min_index;\n    MPI_Allreduce(&my_min_index, &min_index, 1, MPI_INT, MPI_MINLOC, MPI_COMM_WORLD);\n\n    return x[min_index];\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_remainder;\n  }\n\n  int min = INT_MAX;\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2!= 0 && x[i] < min) {\n      min = x[i];\n    }\n  }\n\n  int min_global;\n  MPI_Allreduce(&min, &min_global, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return min_global;\n}",
            "int n = x.size();\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int local_min = 0;\n  int global_min = 0;\n  int local_min_index = 0;\n  int global_min_index = 0;\n  int local_min_index_rank = 0;\n  int global_min_index_rank = 0;\n  int local_min_index_rank_temp = 0;\n  int global_min_index_rank_temp = 0;\n  int local_min_index_rank_temp_temp = 0;\n  int global_min_index_rank_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int local_min_index_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n  int global_min_index_rank_temp_temp_temp_temp_temp",
            "int n = x.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    int local_min = 1000000000;\n    int global_min = 1000000000;\n    int local_min_index = 0;\n    int global_min_index = 0;\n    int local_min_rank = 0;\n    int global_min_rank = 0;\n    int local_min_size = 0;\n    int global_min_size = 0;\n    int local_min_count = 0;\n    int global_min_count = 0;\n    int local_min_count_rank = 0;\n    int global_min_count_rank = 0;\n    int local_min_count_size = 0;\n    int global_min_count_size = 0;\n    int local_min_count_rank_size = 0;\n    int global_min_count_rank_size = 0;\n    int local_min_count_rank_size_count = 0;\n    int global_min_count_rank_size_count = 0;\n    int local_min_count_rank_size_count_rank = 0;\n    int global_min_count_rank_size_count_rank = 0;\n    int local_min_count_rank_size_count_rank_min = 0;\n    int global_min_count_rank_size_count_rank_min = 0;\n    int local_min_count_rank_size_count_rank_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min_min_min = 0;\n    int global_min_count_rank_size_count_rank_min_min_min_min_min_min_min_min_min_min = 0;\n    int local_min_count_rank_size_count_rank_min_min_min_min_min_min_min",
            "int n = x.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunkSize = n / size;\n    int remainder = n % size;\n\n    int start = rank * chunkSize;\n    int end = start + chunkSize;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    int smallest = std::numeric_limits<int>::max();\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2!= 0 && x[i] < smallest) {\n            smallest = x[i];\n        }\n    }\n\n    int result = 0;\n    MPI_Allreduce(&smallest, &result, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int min = 0;\n  int min_rank = 0;\n  int local_min = 0;\n  int local_min_rank = 0;\n  int local_min_size = 0;\n  int local_min_size_rank = 0;\n  int local_min_size_rank_min = 0;\n  int local_min_size_rank_min_min = 0;\n  int local_min_size_rank_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int local_min_size_rank_min_min_min_min_min_min_min_min",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_remainder;\n  }\n\n  int min_odd = std::numeric_limits<int>::max();\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2 == 1 && x[i] < min_odd) {\n      min_odd = x[i];\n    }\n  }\n\n  int min_odd_global;\n  MPI_Allreduce(&min_odd, &min_odd_global, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_odd_global;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n\n  int my_start = rank * n_per_rank;\n  int my_end = my_start + n_per_rank;\n  if (rank == size - 1) {\n    my_end += n_rem;\n  }\n\n  int my_smallest = x[my_start];\n  for (int i = my_start + 1; i < my_end; i++) {\n    if (x[i] < my_smallest && x[i] % 2!= 0) {\n      my_smallest = x[i];\n    }\n  }\n\n  int global_smallest = my_smallest;\n  MPI_Allreduce(&my_smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_smallest;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int min = 0;\n  int min_rank = 0;\n  int min_value = 0;\n  int local_min = 0;\n  int local_min_rank = 0;\n  int local_min_value = 0;\n  int local_min_value_rank = 0;\n\n  int local_min_value_rank_temp = 0;\n  int local_min_value_temp = 0;\n\n  int local_min_value_rank_temp2 = 0;\n  int local_min_value_temp2 = 0;\n\n  int local_min_value_rank_temp3 = 0;\n  int local_min_value_temp3 = 0;\n\n  int local_min_value_rank_temp4 = 0;\n  int local_min_value_temp4 = 0;\n\n  int local_min_value_rank_temp5 = 0;\n  int local_min_value_temp5 = 0;\n\n  int local_min_value_rank_temp6 = 0;\n  int local_min_value_temp6 = 0;\n\n  int local_min_value_rank_temp7 = 0;\n  int local_min_value_temp7 = 0;\n\n  int local_min_value_rank_temp8 = 0;\n  int local_min_value_temp8 = 0;\n\n  int local_min_value_rank_temp9 = 0;\n  int local_min_value_temp9 = 0;\n\n  int local_min_value_rank_temp10 = 0;\n  int local_min_value_temp10 = 0;\n\n  int local_min_value_rank_temp11 = 0;\n  int local_min_value_temp11 = 0;\n\n  int local_min_value_rank_temp12 = 0;\n  int local_min_value_temp12 = 0;\n\n  int local_min_value_rank_temp13 = 0;\n  int local_min_value_temp13 = 0;\n\n  int local_min_value_rank_temp14 = 0;\n  int local_min_value_temp14 = 0;\n\n  int local_min_value_rank_temp15 = 0;\n  int local_min_value_temp15 = 0;\n\n  int local_min_value_rank_temp16 = 0;\n  int local_min_value_temp16 = 0;\n\n  int local_min_value_rank_temp17 = 0;\n  int local_min_value_temp17 = 0;\n\n  int local_min_value_rank_temp18 = 0;\n  int local_min_value_temp18 = 0;\n\n  int local_min_value_rank_temp19 = 0;\n  int local_min_value_temp19 = 0;\n\n  int local_min_value_rank_temp20 = 0;\n  int local_min_value_temp20 = 0;\n\n  int local_min_value_rank_temp21 = 0;\n  int local_min_value_temp21 = 0;\n\n  int local_min_value_rank_temp22 = 0;\n  int local_min_value_temp22 = 0;\n\n  int local_min_value_rank_temp23 = 0;\n  int local_min_value_temp23 = 0;\n\n  int local_min_value_rank_temp24 = 0;\n  int local_min_value_temp24 = 0;\n\n  int local_min_value_rank_temp25 = 0;\n  int local_min_value_temp25 = 0;\n\n  int local_min_value_rank_temp26 = 0;\n  int local_min_value_temp26 = 0;",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    int my_min = INT_MAX;\n    if (rank < n_remainder) {\n        my_min = x[rank * (n_per_rank + 1)];\n    } else {\n        my_min = x[rank * n_per_rank + n_remainder];\n    }\n\n    int min = my_min;\n    MPI_Allreduce(&my_min, &min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return min;\n}",
            "int n = x.size();\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  int min = 0;\n  int min_rank = 0;\n  int min_global = 0;\n  int min_local = 0;\n  int min_local_rank = 0;\n  int min_local_global = 0;\n  int min_local_global_rank = 0;\n  int min_local_global_rank_global = 0;\n  int min_local_global_rank_global_min = 0;\n  int min_local_global_rank_global_min_min = 0;\n  int min_local_global_rank_global_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min = 0;\n  int min_local_global_rank_global_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min_min =",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_chunks = size * num_threads;\n  int chunk_size = x.size() / num_chunks;\n  int remainder = x.size() % num_chunks;\n\n  int my_min = 0;\n  int my_min_index = 0;\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_id = omp_get_thread_num();\n    int chunk_start = thread_id * chunk_size + std::min(thread_id, remainder);\n    int chunk_end = chunk_start + chunk_size + (thread_id < remainder);\n\n    int my_min = x[chunk_start];\n    int my_min_index = chunk_start;\n\n    for (int i = chunk_start; i < chunk_end; i++) {\n      if (x[i] < my_min && x[i] % 2!= 0) {\n        my_min = x[i];\n        my_min_index = i;\n      }\n    }\n  }\n\n  int global_min = my_min;\n  int global_min_index = my_min_index;\n\n  MPI_Reduce(&my_min, &global_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&my_min_index, &global_min_index, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return global_min;\n}",
            "int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_threads = omp_get_max_threads();\n\n    int num_per_thread = x.size() / num_threads;\n    int num_remainder = x.size() % num_threads;\n\n    int start = rank * num_per_thread;\n    int end = start + num_per_thread;\n    if (rank == size - 1) {\n        end += num_remainder;\n    }\n\n    int min = INT_MAX;\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2!= 0 && x[i] < min) {\n            min = x[i];\n        }\n    }\n\n    int min_global = INT_MAX;\n    MPI_Allreduce(&min, &min_global, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return min_global;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "using namespace Kokkos;\n  using namespace Kokkos::RangePolicy<Kokkos::Reduce>;\n  using namespace Kokkos::Parallel;\n\n  // TODO: Your code here\n\n}",
            "// TODO: Replace this with your code\n}",
            "const int n = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, n), KOKKOS_LAMBDA(const int i) {\n    float min = x(i);\n    for (int j = 0; j < i; j++) {\n      min = std::min(min, x(j));\n    }\n    x(i) = min;\n  });\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    for (int i = 1; i < n; ++i) {\n        float min = x[i];\n        for (int j = 0; j < i; ++j) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: Your code here\n\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 1; i < n; i++) {\n    float min = x[0];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "int n = x.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < i; j++) {\n            if (x[j] < x[i]) {\n                x[i] = x[j];\n            }\n        }\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: Your code here\n    int n = x.size();\n    #pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        for(int j = 0; j < i; j++) {\n            if(x[j] < x[i]) {\n                x[i] = x[j];\n            }\n        }\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < i; ++j) {\n            if (x[j] < x[i]) {\n                x[i] = x[j];\n            }\n        }\n    }\n}",
            "int n = x.size();\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "int n = x.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        float min = x[i];\n        for (int j = 0; j < i; ++j) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "serial",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); ++i) {\n    x[i] = std::min(x[i], x[i - 1]);\n  }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n    x[i] = std::min(x[i], x[i - 1]);\n  }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n    x[i] = std::min(x[i], x[i - 1]);\n  }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); ++i) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}",
            "for (int i = 1; i < x.size(); i++) {\n        x[i] = std::min(x[i], x[i - 1]);\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "hip",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  float min = x[i];\n  for (int j = 0; j < i; j++) {\n    min = fminf(min, x[j]);\n  }\n  x[i] = min;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  float min = x[i];\n  for (int j = 0; j < i; j++) {\n    if (x[j] < min) min = x[j];\n  }\n  x[i] = min;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    float min = x[i];\n    for (size_t j = 0; j < i; j++) {\n        if (x[j] < min) min = x[j];\n    }\n    x[i] = min;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (size_t j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  float min = x[i];\n  for (int j = 0; j < i; j++) {\n    min = min < x[j]? min : x[j];\n  }\n  x[i] = min;\n}",
            "// TODO: Implement this kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (size_t j = 0; j < i; j++) {\n            min = fminf(min, x[j]);\n        }\n        x[i] = min;\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Replace this line with your code\n    // You may need to use MPI_Send, MPI_Recv, MPI_Bcast, etc.\n    // You may need to use std::min, std::max, std::abs, etc.\n    // You may need to use std::vector::at, std::vector::size, etc.\n    // You may need to use std::swap, std::min_element, etc.\n\n    // TODO: Replace this line with your code\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int i;\n    if (rank == 0) {\n        for (i = 1; i < size; i++) {\n            MPI_Send(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (i = 0; i < x.size(); i++) {\n        if (rank == 0) {\n            x[i] = std::min(x[i], x[i - 1]);\n        } else {\n            x[i] = std::min(x[i], x[i - 1]);\n            MPI_Send(&x[i], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (i = 1; i < size; i++) {\n            MPI_Recv(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunkSize = n / size;\n  int remainder = n % size;\n\n  std::vector<float> localMin(chunkSize + 1);\n  std::vector<float> localX(chunkSize);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      localX[i % chunkSize] = x[i];\n    }\n  } else {\n    for (int i = 0; i < chunkSize; i++) {\n      localX[i] = x[i + rank * chunkSize];\n    }\n  }\n\n  for (int i = 0; i < chunkSize; i++) {\n    localMin[i] = localX[i];\n    for (int j = i + 1; j < chunkSize; j++) {\n      if (localX[j] < localMin[i]) {\n        localMin[i] = localX[j];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = localMin[i % chunkSize];\n    }\n  } else {\n    for (int i = 0; i < chunkSize; i++) {\n      x[i + rank * chunkSize] = localMin[i];\n    }\n  }\n}",
            "}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int i = rank;\n    int j = i + 1;\n    float min = x[i];\n    while (j < size) {\n        MPI_Send(&x[i], 1, MPI_FLOAT, j, 0, MPI_COMM_WORLD);\n        MPI_Recv(&min, 1, MPI_FLOAT, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        if (min < x[i]) {\n            x[i] = min;\n        }\n        i = j;\n        j = j + 1;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&min, 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (min < x[0]) {\n                x[0] = min;\n            }\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Replace this code with your solution.\n  // You may assume that size is a power of 2.\n  // You may also assume that x has size that is a multiple of size.\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n    }\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Recv(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for (int i = 0; i < rank; i++) {\n      if (x[i] < x[rank]) {\n        x[rank] = x[i];\n      }\n    }\n    MPI_Send(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Replace this line with your code\n  MPI_Allreduce(MPI_IN_PLACE, &x[0], x.size(), MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (int i = rank + 1; i < size; i++) {\n        if (x[i] < x[rank]) {\n            x[rank] = x[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: replace this with your code\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    float min = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      if (x[i] < min) {\n        min = x[i];\n      }\n    }\n    MPI_Send(&min, 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int i = 0;\n    float min = x[i];\n    for (i = 1; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n    }\n\n    MPI_Reduce(&min, &x[0], 1, MPI_FLOAT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Replace this line with your code\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Recv(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int i = 0; i < rank; i++) {\n            if (x[i] < x[rank]) {\n                x[rank] = x[i];\n            }\n        }\n        MPI_Send(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        std::vector<float> x_recv(x.size());\n        MPI_Recv(&x_recv[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int i = 0; i < x.size(); i++) {\n            if (x_recv[i] < x[i]) {\n                x[i] = x_recv[i];\n            }\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Replace this code with your solution\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      if (x[i] < x[i - 1]) {\n        x[i] = x[i - 1];\n      }\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Replace this line with your code\n    // You may use the following variables:\n    //   x: the input vector\n    //   size: the number of ranks\n    //   rank: the rank of this process\n    // You may use the following functions:\n    //   MPI_Send, MPI_Recv, MPI_Bcast, MPI_Reduce\n\n    // TODO: Replace this line with your code\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "cuda",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "// TODO: Implement this function\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n        if (x[j] < min) {\n            min = x[j];\n        }\n    }\n    x[i] = min;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n      if (x[j] < min) {\n        min = x[j];\n      }\n    }\n    x[i] = min;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  float min = x[i];\n  for (int j = 0; j < i; j++) {\n    if (x[j] < min) {\n      min = x[j];\n    }\n  }\n  x[i] = min;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    float min = x[i];\n    for (int j = 0; j < i; j++) {\n        if (x[j] < min) {\n            min = x[j];\n        }\n    }\n    x[i] = min;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "27_scan_scan_with_min_function",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i], 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        for (int i = 0; i < x.size(); i++) {\n            if (i % size == rank) {\n                x[i] = std::min(x[i], x[i - 1]);\n            }\n        }\n        MPI_Send(&x[rank], 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = (rank == size - 1)? n : start + chunk;\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        std::vector<float> x_local(x.size());\n        MPI_Recv(&x_local[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        #pragma omp parallel for\n        for (int i = 0; i < x.size(); i++) {\n            x_local[i] = std::min(x_local[i], x[i]);\n        }\n\n        MPI_Send(&x_local[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<float> x_local(x.size());\n            MPI_Recv(&x_local[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            #pragma omp parallel for\n            for (int i = 0; i < x.size(); i++) {\n                x[i] = std::min(x[i], x_local[i]);\n            }\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunkSize = n / size;\n  int remainder = n % size;\n\n  std::vector<float> localX(chunkSize);\n  std::vector<float> localMin(chunkSize);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[i * chunkSize], chunkSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  if (rank == 0) {\n    localX = x;\n  } else {\n    MPI_Recv(&localX[0], chunkSize, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  for (int i = 0; i < chunkSize; i++) {\n    localMin[i] = localX[i];\n  }\n\n  for (int i = 1; i < chunkSize; i++) {\n    if (localX[i] < localMin[i - 1]) {\n      localMin[i] = localX[i];\n    } else {\n      localMin[i] = localMin[i - 1];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&localX[0], chunkSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < chunkSize; j++) {\n        if (localX[j] < localMin[j]) {\n          localMin[j] = localX[j];\n        }\n      }\n    }\n  } else {\n    MPI_Send(&localMin[0], chunkSize, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = localMin[i];\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int i, j;\n    float min;\n    std::vector<float> x_copy(x);\n    if (rank == 0) {\n        for (i = 0; i < x.size(); i++) {\n            min = x[i];\n            for (j = 0; j < i; j++) {\n                if (x[j] < min) {\n                    min = x[j];\n                }\n            }\n            x[i] = min;\n        }\n    } else {\n        for (i = 0; i < x.size(); i++) {\n            min = x_copy[i];\n            for (j = 0; j < i; j++) {\n                if (x_copy[j] < min) {\n                    min = x_copy[j];\n                }\n            }\n            x[i] = min;\n        }\n    }\n}",
            "}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        std::vector<float> x_recv(x.size());\n        MPI_Recv(&x_recv[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] = std::min(x[i], x_recv[i]);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = (rank == size - 1)? n : start + chunk;\n  float min = x[start];\n  for (int i = start + 1; i < end; i++) {\n    if (x[i] < min) {\n      min = x[i];\n    }\n  }\n  x[start] = min;\n\n  // TODO: Your code here\n  MPI_Reduce(&x[start], &x[0], 1, MPI_FLOAT, MPI_MIN, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunkSize = n / size;\n    int remainder = n % size;\n\n    std::vector<float> localX(chunkSize);\n    std::vector<float> localMin(chunkSize);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * chunkSize], chunkSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < chunkSize; i++) {\n            localX[i] = x[i];\n            localMin[i] = x[i];\n        }\n    } else {\n        MPI_Recv(&localX[0], chunkSize, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&localX[i * chunkSize], chunkSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < chunkSize; j++) {\n                if (localX[j] < localMin[j]) {\n                    localMin[j] = localX[j];\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < chunkSize; i++) {\n            x[i] = localMin[i];\n        }\n    } else {\n        MPI_Send(&localMin[0], chunkSize, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * chunkSize], chunkSize, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_threads = omp_get_max_threads();\n    int num_elements = x.size();\n    int elements_per_thread = num_elements / num_threads;\n    int remainder = num_elements % num_threads;\n\n    std::vector<float> local_min(num_elements);\n    std::vector<float> local_min_buffer(num_elements);\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; i++) {\n        local_min[i] = x[i];\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_threads; i++) {\n        int start = i * elements_per_thread;\n        int end = start + elements_per_thread;\n        if (i == num_threads - 1) {\n            end += remainder;\n        }\n\n        for (int j = start; j < end; j++) {\n            for (int k = start; k < j; k++) {\n                if (local_min[j] > local_min[k]) {\n                    local_min[j] = local_min[k];\n                }\n            }\n        }\n    }\n\n    MPI_Gather(local_min.data(), num_elements, MPI_FLOAT, local_min_buffer.data(), num_elements, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < num_elements; i++) {\n            x[i] = local_min_buffer[i];\n        }\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(Kokkos::View<const double*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n  return 0.0;\n}",
            "// Create a Kokkos view for the output.\n  Kokkos::View<double*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos parallel_for to compute the prefix sum.\n  Kokkos::parallel_for(\n      \"prefix_sum\",\n      Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<double>>>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, double& sum) {\n        if (i == 0) {\n          y(i) = 0;\n        } else {\n          y(i) = y(i - 1) + x(i - 1);\n        }\n        sum += y(i);\n      });\n\n  // Return the sum.\n  return Kokkos::ReduceSum<double>(y.data(), y.extent(0));\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "Kokkos::View<double*> y(\"y\", x.extent(0));\n  Kokkos::parallel_scan(\n      \"sumOfPrefixSum\", x.extent(0), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        if (final) {\n          update += x(i);\n        } else {\n          update = x(i);\n        }\n      });\n  Kokkos::fence();\n  return y(x.extent(0) - 1);\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "Kokkos::View<double*> y(\"y\", x.extent(0));\n  Kokkos::parallel_scan(\n      \"sumOfPrefixSum\", x.extent(0), KOKKOS_LAMBDA(int i, double& update, bool final) {\n        if (final) {\n          update += x(i);\n        } else {\n          update = x(i);\n        }\n      });\n  Kokkos::fence();\n  return y(x.extent(0) - 1);\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a parallel_reduce functor that computes the sum of the prefix sum\n  // array of x.\n  struct PrefixSumFunctor {\n    Kokkos::View<const double*> x;\n    double sum;\n\n    PrefixSumFunctor(Kokkos::View<const double*> const& x_) : x(x_), sum(0) {}\n\n    KOKKOS_INLINE_FUNCTION\n    void operator()(const int i, double& lsum) const {\n      lsum += x(i);\n    }\n\n    KOKKOS_INLINE_FUNCTION\n    void join(volatile double& dst, const volatile double& src) const {\n      dst += src;\n    }\n  };\n\n  // Create a parallel_reduce functor that computes the prefix sum array of x.\n  struct PrefixSumArrayFunctor {\n    Kokkos::View<const double*> x;\n    Kokkos::View<double*> y;\n\n    PrefixSumArrayFunctor(Kokkos::View<const double*> const& x_,\n                          Kokkos::View<double*> const& y_)\n        : x(x_), y(y_) {}\n\n    KOKKOS_INLINE_FUNCTION\n    void operator()(const int i, double& lsum) const {\n      lsum += x(i);\n      y(i) = lsum;\n    }\n\n    KOKKOS_INLINE_FUNCTION\n    void join(volatile double& dst, const volatile double& src) const {\n      dst += src;\n    }\n  };\n\n  // Compute the sum of the prefix sum array of x.\n  double sum = 0;\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n      PrefixSumFunctor(x), sum);\n\n  // Compute the prefix sum array of x.\n  Kokkos::View<double*> y(\"y\", x.extent(0));\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n      PrefixSumArrayFunctor(x, y), sum);\n\n  return sum;\n}",
            "// Create a Kokkos view for the output prefix sum array\n  Kokkos::View<double*> prefix_sum(\"prefix_sum\", x.extent(0));\n\n  // Create a Kokkos parallel for loop to compute the prefix sum\n  Kokkos::parallel_for(\n      \"prefix_sum\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n          prefix_sum(i) = x(i);\n        } else {\n          prefix_sum(i) = x(i) + prefix_sum(i - 1);\n        }\n      });\n\n  // Create a Kokkos parallel reduce to compute the sum of the prefix sum\n  double sum = Kokkos::parallel_reduce(\n      \"sum\", x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n        lsum += prefix_sum(i);\n      },\n      0.0);\n\n  // Sync to make sure the prefix sum is computed\n  Kokkos::fence();\n\n  return sum;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a Kokkos view for the output\n  Kokkos::View<double*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos parallel_for to compute the prefix sum\n  Kokkos::parallel_for(\n      \"prefix_sum\",\n      Kokkos::RangePolicy<Kokkos::RoundRobin<Kokkos::DefaultExecutionSpace>>(\n          0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n          y(i) = x(i);\n        } else {\n          y(i) = x(i) + y(i - 1);\n        }\n      });\n\n  // Create a Kokkos parallel_reduce to compute the sum of the prefix sum\n  double sum = Kokkos::parallel_reduce(\n      \"sum_prefix_sum\",\n      Kokkos::RangePolicy<Kokkos::RoundRobin<Kokkos::DefaultExecutionSpace>>(\n          0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, double& lsum) {\n        lsum += y(i);\n      },\n      0.0);\n\n  // Return the sum\n  return sum;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// TODO: Implement this function\n  return 0.0;\n}",
            "// Create a Kokkos view for the output prefix sum array\n  Kokkos::View<double*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos parallel_for to compute the prefix sum\n  Kokkos::parallel_for(\n      \"prefix_sum\",\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n          y(i) = x(i);\n        } else {\n          y(i) = y(i - 1) + x(i);\n        }\n      });\n\n  // Create a Kokkos parallel_reduce to compute the sum of the prefix sum\n  double sum = Kokkos::parallel_reduce(\n      \"sum_prefix_sum\",\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i, double& lsum) {\n        lsum += y(i);\n      },\n      0.0);\n\n  return sum;\n}",
            "// Create a Kokkos::View to store the prefix sum\n  Kokkos::View<double*> prefixSum(\"prefixSum\", x.extent(0));\n\n  // Create a Kokkos::RangePolicy to distribute the work\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, x.extent(0));\n\n  // Create a Kokkos::parallel_for to compute the prefix sum\n  Kokkos::parallel_for(\n      policy, KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n          prefixSum(i) = x(i);\n        } else {\n          prefixSum(i) = x(i) + prefixSum(i - 1);\n        }\n      });\n\n  // Create a Kokkos::parallel_reduce to sum the prefix sum\n  double sum = 0;\n  Kokkos::parallel_reduce(\n      policy, KOKKOS_LAMBDA(const int i, double& lsum) { lsum += prefixSum(i); },\n      sum);\n\n  return sum;\n}",
            "Kokkos::View<double*> y(\"y\", x.size());\n  Kokkos::parallel_scan(\n      \"prefix sum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        if (final) {\n          y(i) = update;\n        } else {\n          update += x(i);\n        }\n      });\n  Kokkos::fence();\n  double sum = 0;\n  Kokkos::parallel_reduce(\n      \"sum\", x.size(), KOKKOS_LAMBDA(const int i, double& update) { update += y(i); }, sum);\n  Kokkos::fence();\n  return sum;\n}",
            "// Create a Kokkos view for the output prefix sum array\n  Kokkos::View<double*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos parallel_for to compute the prefix sum\n  Kokkos::parallel_for(\n      \"prefix_sum\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n          y(i) = x(i);\n        } else {\n          y(i) = x(i) + y(i - 1);\n        }\n      });\n\n  // Create a Kokkos parallel_reduce to compute the sum of the prefix sum\n  double sum = Kokkos::parallel_reduce(\n      \"sum_prefix_sum\", x.extent(0), 0.0,\n      KOKKOS_LAMBDA(const int i, double& lsum) {\n        lsum += y(i);\n      },\n      Kokkos::Sum<double>());\n\n  // Return the sum\n  return sum;\n}",
            "Kokkos::View<double*> y(\"y\", x.extent(0));\n  Kokkos::parallel_scan(\n      \"prefix_sum\", x.extent(0),\n      KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        if (i == 0) {\n          update = 0;\n        } else {\n          update += x(i - 1);\n        }\n        if (final) {\n          y(i) = update;\n        }\n      });\n  Kokkos::fence();\n  return y(x.extent(0) - 1);\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0;\n    std::vector<double> prefixSum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n\n    return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); ++i) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n    double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n\n    return sum;\n}",
            "double sum = 0;\n    std::vector<double> prefixSum(x.size());\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n    return sum;\n}",
            "double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n\n    return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    if (i == 0) {\n      prefixSum[i] = x[i];\n    } else {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    sum += prefixSum[i];\n  }\n  return sum;\n}",
            "double sum = 0;\n    std::vector<double> prefixSum(x.size());\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); i++) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n    return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    prefixSum[i] = sum;\n  }\n\n  return sum;\n}",
            "// TODO: Implement this function\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    if (i == 0) {\n      prefixSum[i] = x[i];\n    } else {\n      prefixSum[i] = x[i] + prefixSum[i - 1];\n    }\n    sum += prefixSum[i];\n  }\n  return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n\n  return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n    double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n\n    return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); ++i) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n    double sum = 0;\n\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n\n    return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n  double sum = 0;\n\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n\n  return sum;\n}",
            "double sum = 0;\n    std::vector<double> prefixSum(x.size());\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        prefixSum[i] = sum;\n        sum += x[i];\n    }\n    return sum;\n}",
            "double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double sum = 0.0;\n  for (auto const& xi : x) {\n    sum += xi;\n  }\n  return sum;\n}",
            "double sum = 0;\n  for (auto const& xi : x) {\n    sum += xi;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n  double sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    prefixSum[i] = sum;\n  }\n  return sum;\n}",
            "double sum = 0.0;\n  for (auto const& i : x) {\n    sum += i;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size() + 1);\n  prefixSum[0] = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    prefixSum[i + 1] = prefixSum[i] + x[i];\n  }\n  return prefixSum[x.size()];\n}",
            "double sum = 0;\n  for (auto const& x_i : x) {\n    sum += x_i;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n  prefixSum[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n  return prefixSum[x.size() - 1];\n}",
            "std::vector<double> prefixSum(x.size());\n  double sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    prefixSum[i] = sum;\n    sum += x[i];\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n  prefixSum[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n  return prefixSum[x.size() - 1];\n}",
            "std::vector<double> prefixSum(x.size());\n  prefixSum[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n  return prefixSum[x.size() - 1];\n}",
            "double sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size());\n  double sum = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    prefixSum[i] = sum;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size() + 1);\n  for (int i = 0; i < x.size(); ++i) {\n    prefixSum[i + 1] = prefixSum[i] + x[i];\n  }\n  return prefixSum[x.size()];\n}",
            "double sum = 0.0;\n  for (auto const& v : x) {\n    sum += v;\n  }\n  return sum;\n}",
            "std::vector<double> prefixSum(x.size() + 1);\n  for (size_t i = 0; i < x.size(); ++i) {\n    prefixSum[i + 1] = prefixSum[i] + x[i];\n  }\n  return prefixSum[x.size()];\n}",
            "std::vector<double> prefixSum(x.size() + 1);\n  for (size_t i = 0; i < x.size(); ++i) {\n    prefixSum[i + 1] = prefixSum[i] + x[i];\n  }\n  return prefixSum[x.size()];\n}",
            "std::vector<double> prefixSum(x.size() + 1);\n  for (size_t i = 0; i < x.size(); ++i) {\n    prefixSum[i + 1] = prefixSum[i] + x[i];\n  }\n  return prefixSum[x.size()];\n}",
            "double sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n  }\n  return sum;\n}",
            "double sum = 0;\n  for (double xi : x) {\n    sum += xi;\n  }\n  return sum;\n}",
            "double sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n  }\n  return sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "extern __shared__ double sdata[];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n\n  double mySum = 0;\n  for (; i < N; i += gridSize) {\n    mySum += x[i];\n  }\n  sdata[tid] = mySum;\n  __syncthreads();\n\n  // do reduction in shared mem\n  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n  }\n\n  // write result for this block to global mem\n  if (tid == 0) {\n    sum[blockIdx.x] = sdata[0];\n  }\n}",
            "extern __shared__ double sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int end = start + blockDim.x;\n  unsigned int i = start + t;\n  double mySum = 0;\n  if (i < N) {\n    mySum = x[i];\n  }\n  sdata[t] = mySum;\n  __syncthreads();\n  for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n    if (t % (2 * stride) == 0) {\n      sdata[t] += sdata[t + stride];\n    }\n    __syncthreads();\n  }\n  if (t == 0) {\n    sum[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    double temp = 0;\n    for (int i = tid; i < N; i += stride) {\n        temp += x[i];\n        x[i] = temp;\n    }\n    if (tid == 0) {\n        sum[blockIdx.x] = temp;\n    }\n}",
            "extern __shared__ double s[];\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    s[threadIdx.x] = x[i];\n  }\n  __syncthreads();\n  for (size_t stride = 1; stride <= blockDim.x; stride *= 2) {\n    size_t index = 2 * stride * threadIdx.x;\n    if (index < 2 * blockDim.x) {\n      s[index] += s[index - stride];\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x == 0) {\n    sum[blockIdx.x] = s[blockDim.x - 1];\n  }\n}",
            "extern __shared__ double sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int end = start + blockDim.x;\n  double mySum = 0;\n  for (unsigned int i = start + t; i < end; i += blockDim.x) {\n    mySum += x[i];\n    sdata[t] = mySum;\n    __syncthreads();\n    if (t == 0) {\n      sdata[0] = mySum = mySum + sdata[t + 1];\n    }\n    __syncthreads();\n  }\n  if (t == 0) {\n    sum[blockIdx.x] = mySum;\n  }\n}",
            "extern __shared__ double sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int gridSize = blockDim.x * gridDim.x;\n    double mySum = 0;\n\n    // Each thread adds its partial sum to shared memory\n    while (i < N) {\n        mySum += x[i];\n        sdata[tid] = mySum;\n        __syncthreads();\n\n        // do reduction in shared mem\n        if (blockDim.x >= 512) { if (tid < 256) { sdata[tid] = mySum = mySum + sdata[tid + 256]; } __syncthreads(); }\n        if (blockDim.x >= 256) { if (tid < 128) { sdata[tid] = mySum = mySum + sdata[tid + 128]; } __syncthreads(); }\n        if (blockDim.x >= 128) { if (tid < 64) { sdata[tid] = mySum = mySum + sdata[tid + 64]; } __syncthreads(); }\n\n        if (tid < 32) {\n            if (blockDim.x >= 64) { sdata[tid] = mySum = mySum + sdata[tid + 32]; }\n            if (blockDim.x >= 32) { sdata[tid] = mySum = mySum + sdata[tid + 16]; }\n            if (blockDim.x >= 16) { sdata[tid] = mySum = mySum + sdata[tid + 8]; }\n            if (blockDim.x >= 8) { sdata[tid] = mySum = mySum + sdata[tid + 4]; }\n            if (blockDim.x >= 4) { sdata[tid] = mySum = mySum + sdata[tid + 2]; }\n            if (blockDim.x >= 2) { sdata[tid] = mySum = mySum + sdata[tid + 1]; }\n        }\n\n        // write result for this block to global mem\n        if (tid == 0) { sum[blockIdx.x] = sdata[0]; }\n\n        i += gridSize;\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double temp = 0;\n        for (int j = 0; j <= i; j++) {\n            temp += x[j];\n        }\n        sum[i] = temp;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; ++i) {\n            sum += x[i];\n        }\n        sumOfPrefixSum[tid] = sum;\n    }\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    s[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        int index = 2 * s * tid;\n        if (index < blockDim.x) {\n            s[index] += s[index + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        sum[blockIdx.x] = s[0];\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double temp = 0;\n        for (int i = 0; i <= tid; i++) {\n            temp += x[i];\n        }\n        sum[tid] = temp;\n    }\n}",
            "// TODO\n}",
            "extern __shared__ double temp[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int j = 2 * bid * blockDim.x + tid;\n    int k = (bid + 1) * blockDim.x + tid;\n    temp[tid] = 0;\n    if (i < N) {\n        temp[tid] = x[i];\n    }\n    __syncthreads();\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        if (tid >= s) {\n            temp[tid] += temp[tid - s];\n        }\n        __syncthreads();\n    }\n    if (j < N) {\n        x[j] = temp[tid];\n    }\n    if (k < N) {\n        x[k] = temp[tid];\n    }\n    __syncthreads();\n    if (tid == 0) {\n        sum[bid] = temp[blockDim.x - 1];\n    }\n}",
            "extern __shared__ double sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int end = start + blockDim.x;\n  double sum_local = 0.0;\n  for (unsigned int i = start + t; i < end; i += blockDim.x) {\n    sum_local += x[i];\n    sdata[t] = sum_local;\n    __syncthreads();\n    if (t == 0) {\n      sum_local = 0.0;\n    }\n    if (t < blockDim.x / 2) {\n      sum_local += sdata[t + blockDim.x / 2];\n      sdata[t] = sum_local;\n    }\n    __syncthreads();\n    if (t == 0) {\n      sum[blockIdx.x] = sum_local;\n    }\n  }\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int i = bid * bsize + tid;\n    s[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (int s = 1; s < bsize; s *= 2) {\n        if (tid >= s) {\n            s[tid] += s[tid - s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        sum[bid] = s[bsize - 1];\n    }\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int gsize = gridDim.x;\n    int i = bid * bsize + tid;\n    int j = i + bsize;\n    int k = 2 * bsize;\n    double t = 0;\n    s[tid] = 0;\n    while (j < N) {\n        s[tid] += x[i];\n        t += x[i];\n        i = j;\n        j += k;\n        k *= 2;\n    }\n    __syncthreads();\n    for (int s = bsize / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s[tid] += s[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        sum[bid] = s[0];\n    }\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int gsize = gridDim.x;\n    int i = bid * bsize + tid;\n    int j = i + bsize;\n    s[tid] = 0;\n    if (i < N) s[tid] = x[i];\n    __syncthreads();\n    for (int k = 1; k < bsize; k *= 2) {\n        if (tid >= k) s[tid] += s[tid - k];\n        __syncthreads();\n    }\n    if (tid == 0) sum[bid] = s[bsize - 1];\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            sum[i] = x[i];\n        } else {\n            sum[i] = x[i] + sum[i - 1];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    //\n    // Hint:\n    // 1. Use the atomicAdd() function to compute the sum of the prefix sum.\n    // 2. Use the __syncthreads() function to synchronize the threads in the block.\n    // 3. Use the __shfl_down() function to compute the prefix sum.\n    // 4. Use the __shfl_up() function to compute the prefix sum.\n    // 5. Use the __shfl_xor() function to compute the prefix sum.\n    // 6. Use the __shfl() function to compute the prefix sum.\n    // 7. Use the __shfl_down_sync() function to compute the prefix sum.\n    // 8. Use the __shfl_up_sync() function to compute the prefix sum.\n    // 9. Use the __shfl_xor_sync() function to compute the prefix sum.\n    // 10. Use the __shfl_sync() function to compute the prefix sum.\n    // 11. Use the __syncwarp() function to synchronize the threads in the warp.\n    // 12. Use the __any_sync() function to compute the prefix sum.\n    // 13. Use the __ballot_sync() function to compute the prefix sum.\n    // 14. Use the __activemask() function to compute the prefix sum.\n    // 15. Use the __popc() function to compute the prefix sum.\n    // 16. Use the __clz() function to compute the prefix sum.\n    // 17. Use the __ffs() function to compute the prefix sum.\n    // 18. Use the __brev() function to compute the prefix sum.\n    // 19. Use the __byte_perm() function to compute the prefix sum.\n    // 20. Use the __funnelshift_l() function to compute the prefix sum.\n    // 21. Use the __funnelshift_r() function to compute the prefix sum.\n    // 22. Use the __funnelshift_r() function to compute the prefix sum.\n    // 23. Use the __match_any_sync() function to compute the prefix sum.\n    // 24. Use the __match_all_sync() function to compute the prefix sum.\n    // 25. Use the __shfl_sync_down() function to compute the prefix sum.\n    // 26. Use the __shfl_sync_up() function to compute the prefix sum.\n    // 27. Use the __shfl_sync_xor() function to compute the prefix sum.\n    // 28. Use the __shfl_sync() function to compute the prefix sum.\n    // 29. Use the __syncwarp_or() function to synchronize the threads in the warp.\n    // 30. Use the __syncwarp_and() function to synchronize the threads in the warp.\n    // 31. Use the __syncwarp_xor() function to synchronize the threads in the warp.\n    // 32. Use the __syncwarp_nand() function to synchronize the threads in the warp.\n    // 33. Use the __syncwarp_or_sync() function to synchronize the threads in the warp.\n    // 34. Use the __syncwarp_and_sync() function to synchronize the threads in the warp.\n    // 35. Use the __syncwarp_xor_sync() function to synchronize the threads in the warp.\n    // 36. Use the __syncwarp_nand_sync() function to synchronize the threads in the warp.\n    // 37. Use the __syncwarp_or_sync() function to synchronize the threads in the warp.\n    // 38. Use the __syncwarp_and_sync() function to synchronize the threads in the warp.\n    // 39. Use the __syncwarp_xor_sync() function to synchronize the threads in the warp.\n    // 40. Use the __syncwarp_nand_sync() function to synchronize the threads in the warp.\n    // 41. Use the __syncwarp_or_sync() function to synchronize the threads in the warp.\n    // 42. Use the __"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0.0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  if (rank == 0) {\n    prefixSum[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    sum = prefixSum[x.size() - 1];\n  }\n  MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  return sum;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    return 0.0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  double sum = 0;\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n  std::vector<double> local_x(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, local_x.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<double> prefix_sum(n_local);\n  prefix_sum[0] = local_x[0];\n  for (int i = 1; i < n_local; i++) {\n    prefix_sum[i] = prefix_sum[i - 1] + local_x[i];\n  }\n  MPI_Reduce(prefix_sum.data(), NULL, n_local, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(prefix_sum.data() + n_per_proc + (i < n_rem? 1 : 0), n_local, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < n; i++) {\n      sum += x[i];\n    }\n  }\n  return sum;\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  if (rank == 0) {\n    prefixSum[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    sum = prefixSum[x.size() - 1];\n  }\n  MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  return sum;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0.0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n  return 0.0;\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  //...\n\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  // Hint: Use MPI_Reduce\n\n  return 0.0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  if (rank == 0) {\n    prefixSum[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n  }\n\n  MPI_Bcast(&prefixSum[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    for (int i = 0; i < x.size(); i++) {\n      sum += prefixSum[i];\n    }\n  }\n\n  return sum;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> prefixSum(x.size());\n    if (rank == 0) {\n        prefixSum[0] = x[0];\n        for (int i = 1; i < x.size(); i++) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Bcast(prefixSum.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double sum = 0;\n    if (rank == 0) {\n        sum = prefixSum[x.size() - 1];\n    }\n\n    return sum;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      sum += x[i];\n    }\n  }\n  MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  return sum;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> prefixSum(x.size());\n    if (rank == 0) {\n        prefixSum[0] = x[0];\n        for (int i = 1; i < x.size(); ++i) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Bcast(&prefixSum[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double sum = 0;\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); ++i) {\n            sum += prefixSum[i];\n        }\n    }\n\n    return sum;\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  // Hint: Use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Reduce\n  // Hint: Use MPI_IN_PLACE as the send buffer for MPI_Reduce\n  // Hint: Use MPI_SUM as the operation for MPI_Reduce\n  // Hint: Use MPI_Bcast to broadcast the result to all ranks\n\n  return 0.0;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double sum = 0;\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<double> local_sum(chunk + 1);\n    std::vector<double> global_sum(chunk + 1);\n\n    for (int i = 0; i < chunk; i++) {\n        local_sum[i] = x[rank * chunk + i];\n    }\n    local_sum[chunk] = x[rank * chunk + chunk];\n\n    MPI_Reduce(&local_sum[0], &global_sum[0], chunk + 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < chunk + 1; i++) {\n            global_sum[i] += global_sum[i - 1];\n        }\n    }\n\n    MPI_Bcast(&global_sum[0], chunk + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < chunk; i++) {\n            sum += global_sum[i];\n        }\n    } else {\n        for (int i = 0; i < chunk + 1; i++) {\n            sum += global_sum[i];\n        }\n    }\n\n    return sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      sum[i] = x[i];\n    } else {\n      sum[i] = sum[i - 1] + x[i];\n    }\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int gsize = gridDim.x;\n\n    extern __shared__ double sdata[];\n    double *sdata_ptr = sdata + tid;\n\n    // Load the data into shared memory\n    sdata_ptr[tid] = x[bid * bsize + tid];\n    __syncthreads();\n\n    // Compute the prefix sum\n    for (int i = 1; i < bsize; i *= 2) {\n        if (tid >= i) {\n            sdata_ptr[tid] += sdata_ptr[tid - i];\n        }\n        __syncthreads();\n    }\n\n    // Store the result in the global memory\n    if (tid == 0) {\n        sum[bid] = sdata_ptr[0];\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        sum[idx] = x[idx];\n        if (idx > 0) {\n            sum[idx] += sum[idx - 1];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            sum[i] = x[i];\n        } else {\n            sum[i] = sum[i - 1] + x[i];\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum_val = 0;\n        for (int i = 0; i <= idx; i++) {\n            sum_val += x[i];\n        }\n        sum[idx] = sum_val;\n    }\n}",
            "extern __shared__ double temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int bsize = blockDim.x;\n  int gsize = gridDim.x;\n  int i = bid * bsize + tid;\n  int j = i + bsize;\n  int k = 2 * bsize;\n  double mySum = 0;\n  temp[tid] = x[i];\n  __syncthreads();\n  while (j < N) {\n    if (i < N) {\n      temp[tid] += x[i];\n    }\n    __syncthreads();\n    i = j;\n    j += k;\n    k *= 2;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    sum[bid] = temp[bsize - 1];\n  }\n}",
            "// TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function\n    // You can use the atomicAdd() function to compute the sum in parallel\n    // You can use the __syncthreads() function to synchronize threads\n\n    // TODO: Implement this function",
            "int i = threadIdx.x;\n    int j = i + 1;\n    while (j < N) {\n        x[j] += x[i];\n        j += blockDim.x;\n    }\n    __syncthreads();\n    if (i == 0) {\n        sum[0] = x[N - 1];\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = x[i] + (i > 0? sum[i - 1] : 0);\n    }\n}",
            "// TODO: Implement this function\n  int i = threadIdx.x;\n  int j = i + 1;\n  int k = 2 * blockDim.x;\n\n  while (j < N) {\n    x[j] += x[i];\n    j += k;\n  }\n\n  __syncthreads();\n\n  if (i == 0) {\n    *sum = x[N - 1];\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int i = tid;\n    double temp = 0;\n    while (i < N) {\n        temp += x[i];\n        x[i] = temp;\n        i += blockDim.x * gridDim.x;\n    }\n    if (tid == 0) {\n        sum[0] = temp;\n    }\n}",
            "// TODO\n}",
            "extern __shared__ double sdata[];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = 0;\n  while (i < N) {\n    sdata[tid] += x[i];\n    i += gridSize;\n  }\n  __syncthreads();\n\n  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n  }\n\n  if (tid == 0) {\n    sum[blockIdx.x] = sdata[0];\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int i = tid;\n    int j = 2 * tid + 1;\n    if (j < N) {\n        x[j] = x[i] + x[j];\n    }\n    __syncthreads();\n    if (j < N) {\n        x[j] = x[i] + x[j];\n    }\n    __syncthreads();\n    if (tid == 0) {\n        sum[0] = x[N - 1];\n    }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j < N) {\n        if (i == 0) {\n            sum[j] = x[j];\n        } else {\n            sum[j] = sum[j - 1] + x[j];\n        }\n    }\n}",
            "// TODO: Implement this function\n  int i = threadIdx.x;\n  int j = i + 1;\n  double temp = 0;\n  while (j < N) {\n    temp += x[j];\n    x[j] = temp;\n    j += blockDim.x;\n  }\n  __syncthreads();\n  if (i == 0) {\n    sum[0] = temp;\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            sum[i] = x[i];\n        } else {\n            sum[i] = x[i] + sum[i - 1];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "28_scan_sum_of_prefix_sum_array",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  return 0.0;\n}",
            "// TODO: Your code here\n    double sum = 0;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    std::vector<double> prefixSum(x.size(), 0);\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            prefixSum[i] = x[i];\n        }\n    }\n    MPI_Bcast(prefixSum.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (int i = start + 1; i < end; i++) {\n        prefixSum[i] += prefixSum[i - 1];\n    }\n    MPI_Reduce(&prefixSum[end - 1], &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    return sum;\n}",
            "// TODO\n  return 0.0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0.0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  double sum = 0;\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_remainder;\n  }\n  std::vector<double> prefix_sum(n);\n  for (int i = start; i < end; i++) {\n    prefix_sum[i] = x[i];\n  }\n  for (int i = start + 1; i < end; i++) {\n    prefix_sum[i] += prefix_sum[i - 1];\n  }\n  double sum_local = 0;\n  for (int i = start; i < end; i++) {\n    sum_local += prefix_sum[i];\n  }\n  double sum_global;\n  MPI_Reduce(&sum_local, &sum_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    sum = sum_global;\n  }\n  return sum;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  double sum = 0;\n  std::vector<double> prefixSum(n);\n  if (rank == 0) {\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    sum = prefixSum[n - 1];\n  }\n\n  // TODO: Your code here\n  MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  return sum;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<double> prefix_sum(n_local);\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n_local; i++) {\n    prefix_sum[i] = sum;\n    sum += x_local[i];\n  }\n  MPI_Gather(prefix_sum.data(), n_local, MPI_DOUBLE, x.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  return sum;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<double> prefix_sum_local(n_local + 1);\n  double sum = 0;\n  if (rank == 0) {\n    prefix_sum_local[0] = 0;\n  }\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n_local; ++i) {\n    prefix_sum_local[i + 1] = prefix_sum_local[i] + x_local[i];\n    sum += prefix_sum_local[i + 1];\n  }\n  std::vector<double> prefix_sum_global(n + 1);\n  MPI_Gather(prefix_sum_local.data(), n_local + 1, MPI_DOUBLE, prefix_sum_global.data(), n_local + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    return sum;\n  }\n  return 0;\n}",
            "// TODO: Implement this function\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n\n  // TODO: Implement this function\n  if (rank == 0) {\n    prefixSum[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n  }\n\n  MPI_Bcast(prefixSum.data(), prefixSum.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  double sum_local = 0;\n  #pragma omp parallel for reduction(+:sum_local)\n  for (int i = 0; i < prefixSum.size(); i++) {\n    sum_local += prefixSum[i];\n  }\n\n  double sum_global;\n  MPI_Reduce(&sum_local, &sum_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    sum = sum_global;\n  }\n\n  return sum;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  std::vector<double> prefixSum(n);\n  double sum = 0;\n  int start = rank * n / size;\n  int end = (rank + 1) * n / size;\n  for (int i = start; i < end; i++) {\n    prefixSum[i] = x[i];\n    sum += x[i];\n  }\n  for (int i = 1; i < n / size; i++) {\n    MPI_Send(&prefixSum[i], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    MPI_Recv(&prefixSum[i - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD,\n             MPI_STATUS_IGNORE);\n  }\n  if (rank == 0) {\n    for (int i = 1; i < n / size; i++) {\n      MPI_Recv(&prefixSum[i - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD,\n               MPI_STATUS_IGNORE);\n    }\n  }\n  if (rank == size - 1) {\n    for (int i = 0; i < n / size - 1; i++) {\n      MPI_Send(&prefixSum[i], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    }\n  }\n  if (rank!= 0) {\n    MPI_Send(&prefixSum[0], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n  }\n  if (rank!= size - 1) {\n    MPI_Recv(&prefixSum[n / size - 1], 1, MPI_DOUBLE, rank + 1, 0,\n             MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  return sum;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0.0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  double sum = 0;\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_left = n % size;\n  int n_per_rank_rank = n_per_rank + (rank < n_left? 1 : 0);\n  int n_per_rank_rank_left = n_per_rank + (rank < n_left? 0 : 1);\n  std::vector<double> prefix_sum(n_per_rank_rank);\n  std::vector<double> prefix_sum_rank(n_per_rank_rank_left);\n  std::vector<double> prefix_sum_rank_left(n_per_rank_rank_left);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum_rank_left[i] = x[i];\n    }\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum_rank[i] = x[i + n_per_rank_rank_left];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum_rank_left[i] = x[i];\n    }\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum_rank[i] = x[i + n_per_rank_rank_left];\n    }\n  }\n\n  MPI_Gather(prefix_sum_rank_left.data(), n_per_rank_rank_left, MPI_DOUBLE,\n             prefix_sum.data(), n_per_rank_rank_left, MPI_DOUBLE, 0,\n             MPI_COMM_WORLD);\n\n  MPI_Gather(prefix_sum_rank.data(), n_per_rank_rank_left, MPI_DOUBLE,\n             prefix_sum.data(), n_per_rank_rank_left, MPI_DOUBLE, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum[i] += prefix_sum_rank_left[i];\n    }\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      prefix_sum[i] += prefix_sum_rank[i];\n    }\n    for (int i = 0; i < n_per_rank_rank_left; i++) {\n      sum += prefix_sum[i];\n    }\n  }\n\n  return sum;\n}",
            "// TODO: implement this function\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  double sum = 0;\n  std::vector<double> prefixSum(n);\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      prefixSum[i] = x[i];\n    }\n  }\n  MPI_Bcast(prefixSum.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  for (int i = 1; i < n; i++) {\n    prefixSum[i] += prefixSum[i - 1];\n  }\n  MPI_Reduce(&prefixSum[0], &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  return sum;\n}",
            "// TODO\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> prefixSum(n);\n  double sum = 0;\n\n  // TODO: Implement this function\n\n  return sum;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  double sum = 0;\n  std::vector<double> prefixSum(x.size());\n  std::vector<double> recv(x.size());\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  for (int i = start; i < end; i++) {\n    prefixSum[i - start] = x[i];\n  }\n  MPI_Reduce(&prefixSum[0], &recv[0], x.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < x.size(); i++) {\n      recv[i] += recv[i - 1];\n    }\n    sum = recv[x.size() - 1];\n  }\n  return sum;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: your code here\n    double sum = 0;\n    std::vector<double> prefixSum(x.size());\n    if (rank == 0) {\n        prefixSum[0] = x[0];\n        for (int i = 1; i < x.size(); i++) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Bcast(&prefixSum[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank!= 0) {\n        for (int i = 0; i < x.size(); i++) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Reduce(&prefixSum[0], &sum, x.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a Kokkos parallel_for lambda function to compute the prefix sum.\n  // The lambda function takes two arguments:\n  // 1. i: the index of the element to process\n  // 2. sum: the running sum of the prefix sum\n  Kokkos::parallel_for(\n    \"prefix_sum\",\n    Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<double> > >(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& sum) {\n      sum += x(i);\n      output(i) = sum;\n    }\n  );\n\n  // Force the Kokkos execution to complete.\n  Kokkos::fence();\n}",
            "Kokkos::parallel_scan(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n      if (final) {\n        output[i] = update;\n      } else {\n        update += x[i];\n      }\n    });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_scan(\n    \"prefixSum\",\n    x.extent(0),\n    KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n      if (final) {\n        output(i) = update;\n      } else {\n        update += x(i);\n      }\n    }\n  );\n}",
            "// Create a Kokkos::RangePolicy to execute the parallel_for\n  // The range policy will execute the loop in parallel\n  // over all indices in the range [0, x.extent(0))\n  Kokkos::RangePolicy<Kokkos::Rank<1>> policy(0, x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::View<double*> partial_sums(\"Partial Sums\", x.extent(0));\n\n  // Create a Kokkos::View to store the partial sums\n  Kokkos::",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    output.resize(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i - 1] + x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  output[0] = x[0];\n  for (int i = 1; i < n; i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Your code here\n  int n = x.size();\n  output.resize(n);\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < n; i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  double sum = 0;\n  output[0] = x[0];\n  for (int i = 1; i < n; i++) {\n    sum += x[i];\n    output[i] = sum;\n  }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  output[0] = x[0];\n  for (int i = 1; i < n; i++) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement me!\n    // You can use the following variables:\n    // - x\n    // - output\n    // - omp_get_num_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_procs()\n    // - omp_get_thread_limit()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_get_cancellation()\n    // - omp_in_parallel()\n    // - omp_get_max_threads()\n    // - omp_get_wtick()\n    // - omp_get_wtime()\n    // - omp_get_thread_num()\n    // - omp_get_num_procs()\n    // - omp_get_num_threads()\n    // - omp_get_max_threads()\n    // - omp_get_thread_limit()\n    // - omp_get_num_places()\n    // - omp_get_place_num_procs()\n    // - omp_get_place_proc_ids()\n    // - omp_get_place_num()\n    // - omp_get_partition_num_places()\n    // - omp_get_partition_place_nums()\n    // - omp_get_default_device()\n    // - omp_get_num_devices()\n    // - omp_get_num_teams()\n    // - omp_get_team_num()\n    // - omp_get_cancellation()\n    // - omp_get_proc_bind()\n    // - omp_get_default_device()\n    // - omp_get_num_devices()\n    // - omp_get_initial_device()\n    // - omp_get_max_task_priority()\n    // - omp_get_supported_active_levels()\n    // - omp_get_active_level()\n    // - omp_get_level()\n    // - omp_get_ancestor_thread_num()\n    // - omp_get_team_size()\n    // - omp_get_active_level()\n    // - omp_get_cancellation()\n    // - omp_get_proc_bind()\n    // - omp_get_default_device()\n    // - omp_get_num_devices()\n    // - omp_get_initial_device()\n    // - omp_get_max_task_priority()\n    // - omp_get_supported_active_levels()\n    // - omp_get_active_level()\n    // - omp_get_level()\n    // - omp_get_ancestor_thread_num()\n    // - omp_get_team_size()\n    // - omp_get_active_level()\n    // - omp_get_cancellation()\n    // - omp_get_proc_bind()\n    // - omp_get_default_device()\n    // - omp_get_num_devices()\n    // - omp_get_initial_device()\n    // - omp_get_max_task_priority()\n    // - omp_get_supported_active_levels()\n    // - omp_get_active_level()\n    // - omp_get_level()\n    // - omp_get_ancestor_thread_num()\n    // - omp_get_team_size()\n    // - omp_get_active_level()\n    // - omp_get_cancellation()\n    // - omp_get_proc_bind()\n    // - omp_get_default_device()\n    // - omp_get_num_devices()\n    // - omp_get_initial_device()\n    // - omp_get_max_task_priority()\n    // - omp_get_supported_active_levels()\n    // - omp_get_active_level()\n    // - omp_get_level()\n    // - omp_get_ancestor_thread_num()\n    // - omp_get_team_size()\n    // - omp",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    output.resize(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (i == 0) {\n      output[i] = x[i];\n    } else {\n      output[i] = output[i - 1] + x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    output[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i - 1] + x[i];\n    }\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i - 1] + x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n  output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n  output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: implement this function\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); ++i) {\n        output[i] = output[i - 1] + x[i];\n    }\n}",
            "// TODO: implement this function\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n  output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i - 1] + x[i];\n    }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (size_t i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "// TODO: Implement this function\n  output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i - 1] + x[i];\n  }\n}",
            "output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); ++i) {\n        output[i] = output[i - 1] + x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double* x, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "extern __shared__ double sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  unsigned int i = start + t;\n  double mySum = 0;\n  sdata[t] = 0;\n  while (i < N) {\n    mySum += x[i];\n    sdata[t] = mySum;\n    __syncthreads();\n    if (t < 128) {\n      sdata[t] += sdata[t + 128];\n      __syncthreads();\n    }\n    if (t < 64) {\n      sdata[t] += sdata[t + 64];\n      __syncthreads();\n    }\n    if (t < 32) {\n      sdata[t] += sdata[t + 32];\n      __syncthreads();\n    }\n    if (t < 16) {\n      sdata[t] += sdata[t + 16];\n      __syncthreads();\n    }\n    if (t < 8) {\n      sdata[t] += sdata[t + 8];\n      __syncthreads();\n    }\n    if (t < 4) {\n      sdata[t] += sdata[t + 4];\n      __syncthreads();\n    }\n    if (t < 2) {\n      sdata[t] += sdata[t + 2];\n      __syncthreads();\n    }\n    if (t < 1) {\n      sdata[t] += sdata[t + 1];\n      __syncthreads();\n    }\n    if (i + t < N) {\n      output[i + t] = sdata[0];\n    }\n    i += stride;\n  }\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nthreads = blockDim.x;\n  int nblocks = gridDim.x;\n  int i = bid * nthreads + tid;\n  int stride = nthreads * nblocks;\n  double sum = 0.0;\n  for (; i < N; i += stride) {\n    sum += x[i];\n    output[i] = sum;\n  }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    s[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        int index = 2 * s * tid;\n        if (index < blockDim.x) {\n            s[index] += s[index - s];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[i] = s[tid];\n    }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ double s[];\n    unsigned int t = threadIdx.x;\n    unsigned int start = 2 * blockIdx.x * blockDim.x;\n    unsigned int end = start + 2 * blockDim.x;\n    if (end > N) {\n        end = N;\n    }\n    double sum = 0;\n    for (unsigned int i = start + t; i < end; i += blockDim.x) {\n        s[t] = x[i];\n        sum += s[t];\n        output[i] = sum;\n    }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    output[i] = x[i];\n    if (i > 0) {\n      output[i] += output[i - 1];\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        output[i] = x[i];\n    }\n    __syncthreads();\n    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n        int index = 2 * stride * i;\n        if (index < N) {\n            output[index] += output[index - stride];\n        }\n        __syncthreads();\n    }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ double sdata[];\n    unsigned int t = threadIdx.x;\n    unsigned int start = blockIdx.x * blockDim.x;\n    unsigned int end = start + blockDim.x;\n    unsigned int i = start + t;\n    double mySum = 0;\n    sdata[t] = 0;\n    while (i < end) {\n        mySum += x[i];\n        sdata[t] = mySum;\n        i += blockDim.x;\n    }\n    __syncthreads();\n    i = start + (t + 1) / 2;\n    while (i < end) {\n        sdata[t] += sdata[i];\n        i += blockDim.x / 2;\n    }\n    __syncthreads();\n    if (t == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use a shared memory array to store the intermediate results\n    // Hint: Use a for loop to iterate over the elements of x\n    // Hint: Use a for loop to iterate over the elements of output\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint: Use a for loop to iterate over the elements of the shared memory array\n    // Hint",
            "extern __shared__ double sdata[];\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[tid] = x[i];\n  __syncthreads();\n  for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n    if (tid % (2 * s) == 0) {\n      sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    output[blockIdx.x] = sdata[0];\n  }\n  __syncthreads();\n  for (unsigned int s = blockDim.x / 2; s > 0; s /= 2) {\n    if (tid < s) {\n      sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n  }\n  if (i < N) {\n    output[i] = sdata[tid] + x[i];\n  }\n}",
            "extern __shared__ double sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int gridSize = blockDim.x * gridDim.x;\n    sdata[tid] = x[i];\n    __syncthreads();\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n    __syncthreads();\n    for (unsigned int s = blockDim.x / 2; s > 0; s /= 2) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[i] = sdata[tid] + x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = x[i];\n        if (i > 0) {\n            output[i] += output[i-1];\n        }\n    }\n}",
            "extern __shared__ double sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  unsigned int offset = 0;\n  double sum = 0;\n  for (unsigned int i = start + t; i < N; i += stride) {\n    sum += x[i];\n    sdata[t] = sum;\n    __syncthreads();\n    if (t > 0) {\n      sum += sdata[t - 1];\n    }\n    __syncthreads();\n    if (i < N) {\n      output[i] = sum;\n    }\n    __synctrans();\n  }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ double sdata[];\n    unsigned int t = threadIdx.x;\n    unsigned int start = blockIdx.x * blockDim.x;\n    unsigned int end = start + blockDim.x;\n    unsigned int i = start + t;\n    double mySum = 0;\n\n    // Copy input into shared memory\n    sdata[t] = (i < N)? x[i] : 0;\n    __syncthreads();\n\n    // Parallel prefix sum\n    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n        int index = 2 * stride * t;\n        if (index < blockDim.x) {\n            sdata[index] += sdata[index - stride];\n        }\n        __syncthreads();\n    }\n\n    // Copy output from shared memory\n    if (i < N) {\n        output[i] = sdata[t];\n    }\n}",
            "extern __shared__ double sdata[];\n    unsigned int t = threadIdx.x;\n    unsigned int start = blockIdx.x * blockDim.x;\n    unsigned int end = start + blockDim.x;\n    unsigned int i = start + t;\n    sdata[t] = 0;\n    if (i < N) {\n        sdata[t] = x[i];\n    }\n    __syncthreads();\n    for (unsigned int stride = 1; stride <= blockDim.x; stride *= 2) {\n        int index = 2 * stride * t;\n        if (index < blockDim.x) {\n            sdata[index] += sdata[index - stride];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[i] = sdata[t];\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use the __shared__ keyword to declare a shared memory array\n    // Hint: Use the atomicAdd() function to perform a thread-safe addition\n    // Hint: Use the __syncthreads() function to synchronize threads\n    // Hint: Use the threadIdx.x and blockDim.x variables to determine the thread index and the number of threads in the block\n    // Hint: Use the blockIdx.x variable to determine the block index\n    // Hint: Use the gridDim.x variable to determine the number of blocks\n    // Hint: Use the blockDim.x and gridDim.x variables to determine the number of threads per block and the number of blocks\n    // Hint: Use the __shfl_down() function to perform a thread-safe reduction\n    // Hint: Use the __shfl() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_up() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_xor() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_up_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_down_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a thread-safe reduction\n    // Hint: Use the __shfl_sync() function to perform a thread-safe broadcast\n    // Hint: Use the __shfl_sync() function to perform a"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // Hint: Use MPI_Reduce\n  // Hint: Use MPI_IN_PLACE\n  // Hint: Use MPI_SUM\n  // Hint: Use MPI_DOUBLE\n  // Hint: Use MPI_ROOT\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // Hint: You may find the following functions useful\n  // - MPI_Reduce\n  // - MPI_Scan\n  // - MPI_Bcast\n  // - MPI_Gather\n  // - MPI_Allgather\n  // - MPI_Allreduce\n  // - MPI_Scatter\n  // - MPI_Alltoall\n  // - MPI_Alltoallv\n  // - MPI_Alltoallw\n  // - MPI_Reduce_scatter\n  // - MPI_Reduce_scatter_block\n  // - MPI_Exscan\n  // - MPI_Ireduce\n  // - MPI_Ireduce_scatter\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iallreduce\n  // - MPI_Iallreduce\n  // - MPI_Iallgather\n  // - MPI_Iallgatherv\n  // - MPI_Ialltoall\n  // - MPI_Ialltoallv\n  // - MPI_Ialltoallw\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iexscan\n  // - MPI_Iscan\n  // - MPI_Iscatter\n  // - MPI_Igather\n  // - MPI_Igatherv\n  // - MPI_Ibarrier\n  // - MPI_Ibcast\n  // - MPI_Ialltoallw\n  // - MPI_Ialltoallv\n  // - MPI_Ialltoall\n  // - MPI_Iscatterv\n  // - MPI_Iscatter\n  // - MPI_Iallgatherv\n  // - MPI_Iallgather\n  // - MPI_Iallreduce\n  // - MPI_Ireduce\n  // - MPI_Ireduce_scatter\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iscan\n  // - MPI_Iexscan\n  // - MPI_Ibcast\n  // - MPI_Ibarrier\n  // - MPI_Ialltoallw\n  // - MPI_Ialltoallv\n  // - MPI_Ialltoall\n  // - MPI_Iscatterv\n  // - MPI_Iscatter\n  // - MPI_Iallgatherv\n  // - MPI_Iallgather\n  // - MPI_Iallreduce\n  // - MPI_Ireduce\n  // - MPI_Ireduce_scatter\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iscan\n  // - MPI_Iexscan\n  // - MPI_Ibcast\n  // - MPI_Ibarrier\n  // - MPI_Ialltoallw\n  // - MPI_Ialltoallv\n  // - MPI_Ialltoall\n  // - MPI_Iscatterv\n  // - MPI_Iscatter\n  // - MPI_Iallgatherv\n  // - MPI_Iallgather\n  // - MPI_Iallreduce\n  // - MPI_Ireduce\n  // - MPI_Ireduce_scatter\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iscan\n  // - MPI_Iexscan\n  // - MPI_Ibcast\n  // - MPI_Ibarrier\n  // - MPI_Ialltoallw\n  // - MPI_Ialltoallv\n  // - MPI_Ialltoall\n  // - MPI_Iscatterv\n  // - MPI_Iscatter\n  // - MPI_Iallgatherv\n  // - MPI_Iallgather\n  // - MPI_Iallreduce\n  // - MPI_Ireduce\n  // - MPI_Ireduce_scatter\n  // - MPI_Ireduce_scatter_block\n  // - MPI_Iscan",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement the prefix sum algorithm\n    // Hint: Use MPI_Reduce\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int i, j;\n  int n = x.size();\n  int k = 1;\n  int m = n / k;\n  int m_new = n / (k * 2);\n  std::vector<double> x_new(m_new);\n  std::vector<double> x_new_new(m_new);\n  std::vector<double> x_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new(m_new);\n  std::vector<double> x_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_new_",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<double> local_x(n_per_proc + n_rem);\n  std::vector<double> local_output(n_per_proc + n_rem);\n\n  MPI_Scatter(x.data(), n_per_proc + n_rem, MPI_DOUBLE, local_x.data(), n_per_proc + n_rem, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_proc + n_rem; i++) {\n    local_output[i] = local_x[i];\n    if (i > 0) {\n      local_output[i] += local_output[i - 1];\n    }\n  }\n\n  MPI_Gather(local_output.data(), n_per_proc + n_rem, MPI_DOUBLE, output.data(), n_per_proc + n_rem, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<double> local_output(end - start);\n  for (int i = start; i < end; i++) {\n    local_output[i - start] = x[i];\n  }\n  for (int i = 1; i < size; i++) {\n    MPI_Send(&local_output[0], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n  }\n  for (int i = 1; i < size; i++) {\n    MPI_Recv(&local_output[0], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for (int j = 0; j < chunk; j++) {\n      local_output[j] += local_output[j - 1];\n    }\n  }\n  if (rank == 0) {\n    output.resize(n);\n    for (int i = 0; i < size; i++) {\n      MPI_Recv(&output[0], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement me!\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int n_proc = n / n_per_proc;\n  int n_proc_rem = n % n_per_proc;\n  std::vector<double> local_x(n_per_proc);\n  std::vector<double> local_output(n_per_proc);\n  std::vector<double> local_output_sum(n_per_proc);\n  std::vector<double> local_output_sum_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all(n_per_proc);\n  std::vector<double> local_output_sum_all_all_all_all_all_all_",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double *x, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nthreads = blockDim.x;\n    int nblocks = gridDim.x;\n\n    // Compute the prefix sum of the vector x into output.\n    // Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n    // Example:\n    //\n    // input: [1, 7, 4, 6, 6, 2]\n    // output: [1, 8, 12, 18, 24, 26]\n\n    // TODO: Implement this function\n    // You may use the following variables\n    //   - int tid = threadIdx.x;\n    //   - int bid = blockIdx.x;\n    //   - int nthreads = blockDim.x;\n    //   - int nblocks = gridDim.x;\n    //   - double *x = x;\n    //   - double *output = output;\n    //   - size_t N = N;\n    //\n    // You may use the following functions\n    //   - __syncthreads()\n    //   - atomicAdd(double *address, double val)\n    //   - __shfl_down(double var, unsigned int delta, int width = 32)\n    //   - __shfl_up(double var, unsigned int delta, int width = 32)\n    //   - __shfl(double var, int srcLane, int width = 32)\n    //   - __shfl_down_sync(unsigned mask, double var, unsigned int delta, int width = 32)\n    //   - __shfl_up_sync(unsigned mask, double var, unsigned int delta, int width = 32)\n    //   - __shfl_sync(unsigned mask, double var, int srcLane, int width = 32)\n    //   - __syncthreads_or(bool pred)\n    //   - __syncthreads_and(bool pred)\n    //   - __syncthreads_count(bool pred)\n    //   - __syncthreads_or_sync(unsigned mask, bool pred)\n    //   - __syncthreads_and_sync(unsigned mask, bool pred)\n    //   - __syncthreads_count_sync(unsigned mask, bool pred)\n    //   - __threadfence()\n    //   - __threadfence_block()\n    //   - __threadfence_system()\n    //   - __threadfence_block_syn()\n    //   - __threadfence_system_syn()\n    //   - __prof_trigger(int)\n    //   - __trap()\n    //   - __brkpt()\n    //   - __syncthreads_count()\n    //   - __syncthreads_count_sync()\n    //   - __syncthreads_and()\n    //   - __syncthreads_or()\n    //   - __syncthreads_and_sync()\n    //   - __syncthreads_or_sync()\n    //   - __syncthreads()\n    //   - __threadfence()\n    //   - __threadfence_block()\n    //   - __threadfence_system()\n    //   - __threadfence_block_syn()\n    //   - __threadfence_system_syn()\n    //   - __prof_trigger(int)\n    //   - __trap()\n    //   - __brkpt()\n    //   - __syncthreads_count()\n    //   - __syncthreads_count_sync()\n    //   - __syncthreads_and()\n    //   - __syncthreads_or()\n    //   - __syncthreads_and_sync()\n    //   - __syncthreads_or_sync()\n    //   - __syncthreads()\n    //   - __threadfence()\n    //   - __threadfence_block()\n    //   - __threadfence_system()\n    //   - __threadfence_block_syn()\n    //   - __threadfence_system",
            "// TODO: Implement me\n}",
            "extern __shared__ double shared[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int lane = i % warpSize;\n    int wid = i / warpSize;\n    int lane_mask = 1 << lane;\n    int warp_mask = (1 << warpSize) - 1;\n    double mySum = 0;\n    if (i < N) {\n        mySum = x[i];\n    }\n    // Each thread adds its partial sum to shared memory\n    shared[tid] = mySum;\n    __syncthreads();\n    // The first warp does the reduction\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            double y = __shfl_down_sync(warp_mask, mySum, offset);\n            if (lane < offset) {\n                mySum += y;\n            }\n        }\n    }\n    // Broadcast result to all threads in warp\n    mySum = __shfl_sync(warp_mask, mySum, 0);\n    // Write result for this block to global memory\n    if (i < N) {\n        output[i] = mySum;\n    }\n}",
            "extern __shared__ double s[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int lane = i % warpSize;\n    int wid = i / warpSize;\n    s[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (int offset = 1; offset < blockDim.x; offset *= 2) {\n        int index = 2 * offset * lane - (offset - 1) * lane;\n        if (index + offset < blockDim.x) {\n            s[index + offset] += s[index];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[i] = s[tid];\n    }\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = x[i];\n        if (i > 0) {\n            output[i] += output[i-1];\n        }\n    }\n}",
            "// TODO: implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    output[i] = x[i];\n    if (i > 0) {\n      output[i] += output[i - 1];\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ double sdata[];\n    unsigned int t = threadIdx.x;\n    unsigned int start = blockIdx.x * blockDim.x;\n    unsigned int end = start + blockDim.x;\n    unsigned int i = start + t;\n    sdata[t] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n        int index = 2 * stride * t;\n        if (index < blockDim.x) {\n            sdata[index] += sdata[index + stride];\n        }\n        __syncthreads();\n    }\n    if (t == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n}",
            "// TODO: Implement this function\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    output[idx] = x[idx];\n  }\n  __syncthreads();\n  for (int i = 1; i < blockDim.x; i *= 2) {\n    if (idx >= i && idx < N) {\n      output[idx] += output[idx - i];\n    }\n    __syncthreads();\n  }\n}",
            "extern __shared__ double temp[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bsize = blockDim.x;\n    int gsize = gridDim.x;\n    int gtid = bid * bsize + tid;\n    int gsize_m1 = gsize - 1;\n    int lsize = bsize * gsize;\n    int ltid = tid + bid * bsize;\n    int lsize_m1 = lsize - 1;\n    int i = gtid;\n    int j = ltid;\n    int k = 1;\n    int offset = 1;\n    double sum = 0;\n    double t;\n    double *s = temp;\n\n    // Copy input into shared memory\n    s[j] = x[i];\n\n    // Loop to compute the scan\n    while (k < lsize) {\n        __syncthreads();\n        if (j >= k) {\n            t = s[j - k];\n            sum += t;\n            s[j] += sum;\n        }\n        k <<= 1;\n        offset <<= 1;\n    }\n\n    // Copy output from shared memory\n    if (j < N) {\n        output[i] = s[j];\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = x[i];\n        if (i > 0) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    output[i] = x[i];\n    for (int j = 1; j < N; j *= 2) {\n      double tmp = output[i];\n      __syncthreads();\n      if (i >= j) {\n        output[i] += output[i - j];\n      }\n      __syncthreads();\n      output[i] = tmp;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "26_scan_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement me\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    std::vector<double> local_x(n_per_proc + n_rem);\n    std::vector<double> local_output(n_per_proc + n_rem);\n\n    MPI_Scatter(x.data(), n_per_proc + n_rem, MPI_DOUBLE, local_x.data(), n_per_proc + n_rem, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        local_output[0] = local_x[0];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < n_per_proc + n_rem; i++) {\n            local_output[i] = local_output[i - 1] + local_x[i];\n        }\n    }\n\n    MPI_Gather(local_output.data(), n_per_proc + n_rem, MPI_DOUBLE, output.data(), n_per_proc + n_rem, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  std::vector<double> local_x(n_local);\n  std::vector<double> local_output(n_local);\n  std::vector<double> local_output_reduced(n_local);\n  std::vector<double> local_output_reduced_reduced(n_local);\n\n  // Copy the local part of x into local_x\n  for (int i = 0; i < n_local; i++) {\n    local_x[i] = x[i + rank * n_per_rank];\n  }\n\n  // Compute the prefix sum of local_x\n  for (int i = 1; i < n_local; i++) {\n    local_output[i] = local_output[i - 1] + local_x[i];\n  }\n\n  // Reduce the local prefix sum into a single value\n  MPI_Reduce(local_output.data(), local_output_reduced.data(), n_local, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Reduce the local prefix sum into a single value\n  MPI_Reduce(local_output_reduced.data(), local_output_reduced_reduced.data(), n_local, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy the result into output\n  if (rank == 0) {\n    for (int i = 0; i < n_local; i++) {\n      output[i] = local_output_reduced_reduced[i];\n    }\n  }\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n\n    std::vector<double> local_x(n_per_rank);\n    std::vector<double> local_output(n_per_rank);\n\n    if (rank == 0) {\n        local_x = std::vector<double>(x.begin(), x.begin() + n_per_rank);\n    } else {\n        MPI_Status status;\n        MPI_Recv(local_x.data(), n_per_rank, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &status);\n    }\n\n    if (rank == size - 1) {\n        local_x = std::vector<double>(x.begin() + n - n_per_rank, x.end());\n    } else {\n        MPI_Send(local_x.data(), n_per_rank, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD);\n    }\n\n    local_output[0] = local_x[0];\n    for (int i = 1; i < n_per_rank; i++) {\n        local_output[i] = local_output[i - 1] + local_x[i];\n    }\n\n    if (rank == 0) {\n        output = std::vector<double>(n);\n    }\n\n    MPI_Gather(local_output.data(), n_per_rank, MPI_DOUBLE, output.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < n_per_rank; j++) {\n                output[i * n_per_rank + j] += output[j];\n            }\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_rem;\n  }\n  std::vector<double> local_sum(n_per_rank + 1);\n  local_sum[0] = x[start];\n  for (int i = start + 1; i < end; i++) {\n    local_sum[i - start] = x[i] + local_sum[i - start - 1];\n  }\n\n  std::vector<double> global_sum(n);\n  MPI_Gather(&local_sum[0], n_per_rank + 1, MPI_DOUBLE, &global_sum[0], n_per_rank + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    output[0] = global_sum[0];\n    for (int i = 1; i < n; i++) {\n      output[i] = global_sum[i] + output[i - 1];\n    }\n  }\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<double> local_sum(chunk + 1);\n  local_sum[0] = 0;\n  for (int i = 1; i <= chunk; i++) {\n    local_sum[i] = local_sum[i - 1] + x[start + i - 1];\n  }\n  std::vector<double> local_sum_all(chunk + 1);\n  MPI_Reduce(&local_sum[0], &local_sum_all[0], chunk + 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    output[0] = 0;\n    for (int i = 1; i <= chunk; i++) {\n      output[start + i - 1] = local_sum_all[i - 1];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the reverse prefix sum of the array x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(Kokkos::View<const int*> const& x, Kokkos::View<int*> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement me\n}",
            "// Create a parallel_scan to compute the prefix sum\n  Kokkos::parallel_scan(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i, int& update, const bool final) {\n      if (final) {\n        output(i) = update;\n      } else {\n        update += x(i);\n      }\n    }\n  );\n\n  // Create a parallel_for to compute the reverse prefix sum\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      output(i) = output(x.extent(0) - 1) - output(i);\n    }\n  );\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Fill in the code to compute the reverse prefix sum of x into output\n  // using Kokkos.\n  //\n  // Hint: Use Kokkos::parallel_scan to compute the reverse prefix sum.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in the array.\n  //\n  // Hint: Use Kokkos::Experimental::MinMaxScalar to compute the min and max\n  // values in",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j < n; j++) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j >= 0; j--) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int i;\n  int sum = 0;\n  #pragma omp parallel for private(i)\n  for (i = n-1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    output[i] = x[n - 1 - i];\n  }\n  #pragma omp parallel for\n  for (int i = 1; i < n; i++) {\n    output[i] += output[i - 1];\n  }\n}",
            "int n = x.size();\n    output.resize(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int sum = 0;\n        for (int j = i; j >= 0; j--) {\n            sum += x[j];\n        }\n        output[i] = sum;\n    }\n}",
            "// TODO: Implement me!\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j >= 0; j--) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n    output.resize(n);\n    int sum = 0;\n    #pragma omp parallel for\n    for (int i = n - 1; i >= 0; --i) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j < n; j++) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j < n; j++) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    int sum = 0;\n    for (int j = i; j >= 0; --j) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j >= 0; j--) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n    output.resize(n);\n\n    // TODO: implement this function\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        output[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i-1];\n    }\n\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
            "// TODO: Your code here\n    int n = x.size();\n    output.resize(n);\n    int sum = 0;\n    #pragma omp parallel for\n    for (int i = n-1; i >= 0; i--) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  output.resize(n);\n\n  // TODO: Implement this function\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j >= 0; j--) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n\n  // TODO: Fill in the code to compute the reverse prefix sum of x into output\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    output[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    for (int j = 0; j < i; j++) {\n      output[i] += x[j];\n    }\n  }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  int i;\n  int sum = 0;\n  int temp;\n  #pragma omp parallel for private(i, temp)\n  for (i = n - 1; i >= 0; i--) {\n    temp = sum;\n    sum += x[i];\n    output[i] = sum;\n    sum = temp;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j >= 0; j--) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}",
            "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    int sum = 0;\n    for (int j = i; j >= 0; --j) {\n      sum += x[j];\n    }\n    output[i] = sum;\n  }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "// TODO: Your code here\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "// TODO: Your code here\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "// TODO: Your code here\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "int sum = 0;\n    for (int i = x.size() - 1; i >= 0; i--) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}",
            "int sum = 0;\n  for (int i = x.size() - 1; i >= 0; --i) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "output.resize(x.size());\n    output[0] = x[0];\n    for (size_t i = 1; i < x.size(); ++i) {\n        output[i] = output[i - 1] + x[i];\n    }\n}",
            "// TODO: Your code here\n    int sum = 0;\n    for(int i = x.size()-1; i >= 0; i--){\n        sum += x[i];\n        output[i] = sum;\n    }\n}",
            "// TODO: Implement this function\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "int sum = 0;\n  for (int i = x.size() - 1; i >= 0; --i) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "// TODO: Your code here\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    sum += x[i];\n    output[i] = sum;\n  }\n}",
            "// TODO: Your code here\n    int sum = 0;\n    for (int i = x.size() - 1; i >= 0; i--) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n  int sum = 0;\n  for (int i = x.size() - 1; i >= 0; i--) {\n    output[i] = sum;\n    sum += x[i];\n  }\n}",
            "std::vector<int> temp(x.size());\n  std::partial_sum(x.begin(), x.end(), temp.begin(), std::plus<int>());\n  std::partial_sum(temp.rbegin(), temp.rend(), output.begin(), std::plus<int>());\n}",
            "int sum = 0;\n    for (int i = x.size() - 1; i >= 0; i--) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}",
            "// TODO: Implement this function\n    int sum = 0;\n    for (int i = x.size() - 1; i >= 0; i--) {\n        output[i] = sum;\n        sum += x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  extern __shared__ int sdata[];\n  sdata[tid] = x[tid];\n  __syncthreads();\n  for (int i = 1; i < blockDim.x; i *= 2) {\n    int index = 2 * i * tid;\n    if (index < N) {\n      sdata[index] += sdata[index - i];\n    }\n    __syncthreads();\n  }\n  output[tid] = sdata[tid];\n}",
            "extern __shared__ int temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n  int i = bid * blockSize + tid;\n  int j = N - 1 - i;\n  temp[tid] = x[j];\n  __syncthreads();\n  for (unsigned int stride = 1; stride < blockSize; stride *= 2) {\n    int index = 2 * stride * tid;\n    if (index < blockSize) {\n      temp[index] += temp[index - stride];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    output[j] = temp[blockSize - 1];\n  }\n}",
            "// TODO: Implement this function\n}",
            "extern __shared__ int temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nthreads = blockDim.x;\n  int nblocks = gridDim.x;\n  int start = bid * nthreads;\n  int end = (bid + 1) * nthreads;\n  int i;\n  int sum = 0;\n  int tempSum = 0;\n\n  // Load data into shared memory\n  for (i = start + tid; i < end; i += nthreads) {\n    temp[i] = x[i];\n  }\n  __syncthreads();\n\n  // Compute the prefix sum\n  for (i = nthreads / 2; i > 0; i /= 2) {\n    if (tid < i) {\n      tempSum = temp[start + tid];\n      temp[start + tid] = sum;\n      sum += tempSum;\n    }\n    __syncthreads();\n  }\n\n  // Write the results to global memory\n  for (i = start + tid; i < end; i += nthreads) {\n    output[i] = sum;\n    sum += temp[i];\n  }\n}",
            "// TODO: Implement this function\n  int sum = 0;\n  int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    sum = x[index];\n    for (int i = 1; i < N; i++) {\n      int temp = sum;\n      sum += x[N - i];\n      x[N - i] = temp;\n    }\n    output[index] = sum;\n  }\n}",
            "extern __shared__ int temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nthreads = blockDim.x;\n  int nblocks = gridDim.x;\n  int i = bid * nthreads + tid;\n  int j = N - i - 1;\n  int k = nthreads - tid - 1;\n  int sum = 0;\n  if (i < N) {\n    sum = x[j];\n  }\n  temp[tid] = sum;\n  __syncthreads();\n  for (int s = 1; s < nthreads; s *= 2) {\n    if (tid >= s) {\n      temp[k] += temp[k - s];\n    }\n    __syncthreads();\n  }\n  if (i < N) {\n    output[j] = temp[k];\n  }\n}",
            "// TODO: Implement this function\n  int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id >= N) return;\n  int temp = 0;\n  for (int i = N - 1; i > id; i--) {\n    temp += x[i];\n  }\n  output[id] = temp;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = N - 1; i >= 0; i--) {\n            int temp = output[i];\n            output[i] = sum;\n            sum += temp;\n        }\n    }\n}",
            "// TODO: implement me\n}",
            "// TODO: implement this function\n}",
            "extern __shared__ int temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n  int i = bid * blockSize + tid;\n  int j = N - i - 1;\n  int k = N - 1 - tid;\n  int sum = 0;\n  if (i < N) {\n    sum = x[i];\n  }\n  temp[tid] = sum;\n  __syncthreads();\n  for (int s = 1; s < blockSize; s *= 2) {\n    if (tid >= s) {\n      temp[k] += temp[k - s];\n    }\n    __syncthreads();\n  }\n  if (i < N) {\n    output[j] = temp[k];\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = N - 1; i >= 0; i--) {\n            int temp = output[i];\n            output[i] = sum;\n            sum += temp;\n        }\n    }\n}",
            "extern __shared__ int sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int gridSize = blockDim.x * gridDim.x;\n    sdata[tid] = x[i];\n    __syncthreads();\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid >= s) {\n            sdata[tid] += sdata[tid - s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        sdata[N - 1] = 0;\n    }\n    __syncthreads();\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    output[i] = sdata[tid];\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  extern __shared__ int temp[];\n  int tempIndex = threadIdx.x;\n  int stride = blockDim.x;\n  int i = tid;\n  int sum = 0;\n\n  // Load data into shared memory\n  temp[tempIndex] = x[i];\n  __syncthreads();\n\n  // Parallel prefix sum\n  while (i < N) {\n    sum += temp[tempIndex];\n    temp[tempIndex] = sum;\n    i += stride;\n    tempIndex += stride;\n    __syncthreads();\n  }\n\n  // Write results to device memory\n  if (tid < N) {\n    output[tid] = temp[tempIndex - stride];\n  }\n}",
            "// TODO: Implement this function\n    int sum = 0;\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        sum = x[i];\n        for (int j = 1; j < N - i; j++) {\n            sum += x[i + j];\n            output[i + j] = sum;\n        }\n    }\n}",
            "extern __shared__ int temp[];\n  int *temp_ptr = temp;\n  int *output_ptr = output;\n  int *x_ptr = x;\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n  int i = bid * blockSize + tid;\n  int j = N - 1 - i;\n  int temp_index = 2 * tid;\n  int output_index = 2 * i;\n  int x_index = j;\n  temp_ptr[temp_index] = 0;\n  temp_ptr[temp_index + 1] = 0;\n  if (i < N) {\n    temp_ptr[temp_index] = x_ptr[x_index];\n    temp_ptr[temp_index + 1] = x_ptr[x_index];\n  }\n  __syncthreads();\n  for (unsigned int s = 1; s < blockSize; s *= 2) {\n    if (tid >= s) {\n      temp_ptr[temp_index] += temp_ptr[temp_index - s];\n      temp_ptr[temp_index + 1] += temp_ptr[temp_index + 1 - s];\n    }\n    __syncthreads();\n  }\n  if (i < N) {\n    output_ptr[output_index] = temp_ptr[temp_index];\n    output_ptr[output_index + 1] = temp_ptr[temp_index + 1];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int sum = 0;\n  int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    sum = x[index];\n    for (int i = 1; i < N - index; i *= 2) {\n      int y = __shfl_up_sync(0xffffffff, sum, i);\n      if (i <= threadIdx.x) {\n        sum += y;\n      }\n    }\n    output[index] = sum;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use the __shfl_down_sync() intrinsic\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same size on all ranks.\n  // You may assume that the input vector has size >= 1.\n  // You may assume that the input vector has size that is a multiple of size.\n\n  // TODO: Fill in the code to compute the reverse prefix sum.\n  // You may assume that the input vector x is the same on all ranks.\n  // You may assume that the output vector is empty on all ranks except rank 0.\n  // You may assume that the output vector is empty on rank 0.\n  // You may assume that the output vector has the same size as the input vector.\n  // You may assume that the input vector has the same",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  if (rank == 0) {\n    output = x;\n  }\n  MPI_Bcast(output.data(), output.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  for (int i = 1; i < size; i++) {\n    int offset = i * (x.size() / size);\n    std::vector<int> temp(offset, 0);\n    MPI_Scatter(output.data(), offset, MPI_INT, temp.data(), offset, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int j = 0; j < offset; j++) {\n      temp[j] += output[j + offset - 1];\n    }\n    MPI_Gather(temp.data(), offset, MPI_INT, output.data(), offset, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<int> local_x(n_per_proc + n_rem);\n  std::vector<int> local_output(n_per_proc + n_rem);\n\n  MPI_Scatter(x.data(), n_per_proc + n_rem, MPI_INT, local_x.data(), n_per_proc + n_rem, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_proc + n_rem; i++) {\n    local_output[i] = local_x[i];\n  }\n\n  for (int i = 1; i < size; i++) {\n    MPI_Send(local_output.data(), n_per_proc + n_rem, MPI_INT, i, 0, MPI_COMM_WORLD);\n  }\n\n  for (int i = 0; i < n_per_proc + n_rem; i++) {\n    local_output[i] = local_output[i] + local_output[i - 1];\n  }\n\n  for (int i = 0; i < n_per_proc + n_rem; i++) {\n    local_output[i] = local_output[i] - local_x[i];\n  }\n\n  MPI_Gather(local_output.data(), n_per_proc + n_rem, MPI_INT, output.data(), n_per_proc + n_rem, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size() / size;\n  int local_rank = rank;\n  int local_start = local_rank * local_size;\n  int local_end = local_start + local_size;\n\n  std::vector<int> local_x(local_size);\n  std::vector<int> local_output(local_size);\n\n  for (int i = 0; i < local_size; i++) {\n    local_x[i] = x[local_start + i];\n  }\n\n  for (int i = 0; i < local_size; i++) {\n    local_output[i] = local_x[i];\n  }\n\n  for (int i = 1; i < local_size; i++) {\n    local_output[i] += local_output[i - 1];\n  }\n\n  for (int i = local_size - 2; i >= 0; i--) {\n    local_output[i] += local_output[i + 1];\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < local_size; i++) {\n      output[local_start + i] = local_output[i];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      int start = i * local_size;\n      int end = start + local_size;\n      MPI_Recv(&output[start], local_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&local_output[0], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    // You may assume that x is the same length as output\n    // You may assume that output has already been allocated\n    // You may assume that x has already been allocated\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You may assume that x and output are the same length\n    // You",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int sum = 0;\n    if (i < N) {\n        sum = x[i];\n        for (int j = i + 1; j < N; j++) {\n            sum += x[j];\n            output[j] = sum;\n        }\n    }\n}",
            "extern __shared__ int temp[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int j = N - 1 - i;\n    int temp_sum = 0;\n    if (i < N) {\n        temp[tid] = x[j];\n        __syncthreads();\n        for (int s = 1; s < blockDim.x; s *= 2) {\n            if (tid >= s) {\n                temp[tid] += temp[tid - s];\n            }\n            __syncthreads();\n        }\n        if (tid == 0) {\n            output[j] = temp[blockDim.x - 1];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int sum = 0;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        sum = x[N - idx - 1];\n        if (idx > 0) {\n            sum += output[N - idx];\n        }\n        output[N - idx - 1] = sum;\n    }\n}",
            "// TODO: Implement this function\n  int sum = 0;\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    sum = x[N - index - 1];\n    for (int i = 1; i < blockDim.x; i *= 2) {\n      int n = __shfl_up_sync(0xffffffff, sum, i);\n      if (threadIdx.x >= i) {\n        sum += n;\n      }\n    }\n    output[N - index - 1] = sum;\n  }\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int blockSize = blockDim.x;\n    int index = bid * blockSize + tid;\n    int stride = blockSize * gridDim.x;\n\n    __shared__ int temp[1024];\n\n    int sum = 0;\n    for (int i = index; i < N; i += stride) {\n        temp[tid] = x[i];\n        __syncthreads();\n        if (tid == 0) {\n            for (int j = 0; j < blockSize; j++) {\n                sum += temp[j];\n                temp[j] = sum;\n            }\n        }\n        __syncthreads();\n        output[i] = temp[tid];\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = i - 1;\n    int sum = 0;\n    if (i < N) {\n        sum = x[i];\n        while (j >= 0) {\n            sum += x[j];\n            output[j] = sum;\n            j--;\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int sum = 0;\n        for (int i = idx; i >= 0; i -= blockDim.x) {\n            sum += x[i];\n            output[i] = sum;\n        }\n    }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int sum = 0;\n    for (int j = i; j >= 0; j -= blockDim.x) {\n      sum += x[j];\n      output[j] = sum;\n    }\n  }\n}",
            "extern __shared__ int temp[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int j = N - 1 - i;\n    temp[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (int d = 1; d < blockDim.x; d *= 2) {\n        int n = 2 * d * tid;\n        if (n + d < blockDim.x) {\n            temp[n + d] += temp[n];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[j] = temp[blockDim.x - 1];\n    }\n}",
            "extern __shared__ int temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int bsize = blockDim.x;\n  int gsize = gridDim.x;\n  int i = bid * bsize + tid;\n  int j = N - i - 1;\n  int k = 0;\n  int sum = 0;\n\n  // Load data into shared memory\n  if (i < N) {\n    temp[tid] = x[i];\n  }\n\n  // Wait for all threads to finish loading data\n  __syncthreads();\n\n  // Compute the sum of the elements in the block\n  for (k = 0; k < bsize; k++) {\n    sum += temp[k];\n  }\n\n  // Wait for all threads to finish computing the sum\n  __syncthreads();\n\n  // Write the sum of the block into the output array\n  if (tid == 0) {\n    output[j] = sum;\n  }\n\n  // Wait for all threads to finish writing the sum\n  __syncthreads();\n\n  // Compute the sum of the elements in the block\n  for (k = 0; k < bsize; k++) {\n    sum += temp[k];\n  }\n\n  // Wait for all threads to finish computing the sum\n  __syncthreads();\n\n  // Write the sum of the block into the output array\n  if (tid == 0) {\n    output[j] = sum;\n  }\n}",
            "extern __shared__ int temp[];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n    int j = N - i - 1;\n    temp[tid] = (i < N)? x[i] : 0;\n    __syncthreads();\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        if (tid >= s) {\n            temp[tid] += temp[tid - s];\n        }\n        __syncthreads();\n    }\n    if (i < N) {\n        output[j] = temp[tid];\n    }\n}",
            "// TODO: Implement this function\n  int sum = 0;\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    sum = x[index];\n    for (int i = 1; i < N - index; i++) {\n      sum += x[index + i];\n      output[index + i] = sum;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum\n    // Hint: Use __syncthreads() to make sure all threads have finished their work before continuing\n    // Hint: Use __shfl_up() to share values between threads\n\n    // TODO: Compute the reverse prefix sum of x into output\n    // Hint: Use atomicAdd() to compute the sum",
            "// TODO: Implement this function\n    int sum = 0;\n    for (int i = N - 1; i >= 0; i--) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "29_scan_reverse_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = x.size() / num_threads;\n    int num_extra = x.size() % num_threads;\n    int start = rank * num_per_thread;\n    int end = start + num_per_thread;\n    if (rank == 0) {\n        end += num_extra;\n    }\n    std::vector<int> local_output(end - start);\n    for (int i = start; i < end; i++) {\n        local_output[i - start] = x[i];\n    }\n    for (int i = 1; i < num_threads; i++) {\n        if (rank == i) {\n            local_output[0] += x[start - 1];\n        }\n        MPI_Send(&local_output[0], local_output.size(), MPI_INT, i - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&local_output[0], local_output.size(), MPI_INT, i - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&local_output[0], local_output.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < local_output.size(); j++) {\n                local_output[j] += x[end + j - 1];\n            }\n        }\n    }\n    if (rank == 0) {\n        output = local_output;\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  // TODO: Implement this function\n  if (rank == 0) {\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      output[i] = output[i - 1] + x[i];\n    }\n  }\n\n  // TODO: Implement this function\n  if (rank!= 0) {\n    MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // TODO: Implement this function\n  if (rank == 0) {\n    std::vector<int> temp(x.size());\n    MPI_Status status;\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n      for (int j = 0; j < x.size(); j++) {\n        output[j] += temp[j];\n      }\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n    std::vector<int> local_x(n_local);\n    std::vector<int> local_output(n_local);\n    std::vector<int> local_output_reduced(n_local);\n    std::vector<int> local_output_reduced_reduced(n_local);\n    std::vector<int> local_output_reduced_reduced_reduced(n_local);\n\n    MPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, local_x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        local_output[0] = local_x[0];\n        for (int i = 1; i < n_local; i++) {\n            local_output[i] = local_output[i - 1] + local_x[i];\n        }\n    } else {\n        local_output[0] = local_x[0];\n        for (int i = 1; i < n_local; i++) {\n            local_output[i] = local_output[i - 1] + local_x[i];\n        }\n    }\n\n    MPI_Gather(local_output.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, local_output_reduced.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        local_output_reduced_reduced[0] = local_output_reduced[0];\n        for (int i = 1; i < n_local; i++) {\n            local_output_reduced_reduced[i] = local_output_reduced_reduced[i - 1] + local_output_reduced[i];\n        }\n    } else {\n        local_output_reduced_reduced[0] = local_output_reduced[0];\n        for (int i = 1; i < n_local; i++) {\n            local_output_reduced_reduced[i] = local_output_reduced_reduced[i - 1] + local_output_reduced[i];\n        }\n    }\n\n    MPI_Gather(local_output_reduced_reduced.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, local_output_reduced_reduced_reduced.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = local_output_reduced_reduced_reduced[0];\n        for (int i = 1; i < n_local; i++) {\n            output[i] = local_output_reduced_reduced_reduced[i - 1] + local_output_reduced_reduced_reduced[i];\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n    int rank = 0;\n    int num_ranks = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO: Your code here\n    int chunk = size / num_ranks;\n    int remainder = size % num_ranks;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == num_ranks - 1) {\n        end += remainder;\n    }\n    std::vector<int> local_output(end - start);\n    std::vector<int> local_x(x.begin() + start, x.begin() + end);\n    int local_size = local_x.size();\n    int local_rank = rank;\n    int local_num_ranks = num_ranks;\n\n    #pragma omp parallel\n    {\n        int local_thread_id = omp_get_thread_num();\n        int local_num_threads = omp_get_num_threads();\n        int local_chunk = local_size / local_num_threads;\n        int local_remainder = local_size % local_num_threads;\n        int local_start = local_thread_id * local_chunk;\n        int local_end = local_start + local_chunk;\n        if (local_thread_id == local_num_threads - 1) {\n            local_end += local_remainder;\n        }\n        std::vector<int> local_local_output(local_end - local_start);\n        std::vector<int> local_local_x(local_x.begin() + local_start, local_x.begin() + local_end);\n        int local_local_size = local_local_x.size();\n        int local_local_rank = local_rank;\n        int local_local_num_ranks = local_num_ranks;\n\n        #pragma omp for\n        for (int i = 0; i < local_local_size; i++) {\n            local_local_output[i] = local_local_x[i];\n        }\n\n        #pragma omp barrier\n\n        #pragma omp for\n        for (int i = 1; i < local_local_size; i++) {\n            local_local_output[i] += local_local_output[i - 1];\n        }\n\n        #pragma omp barrier\n\n        #pragma omp single\n        {\n            int global_chunk = size / num_ranks;\n            int global_remainder = size % num_ranks;\n            int global_start = rank * global_chunk;\n            int global_end = global_start + global_chunk;\n            if (rank == num_ranks - 1) {\n                global_end += global_remainder;\n            }\n            std::vector<int> global_output(global_end - global_start);\n            std::vector<int> global_x(x.begin() + global_start, x.begin() + global_end);\n            int global_size = global_x.size();\n            int global_rank = rank;\n            int global_num_ranks = num_ranks;\n\n            #pragma omp for\n            for (int i = 0; i < global_size; i++) {\n                global_output[i] = global_x[i];\n            }\n\n            #pragma omp barrier\n\n            #pragma omp for\n            for (int i = 1; i < global_size; i++) {\n                global_output[i] += global_output[i - 1];\n            }\n\n            #pragma omp barrier\n\n            #pragma omp single\n            {\n                for (int i = 0; i < local_size; i++) {\n                    local_output[i] = global_output[i + start];\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        output = local_output;\n    }\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int num_threads = omp_get_max_threads();\n  int num_per_thread = x.size() / num_threads;\n  int num_remainder = x.size() % num_threads;\n  int start_index = rank * num_per_thread;\n  int end_index = start_index + num_per_thread;\n  if (rank == 0) {\n    end_index += num_remainder;\n  }\n  std::vector<int> local_output(num_per_thread, 0);\n  std::vector<int> local_x(x.begin() + start_index, x.begin() + end_index);\n  std::vector<int> local_output_temp(num_per_thread, 0);\n  std::vector<int> local_x_temp(num_per_thread, 0);\n  std::vector<int> local_output_final(num_per_thread, 0);\n  std::vector<int> local_x_final(num_per_thread, 0);\n  if (rank == 0) {\n    local_output_final = local_output;\n    local_x_final = local_x;\n  }\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_id = omp_get_thread_num();\n    int start_index_temp = thread_id * num_per_thread;\n    int end_index_temp = start_index_temp + num_per_thread;\n    if (thread_id == 0) {\n      end_index_temp += num_remainder;\n    }\n    local_output_temp = local_output;\n    local_x_temp = local_x;\n    for (int i = 1; i < num_per_thread; i++) {\n      local_output_temp[i] = local_output_temp[i] + local_output_temp[i - 1];\n    }\n    for (int i = 1; i < num_per_thread; i++) {\n      local_x_temp[i] = local_x_temp[i] + local_x_temp[i - 1];\n    }\n    #pragma omp barrier\n    #pragma omp master\n    {\n      for (int i = 0; i < num_per_thread; i++) {\n        local_output_final[i] = local_output_final[i] + local_output_temp[i];\n        local_x_final[i] = local_x_final[i] + local_x_temp[i];\n      }\n    }\n  }\n  if (rank == 0) {\n    output = local_output_final;\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement me!\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int n_per_proc_plus_one = n_per_proc + 1;\n    int n_per_proc_plus_one_rem = n_per_proc_plus_one + n_rem;\n    int n_per_proc_plus_one_rem_per_proc = n_per_proc_plus_one_rem / size;\n    int n_per_proc_plus_one_rem_per_proc_plus_one = n_per_proc_plus_one_rem_per_proc + 1;\n    int n_per_proc_plus_one_rem_per_proc_plus_one_rem = n_per_proc_plus_one_rem_per_proc_plus_one + n_rem;\n\n    std::vector<int> x_local(n_per_proc_plus_one_rem_per_proc_plus_one_rem);\n    std::vector<int> output_local(n_per_proc_plus_one_rem_per_proc_plus_one_rem);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_proc_plus_one_rem_per_proc_plus_one_rem; i++) {\n            x_local[i] = x[i];\n        }\n    } else {\n        for (int i = 0; i < n_per_proc_plus_one_rem_per_proc_plus_one_rem; i++) {\n            x_local[i] = x[i + rank * n_per_proc_plus_one_rem_per_proc_plus_one_rem];\n        }\n    }\n\n    std::vector<int> x_local_copy(n_per_proc_plus_one_rem_per_proc_plus_one_rem);\n    std::vector<int> output_local_copy(n_per_proc_plus_one_rem_per_proc_plus_one_rem);\n\n    for (int i = 0; i < n_per_proc_plus_one_rem_per_proc_plus_one_rem; i++) {\n        x_local_copy[i] = x_local[i];\n        output_local_copy[i] = output_local[i];\n    }\n\n    int n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread = n_per_proc_plus_one_rem_per_proc_plus_one_rem / omp_get_max_threads();\n    int n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread_rem = n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread + n_rem;\n\n    std::vector<int> x_local_copy_per_thread(n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread_rem);\n    std::vector<int> output_local_copy_per_thread(n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread_rem);\n\n    for (int i = 0; i < n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread_rem; i++) {\n        x_local_copy_per_thread[i] = x_local_copy[i];\n        output_local_copy_per_thread[i] = output_local_copy[i];\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_proc_plus_one_rem_per_proc_plus_one_rem_per_thread_rem; i++) {\n        int sum = 0;\n        for (int j = i; j >= 0; j--) {\n            sum += x",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int local_size = x.size();\n  int local_rank = rank;\n  int local_start = 0;\n  int local_end = local_size;\n  int local_sum = 0;\n  int global_sum = 0;\n  int global_start = 0;\n  int global_end = local_size;\n  int global_rank = rank;\n  int global_size = size;\n  int global_sum_temp = 0;\n\n  if (rank == 0) {\n    global_start = 0;\n    global_end = local_size;\n    global_rank = 0;\n    global_size = size;\n  }\n  else {\n    global_start = local_size * (rank - 1);\n    global_end = local_size * rank;\n    global_rank = rank - 1;\n    global_size = size - 1;\n  }\n\n  for (int i = global_end - 1; i >= global_start; i--) {\n    local_sum += x[i];\n  }\n\n  MPI_Reduce(&local_sum, &global_sum_temp, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    global_sum = global_sum_temp;\n    for (int i = global_end - 1; i >= global_start; i--) {\n      output[i] = global_sum;\n      global_sum -= x[i];\n    }\n  }\n  else {\n    MPI_Send(&local_sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Bcast(&global_sum, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = global_start; i < global_end; i++) {\n    output[i] = global_sum - output[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&local_sum, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < local_size; j++) {\n        output[local_size * i + j] = local_sum;\n        local_sum -= x[local_size * i + j];\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remain = n % size;\n    int n_per_rank_plus = n_per_rank + 1;\n    int n_per_rank_minus = n_per_rank - 1;\n    int n_per_rank_plus_remain = n_per_rank_plus + n_remain;\n    int n_per_rank_minus_remain = n_per_rank_minus + n_remain;\n    int n_per_rank_plus_remain_last = n_per_rank_plus_remain - 1;\n    int n_per_rank_minus_remain_last = n_per_rank_minus_remain - 1;\n    int n_per_rank_plus_remain_last_rank = n_per_rank_plus_remain_last / size;\n    int n_per_rank_minus_remain_last_rank = n_per_rank_minus_remain_last / size;\n    int n_per_rank_plus_remain_last_rank_remain = n_per_rank_plus_remain_last % size;\n    int n_per_rank_minus_remain_last_rank_remain = n_per_rank_minus_remain_last % size;\n    int n_per_rank_plus_remain_last_rank_remain_last = n_per_rank_plus_remain_last_rank_remain - 1;\n    int n_per_rank_minus_remain_last_rank_remain_last = n_per_rank_minus_remain_last_rank_remain - 1;\n    int n_per_rank_plus_remain_last_rank_remain_last_rank = n_per_rank_plus_remain_last_rank_remain_last / size;\n    int n_per_rank_minus_remain_last_rank_remain_last_rank = n_per_rank_minus_remain_last_rank_remain_last / size;\n    int n_per_rank_plus_remain_last_rank_remain_last_rank_remain = n_per_rank_plus_remain_last_rank_remain_last % size;\n    int n_per_rank_minus_remain_last_rank_remain_last_rank_remain = n_per_rank_minus_remain_last_rank_remain_last % size;\n    int n_per_rank_plus_remain_last_rank_remain_last_rank_remain_last = n_per_rank_plus_remain_last_rank_remain_last_rank_remain - 1;\n    int n_per_rank_minus_remain_last_rank_remain_last_rank_remain_last = n_per_rank_minus_remain_last_rank_remain_last_rank_remain - 1;\n    int n_per_rank_plus_remain_last_rank_remain_last_rank_remain_last_rank = n_per_rank_plus_remain_last_rank_remain_last_rank_remain_last / size;\n    int n_per_rank_minus_remain_last_rank_remain_last_rank_remain_last_rank = n_per_rank_minus_remain_last_rank_remain_last_rank_remain_last / size;\n    int n_per_rank_plus_remain_last_rank_remain_last_rank_remain_last_rank_remain = n_per_rank_plus_remain_last_rank_remain_last_rank_remain_last_rank_remain - 1;\n    int n_per_rank_minus_remain_last_rank_remain_last_rank_remain_last_rank_remain = n_per_rank_minus_remain_last_rank_",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size() / size;\n  int local_start = rank * local_size;\n  int local_end = (rank + 1) * local_size;\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  std::vector<int> local_output(local_size);\n  std::vector<int> local_x(local_size);\n  std::copy(x.begin() + local_start, x.begin() + local_end, local_x.begin());\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_size; i++) {\n    local_output[i] = local_x[i];\n  }\n\n  for (int i = 1; i < local_size; i++) {\n    local_output[i] += local_output[i - 1];\n  }\n\n  std::reverse(local_output.begin(), local_output.end());\n\n  MPI_Gather(local_output.data(), local_size, MPI_INT, output.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::reverse(output.begin(), output.end());\n  }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int n_proc = rank < n_rem? rank * (n_per_proc + 1) : rank * n_per_proc + n_rem;\n  int n_proc_next = rank < n_rem? (rank + 1) * (n_per_proc + 1) : (rank + 1) * n_per_proc + n_rem;\n\n  std::vector<int> local_x(n_per_proc + (rank < n_rem));\n  std::vector<int> local_y(n_per_proc + (rank < n_rem));\n  std::vector<int> local_z(n_per_proc + (rank < n_rem));\n\n  if (rank == 0) {\n    output.resize(n);\n  }\n\n  for (int i = 0; i < n_per_proc + (rank < n_rem); i++) {\n    local_x[i] = x[n_proc + i];\n  }\n\n  int sum = 0;\n  for (int i = 0; i < n_per_proc + (rank < n_rem); i++) {\n    sum += local_x[i];\n    local_y[i] = sum;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_proc + (rank < n_rem); i++) {\n      output[n_proc + i] = local_y[i];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Gather(&local_y[0], n_per_proc + (rank < n_rem), MPI_INT, &local_z[0], n_per_proc + (rank < n_rem), MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_proc; i++) {\n      output[i] = local_z[n_proc_next - i - 1];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_proc; i++) {\n      output[i] = local_z[n_proc_next - i - 1];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Bcast(&output[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(Kokkos::View<int*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Implement me\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement me!\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    int max_sum_index = 0;\n    int sum_index = 0;\n    #pragma omp parallel for reduction(max:max_sum) reduction(max:max_sum_index)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n            max_sum_index = sum_index;\n        }\n        if (sum < 0) {\n            sum = 0;\n            sum_index = i + 1;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    int i;\n\n    #pragma omp parallel for private(sum) reduction(max:max_sum)\n    for (i = 0; i < n; i++) {\n        sum = 0;\n        for (int j = i; j < n; j++) {\n            sum += x[j];\n            if (sum > max_sum) {\n                max_sum = sum;\n            }\n        }\n    }\n\n    return max_sum;\n}",
            "int n = x.size();\n    std::vector<int> max_sum(n, 0);\n    std::vector<int> sum(n, 0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        sum[i] = x[i];\n        if (i > 0) {\n            sum[i] += sum[i - 1];\n        }\n    }\n\n    int max_sum_value = sum[0];\n    for (int i = 0; i < n; i++) {\n        if (sum[i] > max_sum_value) {\n            max_sum_value = sum[i];\n        }\n    }\n\n    return max_sum_value;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  #pragma omp parallel for reduction(max: max_sum)\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j < n; j++) {\n      sum += x[j];\n      max_sum = std::max(max_sum, sum);\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n    int maxSum = 0;\n    int sum = 0;\n    #pragma omp parallel for reduction(max:maxSum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > maxSum) {\n            maxSum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return maxSum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  std::vector<int> partial_sum(n);\n  partial_sum[0] = x[0];\n  for (int i = 1; i < n; ++i) {\n    partial_sum[i] = partial_sum[i - 1] + x[i];\n  }\n\n  int max_sum = partial_sum[0];\n  for (int i = 1; i < n; ++i) {\n    if (partial_sum[i] > max_sum) {\n      max_sum = partial_sum[i];\n    }\n  }\n\n  return max_sum;\n}",
            "int n = x.size();\n    int max_sum = x[0];\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int maxSum = 0;\n  int sum = 0;\n  int maxSumIdx = 0;\n  int sumIdx = 0;\n\n  #pragma omp parallel for reduction(max:maxSum) reduction(max:maxSumIdx)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n      maxSumIdx = sumIdx;\n    }\n    if (sum < 0) {\n      sum = 0;\n      sumIdx = i + 1;\n    }\n  }\n\n  return maxSum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int maxSum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return maxSum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  int max_sum_thread = 0;\n  int sum_thread = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; i++) {\n    sum_thread += x[i];\n    if (sum_thread > max_sum_thread) {\n      max_sum_thread = sum_thread;\n    }\n    if (sum_thread < 0) {\n      sum_thread = 0;\n    }\n  }\n  return max_sum_thread;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    #pragma omp parallel for reduction(max:max_sum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:max_sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    #pragma omp parallel for reduction(max:max_sum)\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int maxSum = 0;\n  int sum = 0;\n  #pragma omp parallel for reduction(max:maxSum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return maxSum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "serial",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max_so_far = x[0];\n    int max_ending_here = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        max_ending_here = std::max(x[i], max_ending_here + x[i]);\n        max_so_far = std::max(max_so_far, max_ending_here);\n    }\n    return max_so_far;\n}",
            "int max_sum = x[0];\n    int sum = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        sum = std::max(sum + x[i], x[i]);\n        max_sum = std::max(max_sum, sum);\n    }\n    return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    max_sum = std::max(max_sum, sum);\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_so_far = x[0];\n  int max_ending_here = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    max_ending_here = std::max(x[i], max_ending_here + x[i]);\n    max_so_far = std::max(max_so_far, max_ending_here);\n  }\n  return max_so_far;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = x[0];\n  int current_sum = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    current_sum = std::max(x[i], current_sum + x[i]);\n    max_sum = std::max(max_sum, current_sum);\n  }\n  return max_sum;\n}",
            "int max_sum = x[0];\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (auto i : x) {\n        sum += i;\n        max_sum = std::max(max_sum, sum);\n        sum = std::max(sum, 0);\n    }\n    return max_sum;\n}",
            "int max_so_far = x[0];\n  int max_ending_here = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    max_ending_here = std::max(x[i], max_ending_here + x[i]);\n    max_so_far = std::max(max_so_far, max_ending_here);\n  }\n  return max_so_far;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int current_sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    current_sum += x[i];\n    if (current_sum > max_sum) {\n      max_sum = current_sum;\n    }\n    if (current_sum < 0) {\n      current_sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    } else if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    } else if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (auto i : x) {\n    sum += i;\n    max_sum = std::max(max_sum, sum);\n    sum = std::max(0, sum);\n  }\n  return max_sum;\n}",
            "int max_sum = x[0];\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = x[0];\n  int sum = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_so_far = x[0];\n    int max_ending_here = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        max_ending_here = std::max(x[i], max_ending_here + x[i]);\n        max_so_far = std::max(max_so_far, max_ending_here);\n    }\n    return max_so_far;\n}",
            "int max_sum = x[0];\n  int sum = 0;\n  for (auto const& i : x) {\n    sum += i;\n    max_sum = std::max(max_sum, sum);\n    sum = std::max(sum, 0);\n  }\n  return max_sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int sum_local = 0;\n  int max_sum = 0;\n\n  if (tid < N) {\n    sum_local += x[tid];\n    max_sum = sum_local;\n\n    for (int i = tid + 1; i < N; i++) {\n      sum_local += x[i];\n      if (sum_local > max_sum) {\n        max_sum = sum_local;\n      }\n    }\n  }\n\n  atomicMax(sum, max_sum);\n}",
            "extern __shared__ int sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int temp = 0;\n  if (i < N) {\n    temp = x[i];\n  }\n  sdata[t] = temp;\n  __syncthreads();\n  for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n    if (t % (2 * s) == 0) {\n      sdata[t] = sdata[t] + sdata[t + s];\n    }\n    __syncthreads();\n  }\n  if (t == 0) {\n    *sum = sdata[0];\n  }\n}",
            "int max_sum = 0;\n  int sum_so_far = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    sum_so_far += x[i];\n    if (sum_so_far > max_sum) {\n      max_sum = sum_so_far;\n    }\n  }\n  *sum = max_sum;\n}",
            "extern __shared__ int s[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int i = bid * blockDim.x + tid;\n  int j = bid * blockDim.x + tid + blockDim.x;\n  int max_sum = 0;\n  int max_index = 0;\n  int sum_so_far = 0;\n  int max_so_far = 0;\n  int max_index_so_far = 0;\n  if (i < N) {\n    s[tid] = x[i];\n  } else {\n    s[tid] = 0;\n  }\n  if (j < N) {\n    s[tid + blockDim.x] = x[j];\n  } else {\n    s[tid + blockDim.x] = 0;\n  }\n  __syncthreads();\n  for (int k = 0; k < blockDim.x; k++) {\n    sum_so_far += s[k];\n    if (sum_so_far > max_so_far) {\n      max_so_far = sum_so_far;\n      max_index_so_far = k;\n    }\n  }\n  if (max_so_far > max_sum) {\n    max_sum = max_so_far;\n    max_index = max_index_so_far;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    atomicMax(sum, max_sum);\n  }\n}",
            "int max_so_far = 0;\n  int max_ending_here = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    max_ending_here += x[i];\n    if (max_so_far < max_ending_here) {\n      max_so_far = max_ending_here;\n    }\n    if (max_ending_here < 0) {\n      max_ending_here = 0;\n    }\n  }\n  atomicMax(sum, max_so_far);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int max_sum = INT_MIN;\n  int temp_sum = 0;\n  for (int i = tid; i < N; i += stride) {\n    temp_sum += x[i];\n    if (temp_sum > max_sum)\n      max_sum = temp_sum;\n    if (temp_sum < 0)\n      temp_sum = 0;\n  }\n  atomicMax(sum, max_sum);\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  extern __shared__ int s[];\n  int *s_x = s;\n  int *s_sum = s + N;\n  int *s_max = s + 2 * N;\n  int *s_min = s + 3 * N;\n  int *s_max_sum = s + 4 * N;\n  int *s_min_sum = s + 5 * N;\n\n  s_x[tid] = x[bid * N + tid];\n  __syncthreads();\n\n  // Compute the sum of the contiguous subarray\n  int sum_x = 0;\n  for (int i = 0; i < N; i++) {\n    sum_x += s_x[i];\n  }\n\n  // Compute the maximum sum of any contiguous subarray\n  s_sum[tid] = sum_x;\n  __syncthreads();\n  for (int i = 1; i < N; i *= 2) {\n    int j = 2 * i * tid;\n    if (j < N) {\n      s_sum[j] = max(s_sum[j], s_sum[j + i]);\n    }\n    __syncthreads();\n  }\n  *sum = s_sum[0];\n\n  // Compute the maximum sum of any contiguous subarray\n  s_max[tid] = s_x[tid];\n  __syncthreads();\n  for (int i = 1; i < N; i *= 2) {\n    int j = 2 * i * tid;\n    if (j < N) {\n      s_max[j] = max(s_max[j], s_max[j + i]);\n    }\n    __syncthreads();\n  }\n  *sum = max(*sum, s_max[0]);\n\n  // Compute the minimum sum of any contiguous subarray\n  s_min[tid] = s_x[tid];\n  __syncthreads();\n  for (int i = 1; i < N; i *= 2) {\n    int j = 2 * i * tid;\n    if (j < N) {\n      s_min[j] = min(s_min[j], s_min[j + i]);\n    }\n    __syncthreads();\n  }\n  *sum = max(*sum, s_min[0]);\n\n  // Compute the maximum sum of any contiguous subarray\n  s_max_sum[tid] = sum_x;\n  __syncthreads();\n  for (int i = 1; i < N; i *= 2) {\n    int j = 2 * i * tid;\n    if (j < N) {\n      s_max_sum[j] = max(s_max_sum[j], s_max_sum[j + i]);\n    }\n    __syncthreads();\n  }\n  *sum = max(*sum, s_max_sum[0]);\n\n  // Compute the minimum sum of any contiguous subarray\n  s_min_sum[tid] = sum_x;\n  __syncthreads();\n  for (int i = 1; i < N; i *= 2) {\n    int j = 2 * i * tid;\n    if (j < N) {\n      s_min_sum[j] = min(s_min_sum[j], s_min_sum[j + i]);\n    }\n    __syncthreads();\n  }\n  *sum = max(*sum, s_min_sum[0]);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    extern __shared__ int sdata[];\n    sdata[tid] = x[tid];\n    __syncthreads();\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n      int index = 2 * s * tid;\n      if (index < N) {\n        sdata[index] = sdata[index] + sdata[index + s];\n      }\n      __syncthreads();\n    }\n    if (tid == 0) {\n      *sum = sdata[0];\n    }\n  }\n}",
            "int max_so_far = 0;\n  int max_ending_here = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    max_ending_here = x[i];\n    if (i > 0) {\n      max_ending_here += max_so_far;\n      if (max_ending_here < 0) {\n        max_ending_here = 0;\n      }\n    }\n    max_so_far = max(max_so_far, max_ending_here);\n  }\n  __syncthreads();\n  if (i == 0) {\n    *sum = max_so_far;\n  }\n}",
            "int max_so_far = 0;\n  int max_ending_here = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    max_ending_here = x[i];\n    if (i > 0) {\n      max_ending_here += x[i - 1];\n    }\n    if (max_so_far < max_ending_here) {\n      max_so_far = max_ending_here;\n    }\n  }\n  atomicMax(sum, max_so_far);\n}",
            "extern __shared__ int sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[t] = 0;\n  __syncthreads();\n  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (i < N) {\n      sdata[t] = max(sdata[t], sdata[t + s]);\n    }\n    __syncthreads();\n  }\n  if (t == 0) {\n    atomicMax(sum, sdata[0]);\n  }\n}",
            "int *sdata = SharedMemory<int>();\n\n  // perform first level of reduction,\n  // reading from global memory, writing to shared memory\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n\n  int mySum = 0;\n\n  // we reduce multiple elements per thread.  The number is determined by the\n  // number of active thread blocks (via gridDim).  More blocks will result\n  // in a larger gridSize and therefore fewer elements per thread\n  while (i < N) {\n    mySum += x[i];\n    i += gridSize;\n  }\n\n  // each thread puts its local sum into shared memory\n  sdata[tid] = mySum;\n  __syncthreads();\n\n  // do reduction in shared mem\n  if (blockDim.x >= 1024) {\n    if (tid < 512) {\n      sdata[tid] = mySum = mySum + sdata[tid + 512];\n    }\n    __syncthreads();\n  }\n\n  if (blockDim.x >= 512) {\n    if (tid < 256) {\n      sdata[tid] = mySum = mySum + sdata[tid + 256];\n    }\n    __syncthreads();\n  }\n\n  if (blockDim.x >= 256) {\n    if (tid < 128) {\n      sdata[tid] = mySum = mySum + sdata[tid + 128];\n    }\n    __syncthreads();\n  }\n\n  if (blockDim.x >= 128) {\n    if (tid < 64) {\n      sdata[tid] = mySum = mySum + sdata[tid + 64];\n    }\n    __syncthreads();\n  }\n\n  if (tid < 32) {\n    warpReduce(sdata, tid);\n  }\n\n  // write result for this block to global mem\n  if (tid == 0) {\n    sum[blockIdx.x] = sdata[0];\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n  extern __shared__ int s[];\n  int i = bid * blockSize + tid;\n  int mySum = 0;\n  if (i < N) {\n    mySum = x[i];\n    for (int j = 1; j < blockSize; j++) {\n      int index = i + j * blockSize;\n      if (index < N) {\n        mySum += x[index];\n      } else {\n        break;\n      }\n    }\n  }\n  s[tid] = mySum;\n  __syncthreads();\n  if (tid == 0) {\n    int maxSum = s[0];\n    for (int j = 1; j < blockSize; j++) {\n      maxSum = max(maxSum, s[j]);\n    }\n    *sum = maxSum;\n  }\n}",
            "int max_so_far = 0;\n  int max_ending_here = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    max_ending_here = x[i];\n    if (i > 0) {\n      max_ending_here += x[i - 1];\n    }\n    if (max_ending_here > max_so_far) {\n      max_so_far = max_ending_here;\n    }\n  }\n  atomicMax(sum, max_so_far);\n}",
            "extern __shared__ int sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int start = blockIdx.x * blockDim.x;\n  unsigned int end = start + blockDim.x;\n  unsigned int mySum = 0;\n\n  for (unsigned int i = start + t; i < end; i += blockDim.x) {\n    mySum += x[i];\n  }\n  sdata[t] = mySum;\n  __syncthreads();\n\n  for (unsigned int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n    if (t < stride) {\n      sdata[t] = sdata[t] + sdata[t + stride];\n    }\n    __syncthreads();\n  }\n\n  if (t == 0) {\n    *sum = sdata[0];\n  }\n}",
            "extern __shared__ int sdata[];\n  int tx = threadIdx.x;\n  int bx = blockIdx.x;\n  int bsize = blockDim.x;\n  int gsize = gridDim.x * bsize;\n  int start = bx * bsize + tx;\n  int stride = gsize;\n  int i;\n  int mySum = 0;\n  for (i = start; i < N; i += stride) {\n    mySum += x[i];\n  }\n  sdata[tx] = mySum;\n  __syncthreads();\n  for (i = bsize / 2; i > 0; i /= 2) {\n    if (tx < i) {\n      sdata[tx] = max(sdata[tx], sdata[tx + i]);\n    }\n    __syncthreads();\n  }\n  if (tx == 0) {\n    sum[bx] = sdata[0];\n  }\n}",
            "extern __shared__ int sdata[];\n  unsigned int t = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int gridSize = blockDim.x * gridDim.x;\n  sdata[t] = 0;\n  __syncthreads();\n  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (i < N) {\n      sdata[t] = max(sdata[t], sdata[t + s]);\n    }\n    __syncthreads();\n  }\n  if (t == 0) {\n    atomicMax(sum, sdata[0]);\n  }\n}",
            "int max_sum = 0;\n  int sum_so_far = 0;\n  for (int i = 0; i < N; i++) {\n    sum_so_far += x[i];\n    if (sum_so_far > max_sum) {\n      max_sum = sum_so_far;\n    }\n    if (sum_so_far < 0) {\n      sum_so_far = 0;\n    }\n  }\n  *sum = max_sum;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int max_sum = INT_MIN;\n  int local_sum = 0;\n  for (int i = tid; i < N; i += stride) {\n    local_sum += x[i];\n    if (local_sum > max_sum) {\n      max_sum = local_sum;\n    }\n    if (local_sum < 0) {\n      local_sum = 0;\n    }\n  }\n  atomicMax(sum, max_sum);\n}",
            "extern __shared__ int sdata[];\n\n  // each thread loads one element from global to shared mem\n  unsigned int t = threadIdx.x;\n  unsigned int i = blockIdx.x * blockDim.x + t;\n  sdata[t] = (i < N)? x[i] : 0;\n\n  __syncthreads();\n\n  // do reduction in shared mem\n  for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n    int index = 2 * s * t;\n    if (index < blockDim.x) {\n      sdata[index] += sdata[index + s];\n    }\n    __syncthreads();\n  }\n\n  // write result for this block to global mem\n  if (t == 0) {\n    sum[blockIdx.x] = sdata[0];\n  }\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max) {\n            max = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int const n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n  int max_sum = x[0];\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n    int maxSum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > maxSum) {\n            maxSum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return maxSum;\n}",
            "int n = x.size();\n  int maxSum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return maxSum;\n}",
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int max_sum = 0;\n    int local_max_sum = 0;\n    int local_sum = 0;\n    int local_start = 0;\n    int local_end = 0;\n    int global_start = 0;\n    int global_end = 0;\n    int global_max_sum = 0;\n    int global_max_start = 0;\n    int global_max_end = 0;\n\n    for (int i = 0; i < n; i++) {\n        local_sum += x[i];\n        if (local_sum > local_max_sum) {\n            local_max_sum = local_sum;\n            local_end = i;\n        }\n        if (local_sum < 0) {\n            local_sum = 0;\n            local_start = i + 1;\n        }\n    }\n\n    MPI_Reduce(&local_max_sum, &global_max_sum, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_start, &global_start, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_end, &global_end, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"Maximum subarray sum is \" << global_max_sum << \" from \" << global_start << \" to \" << global_end << std::endl;\n    }\n\n    return global_max_sum;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int max = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max) {\n            max = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max;\n}",
            "int max = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max) {\n            max = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max;\n}",
            "int const size = x.size();\n    int const rank = MPI::COMM_WORLD.Get_rank();\n    int const p = MPI::COMM_WORLD.Get_size();\n\n    int const chunk = size / p;\n    int const remainder = size % p;\n\n    int const start = rank * chunk + std::min(rank, remainder);\n    int const end = start + chunk + (rank < remainder? 1 : 0);\n\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = start; i < end; ++i) {\n        sum += x[i];\n        max_sum = std::max(max_sum, sum);\n    }\n\n    int max_sum_global;\n    MPI::COMM_WORLD.Reduce(&max_sum, &max_sum_global, 1, MPI::INT, MPI::MAX, 0);\n\n    return max_sum_global;\n}",
            "// TODO: implement\n  return 0;\n}",
            "int max = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    if (sum > max) {\n      max = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int maxSum = 0;\n  int sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return maxSum;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int maxSum = 0;\n  int currentSum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    currentSum += x[i];\n    if (currentSum > maxSum) {\n      maxSum = currentSum;\n    }\n    if (currentSum < 0) {\n      currentSum = 0;\n    }\n  }\n  return maxSum;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int max_sum = 0;\n    int sum_so_far = 0;\n    for (int i = 0; i < N; i++) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n        if (sum_so_far < 0) {\n            sum_so_far = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int max_sum = 0;\n    int current_sum = 0;\n    if (tid < N) {\n        for (int i = tid; i < N; i++) {\n            current_sum += x[i];\n            if (current_sum > max_sum) {\n                max_sum = current_sum;\n            }\n            if (current_sum < 0) {\n                current_sum = 0;\n            }\n        }\n    }\n    *sum = max_sum;\n}",
            "// TODO: Implement this function\n    int max_sum = 0;\n    int sum_so_far = 0;\n    for (int i = 0; i < N; i++) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n        if (sum_so_far < 0) {\n            sum_so_far = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    __shared__ int s[BLOCK_SIZE];\n    int i = bid * BLOCK_SIZE + tid;\n    int j = bid * BLOCK_SIZE + tid + BLOCK_SIZE;\n    int max = 0;\n    int sum = 0;\n    if (i < N) {\n        sum = x[i];\n    }\n    if (j < N) {\n        sum += x[j];\n    }\n    s[tid] = sum;\n    __syncthreads();\n    if (BLOCK_SIZE >= 512) {\n        if (tid < 256) {\n            s[tid] = max(s[tid], s[tid + 256]);\n        }\n        __syncthreads();\n    }\n    if (BLOCK_SIZE >= 256) {\n        if (tid < 128) {\n            s[tid] = max(s[tid], s[tid + 128]);\n        }\n        __syncthreads();\n    }\n    if (BLOCK_SIZE >= 128) {\n        if (tid < 64) {\n            s[tid] = max(s[tid], s[tid + 64]);\n        }\n        __syncthreads();\n    }\n    if (tid < 32) {\n        warpReduce(s, tid);\n    }\n    if (tid == 0) {\n        atomicMax(sum, s[0]);\n    }\n}",
            "int tid = threadIdx.x;\n    __shared__ int s[100];\n    int max_sum = 0;\n    int local_sum = 0;\n    for (int i = tid; i < N; i += blockDim.x) {\n        local_sum += x[i];\n        s[tid] = local_sum;\n        __syncthreads();\n        if (tid == 0) {\n            max_sum = s[0];\n            for (int j = 1; j < blockDim.x; j++) {\n                max_sum = max(max_sum, s[j]);\n            }\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *sum = max_sum;\n    }\n}",
            "int max_sum = 0;\n    int temp_sum = 0;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        temp_sum = x[i];\n        for (int j = i + 1; j < N; j++) {\n            temp_sum += x[j];\n            if (temp_sum > max_sum) {\n                max_sum = temp_sum;\n            }\n        }\n    }\n    atomicMax(sum, max_sum);\n}",
            "int max_sum = 0;\n    int current_sum = 0;\n    for (int i = 0; i < N; i++) {\n        current_sum += x[i];\n        if (current_sum > max_sum) {\n            max_sum = current_sum;\n        }\n        if (current_sum < 0) {\n            current_sum = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int max_sum = 0;\n    int sum_so_far = 0;\n    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n        if (sum_so_far < 0) {\n            sum_so_far = 0;\n        }\n    }\n    atomicMax(sum, max_sum);\n}",
            "extern __shared__ int sdata[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int temp = 0;\n    if (i < N) {\n        temp = x[i];\n    }\n    sdata[tid] = temp;\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            sdata[tid] = sdata[tid] + sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        *sum = sdata[0];\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int max_sum = 0;\n    int current_sum = 0;\n    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        current_sum += x[i];\n        if (current_sum > max_sum) {\n            max_sum = current_sum;\n        }\n        if (current_sum < 0) {\n            current_sum = 0;\n        }\n    }\n    atomicMax(sum, max_sum);\n}",
            "int max_sum = 0;\n  int sum_so_far = 0;\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    sum_so_far += x[i];\n    if (sum_so_far > max_sum) {\n      max_sum = sum_so_far;\n    } else if (sum_so_far < 0) {\n      sum_so_far = 0;\n    }\n  }\n  atomicMax(sum, max_sum);\n}",
            "int max_sum = 0;\n  int current_sum = 0;\n  for (size_t i = 0; i < N; i++) {\n    current_sum += x[i];\n    if (current_sum > max_sum) {\n      max_sum = current_sum;\n    }\n    if (current_sum < 0) {\n      current_sum = 0;\n    }\n  }\n  *sum = max_sum;\n}",
            "int max_sum = 0;\n    int sum_so_far = 0;\n    for (int i = 0; i < N; i++) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n        if (sum_so_far < 0) {\n            sum_so_far = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int max_sum = 0;\n  int current_sum = 0;\n  int i;\n  for (i = 0; i < N; i++) {\n    current_sum += x[i];\n    if (current_sum > max_sum) {\n      max_sum = current_sum;\n    }\n    if (current_sum < 0) {\n      current_sum = 0;\n    }\n  }\n  *sum = max_sum;\n}",
            "int max_sum = 0;\n    int current_sum = 0;\n\n    for (int i = 0; i < N; i++) {\n        current_sum += x[i];\n        if (current_sum > max_sum) {\n            max_sum = current_sum;\n        }\n        if (current_sum < 0) {\n            current_sum = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int max_sum = 0;\n    int sum_so_far = 0;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n    }\n    *sum = max_sum;\n}",
            "int *s = new int[N];\n    s[0] = x[0];\n    for (int i = 1; i < N; i++) {\n        s[i] = max(x[i], s[i - 1] + x[i]);\n    }\n    *sum = s[N - 1];\n    delete[] s;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    int max_sum = 0;\n    int temp_sum = 0;\n\n    for (int i = tid; i < N; i += stride) {\n        temp_sum += x[i];\n        if (temp_sum > max_sum) {\n            max_sum = temp_sum;\n        }\n        if (temp_sum < 0) {\n            temp_sum = 0;\n        }\n    }\n    atomicMax(sum, max_sum);\n}",
            "int max_sum = 0;\n    int sum_so_far = 0;\n    for (int i = 0; i < N; i++) {\n        sum_so_far += x[i];\n        if (sum_so_far > max_sum) {\n            max_sum = sum_so_far;\n        }\n        if (sum_so_far < 0) {\n            sum_so_far = 0;\n        }\n    }\n    *sum = max_sum;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int max_sum = 0;\n  int sum_so_far = 0;\n  if (i < N) {\n    for (int j = i; j < N; j++) {\n      sum_so_far += x[j];\n      if (sum_so_far > max_sum) {\n        max_sum = sum_so_far;\n      }\n    }\n  }\n  *sum = max_sum;\n}"
        ]
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n  int maxSum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > maxSum) {\n      maxSum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return maxSum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int size = x.size();\n    int max = 0;\n    int sum = 0;\n    for (int i = 0; i < size; i++) {\n        sum += x[i];\n        if (sum > max) {\n            max = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * n / size;\n    int end = (rank + 1) * n / size;\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = start; i < end; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n    }\n    int max_sum_global;\n    MPI_Reduce(&max_sum, &max_sum_global, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return max_sum_global;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    max_sum = std::max(sum, max_sum);\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  for (int i = 0; i < n; i++) {\n    int sum = 0;\n    for (int j = i; j < n; j++) {\n      sum += x[j];\n      if (sum > max_sum) {\n        max_sum = sum;\n      }\n    }\n  }\n  return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int const n = x.size();\n  int const rank = omp_get_thread_num();\n  int const size = omp_get_num_threads();\n\n  int const chunk = n / size;\n  int const remainder = n % size;\n\n  int const start = rank * chunk + std::min(rank, remainder);\n  int const end = start + chunk + (rank < remainder);\n\n  int max_sum = 0;\n  int sum = 0;\n  for (int i = start; i < end; ++i) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n  }\n\n  return max_sum;\n}",
            "int n = x.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = n / size;\n    int local_start = rank * local_size;\n    int local_end = (rank + 1) * local_size;\n    if (rank == size - 1) {\n        local_end = n;\n    }\n\n    std::vector<int> local_x(local_end - local_start);\n    for (int i = 0; i < local_x.size(); i++) {\n        local_x[i] = x[local_start + i];\n    }\n\n    int local_max = 0;\n    int local_max_start = 0;\n    int local_max_end = 0;\n    int local_max_sum = 0;\n    int local_sum = 0;\n    for (int i = 0; i < local_x.size(); i++) {\n        local_sum += local_x[i];\n        if (local_sum > local_max_sum) {\n            local_max_sum = local_sum;\n            local_max_start = local_start + i - local_sum + 1;\n            local_max_end = local_start + i + 1;\n        } else if (local_sum < 0) {\n            local_sum = 0;\n            local_max_start = local_start + i + 1;\n        }\n    }\n\n    int global_max = 0;\n    int global_max_start = 0;\n    int global_max_end = 0;\n    MPI_Reduce(&local_max_sum, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_max_start, &global_max_start, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_max_end, &global_max_end, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"Maximum subarray: \" << global_max << \" from \" << global_max_start << \" to \" << global_max_end << std::endl;\n    }\n\n    return global_max;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; ++i) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}",
            "int n = x.size();\n  int max_sum = 0;\n  int sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n    if (sum > max_sum) {\n      max_sum = sum;\n    }\n    if (sum < 0) {\n      sum = 0;\n    }\n  }\n  return max_sum;\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a parallel_for lambda function to negate the odd values and divide the even values by 2.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (i % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) *= -1;\n    }\n  });\n\n  // Force the parallel_for to complete before returning.\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n\n  // Create a parallel_for lambda that will be executed in parallel.\n  Kokkos::parallel_for(\n      \"negateOddsAndHalveEvens\",\n      PolicyType(0, x.size()),\n      KOKKOS_LAMBDA(const int i) {\n        if (i % 2 == 0) {\n          x(i) /= 2;\n        } else {\n          x(i) = -x(i);\n        }\n      });\n\n  // Wait for the parallel_for to finish.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda that negates the odd values and halves the even values\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (i % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n\n  // Force the parallel_for to finish before returning\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = typename PolicyType::member_type;\n\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    PolicyType(0, x.extent(0)),\n    KOKKOS_LAMBDA(const MemberType& i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) *= -1;\n      }\n    });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n\n  // Create a Kokkos parallel_for lambda that will be executed in parallel.\n  // The lambda takes as input the index of the loop iteration.\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    PolicyType(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 1) {\n        x(i) = -x(i);\n      } else {\n        x(i) /= 2;\n      }\n    });\n\n  // Force the parallel section to complete before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda that will be executed in parallel.\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) *= -1;\n      }\n    });\n  // Wait for the parallel_for to finish.\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  const int n = x.extent(0);\n  Kokkos::parallel_for(\n      \"negateOddsAndHalveEvens\",\n      PolicyType(0, n),\n      KOKKOS_LAMBDA(const int i) {\n        if (i % 2 == 0) {\n          x(i) /= 2;\n        } else {\n          x(i) = -x(i);\n        }\n      });\n}",
            "// Create a parallel_for lambda that will be executed in parallel\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 1) {\n        x(i) = -x(i);\n      } else {\n        x(i) /= 2;\n      }\n    });\n\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) = -x(i);\n      }\n    }\n  );\n\n  // Force the parallel_for to finish\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) *= -1;\n      }\n    }\n  );\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda that will be executed in parallel\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) *= -1;\n      }\n    }\n  );\n\n  // Force the parallel_for to complete before returning\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda that will be executed by Kokkos\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 1) {\n        x(i) = -x(i);\n      } else {\n        x(i) /= 2;\n      }\n    });\n\n  // Force Kokkos to finish all work\n  Kokkos::fence();\n}",
            "using namespace Kokkos;\n  using namespace Kokkos::RangePolicy<Kokkos::RoundRobin>;\n  using namespace Kokkos::Experimental;\n\n  // Create a parallel_for lambda function that will be executed in parallel\n  // on the device.\n  parallel_for(\n    \"negateOddsAndHalveEvens\",\n    RangePolicy<RoundRobin>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 1) {\n        x(i) = -x(i);\n      } else {\n        x(i) /= 2;\n      }\n    });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = typename PolicyType::member_type;\n\n  // Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n      \"negateOddsAndHalveEvens\",\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(const MemberType &i) {\n        if (i % 2 == 0) {\n          x(i) /= 2;\n        } else {\n          x(i) = -x(i);\n        }\n      });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::Member",
            "// Create a parallel_for lambda that will be executed in parallel\n  Kokkos::parallel_for(\n    \"negateOddsAndHalveEvens\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) = -x(i);\n      }\n    }\n  );\n\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n}",
            "using Kokkos::parallel_for;\n  using Kokkos::RangePolicy;\n\n  const int n = x.extent(0);\n  parallel_for(RangePolicy<>(0, n), KOKKOS_LAMBDA(const int i) {\n    if (i % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) *= -1;\n    }\n  });\n}",
            "using Kokkos::parallel_for;\n  using Kokkos::RangePolicy;\n\n  // Create a parallel_for functor to do the work.\n  struct Functor {\n    Kokkos::View<int*> x;\n    Functor(Kokkos::View<int*> x_) : x(x_) {}\n    KOKKOS_INLINE_FUNCTION\n    void operator()(const int i) const {\n      if (i % 2 == 0) {\n        x(i) /= 2;\n      } else {\n        x(i) = -x(i);\n      }\n    }\n  };\n\n  // Run the functor in parallel.\n  const int n = x.extent(0);\n  parallel_for(RangePolicy<>(0, n), Functor(x));\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes a single argument, which is the index of the\n  // element in the View.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n\n    // The lambda function can access the View as if it were a C++ array.\n    if (i % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function to negate the odd values and divide the even values by 2.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (i % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  // Force the parallel_for to complete before returning.\n  Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int n = x.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "serial",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] *= -1;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] = x[i] / 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] *= -1;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] *= -1;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "hip",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        if (idx % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        if (idx % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (idx % 2 == 0) {\n      x[idx] /= 2;\n    } else {\n      x[idx] = -x[idx];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here\n\n  // TODO: your code here",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_left = n % size;\n\n    std::vector<int> x_local(n_per_proc + (rank < n_left? 1 : 0));\n    MPI_Scatter(x.data(), n_per_proc + (rank < n_left? 1 : 0), MPI_INT,\n                x_local.data(), n_per_proc + (rank < n_left? 1 : 0), MPI_INT,\n                0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < n_per_proc + (rank < n_left? 1 : 0); ++i) {\n        if (i % 2 == 1) {\n            x_local[i] = -x_local[i];\n        } else {\n            x_local[i] /= 2;\n        }\n    }\n\n    MPI_Gather(x_local.data(), n_per_proc + (rank < n_left? 1 : 0), MPI_INT,\n               x.data(), n_per_proc + (rank < n_left? 1 : 0), MPI_INT, 0,\n               MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  std::vector<int> x_local(n_per_rank + (rank < n_extra));\n  MPI_Scatter(x.data(), n_per_rank + (rank < n_extra), MPI_INT,\n              x_local.data(), n_per_rank + (rank < n_extra), MPI_INT, 0,\n              MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_rank + (rank < n_extra); i++) {\n    if (i % 2 == 1) {\n      x_local[i] = -x_local[i];\n    } else {\n      x_local[i] /= 2;\n    }\n  }\n\n  std::vector<int> x_global(n);\n  MPI_Gather(x_local.data(), n_per_rank + (rank < n_extra), MPI_INT,\n             x_global.data(), n_per_rank + (rank < n_extra), MPI_INT, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = x_global;\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_per_rank = x.size() / size;\n    int num_left = x.size() % size;\n\n    int start = rank * num_per_rank;\n    int end = start + num_per_rank;\n    if (rank == size - 1) {\n        end += num_left;\n    }\n\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(&x[0], num_per_rank, MPI_INT, &result[0], num_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], num_per_rank, MPI_INT, NULL, num_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_remainder;\n  }\n\n  for (int i = start; i < end; i++) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n\n  // Reduce the results from all ranks to rank 0\n  std::vector<int> x_all(n);\n  MPI_Reduce(x.data(), x_all.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    x = x_all;\n  }\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int nPerRank = n / size;\n    int nRemainder = n % size;\n\n    std::vector<int> xLocal(nPerRank);\n    if (rank == 0) {\n        for (int i = 0; i < nPerRank; i++) {\n            xLocal[i] = x[i];\n        }\n    } else {\n        for (int i = 0; i < nPerRank; i++) {\n            xLocal[i] = x[rank * nPerRank + i];\n        }\n    }\n\n    for (int i = 0; i < nPerRank; i++) {\n        if (xLocal[i] % 2 == 0) {\n            xLocal[i] /= 2;\n        } else {\n            xLocal[i] *= -1;\n        }\n    }\n\n    std::vector<int> xGlobal(n);\n    MPI_Gather(xLocal.data(), nPerRank, MPI_INT, xGlobal.data(), nPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < nRemainder; i++) {\n            xGlobal[i + nPerRank * size] = x[i + nPerRank * size];\n        }\n        x = xGlobal;\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "cuda",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (idx % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (idx % 2 == 0) {\n      x[idx] /= 2;\n    } else {\n      x[idx] = -x[idx];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "53_transform_negate_odds",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_values = x.size();\n  int num_values_per_rank = num_values / size;\n  int num_values_per_rank_extra = num_values % size;\n\n  std::vector<int> x_local(num_values_per_rank);\n  std::vector<int> x_local_extra(num_values_per_rank_extra);\n\n  if (rank == 0) {\n    for (int i = 0; i < num_values_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n    for (int i = 0; i < num_values_per_rank_extra; i++) {\n      x_local_extra[i] = x[num_values_per_rank + i];\n    }\n  } else {\n    MPI_Recv(&x_local[0], num_values_per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    if (rank < num_values_per_rank_extra) {\n      MPI_Recv(&x_local_extra[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // Negate odd values and divide even values by 2\n  #pragma omp parallel for\n  for (int i = 0; i < num_values_per_rank; i++) {\n    if (x_local[i] % 2 == 0) {\n      x_local[i] /= 2;\n    } else {\n      x_local[i] *= -1;\n    }\n  }\n\n  // Send results back to rank 0\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      if (i < num_values_per_rank_extra) {\n        MPI_Send(&x_local_extra[0], 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n      }\n      MPI_Send(&x_local[0], num_values_per_rank, MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Send(&x_local[0], num_values_per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    if (rank < num_values_per_rank_extra) {\n      MPI_Send(&x_local_extra[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  // Rank 0 combines the results\n  if (rank == 0) {\n    for (int i = 0; i < num_values_per_rank; i++) {\n      x[i] = x_local[i];\n    }\n    for (int i = 0; i < num_values_per_rank_extra; i++) {\n      x[num_values_per_rank + i] = x_local_extra[i];\n    }\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    std::vector<int> x_local(n_per_rank);\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            x_local[i] = x[i];\n        }\n    } else {\n        for (int i = 0; i < n_per_rank; i++) {\n            x_local[i] = x[rank * n_per_rank + i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_extra; i++) {\n            x_local[i] = x[i];\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank; i++) {\n        if (x_local[i] % 2 == 0) {\n            x_local[i] /= 2;\n        } else {\n            x_local[i] = -x_local[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            x[i] = x_local[i];\n        }\n    } else {\n        for (int i = 0; i < n_per_rank; i++) {\n            x[rank * n_per_rank + i] = x_local[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_extra; i++) {\n            x[i] = x_local[i];\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = x.size() / size;\n    int start = rank * chunkSize;\n    int end = (rank + 1) * chunkSize;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (i % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Reduce(x.data(), result.data(), x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Reduce(x.data(), nullptr, x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n\t\tmask(i) = isPowerOfTwo(x(i));\n\t});\n}",
            "// TODO: Implement this function\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int n = x.size();\n\tmask.resize(n);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "serial",
        "prompt": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask.push_back(isPowerOfTwo(x[i]));\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask.push_back(isPowerOfTwo(x[i]));\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask.push_back(isPowerOfTwo(x[i]));\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask.push_back(isPowerOfTwo(x[i]));\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask.push_back(isPowerOfTwo(x[i]));\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "hip",
        "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tmask[idx] = isPowerOfTwo(x[idx]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> x_rank(chunkSize + (rank < remainder? 1 : 0));\n\tstd::vector<bool> mask_rank(chunkSize + (rank < remainder? 1 : 0));\n\n\tMPI_Scatter(x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, x_rank.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < x_rank.size(); i++) {\n\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t}\n\n\tMPI_Gather(mask_rank.data(), chunkSize + (rank < remainder? 1 : 0), MPI_C_BOOL, mask.data(), chunkSize + (rank < remainder? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tstd::vector<int> x_rank(count_per_rank);\n\tstd::vector<bool> mask_rank(count_per_rank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tx_rank[i] = x[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tx_rank[i] = x[rank * count_per_rank + i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tmask[i] = mask_rank[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < count_per_rank; i++) {\n\t\t\tmask[rank * count_per_rank + i] = mask_rank[i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < count_remainder; i++) {\n\t\t\tmask[count - count_remainder + i] = isPowerOfTwo(x[count - count_remainder + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint chunk = count / size;\n\tint remainder = count % size;\n\n\tstd::vector<int> x_rank;\n\tstd::vector<bool> mask_rank;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tx_rank.push_back(x[i]);\n\t\t}\n\t\tfor (int i = remainder; i < count; i++) {\n\t\t\tx_rank.push_back(x[i + 1]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tx_rank.push_back(x[i]);\n\t\t}\n\t}\n\n\tfor (int i = 0; i < x_rank.size(); i++) {\n\t\tmask_rank.push_back(isPowerOfTwo(x_rank[i]));\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[i] = mask_rank[i];\n\t\t}\n\t\tfor (int i = remainder; i < count; i++) {\n\t\t\tmask[i + 1] = mask_rank[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tmask[i] = mask_rank[i];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint countPerRank = count / size;\n\tint countRemainder = count % size;\n\n\tstd::vector<int> xRank(countPerRank + (rank < countRemainder));\n\tstd::vector<bool> maskRank(countPerRank + (rank < countRemainder));\n\n\tMPI_Scatter(x.data(), countPerRank + (rank < countRemainder), MPI_INT, xRank.data(), countPerRank + (rank < countRemainder), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < xRank.size(); i++) {\n\t\tmaskRank[i] = isPowerOfTwo(xRank[i]);\n\t}\n\n\tMPI_Gather(maskRank.data(), countPerRank + (rank < countRemainder), MPI_C_BOOL, mask.data(), countPerRank + (rank < countRemainder), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tstd::vector<int> x_local(n_per_rank + (rank < n_extra));\n\tstd::vector<bool> mask_local(n_per_rank + (rank < n_extra));\n\n\tMPI_Scatter(x.data(), n_per_rank + (rank < n_extra), MPI_INT, x_local.data(), n_per_rank + (rank < n_extra), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < n_per_rank + (rank < n_extra); i++) {\n\t\tmask_local[i] = isPowerOfTwo(x_local[i]);\n\t}\n\n\tMPI_Gather(mask_local.data(), n_per_rank + (rank < n_extra), MPI_C_BOOL, mask.data(), n_per_rank + (rank < n_extra), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_proc = n / size;\n\tint n_extra = n % size;\n\n\tstd::vector<int> x_per_proc(n_per_proc);\n\tstd::vector<bool> mask_per_proc(n_per_proc);\n\n\tMPI_Scatter(x.data(), n_per_proc, MPI_INT, x_per_proc.data(), n_per_proc, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < n_per_proc; i++) {\n\t\tmask_per_proc[i] = isPowerOfTwo(x_per_proc[i]);\n\t}\n\n\tMPI_Gather(mask_per_proc.data(), n_per_proc, MPI_C_BOOL, mask.data(), n_per_proc, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_extra; i++) {\n\t\t\tmask[n_per_proc * size + i] = isPowerOfTwo(x[n_per_proc * size + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tstd::vector<int> x_rank(n_per_rank);\n\tstd::vector<bool> mask_rank(n_per_rank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_rank[i] = x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_rank[i] = x[rank * n_per_rank + i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t}\n\n\tMPI_Gather(mask_rank.data(), n_per_rank, MPI_C_BOOL, mask.data(), n_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tmask[n_per_rank * size + i] = isPowerOfTwo(x[n_per_rank * size + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_elements = x.size();\n\tint num_elements_per_rank = num_elements / size;\n\tint num_elements_remainder = num_elements % size;\n\n\tstd::vector<int> x_rank;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < num_elements_per_rank; i++) {\n\t\t\tx_rank.push_back(x[i]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < num_elements_per_rank + num_elements_remainder; i++) {\n\t\t\tx_rank.push_back(x[i]);\n\t\t}\n\t}\n\n\tstd::vector<bool> mask_rank;\n\tfor (int i = 0; i < x_rank.size(); i++) {\n\t\tmask_rank.push_back(isPowerOfTwo(x_rank[i]));\n\t}\n\n\tstd::vector<bool> mask_rank_all;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\tif (i == 0) {\n\t\t\t\tfor (int j = 0; j < num_elements_per_rank; j++) {\n\t\t\t\t\tmask_rank_all.push_back(mask_rank[j]);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (int j = 0; j < num_elements_per_rank + num_elements_remainder; j++) {\n\t\t\t\t\tmask_rank_all.push_back(mask_rank[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tmask = mask_rank_all;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint remainder = count % size;\n\n\tstd::vector<int> x_per_rank(count_per_rank);\n\tstd::vector<bool> mask_per_rank(count_per_rank);\n\n\tMPI_Scatter(x.data(), count_per_rank, MPI_INT, x_per_rank.data(), count_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < count_per_rank; i++) {\n\t\tmask_per_rank[i] = isPowerOfTwo(x_per_rank[i]);\n\t}\n\n\tMPI_Gather(mask_per_rank.data(), count_per_rank, MPI_C_BOOL, mask.data(), count_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[i + count_per_rank * size] = isPowerOfTwo(x[i + count_per_rank * size]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint countPerRank = count / size;\n\tint remainder = count % size;\n\n\tstd::vector<int> xRank(countPerRank);\n\tstd::vector<bool> maskRank(countPerRank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < countPerRank; i++) {\n\t\t\txRank[i] = x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < countPerRank; i++) {\n\t\t\txRank[i] = x[i + rank * countPerRank];\n\t\t}\n\t}\n\n\tfor (int i = 0; i < countPerRank; i++) {\n\t\tmaskRank[i] = isPowerOfTwo(xRank[i]);\n\t}\n\n\tMPI_Gather(maskRank.data(), countPerRank, MPI_C_BOOL, mask.data(), countPerRank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> chunk(chunkSize);\n\tstd::vector<bool> chunkMask(chunkSize);\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Send(&x[i * chunkSize], chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < chunkSize; i++) {\n\t\t\tchunk[i] = x[i];\n\t\t\tchunkMask[i] = isPowerOfTwo(chunk[i]);\n\t\t}\n\t} else {\n\t\tMPI_Recv(&chunk[0], chunkSize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tfor (int i = 0; i < chunkSize; i++) {\n\t\t\tchunkMask[i] = isPowerOfTwo(chunk[i]);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[i] = isPowerOfTwo(x[i + size * chunkSize]);\n\t\t}\n\t}\n\n\tMPI_Gather(&chunkMask[0], chunkSize, MPI_C_BOOL, &mask[0], chunkSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<int> x_proc(n_per_proc + (rank < n_rem? 1 : 0));\n\tstd::vector<bool> mask_proc(n_per_proc + (rank < n_rem? 1 : 0));\n\n\tMPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, x_proc.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < x_proc.size(); i++) {\n\t\tmask_proc[i] = isPowerOfTwo(x_proc[i]);\n\t}\n\n\tMPI_Gather(mask_proc.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_BOOL, mask.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> local_x(chunk + (rank < remainder? 1 : 0));\n\tstd::vector<bool> local_mask(chunk + (rank < remainder? 1 : 0));\n\n\tMPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_INT, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(local_x[i]);\n\t}\n\n\tMPI_Gather(local_mask.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, mask.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint localSize = x.size() / size;\n\tint localOffset = rank * localSize;\n\tstd::vector<bool> localMask(localSize);\n\n\tfor (int i = 0; i < localSize; i++) {\n\t\tlocalMask[i] = isPowerOfTwo(x[localOffset + i]);\n\t}\n\n\tstd::vector<bool> globalMask(x.size());\n\tMPI_Gather(localMask.data(), localSize, MPI_C_BOOL, globalMask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = globalMask;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<int> x_proc(n_per_proc + (rank < n_rem? 1 : 0));\n\tstd::vector<bool> mask_proc(n_per_proc + (rank < n_rem? 1 : 0));\n\n\tMPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, x_proc.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < x_proc.size(); i++) {\n\t\tmask_proc[i] = isPowerOfTwo(x_proc[i]);\n\t}\n\n\tMPI_Gather(mask_proc.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_C_BOOL, mask.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint chunk = count / size;\n\tint remainder = count % size;\n\n\tstd::vector<int> local_x;\n\tstd::vector<bool> local_mask;\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tlocal_x.push_back(x[i]);\n\t\t}\n\t\tfor (int i = remainder; i < count; i++) {\n\t\t\tlocal_x.push_back(x[i + 1]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tlocal_x.push_back(x[i]);\n\t\t}\n\t}\n\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tlocal_mask.push_back(isPowerOfTwo(local_x[i]));\n\t}\n\n\tstd::vector<bool> global_mask;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tglobal_mask.push_back(local_mask[i]);\n\t\t}\n\t\tfor (int i = remainder; i < count; i++) {\n\t\t\tglobal_mask.push_back(local_mask[i + 1]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tglobal_mask.push_back(local_mask[i]);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tmask = global_mask;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint remainder = count % size;\n\n\tstd::vector<int> x_rank(count_per_rank + (rank < remainder? 1 : 0));\n\tstd::vector<bool> mask_rank(count_per_rank + (rank < remainder? 1 : 0));\n\n\tMPI_Scatter(x.data(), count_per_rank + (rank < remainder? 1 : 0), MPI_INT, x_rank.data(), count_per_rank + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < x_rank.size(); i++) {\n\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t}\n\n\tMPI_Gather(mask_rank.data(), count_per_rank + (rank < remainder? 1 : 0), MPI_C_BOOL, mask.data(), count_per_rank + (rank < remainder? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint count = x.size();\n\tint count_per_rank = count / size;\n\tint count_remainder = count % size;\n\n\tint start = rank * count_per_rank;\n\tint end = start + count_per_rank;\n\tif (rank == size - 1) {\n\t\tend += count_remainder;\n\t}\n\n\tstd::vector<bool> mask_local(count_per_rank);\n\tfor (int i = start; i < end; i++) {\n\t\tmask_local[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tstd::vector<bool> mask_global(count);\n\tMPI_Gather(&mask_local[0], count_per_rank, MPI_C_BOOL, &mask_global[0], count_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = mask_global;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> myX;\n\tif (rank == 0) {\n\t\tmyX.resize(chunkSize + remainder);\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmyX[i] = x[i];\n\t\t}\n\t}\n\telse {\n\t\tmyX.resize(chunkSize);\n\t}\n\n\tMPI_Scatter(x.data(), chunkSize, MPI_INT, myX.data(), chunkSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tstd::vector<bool> myMask(myX.size());\n\tfor (int i = 0; i < myX.size(); i++) {\n\t\tmyMask[i] = isPowerOfTwo(myX[i]);\n\t}\n\n\tMPI_Gather(myMask.data(), chunkSize, MPI_C_BOOL, mask.data(), chunkSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> local_x(chunk + (rank < remainder? 1 : 0));\n\tstd::vector<bool> local_mask(chunk + (rank < remainder? 1 : 0));\n\n\tMPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_INT, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(local_x[i]);\n\t}\n\n\tMPI_Gather(local_mask.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, mask.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "cuda",
        "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tmask[idx] = isPowerOfTwo(x[idx]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_map_function",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint local_size = x.size() / size;\n\tint local_rank = rank * local_size;\n\n\tstd::vector<bool> local_mask(local_size);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_size; i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(x[i + local_rank]);\n\t}\n\n\tif (rank == 0) {\n\t\tmask.resize(x.size());\n\t}\n\n\tMPI_Gather(local_mask.data(), local_size, MPI_C_BOOL, mask.data(), local_size, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tint start = rank * chunkSize;\n\tint end = start + chunkSize;\n\n\tif (rank == size - 1) {\n\t\tend += remainder;\n\t}\n\n\tstd::vector<bool> localMask(chunkSize);\n\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tlocalMask[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tstd::vector<bool> globalMask(x.size());\n\tMPI_Gather(localMask.data(), chunkSize, MPI_C_BOOL, globalMask.data(), chunkSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = globalMask;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tstd::vector<int> x_per_rank(n_per_rank);\n\tstd::vector<bool> mask_per_rank(n_per_rank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_per_rank[i] = x[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_per_rank[i] = x[rank * n_per_rank + i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tx_per_rank[n_per_rank + i] = x[size * n_per_rank + i];\n\t\t}\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n_per_rank; i++) {\n\t\tmask_per_rank[i] = isPowerOfTwo(x_per_rank[i]);\n\t}\n\n\tMPI_Gather(mask_per_rank.data(), n_per_rank, MPI_C_BOOL, mask.data(), n_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tmask[size * n_per_rank + i] = isPowerOfTwo(x[size * n_per_rank + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_threads = omp_get_max_threads();\n\tint num_values_per_thread = x.size() / num_threads;\n\tint num_values_remaining = x.size() % num_threads;\n\n\tstd::vector<int> x_per_thread(num_values_per_thread);\n\tstd::vector<bool> mask_per_thread(num_values_per_thread);\n\n\t#pragma omp parallel\n\t{\n\t\tint thread_id = omp_get_thread_num();\n\t\tint start_index = thread_id * num_values_per_thread;\n\t\tint end_index = start_index + num_values_per_thread;\n\n\t\tif (thread_id == num_threads - 1) {\n\t\t\tend_index += num_values_remaining;\n\t\t}\n\n\t\tfor (int i = start_index; i < end_index; i++) {\n\t\t\tx_per_thread[i - start_index] = x[i];\n\t\t}\n\n\t\t#pragma omp for\n\t\tfor (int i = 0; i < num_values_per_thread; i++) {\n\t\t\tmask_per_thread[i] = isPowerOfTwo(x_per_thread[i]);\n\t\t}\n\t}\n\n\tstd::vector<bool> mask_per_rank(x.size());\n\tMPI_Gather(mask_per_thread.data(), num_values_per_thread, MPI_C_BOOL, mask_per_rank.data(), num_values_per_thread, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = mask_per_rank;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> x_chunk(chunk + (rank < remainder? 1 : 0));\n\tstd::vector<bool> mask_chunk(chunk + (rank < remainder? 1 : 0));\n\n\tif (rank < remainder) {\n\t\tfor (int i = 0; i < chunk + 1; i++) {\n\t\t\tx_chunk[i] = x[rank * chunk + i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < chunk; i++) {\n\t\t\tx_chunk[i] = x[rank * chunk + i];\n\t\t}\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < x_chunk.size(); i++) {\n\t\tmask_chunk[i] = isPowerOfTwo(x_chunk[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tmask.resize(x.size());\n\t}\n\n\tMPI_Gather(mask_chunk.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, mask.data(), chunk + (rank < remainder? 1 : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_threads = omp_get_max_threads();\n\tint num_per_thread = x.size() / num_threads;\n\tint num_extra = x.size() % num_threads;\n\n\tstd::vector<bool> mask_local(num_per_thread + (rank < num_extra));\n\n\t#pragma omp parallel\n\t{\n\t\tint thread_id = omp_get_thread_num();\n\t\tint start = thread_id * num_per_thread + std::min(thread_id, num_extra);\n\t\tint end = start + num_per_thread + (thread_id < num_extra);\n\n\t\tfor (int i = start; i < end; i++) {\n\t\t\tmask_local[i - start] = isPowerOfTwo(x[i]);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tmask.resize(x.size());\n\t}\n\n\tMPI_Gather(mask_local.data(), mask_local.size(), MPI_C_BOOL, mask.data(), mask_local.size(), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\tint start = rank * chunk + std::min(rank, remainder);\n\tint end = (rank + 1) * chunk + std::min(rank + 1, remainder);\n\n\tstd::vector<bool> localMask(end - start);\n\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tlocalMask[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tmask.resize(x.size());\n\t}\n\n\tMPI_Gather(localMask.data(), localMask.size(), MPI_C_BOOL, mask.data(), localMask.size(), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> localX;\n\tstd::vector<bool> localMask;\n\n\tif (rank == 0) {\n\t\tlocalX.resize(chunkSize + remainder);\n\t\tlocalMask.resize(chunkSize + remainder);\n\t}\n\telse {\n\t\tlocalX.resize(chunkSize);\n\t\tlocalMask.resize(chunkSize);\n\t}\n\n\tMPI_Scatter(x.data(), chunkSize, MPI_INT, localX.data(), chunkSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < localX.size(); i++) {\n\t\tlocalMask[i] = isPowerOfTwo(localX[i]);\n\t}\n\n\tMPI_Gather(localMask.data(), chunkSize, MPI_C_BOOL, mask.data(), chunkSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[chunkSize + i] = isPowerOfTwo(x[chunkSize + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint local_size = x.size() / size;\n\tint local_rank = rank;\n\n\tstd::vector<int> local_x(local_size);\n\tstd::vector<bool> local_mask(local_size);\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Send(&x[i * local_size], local_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < local_size; i++) {\n\t\t\tlocal_x[i] = x[i];\n\t\t}\n\t} else {\n\t\tMPI_Recv(&local_x[0], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_size; i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(local_x[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(&local_mask[i * local_size], local_size, MPI_C_BOOL, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t} else {\n\t\tMPI_Send(&local_mask[0], local_size, MPI_C_BOOL, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < x.size(); i++) {\n\t\t\tmask[i] = local_mask[i];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_extra;\n\t}\n\n\tstd::vector<bool> mask_local(n_per_rank);\n\tfor (int i = start; i < end; i++) {\n\t\tmask_local[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tstd::vector<bool> mask_global(n);\n\tMPI_Gather(mask_local.data(), n_per_rank, MPI_C_BOOL, mask_global.data(), n_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = mask_global;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint num_threads = omp_get_max_threads();\n\tint num_per_thread = x.size() / num_threads;\n\tint num_remainder = x.size() % num_threads;\n\n\tstd::vector<int> local_x;\n\tstd::vector<bool> local_mask;\n\n\tif (rank == 0) {\n\t\tlocal_x = std::vector<int>(x.begin(), x.begin() + num_per_thread + num_remainder);\n\t\tlocal_mask = std::vector<bool>(local_x.size());\n\t}\n\telse {\n\t\tlocal_x = std::vector<int>(x.begin() + num_per_thread * rank + num_remainder * (rank - 1), x.begin() + num_per_thread * (rank + 1) + num_remainder * rank);\n\t\tlocal_mask = std::vector<bool>(local_x.size());\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(local_x[i]);\n\t}\n\n\tstd::vector<int> recvcounts(size);\n\tstd::vector<int> displs(size);\n\n\tfor (int i = 0; i < size; i++) {\n\t\trecvcounts[i] = num_per_thread;\n\t\tdispls[i] = num_per_thread * i;\n\t}\n\n\trecvcounts[0] += num_remainder;\n\tdispls[0] = 0;\n\n\tstd::vector<bool> global_mask(x.size());\n\n\tMPI_Gatherv(local_mask.data(), num_per_thread + num_remainder, MPI_C_BOOL, global_mask.data(), recvcounts.data(), displs.data(), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = global_mask;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<int> x_local(n_per_rank);\n\tstd::vector<bool> mask_local(n_per_rank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_local[i] = x[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_local[i] = x[rank * n_per_rank + i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_rem; i++) {\n\t\t\tx_local[n_per_rank + i] = x[n_per_rank * size + i];\n\t\t}\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n_per_rank; i++) {\n\t\tmask_local[i] = isPowerOfTwo(x_local[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask[i] = mask_local[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask[rank * n_per_rank + i] = mask_local[i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_rem; i++) {\n\t\t\tmask[n_per_rank * size + i] = mask_local[n_per_rank + i];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\tif (rank == size - 1) {\n\t\tend += n_remainder;\n\t}\n\n\tstd::vector<bool> local_mask(n_per_rank);\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tlocal_mask[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tstd::vector<int> local_mask_size(size);\n\tlocal_mask_size[rank] = local_mask.size();\n\tMPI_Gather(local_mask_size.data(), 1, MPI_INT, mask.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tint offset = 0;\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\tfor (int j = 0; j < local_mask_size[i]; j++) {\n\t\t\t\tmask[offset + j] = local_mask[j];\n\t\t\t}\n\t\t\toffset += local_mask_size[i];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint n_per_rank_local = n_per_rank;\n\tint n_remainder_local = 0;\n\tif (rank < n_remainder) {\n\t\tn_per_rank_local++;\n\t\tn_remainder_local = 1;\n\t}\n\n\tstd::vector<int> x_local(n_per_rank_local + n_remainder_local);\n\tstd::vector<bool> mask_local(n_per_rank_local + n_remainder_local);\n\n\tif (rank < n_remainder) {\n\t\tfor (int i = 0; i < n_per_rank_local; i++) {\n\t\t\tx_local[i] = x[rank * n_per_rank + i];\n\t\t}\n\t\tx_local[n_per_rank_local] = x[rank * n_per_rank + n_per_rank];\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank_local; i++) {\n\t\t\tx_local[i] = x[rank * n_per_rank + i - n_remainder];\n\t\t}\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n_per_rank_local + n_remainder_local; i++) {\n\t\tmask_local[i] = isPowerOfTwo(x_local[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tmask.resize(n);\n\t}\n\n\tMPI_Gather(mask_local.data(), n_per_rank_local + n_remainder_local, MPI_C_BOOL, mask.data(), n_per_rank_local + n_remainder_local, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tmask[i * n_per_rank + n_per_rank] = mask[i * n_per_rank + n_per_rank - 1];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> localX;\n\tstd::vector<bool> localMask;\n\n\tif (rank == 0) {\n\t\tlocalX.resize(chunk + remainder);\n\t\tlocalMask.resize(chunk + remainder);\n\t} else {\n\t\tlocalX.resize(chunk);\n\t\tlocalMask.resize(chunk);\n\t}\n\n\tMPI_Scatter(x.data(), chunk, MPI_INT, localX.data(), chunk, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < localX.size(); i++) {\n\t\tlocalMask[i] = isPowerOfTwo(localX[i]);\n\t}\n\n\tMPI_Gather(localMask.data(), chunk, MPI_C_BOOL, mask.data(), chunk, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[chunk + i] = isPowerOfTwo(x[chunk + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunkSize = x.size() / size;\n\tint remainder = x.size() % size;\n\n\tstd::vector<int> myX;\n\tif (rank == 0) {\n\t\tmyX.resize(chunkSize + remainder);\n\t}\n\telse {\n\t\tmyX.resize(chunkSize);\n\t}\n\n\tMPI_Scatter(x.data(), chunkSize, MPI_INT, myX.data(), chunkSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tstd::vector<bool> myMask;\n\tif (rank == 0) {\n\t\tmyMask.resize(chunkSize + remainder);\n\t}\n\telse {\n\t\tmyMask.resize(chunkSize);\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < myX.size(); i++) {\n\t\tmyMask[i] = isPowerOfTwo(myX[i]);\n\t}\n\n\tMPI_Gather(myMask.data(), chunkSize, MPI_C_BOOL, mask.data(), chunkSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < remainder; i++) {\n\t\t\tmask[chunkSize + i] = isPowerOfTwo(x[chunkSize + i]);\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint localSize = x.size() / size;\n\tint localStart = rank * localSize;\n\tint localEnd = (rank + 1) * localSize;\n\tif (rank == size - 1) {\n\t\tlocalEnd = x.size();\n\t}\n\n\tstd::vector<bool> localMask(localEnd - localStart);\n\n\t#pragma omp parallel for\n\tfor (int i = localStart; i < localEnd; i++) {\n\t\tlocalMask[i - localStart] = isPowerOfTwo(x[i]);\n\t}\n\n\tstd::vector<int> localSizes(size);\n\tstd::vector<int> localDispls(size);\n\tfor (int i = 0; i < size; i++) {\n\t\tlocalSizes[i] = localSize;\n\t\tlocalDispls[i] = i * localSize;\n\t}\n\n\tMPI_Gatherv(&localMask[0], localSize, MPI_C_BOOL, &mask[0], &localSizes[0], &localDispls[0], MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tint start = rank * n_per_rank;\n\tint end = start + n_per_rank;\n\n\tif (rank == 0) {\n\t\tmask.resize(n);\n\t}\n\n\tstd::vector<bool> mask_local(n_per_rank);\n\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tmask_local[i - start] = isPowerOfTwo(x[i]);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask[i] = mask_local[i];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tmask[i + n_per_rank] = isPowerOfTwo(x[i + n_per_rank]);\n\t\t}\n\t}\n\n\tMPI_Gather(mask_local.data(), n_per_rank, MPI_C_BOOL, mask.data(), n_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint chunk = x.size() / size;\n\tint rem = x.size() % size;\n\n\tstd::vector<int> local_x;\n\tif (rank == 0) {\n\t\tlocal_x.resize(chunk + rem);\n\t\tstd::copy(x.begin(), x.begin() + chunk + rem, local_x.begin());\n\t} else {\n\t\tlocal_x.resize(chunk);\n\t\tstd::copy(x.begin() + rank * chunk, x.begin() + rank * chunk + chunk, local_x.begin());\n\t}\n\n\tstd::vector<bool> local_mask(local_x.size());\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tlocal_mask[i] = isPowerOfTwo(local_x[i]);\n\t}\n\n\tstd::vector<bool> global_mask(x.size());\n\tMPI_Gather(local_mask.data(), chunk + (rank == 0? rem : 0), MPI_C_BOOL, global_mask.data(), chunk + (rank == 0? rem : 0), MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tmask = global_mask;\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_remainder = n % size;\n\n\tstd::vector<int> x_rank(n_per_rank);\n\tstd::vector<bool> mask_rank(n_per_rank);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_rank[i] = x[i];\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_rank[i] = x[i + n_per_rank * (rank - 1)];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tmask_rank[i] = isPowerOfTwo(x_rank[i]);\n\t\t}\n\t}\n\n\tMPI_Gather(mask_rank.data(), n_per_rank, MPI_C_BOOL, mask.data(), n_per_rank, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n_remainder; i++) {\n\t\t\tmask[n_per_rank * (size - 1) + i] = isPowerOfTwo(x[n_per_rank * (size - 1) + i]);\n\t\t}\n\t}\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that squares each element of x.\n  Kokkos::parallel_for(\n    \"Square each element\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = x(i) * x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Fill in this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that squares each element of x\n  Kokkos::parallel_for(\n    \"Square each element\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = x(i) * x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Replace this code with your solution.\n  // Note: You may not use the [] operator; use x.at(i) to access elements.\n  int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    x.at(i) = x.at(i) * x.at(i);\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] *= x[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "// TODO: Fill in the code to square each element of x\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "// TODO: replace this line with your code\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int n = x.size();\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] *= x[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] *= x[i];\n    }\n}",
            "// TODO: Fill this in\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] *= x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "hip",
        "prompt": "/* Replace every element of x with the square of its value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunkSize = x.size() / size;\n  int remainder = x.size() % size;\n\n  std::vector<int> localX(chunkSize + (rank < remainder? 1 : 0));\n  std::copy(x.begin() + rank * chunkSize,\n            x.begin() + rank * chunkSize + localX.size(),\n            localX.begin());\n\n  for (int i = 0; i < localX.size(); i++) {\n    localX[i] *= localX[i];\n  }\n\n  std::vector<int> globalX(x.size());\n  MPI_Gather(localX.data(), localX.size(), MPI_INT,\n             globalX.data(), localX.size(), MPI_INT,\n             0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = globalX;\n  }\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    std::vector<int> local_x(n_per_rank + (rank < n_remainder));\n    MPI_Scatter(x.data(), n_per_rank + (rank < n_remainder), MPI_INT,\n                local_x.data(), n_per_rank + (rank < n_remainder), MPI_INT,\n                0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_x[i] *= local_x[i];\n    }\n\n    std::vector<int> global_x(n);\n    MPI_Gather(local_x.data(), n_per_rank + (rank < n_remainder), MPI_INT,\n               global_x.data(), n_per_rank + (rank < n_remainder), MPI_INT,\n               0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_x;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk = x.size() / size;\n    int remainder = x.size() % size;\n\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    for (int i = start; i < end; i++) {\n        x[i] *= x[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(x.data() + i * chunk, chunk, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(x.data() + start, chunk, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int start = rank * x.size() / size;\n    int end = (rank + 1) * x.size() / size;\n\n    for (int i = start; i < end; i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * x.size() / size], x.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start], end - start, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Replace this code with your solution\n  // You may assume that the length of x is divisible by size\n  // You may also assume that x has been initialized\n  // You may not use any MPI routines other than MPI_Send and MPI_Recv\n  // You may not use any routines from <vector>\n  // You may not use any routines from <algorithm>\n  // You may not use any routines from <numeric>\n  // You may not use any routines from <iterator>\n  // You may not use any routines from <iostream>\n  // You may not use any routines from <cmath>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <random>\n  // You may not use any routines from <ctime>\n  // You may not use any routines from <cstring>\n  // You may not use any routines from <cstdarg>\n  // You may not use any routines from <cstdio>\n  // You may not use any routines from <cstddef>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <cstddef>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <cstring>\n  // You may not use any routines from <cstdio>\n  // You may not use any routines from <cstdarg>\n  // You may not use any routines from <cmath>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <ctime>\n  // You may not use any routines from <random>\n  // You may not use any routines from <iterator>\n  // You may not use any routines from <numeric>\n  // You may not use any routines from <algorithm>\n  // You may not use any routines from <vector>\n  // You may not use any routines from <iostream>\n  // You may not use any routines from <cstring>\n  // You may not use any routines from <cstdio>\n  // You may not use any routines from <cstdarg>\n  // You may not use any routines from <cmath>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <ctime>\n  // You may not use any routines from <random>\n  // You may not use any routines from <iterator>\n  // You may not use any routines from <numeric>\n  // You may not use any routines from <algorithm>\n  // You may not use any routines from <vector>\n  // You may not use any routines from <iostream>\n  // You may not use any routines from <cstring>\n  // You may not use any routines from <cstdio>\n  // You may not use any routines from <cstdarg>\n  // You may not use any routines from <cmath>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <ctime>\n  // You may not use any routines from <random>\n  // You may not use any routines from <iterator>\n  // You may not use any routines from <numeric>\n  // You may not use any routines from <algorithm>\n  // You may not use any routines from <vector>\n  // You may not use any routines from <iostream>\n  // You may not use any routines from <cstring>\n  // You may not use any routines from <cstdio>\n  // You may not use any routines from <cstdarg>\n  // You may not use any routines from <cmath>\n  // You may not use any routines from <cstdlib>\n  // You may not use any routines from <ctime>\n  // You may not use any routines from <random>\n  // You may not use any routines from <iterator>\n  // You may not use any routines from <numeric>\n  // You may not use any rout",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "// Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Replace this code with your solution.\n  // You may use MPI_Send, MPI_Recv, MPI_Sendrecv, etc.\n  // You may also use MPI_Bcast, MPI_Reduce, etc.\n  // You may use any other MPI functions you wish.\n  // You may not use any non-MPI functions.\n  // You may not use global variables.\n  // You may not use std::vector.\n  // You may not use std::cout.\n  // You may not use std::endl.\n  // You may not use any other standard library functions.\n  // You may not use any other functions.\n  // You may not use any other data types.\n  // You may not use any other variables.\n  // You may not use any other statements.\n  // You may not use any other comments.\n  // You may not use any other code.\n  // You may not use any other text.\n  // You may not use any other files.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.\n  // You may not use any other MPI functions.",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunkSize = n / size;\n  int remainder = n % size;\n\n  std::vector<int> localX(chunkSize + (rank < remainder? 1 : 0));\n  MPI_Scatter(x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n              localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n              0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < localX.size(); i++) {\n    localX[i] *= localX[i];\n  }\n\n  std::vector<int> globalX(n);\n  MPI_Gather(localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n             globalX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n             0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = globalX;\n  }\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "cuda",
        "prompt": "/* Replace every element of x with the square of its value.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] *= x[i];\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] *= x[idx];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = x[idx] * x[idx];\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_squaring",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunkSize = n / size;\n  int remainder = n % size;\n\n  std::vector<int> localX(chunkSize + (rank < remainder? 1 : 0));\n  std::vector<int> localY(chunkSize + (rank < remainder? 1 : 0));\n\n  MPI_Scatter(x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n              localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0,\n              MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < localX.size(); i++) {\n    localY[i] = localX[i] * localX[i];\n  }\n\n  MPI_Gather(localY.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n             x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0,\n             MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[start], chunk, MPI_INT, i, 0, MPI_COMM_WORLD);\n      start += chunk;\n      end += chunk;\n    }\n  }\n\n  if (rank!= 0) {\n    MPI_Recv(&x[start], chunk, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    x[i] = x[i] * x[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x[start], chunk, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      start += chunk;\n      end += chunk;\n    }\n  }\n\n  if (rank!= 0) {\n    MPI_Send(&x[start], chunk, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<int> localX(chunkSize + (rank < remainder? 1 : 0));\n\n    MPI_Scatter(x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = x.size() / size;\n    int start = rank * chunkSize;\n    int end = (rank + 1) * chunkSize;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * chunkSize], chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&x[start], chunkSize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        x[i] *= x[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * chunkSize], chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start], chunkSize, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<int> localX;\n    std::vector<int> localY;\n\n    if (rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            localX.push_back(x[i]);\n        }\n        for (int i = remainder; i < x.size(); i++) {\n            localX.push_back(x[i]);\n        }\n    } else {\n        for (int i = 0; i < chunkSize; i++) {\n            localX.push_back(x[i]);\n        }\n    }\n\n    localY.resize(localX.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < localX.size(); i++) {\n        localY[i] = localX[i] * localX[i];\n    }\n\n    std::vector<int> globalY;\n    if (rank == 0) {\n        globalY.resize(x.size());\n    }\n\n    MPI_Gather(&localY[0], localY.size(), MPI_INT, &globalY[0], localY.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = globalY;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = omp_get_max_threads();\n    int num_elements = x.size();\n    int elements_per_thread = num_elements / num_threads;\n    int remainder = num_elements % num_threads;\n\n    std::vector<int> local_x;\n    std::vector<int> local_y;\n\n    if (rank == 0) {\n        local_x = std::vector<int>(x.begin(), x.begin() + elements_per_thread);\n        local_y = std::vector<int>(x.begin(), x.begin() + elements_per_thread);\n    } else {\n        local_x = std::vector<int>(x.begin() + elements_per_thread + remainder, x.end());\n        local_y = std::vector<int>(x.begin() + elements_per_thread + remainder, x.end());\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        local_y[i] = local_x[i] * local_x[i];\n    }\n\n    std::vector<int> global_y;\n    if (rank == 0) {\n        global_y = std::vector<int>(num_elements);\n    }\n\n    MPI_Gather(local_y.data(), local_y.size(), MPI_INT, global_y.data(), local_y.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_y;\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_elements = x.size();\n    int num_elements_per_rank = num_elements / size;\n    int num_elements_remainder = num_elements % size;\n\n    int start_index = rank * num_elements_per_rank;\n    int end_index = start_index + num_elements_per_rank;\n\n    if (rank == 0) {\n        end_index += num_elements_remainder;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < num_elements_remainder; i++) {\n            x[i] *= x[i];\n        }\n    }\n\n    for (int i = start_index; i < end_index; i++) {\n        x[i] *= x[i];\n    }\n\n    if (rank == 0) {\n        for (int i = end_index; i < num_elements; i++) {\n            x[i] *= x[i];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * num_elements_per_rank], num_elements_per_rank, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start_index], num_elements_per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<int> localX(chunkSize + (rank < remainder? 1 : 0));\n\n    MPI_Scatter(x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n                localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT,\n               x.data(), chunkSize + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Replace this line with your code\n    int size = x.size();\n    int rank = 0;\n    int num_threads = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    omp_set_num_threads(size);\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n    int chunk = size / num_threads;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = size;\n    }\n    for (int i = start; i < end; i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement the ReLU function\n}",
            "// TODO: Implement the ReLU function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that takes an index and computes the ReLU\n  // function on the element at that index.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  // Wait for the parallel_for to finish.\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"relu\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      if (x(i) < 0) {\n        x(i) = 0;\n      }\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\n    \"relu\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n        x(i) = 0;\n      }\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of the function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of the function\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n}",
            "// Create a parallel_for lambda function to compute the ReLU function\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"relu\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      if (x(i) < 0) {\n        x(i) = 0;\n      }\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "int n = x.size();\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "// TODO: implement\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "serial",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "hip",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0.0)? x[i] : 0.0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] > 0? x[i] : 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] < 0? 0 : x[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0)? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] > 0? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0.0)? x[i] : 0.0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0)? x[i] : 0;\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] > 0? x[idx] : 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0.0)? x[i] : 0.0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0.0)? x[i] : 0.0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = fmax(x[i], 0.0);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0.0)? x[i] : 0.0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] < 0? 0 : x[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] > 0? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0)? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] > 0? x[i] : 0;\n  }\n}",
            "// Get the index of the current thread\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check if the current thread is within bounds\n    if (i < N) {\n        // Compute the ReLU function\n        x[i] = (x[i] > 0.0)? x[i] : 0.0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] < 0.0? 0.0 : x[i];\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n    int n_start = rank * n_per_rank;\n    int n_end = n_start + n_per_rank;\n    if (rank == size - 1) {\n        n_end += n_remainder;\n    }\n\n    std::vector<double> x_local(n_per_rank);\n    for (int i = n_start; i < n_end; i++) {\n        x_local[i - n_start] = x[i];\n    }\n\n    for (int i = 0; i < n_per_rank; i++) {\n        if (x_local[i] < 0) {\n            x_local[i] = 0;\n        }\n    }\n\n    std::vector<double> x_all(n);\n    MPI_Gather(&x_local[0], n_per_rank, MPI_DOUBLE, &x_all[0], n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_all[i];\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Compute the ReLU function on every element of x.\n    //       Use MPI to compute in parallel.\n    //       Assume MPI has already been initialized.\n    //       Every rank has a complete copy of x. The final result is stored on rank 0.\n    //       Example:\n    //\n    //       input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n    //       output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n\n    // TODO: Your code here\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_remain = n % size;\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n    if (rank == size - 1)\n        end += n_remain;\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0)\n            x[i] = 0;\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * n_per_proc], n_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start], n_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  std::vector<double> x_local(n_per_rank + (rank < n_extra));\n  std::vector<double> x_local_relu(n_per_rank + (rank < n_extra));\n\n  MPI_Scatter(x.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE,\n              x_local.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_rank + (rank < n_extra); i++) {\n    if (x_local[i] < 0) {\n      x_local_relu[i] = 0;\n    } else {\n      x_local_relu[i] = x_local[i];\n    }\n  }\n\n  MPI_Gather(x_local_relu.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE,\n             x.data(), n_per_rank + (rank < n_extra), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Compute the ReLU function on x in parallel\n  // Hint: Use MPI_Send and MPI_Recv to send and receive data between ranks\n  // Hint: Use MPI_Scatter to distribute the data to each rank\n  // Hint: Use MPI_Gather to collect the data from each rank\n  // Hint: Use MPI_Bcast to broadcast the data from rank 0 to all ranks\n\n  // TODO: Use MPI_Reduce to compute the sum of the data on each rank\n  // Hint: Use MPI_Reduce with MPI_SUM to compute the sum of the data on each rank\n  // Hint: Use MPI_Reduce with MPI_SUM and MPI_ROOT to compute the sum of the data on rank 0\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<double> local_x(chunk_size + (rank < remainder? 1 : 0));\n    MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE,\n                local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    std::vector<double> result(chunk_size + (rank < remainder? 1 : 0));\n    MPI_Gather(local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE,\n               result.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = result;\n    }\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size()/size;\n    int start = rank*chunk;\n    int end = (rank+1)*chunk;\n    if(rank == size-1) end = x.size();\n    for(int i = start; i < end; i++) {\n        if(x[i] < 0) x[i] = 0;\n    }\n    std::vector<double> temp(chunk);\n    MPI_Gather(&x[start], chunk, MPI_DOUBLE, &temp[0], chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if(rank == 0) x = temp;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_proc;\n  int end = start + n_per_proc;\n  if (rank == size - 1) {\n    end += n_rem;\n  }\n  std::vector<double> x_proc(n_per_proc);\n  for (int i = 0; i < n_per_proc; i++) {\n    x_proc[i] = x[start + i];\n  }\n  for (int i = 0; i < n_per_proc; i++) {\n    if (x_proc[i] < 0) {\n      x_proc[i] = 0;\n    }\n  }\n  std::vector<double> x_proc_recv(n_per_proc);\n  MPI_Gather(&x_proc[0], n_per_proc, MPI_DOUBLE, &x_proc_recv[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = x_proc_recv[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<double> local_x(chunk_size + (rank < remainder? 1 : 0));\n    MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    std::vector<double> global_x(x.size());\n    MPI_Gather(local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, global_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_x;\n    }\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_proc;\n  int end = start + n_per_proc;\n  if (rank == size - 1) {\n    end += n_rem;\n  }\n  for (int i = start; i < end; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n\n  // TODO: Your code here\n  if (rank == 0) {\n    std::vector<double> result(n);\n    MPI_Gather(&x[0], n_per_proc, MPI_DOUBLE, &result[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    x = result;\n  } else {\n    MPI_Gather(&x[0], n_per_proc, MPI_DOUBLE, NULL, n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_proc;\n    int end = (rank + 1) * n_per_proc;\n    if (rank == size - 1) {\n        end += n_rem;\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    // TODO: Your code here\n    if (rank == 0) {\n        std::vector<double> recv_buf(n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recv_buf[0], n_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n_per_proc; j++) {\n                x[i * n_per_proc + j] = recv_buf[j];\n            }\n        }\n    } else {\n        MPI_Send(&x[0], n_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // Hint: Use MPI_Send and MPI_Recv to send and receive data between ranks\n  // Hint: Use MPI_Reduce to compute the final result\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  // TODO: Send the result to rank 0\n\n  // TODO: Receive the result from rank 0\n\n  // TODO: Broadcast the result to all ranks\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] > 0? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = (x[i] > 0)? x[i] : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "// TODO: Implement the ReLU function\n    // Use the thread ID to index into the array\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "// TODO: Implement the ReLU function\n    // Use the thread ID to index into the array\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] > 0? x[i] : 0;\n    }\n}",
            "// TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use the following formula to compute the ReLU function:\n    // f(x) = max(0, x)\n\n    // TODO: Implement the ReLU function\n    // Use",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0.0)? x[i] : 0.0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] > 0? x[i] : 0;\n    }\n}",
            "// TODO: Implement the ReLU function\n    // Use the thread id to index into the array\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = (x[idx] > 0)? x[idx] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] > 0? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] < 0? 0 : x[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}",
            "// TODO: Implement the ReLU function\n  // Use the following formula to compute the ReLU function:\n  // x_new = max(0, x)\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double fmax(double x, double y);\n  // float fmaxf(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double fmin(double x, double y);\n  // float fminf(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double fabs(double x);\n  // float fabsf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double sqrt(double x);\n  // float sqrtf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double exp(double x);\n  // float expf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double log(double x);\n  // float logf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double pow(double x, double y);\n  // float powf(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double sin(double x);\n  // float sinf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double cos(double x);\n  // float cosf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double tan(double x);\n  // float tanf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double atan(double x);\n  // float atanf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double atan2(double x, double y);\n  // float atan2f(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double ceil(double x);\n  // float ceilf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double floor(double x);\n  // float floorf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double fmod(double x, double y);\n  // float fmodf(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double fdim(double x, double y);\n  // float fdimf(float x, float y);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double trunc(double x);\n  // float truncf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double round(double x);\n  // float roundf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double rint(double x);\n  // float rintf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double nearbyint(double x);\n  // float nearbyintf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double lrint(double x);\n  // long int lrintf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double lround(double x);\n  // long int lroundf(float x);\n  //\n  // You can use the following functions to compute the max:\n  //\n  // double llrint(double x);\n  // long long int llrintf(float x);\n  //\n  // You can use the following functions to compute",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = (x[i] > 0)? x[i] : 0;\n    }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "52_transform_relu",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<double> local_x(chunk + (rank < remainder? 1 : 0));\n    MPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    std::vector<double> global_x(x.size());\n    MPI_Gather(local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, global_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_x;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_per_rank = x.size() / size;\n    int num_extra = x.size() % size;\n\n    int start = rank * num_per_rank + std::min(rank, num_extra);\n    int end = start + num_per_rank + (rank < num_extra? 1 : 0);\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, x.data(), x.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    std::vector<double> x_local(n_per_rank);\n    std::vector<double> x_local_result(n_per_rank);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        x_local = std::vector<double>(x.begin(), x.begin() + n_per_rank);\n    } else {\n        MPI_Recv(&x_local[0], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank; i++) {\n        if (x_local[i] < 0) {\n            x_local_result[i] = 0;\n        } else {\n            x_local_result[i] = x_local[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x_local_result[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x_local_result[0], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_local_result[i];\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_per_rank = x.size() / size;\n    int start = rank * num_per_rank;\n    int end = (rank + 1) * num_per_rank;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    std::vector<double> local_x(x.begin() + start, x.begin() + end);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        x.resize(x.size() - (size - 1) * num_per_rank);\n    }\n\n    MPI_Gather(local_x.data(), num_per_rank, MPI_DOUBLE, x.data(), num_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_per_rank_extra = num_elements % size;\n\n  int start_index = rank * num_elements_per_rank;\n  int end_index = start_index + num_elements_per_rank;\n  if (rank == size - 1) {\n    end_index += num_elements_per_rank_extra;\n  }\n\n  std::vector<double> local_x(num_elements_per_rank);\n  for (int i = 0; i < num_elements_per_rank; i++) {\n    local_x[i] = x[start_index + i];\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < num_elements_per_rank; i++) {\n    if (local_x[i] < 0) {\n      local_x[i] = 0;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < num_elements_per_rank; i++) {\n      x[i] = local_x[i];\n    }\n  } else {\n    MPI_Send(local_x.data(), num_elements_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank!= 0) {\n    MPI_Recv(local_x.data(), num_elements_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for (int i = 0; i < num_elements_per_rank; i++) {\n      x[start_index + i] = local_x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_per_rank = x.size() / size;\n    int num_left = x.size() % size;\n\n    std::vector<double> local_x(num_per_rank + (rank < num_left));\n    MPI_Scatter(x.data(), num_per_rank + (rank < num_left), MPI_DOUBLE, local_x.data(), num_per_rank + (rank < num_left), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    std::vector<double> global_x(x.size());\n    MPI_Gather(local_x.data(), num_per_rank + (rank < num_left), MPI_DOUBLE, global_x.data(), num_per_rank + (rank < num_left), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_x;\n    }\n}",
            "// TODO: Implement this function\n}",
            "int num_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int num_threads = omp_get_max_threads();\n    int num_ranks = omp_get_num_threads();\n    int rank = omp_get_thread_num();\n    int size = x.size();\n    int chunk_size = size / num_ranks;\n    int start = rank * chunk_size;\n    int end = start + chunk_size;\n    if (rank == num_ranks - 1) {\n        end = size;\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  // TODO: Compute the ReLU function on every element of x.\n  //       Use MPI and OpenMP to compute in parallel.\n  //       Assume MPI has already been initialized.\n  //       Every rank has a complete copy of x. The final result is stored on rank 0.\n\n  // TODO: Use MPI_Scatter to distribute the data to the ranks.\n  //       Use MPI_Gather to collect the data from the ranks.\n  //       Use OpenMP to compute the ReLU function in parallel.\n\n  // TODO: Use MPI_Reduce to collect the data from the ranks.\n  //       Use OpenMP to compute the ReLU function in parallel.\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_per_rank = x.size() / size;\n    int num_extra = x.size() % size;\n\n    std::vector<double> local_x;\n    if (rank == 0) {\n        local_x.resize(num_per_rank + num_extra);\n        std::copy(x.begin(), x.end(), local_x.begin());\n    } else {\n        local_x.resize(num_per_rank);\n        MPI_Status status;\n        MPI_Recv(local_x.data(), num_per_rank, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &status);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::copy(local_x.begin(), local_x.end(), x.begin());\n    } else {\n        MPI_Send(local_x.data(), num_per_rank, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_rank + std::min(rank, n_rem);\n  int end = (rank + 1) * n_per_rank + std::min(rank + 1, n_rem);\n  std::vector<double> x_local(x.begin() + start, x.begin() + end);\n\n  #pragma omp parallel for\n  for (int i = 0; i < x_local.size(); i++) {\n    if (x_local[i] < 0) {\n      x_local[i] = 0;\n    }\n  }\n\n  // TODO: Your code here\n  MPI_Reduce(x_local.data(), x.data(), x.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int size = x.size();\n    int chunk = size / num_ranks;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == num_ranks - 1) {\n        end = size;\n    }\n    std::vector<double> local_x(x.begin() + start, x.begin() + end);\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n    MPI_Gather(local_x.data(), chunk, MPI_DOUBLE, x.data(), chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  // For debugging\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      printf(\"%f \", x[i]);\n    }\n    printf(\"\\n\");\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<double> local_x(chunk_size + (rank < remainder? 1 : 0));\n\n    MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE,\n                local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    std::vector<double> global_x(x.size());\n    MPI_Gather(local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE,\n               global_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_x;\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a parallel_for lambda that will be executed in parallel.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel section to finish.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function to do the work.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel_for to finish.\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes an integer index as an argument.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    }\n  );\n\n  // Wait for all parallel operations to finish.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes a single integer argument, i.\n  // The lambda function will be executed once for each element of the View.\n  // The lambda function will be executed in parallel.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n                       KOKKOS_LAMBDA(const int i) {\n                         x(i) = 1 - 1 / x(i);\n                       });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n}",
            "// Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "// Create a parallel_for lambda to do the work\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1.0 - 1.0 / x(i);\n  });\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::Rank<1>>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    }\n  );\n}",
            "// Create a parallel_for lambda to do the work.\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n\n  // Force the parallel_for to complete before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda to compute 1-1/x.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// Create a parallel_for lambda that will be executed in parallel.\n  // The lambda takes one argument: the index of the element to process.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    x.extent(0),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel section to finish.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function will be called once for each element of the View.\n  // The lambda function will take as input the index of the element.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceMax<int> > >(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    });\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes a single integer argument, which is the index\n  // of the element to be processed.\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    x.extent(0),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    }\n  );\n\n  // Force the parallel section to finish before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda to do the work\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1.0 - 1.0 / x(i);\n  });\n\n  // Force the parallel_for to finish\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1 / x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// Create a parallel_for lambda function to perform the computation\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    x.extent(0),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1.0 - 1.0 / x(i);\n    }\n  );\n\n  // Force the parallel_for to complete before returning\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that takes an index i and\n  // computes x(i) = 1 - 1/x(i).\n  Kokkos::parallel_for(\n    \"oneMinusInverse\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = 1 - 1/x(i);\n    }\n  );\n\n  // Force the parallel_for to finish before returning.\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function to do the work\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  // Force the parallel_for to finish before returning\n  Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (auto &i : x) {\n        i = 1.0 - 1.0 / i;\n    }\n}",
            "for (auto &i : x) {\n    i = 1.0 / i;\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (auto &v : x) {\n    v = 1 - 1 / v;\n  }\n}",
            "for (auto &x_i : x) {\n    x_i = 1 - 1 / x_i;\n  }\n}",
            "for (auto &i : x) {\n    i = 1 - 1 / i;\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (auto &e : x) {\n    e = 1 - 1 / e;\n  }\n}",
            "for (auto &v : x) {\n    v = 1.0 - 1.0 / v;\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (auto &i : x) {\n    i = 1 - 1 / i;\n  }\n}",
            "for (auto &x_i : x) {\n    x_i = 1 - 1 / x_i;\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (auto &v : x) {\n    v = 1 - 1 / v;\n  }\n}",
            "for (auto &i : x) {\n    i = 1 - 1 / i;\n  }\n}",
            "for (auto &i : x) {\n    i = 1 - 1 / i;\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "for (auto &elem : x) {\n    elem = 1.0 - 1.0 / elem;\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "hip",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1 / x[i];\n  }\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<double> my_x(chunk);\n    std::vector<double> my_y(chunk);\n\n    MPI_Scatter(x.data(), chunk, MPI_DOUBLE, my_x.data(), chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < chunk; i++) {\n        my_y[i] = 1 - 1 / my_x[i];\n    }\n\n    MPI_Gather(my_y.data(), chunk, MPI_DOUBLE, x.data(), chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            x[i + chunk * size] = 1 - 1 / x[i + chunk * size];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Replace this with your code\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_extra = n % size;\n\n    std::vector<double> y(n_per_proc + (rank < n_extra));\n\n    MPI_Scatter(x.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE,\n                y.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE, 0,\n                MPI_COMM_WORLD);\n\n    for (int i = 0; i < y.size(); ++i) {\n        y[i] = 1 - 1 / y[i];\n    }\n\n    MPI_Gather(y.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE,\n               x.data(), n_per_proc + (rank < n_extra), MPI_DOUBLE, 0,\n               MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  std::vector<double> y(chunk);\n  std::vector<double> z(chunk);\n\n  MPI_Scatter(x.data(), chunk, MPI_DOUBLE, y.data(), chunk, MPI_DOUBLE, 0,\n              MPI_COMM_WORLD);\n\n  for (int i = 0; i < chunk; i++) {\n    y[i] = 1 - 1 / y[i];\n  }\n\n  MPI_Gather(y.data(), chunk, MPI_DOUBLE, z.data(), chunk, MPI_DOUBLE, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < remainder; i++) {\n      z[i] = 1 - 1 / z[i];\n    }\n    for (int i = remainder; i < n; i++) {\n      x[i] = z[i - remainder];\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Replace this with your code\n    //\n    // 1. Compute the number of elements per rank\n    // 2. Compute the starting index of each rank\n    // 3. Compute the number of elements to be computed by this rank\n    // 4. Compute the local result\n    // 5. Use MPI_Reduce to compute the global result\n    //\n    // Note:\n    // - Use MPI_Reduce to compute the global result\n    // - Use MPI_IN_PLACE as the send buffer if you want to use the local result\n    //   as the receive buffer\n    // - Use MPI_SUM as the operation\n    // - Use MPI_DOUBLE as the data type\n    // - Use MPI_COMM_WORLD as the communicator\n    // - Use rank 0 as the root\n    //\n    // Hint:\n    // - Use MPI_Bcast to broadcast the number of elements per rank\n    // - Use MPI_Bcast to broadcast the starting index of each rank\n    // - Use MPI_Bcast to broadcast the number of elements to be computed by this rank\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Replace this with your code\n  // You may assume that the size of x is a multiple of size\n  // You may also assume that x is a complete copy of the vector on every rank\n  // You may also assume that the input is valid (i.e. no division by zero)\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<double> localX(chunk + (rank < remainder? 1 : 0));\n    MPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE,\n                localX.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE,\n                0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] = 1 - 1 / localX[i];\n    }\n\n    std::vector<double> globalX(n);\n    MPI_Gather(localX.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE,\n               globalX.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE,\n               0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = globalX;\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: replace this with your code\n    //\n    // Hint:\n    // - You can use MPI_Send and MPI_Recv to send and receive data between\n    //   ranks.\n    // - You can use MPI_Bcast to broadcast data from rank 0 to all other ranks.\n    // - You can use MPI_Reduce to combine data from all ranks into a single\n    //   value.\n    // - You can use MPI_Scatter to distribute data from rank 0 to all other\n    //   ranks.\n    // - You can use MPI_Gather to collect data from all ranks into rank 0.\n    // - You can use MPI_Allgather to collect data from all ranks into all\n    //   ranks.\n    // - You can use MPI_Allreduce to combine data from all ranks into a single\n    //   value.\n    // - You can use MPI_Reduce_scatter to combine data from all ranks into a\n    //   single value.\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "cuda",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1 - 1 / x[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = 1.0 - 1.0 / x[i];\n    }\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "54_transform_inverse_offset",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n    int n_local = n_per_rank + (rank < n_remainder);\n\n    std::vector<double> x_local(n_local);\n    std::vector<double> x_local_result(n_local);\n\n    MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; i++) {\n        x_local_result[i] = 1 - 1 / x_local[i];\n    }\n\n    MPI_Gather(x_local_result.data(), n_local, MPI_DOUBLE, x.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // TODO: Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < n_local; i++) {\n    x_local[i] = 1.0 - 1.0 / x_local[i];\n  }\n\n  // TODO: Your code here\n  MPI_Gather(x_local.data(), n_local, MPI_DOUBLE, x.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end = n;\n  }\n\n  for (int i = start; i < end; i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n\n  // TODO: Your code here\n  std::vector<double> recv(chunk);\n  MPI_Gather(&x[start], chunk, MPI_DOUBLE, &recv[0], chunk, MPI_DOUBLE, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      for (int j = 0; j < chunk; j++) {\n        x[i * chunk + j] = recv[j];\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunk = n / size;\n    int remainder = n % size;\n\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    std::vector<double> local_x(end - start);\n    for (int i = start; i < end; i++) {\n        local_x[i - start] = x[i];\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < chunk; i++) {\n            x[i] = local_x[i];\n        }\n    } else {\n        MPI_Send(local_x.data(), local_x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(x.data() + chunk, remainder, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n\n  std::vector<double> local_x(chunk + (rank < remainder? 1 : 0));\n  MPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_x[i] = 1 - 1 / local_x[i];\n  }\n\n  std::vector<double> global_x(x.size());\n  MPI_Gather(local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, global_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = global_x;\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: replace this with your code\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n  int start = rank * n_per_proc;\n  int end = start + n_per_proc;\n  if (rank == size - 1) {\n    end += n_rem;\n  }\n  for (int i = start; i < end; i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the code to compute Y=AX\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M), [&](const int i) {\n      for (size_t j = 0; j < N; j++) {\n         Y(i, j) = 0;\n         for (size_t k = 0; k < K; k++) {\n            Y(i, j) += A(i*K+k).value * X(k*N+j).value;\n         }\n      }\n   });\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\"spmm\", M, KOKKOS_LAMBDA(const int i) {\n      for (size_t j = 0; j < N; j++) {\n         Y(i, j) = 0;\n         for (size_t k = 0; k < K; k++) {\n            Y(i, j) += A(i*K+k).value * X(k*N+j).value;\n         }\n      }\n   });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         double sum = 0;\n         for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k].value * X[k * N + j].value;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         double sum = 0;\n         for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k].value * X[k * N + j].value;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement the matrix multiplication\n}",
            "// TODO: implement this function\n}",
            "// Initialize Y to zero\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         Y[i * N + j] = 0;\n      }\n   }\n\n   // Compute Y = AX\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (column == X[j].column) {\n            Y[row * N + X[j].row] += value * X[j].value;\n         }\n      }\n   }\n}",
            "// TODO: implement me\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i*K + k].value * X[k*N + j].value;\n         }\n         Y[i*N + j] = sum;\n      }\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement the matrix multiplication\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i*K+k].value * X[k*N+j].value;\n         }\n         Y[i*N+j] = sum;\n      }\n   }\n}",
            "// TODO: implement the matrix multiplication\n   // Y[i][j] = A[i][k] * X[k][j]\n   // for all i in [0, M), j in [0, N), k in [0, K)\n\n   // initialize Y to zero\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         Y[i * N + j] = 0;\n      }\n   }\n\n   // TODO: parallelize this loop\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < K; k++) {\n            Y[i * N + j] += A[i * K + k].value * X[k * N + j].value;\n         }\n      }\n   }\n}",
            "// TODO: implement the matrix multiplication\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k].value * X[k * N + j].value;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "// TODO: implement the matrix multiplication\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i*K + k].value * X[k*N + j].value;\n         }\n         Y[i*N + j] = sum;\n      }\n   }\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k].value * X[k * N + j].value;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         double sum = 0;\n         for (size_t k = 0; k < K; ++k) {\n            double a = 0, x = 0;\n            if (A[i * K + k].row == i && A[i * K + k].column == k) {\n               a = A[i * K + k].value;\n            }\n            if (X[k * N + j].row == k && X[k * N + j].column == j) {\n               x = X[k * N + j].value;\n            }\n            sum += a * x;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "// TODO: Implement the matrix multiplication\n   // Hint: Use OpenMP to parallelize the loop\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k].value * X[k * N + j].value;\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "std::vector<double> A_values(A.size());\n   std::vector<double> X_values(X.size());\n   std::vector<double> Y_values(M*N);\n\n   for (size_t i = 0; i < A.size(); i++) {\n      A_values[i] = A[i].value;\n   }\n   for (size_t i = 0; i < X.size(); i++) {\n      X_values[i] = X[i].value;\n   }\n\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         Y_values[i*N + j] = 0;\n         for (size_t k = 0; k < K; k++) {\n            Y_values[i*N + j] += A_values[i*K + k] * X_values[k*N + j];\n         }\n      }\n   }\n\n   Y = Y_values;\n}",
            "std::vector<double> Y_temp(M*N, 0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Y_temp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = Y_temp;\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement the matrix multiplication\n}",
            "// TODO: implement this function\n}",
            "std::vector<double> Y_temp(M*N, 0);\n   for (auto const& x : X) {\n      for (auto const& a : A) {\n         if (a.column == x.row) {\n            Y_temp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = Y_temp;\n}",
            "std::vector<double> Y_tmp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Y_tmp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = Y_tmp;\n}",
            "std::vector<double> Y_temp(M*N, 0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Y_temp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = Y_temp;\n}",
            "std::vector<double> Y_temp(M*N, 0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Y_temp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = Y_temp;\n}",
            "std::vector<double> A_values(A.size());\n   std::vector<double> X_values(X.size());\n   std::vector<double> Y_values(M*N);\n\n   for (size_t i = 0; i < A.size(); i++) {\n      A_values[i] = A[i].value;\n   }\n   for (size_t i = 0; i < X.size(); i++) {\n      X_values[i] = X[i].value;\n   }\n\n   // Compute the matrix multiplication Y=AX.\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         double sum = 0;\n         for (size_t k = 0; k < K; k++) {\n            sum += A_values[i*K + k] * X_values[k*N + j];\n         }\n         Y_values[i*N + j] = sum;\n      }\n   }\n\n   // Copy the result to Y.\n   for (size_t i = 0; i < M; i++) {\n      for (size_t j = 0; j < N; j++) {\n         Y[i*N + j] = Y_values[i*N + j];\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "std::vector<double> A_values(A.size());\n   std::vector<double> X_values(X.size());\n   for (size_t i = 0; i < A.size(); ++i) {\n      A_values[i] = A[i].value;\n   }\n   for (size_t i = 0; i < X.size(); ++i) {\n      X_values[i] = X[i].value;\n   }\n\n   // TODO: implement the matrix multiplication\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "std::vector<double> A_values(A.size());\n   std::vector<double> X_values(X.size());\n   std::vector<double> Y_values(M*N);\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      A_values[i] = A[i].value;\n   }\n\n   for (size_t i = 0; i < X.size(); ++i) {\n      X_values[i] = X[i].value;\n   }\n\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         Y_values[i*N+j] = 0;\n      }\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A_values[i];\n\n      for (size_t j = 0; j < X.size(); ++j) {\n         size_t x_row = X[j].row;\n         size_t x_column = X[j].column;\n         double x_value = X_values[j];\n\n         if (column == x_row) {\n            Y_values[row*N+x_column] += value*x_value;\n         }\n      }\n   }\n\n   Y = Y_values;\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == A[i].column) {\n               sum += A[i].value * X[j].value;\n               break;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * X[A[i].column].value;\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (column == X[j].column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         size_t column = A[i].column;\n         double value = A[i].value;\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == column) {\n               Y[row * N + X[j].row] += value * X[j].value;\n            }\n         }\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   for (size_t j = 0; j < sizeX; j++) {\n      if (column == X[j].column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         sum += X[j].value * value;\n      }\n   }\n   Y[row * N + column] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; ++i) {\n      if (A[i].row == row) {\n         sum += A[i].value * X[A[i].column].value;\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; ++i) {\n      if (A[i].row == row) {\n         sum += A[i].value * X[A[i].column].value;\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         size_t column = A[i].column;\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == column) {\n               sum += A[i].value * X[j].value;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      for (size_t j = 0; j < sizeX; j++) {\n         if (X[j].column == column) {\n            Y[row * N + X[j].row] += value * X[j].value;\n         }\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         size_t column = A[i].column;\n         double value = A[i].value;\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == column) {\n               sum += value * X[j].value;\n               break;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0.0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * X[A[i].column].value;\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         sum += X[j].value * value;\n      }\n   }\n   Y[row * N + column] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == A[i].column) {\n               sum += A[i].value * X[j].value;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         sum += X[j].value * value;\n      }\n   }\n   Y[row * N + column] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0.0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         size_t column = A[i].column;\n         double value = A[i].value;\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == column) {\n               sum += value * X[j].value;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: your code here\n}",
            "// TODO: your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: implement the kernel\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == A[i].column) {\n               sum += A[i].value * X[j].value;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "// TODO: implement the kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         sum += X[j].value * value;\n      }\n   }\n\n   Y[row * N + column] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      for (size_t j = 0; j < sizeX; j++) {\n         if (X[j].column == column) {\n            Y[row * N + X[j].row] += value * X[j].value;\n         }\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (column == X[j].column) {\n         atomicAdd(&Y[row * N + X[j].row], value * X[j].value);\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         sum += X[j].value * value;\n      }\n   }\n   Y[row * N + column] = sum;\n}",
            "// TODO: implement the kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      double sum = 0;\n      for (size_t j = 0; j < sizeX; j++) {\n         if (X[j].column == column) {\n            sum += X[j].value * value;\n         }\n      }\n      Y[row * N + column] = sum;\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   for (size_t i = 0; i < sizeX; i++) {\n      if (X[i].column == row) {\n         double sum = 0;\n         for (size_t j = 0; j < sizeA; j++) {\n            if (A[j].row == X[i].row) {\n               sum += A[j].value * X[i].value;\n            }\n         }\n         Y[row * N + X[i].column] = sum;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   double sum = 0;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (column == X[j].column) {\n         sum += value * X[j].value;\n      }\n   }\n   Y[row * N + column] = sum;\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         for (size_t j = 0; j < sizeX; j++) {\n            if (X[j].column == A[i].column) {\n               sum += A[i].value * X[j].value;\n            }\n         }\n      }\n   }\n   Y[row] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   for (size_t j = 0; j < sizeX; j++) {\n      if (X[j].column == column) {\n         Y[row * N + X[j].row] += value * X[j].value;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "43_sparse_la_spmm",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get_num_threads\n   // - omp_get_max_threads\n   // - omp_get_num_procs\n   // - omp_get_dynamic\n   // - omp_set_dynamic\n   // - omp_set_num_threads\n\n   // TODO: Your code here\n   // You may use the following functions:\n   // - MPI_Comm_rank\n   // - MPI_Comm_size\n   // - MPI_Send\n   // - MPI_Recv\n   // - MPI_Bcast\n   // - MPI_Allreduce\n   // - omp_get_thread_num\n   // - omp_get",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n   // Hint: Use Kokkos::parallel_for\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::TeamThreadRange class\n   // You may need to use the Kokkos::parallel_for function\n   // You may need to use the Kokkos::subview function\n   // You may need to use the Kokkos::atomic_add function\n   // You may need to use the Kokkos::Atomic<Kokkos::OpenMP> class\n   // You may need to use the Kokkos::RangePolicy class\n   // You may need to use the Kokkos::TeamPolicy class\n   // You may need to use",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix is stored in COO format\n   // You may assume that the matrix is square\n   // You may assume that the matrix is symmetric\n   // You may assume that the matrix is positive definite\n   // You may assume that the matrix has no duplicate entries\n   // You may assume that the matrix has no zeros on the diagonal\n   // You may assume that the matrix has no zeros on the diagonal\n   //",
            "// TODO: Implement this function\n   // Hint: You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i].column, A[i].value\n   //       You can use the following functions to access the elements of the vectors\n   //       x[i] and y[i]\n   //       A[i].row, A[i",
            "// TODO\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0.0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n\n   // TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n\n   // TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n\n   // TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n\n   // TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n\n   // TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread id\n   // Hint: You can use the omp_get_num_threads()",
            "// TODO: implement\n\n}",
            "// TODO: implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to determine the thread number\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_in_parallel() function to determine if you are in parallel mode\n   // Hint: You can use the omp_set_num_threads(int) function to set the number of threads\n   // Hint: You can use the omp_get_dynamic() function to determine if dynamic thread adjustment is enabled\n   // Hint: You can use the omp_set_dynamic(int) function to enable or disable dynamic thread adjustment\n   // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n   // Hint: You can use the omp_get_thread_limit() function to determine the thread limit\n   // Hint: You can use the omp_set_nested(int) function to enable or disable nested parallelism\n   // Hint: You can use the omp_get_nested() function to determine if nested parallelism is enabled\n   // Hint: You can use the omp_set_schedule(omp_sched_t, int) function to set the schedule type and chunk size\n   // Hint: You can use the omp_get_schedule() function to determine the schedule type and chunk size\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread number\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_in_parallel() function to determine if you are in parallel mode\n   // Hint: You can use the omp_set_num_threads(int) function to set the number of threads\n   // Hint: You can use the omp_get_dynamic() function to determine if dynamic thread adjustment is enabled\n   // Hint: You can use the omp_set_dynamic(int) function to enable or disable dynamic thread adjustment\n   // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n   // Hint: You can use the omp_get_thread_limit() function to determine the thread limit\n   // Hint: You can use the omp_set_nested(int) function to enable or disable nested parallelism\n   // Hint: You can use the omp_get_nested() function to determine if nested parallelism is enabled\n   // Hint: You can use the omp_set_schedule(omp_sched_t, int) function to set the schedule type and chunk size\n   // Hint: You can use the omp_get_schedule() function to determine the schedule type and chunk size\n   // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n   // Hint: You can use the omp_get_thread_num() function to determine the thread number\n   // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n   // Hint: You can use the omp_in_parallel() function to determine if you are in parallel mode\n   // Hint: You can use the omp_set_num_threads(int) function to set the number of threads\n   // Hint: You can use the omp_get_dynamic() function to determine if dynamic thread adjustment is enabled\n   // Hint: You can use the omp_set_dynamic(int) function to enable or disable dynamic thread adjustment\n   // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n   // Hint: You can use the omp_get_thread_limit() function to determine the thread limit\n   // Hint: You can use the omp_set_nested(int) function to enable or disable nested parallelism\n   // Hint: You can use the omp_get_nested() function to determine if nested parallelism is enabled\n   // Hint: You can use the",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0.0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for(size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for(size_t j = 0; j < N; j++) {\n         for(size_t k = 0; k < A.size(); k++) {\n            if(A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the omp_get_num_procs() function to get the number of processors\n   // Hint: You can use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: You can use the omp_get_thread_num() function to get the thread number\n   // Hint: You can use the omp_get_num_threads() function to get the number of threads\n   // Hint: You can use the",
            "// TODO: Implement this function\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y\n   // double x_i = x[i];\n   // double& y_i = y[i];\n\n   // Hint: You can use the following code to access the (i,j)-th element of A\n   // double A_ij = A[i].value;\n   // size_t A_ij_row = A[i].row;\n   // size_t A_ij_column = A[i].column;\n\n   // Hint: You can use the following code to access the i-th element of x and y",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         sum += A[i*N + j].value * x[j];\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         sum += A[i*N + j].value * x[j];\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // Hint: Use OpenMP to parallelize\n   // Hint: Use the following formula to compute y[i]\n   // y[i] = alpha * sum(A[i,j] * x[j]) + beta * y[i]\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0.0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint: You can use the following code as a starting point\n   // for (size_t i = 0; i < M; i++) {\n   //    double sum = 0.0;\n   //    for (size_t j = 0; j < N; j++) {\n   //       sum += A[i*N + j] * x[j];\n   //    }\n   //    y[i] = alpha * sum + beta * y[i];\n   // }\n\n   // TODO: Implement this function\n   // Hint:",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; i++) {\n      double sum = 0.0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}",
            "// TODO: implement this\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n   // Hint: You can use the following functions:\n   //   - std::vector::at()\n   //   - std::vector::size()\n   //   - std::vector::begin()\n   //   - std::vector::end()\n   //   - std::vector::push_back()\n   //   - std::vector::insert()\n   //   - std::vector::erase()\n   //   - std::vector::clear()\n   //   - std::vector::empty()\n   //   - std::vector::resize()\n   //   - std::vector::reserve()\n   //   - std::vector::swap()\n   //   - std::vector::assign()\n   //   - std::vector::insert()\n   //   - std::vector::erase()\n   //   - std::vector::emplace()\n   //   - std::vector::emplace_back()\n   //   - std::vector::pop_back()\n   //   - std::vector::swap()\n   //   - std::vector::shrink_to_fit()\n   //   - std::vector::operator[]()\n   //   - std::vector::operator=()\n   //   - std::vector::operator==()\n   //   - std::vector::operator!=()\n   //   - std::vector::operator<()\n   //   - std::vector::operator>()\n   //   - std::vector::operator<=()\n   //   - std::vector::operator>=()\n   //   - std::vector::get_allocator()\n   //   - std::vector::swap()\n   //   - std::vector::size()\n   //   - std::vector::max_size()\n   //   - std::vector::capacity()\n   //   - std::vector::empty()\n   //   - std::vector::reserve()\n   //   - std::vector::shrink_to_fit()\n   //   - std::vector::clear()\n   //   - std::vector::insert()\n   //   - std::vector::emplace()\n   //   - std::vector::emplace_back()\n   //   - std::vector::push_back()\n   //   - std::vector::pop_back()\n   //   - std::vector::erase()\n   //   - std::vector::swap()\n   //   - std::vector::assign()\n   //   - std::vector::at()\n   //   - std::vector::operator[]()\n   //   - std::vector::front()\n   //   - std::vector::back()\n   //   - std::vector::data()\n   //   - std::vector::begin()\n   //   - std::vector::end()\n   //   - std::vector::rbegin()\n   //   - std::vector::rend()\n   //   - std::vector::cbegin()\n   //   - std::vector::cend()\n   //   - std::vector::crbegin()\n   //   - std::vector::crend()\n   //   - std::vector::empty()\n   //   - std::vector::size()\n   //   - std::vector::max_size()\n   //   - std::vector::capacity()\n   //   - std::vector::shrink_to_fit()\n   //   - std::vector::reserve()\n   //   - std::vector::clear()\n   //   - std::vector::insert()\n   //   - std::vector::emplace()\n   //   - std::vector::emplace_back()\n   //   - std::vector::push_back()\n   //   - std::vector::pop_back()\n   //   - std::vector::erase()\n   //   - std::vector::swap()\n   //   - std::vector::assign()\n   //   - std::vector::at()\n   //   - std::vector::operator[]()\n   //   - std::vector::front()\n   //   - std::vector::back()\n   //   - std::vector::data()\n   //   - std::vector::begin()\n   //   - std::vector::end()\n   //   - std::vector::rbegin()\n   //   - std::vector::rend()\n   //   - std::vector::cbegin()\n   //   - std::vector::cend()\n   //",
            "for (size_t i = 0; i < M; ++i) {\n      y[i] = 0;\n   }\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n}",
            "// TODO: implement this\n}",
            "// TODO: implement this function\n   std::vector<double> y_temp(M, 0.0);\n   for (auto const& element : A) {\n      y_temp[element.row] += element.value * x[element.column];\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] = alpha * y_temp[i] + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   for (size_t i = 0; i < M; i++) {\n      y[i] = 0;\n   }\n   for (size_t i = 0; i < A.size(); i++) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] *= beta;\n   }\n}",
            "// TODO: implement this function\n}",
            "for (size_t i = 0; i < M; ++i) {\n      y[i] = 0;\n   }\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= alpha;\n   }\n   for (size_t i = 0; i < M; ++i) {\n      y[i] += beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   for (size_t i = 0; i < M; i++) {\n      y[i] = 0;\n   }\n   for (size_t i = 0; i < A.size(); i++) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] *= beta;\n   }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n   // Hint: you can use the following functions:\n   //  - std::vector::at()\n   //  - std::vector::size()\n   //  - std::vector::begin()\n   //  - std::vector::end()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::push_back()\n   //  - std::vector::clear()\n   //  - std::vector::reserve()\n   //  - std::vector::resize()\n   //  - std::vector::empty()\n   //  - std::vector::swap()\n   //  - std::vector::assign()\n   //  - std::vector::insert()\n   //  - std::vector::erase()\n   //  - std::vector::",
            "// TODO: implement this function\n}",
            "for (size_t i = 0; i < M; ++i) {\n      y[i] = 0;\n      for (size_t j = 0; j < A.size(); ++j) {\n         if (A[j].row == i) {\n            y[i] += A[j].value * x[A[j].column];\n         }\n      }\n      y[i] *= alpha;\n      y[i] += beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   for (size_t i = 0; i < M; i++) {\n      y[i] = beta * y[i];\n   }\n   for (size_t i = 0; i < A.size(); i++) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}",
            "// TODO: implement this function\n   for (size_t i = 0; i < M; i++) {\n      y[i] = 0;\n   }\n\n   for (size_t i = 0; i < A.size(); i++) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n\n   for (size_t i = 0; i < M; i++) {\n      y[i] *= beta;\n   }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n   for (size_t i = 0; i < M; i++) {\n      y[i] = 0;\n   }\n   for (size_t i = 0; i < A.size(); i++) {\n      y[A[i].row] += A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] *= alpha;\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] += beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n   std::vector<double> temp(M, 0);\n   for (size_t i = 0; i < A.size(); i++) {\n      temp[A[i].row] += A[i].value * x[A[i].column];\n   }\n   for (size_t i = 0; i < M; i++) {\n      y[i] = alpha * temp[i] + beta * y[i];\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n   // You can use the following functions:\n   // - std::lower_bound\n   // - std::upper_bound\n   // - std::sort\n   // - std::fill\n   // - std::copy\n   // - std::transform\n   // - std::inner_product\n   // - std::accumulate\n   // - std::for_each\n   // - std::count\n   // - std::count_if\n   // - std::find\n   // - std::find_if\n   // - std::find_if_not\n   // - std::all_of\n   // - std::any_of\n   // - std::none_of\n   // - std::min_element\n   // - std::max_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::equal_range\n   // - std::is_sorted\n   // - std::is_sorted_until\n   // - std::is_permutation\n   // - std::mismatch\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::is_permutation\n   // - std::",
            "// TODO: Implement this function\n   // Hint: You can use the following functions:\n   //   - std::vector::at()\n   //   - std::vector::size()\n   //   - std::vector::begin()\n   //   - std::vector::end()\n   //   - std::lower_bound()\n   //   - std::upper_bound()\n   //   - std::distance()\n   //   - std::sort()\n   //   - std::inplace_merge()\n   //   - std::transform()\n   //   - std::inner_product()\n   //   - std::accumulate()\n   //   - std::partial_sum()\n   //   - std::adjacent_difference()\n   //   - std::fill()\n   //   - std::fill_n()\n   //   - std::copy()\n   //   - std::copy_n()\n   //   - std::swap()\n   //   - std::swap_ranges()\n   //   - std::transform()\n   //   - std::transform_reduce()\n   //   - std::transform_exclusive_scan()\n   //   - std::transform_inclusive_scan()\n   //   - std::inner_product()\n   //   - std::accumulate()\n   //   - std::adjacent_difference()\n   //   - std::partial_sum()\n   //   - std::iota()\n   //   - std::generate()\n   //   - std::generate_n()\n   //   - std::remove()\n   //   - std::remove_if()\n   //   - std::remove_copy()\n   //   - std::remove_copy_if()\n   //   - std::unique()\n   //   - std::unique_copy()\n   //   - std::reverse()\n   //   - std::reverse_copy()\n   //   - std::rotate()\n   //   - std::rotate_copy()\n   //   - std::random_shuffle()\n   //   - std::random_sample()\n   //   - std::random_sample_n()\n   //   - std::random_device()\n   //   - std::min_element()\n   //   - std::max_element()\n   //   - std::minmax_element()\n   //   - std::minmax()\n   //   - std::minmax_element()\n   //   - std::lexicographical_compare()\n   //   - std::lexicographical_compare_three_way()\n   //   - std::next_permutation()\n   //   - std::prev_permutation()\n   //   - std::nth_element()\n   //   - std::is_sorted()\n   //   - std::is_sorted_until()\n   //   - std::is_heap()\n   //   - std::is_heap_until()\n   //   - std::is_heap_until()\n   //   - std::is_permutation()\n   //   - std::is_permutation_until()\n   //   - std::is_partitioned()\n   //   - std::is_partitioned_until()\n   //   - std::partition()\n   //   - std::partition_copy()\n   //   - std::stable_partition()\n   //   - std::stable_partition_copy()\n   //   - std::partition_point()\n   //   - std::nth_element()\n   //   - std::sort()\n   //   - std::stable_sort()\n   //   - std::partial_sort()\n   //   - std::partial_sort_copy()\n   //   - std::nth_element()\n   //   - std::sort_heap()\n   //   - std::make_heap()\n   //   - std::push_heap()\n   //   - std::pop_heap()\n   //   - std::is_heap()\n   //   - std::is_heap_until()\n   //   - std::is_heap_until()\n   //   - std::is_permutation()\n   //   - std::is_permutation_until()\n   //   - std::is_partitioned()\n   //   - std::is_partitioned_until()\n   //   - std::partition()"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   x and y are length N and A is M x N.\n   Use AMD HIP to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   atomicAdd(&y[row], alpha * value * x[column]);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   atomicAdd(&y[row], alpha * value * x[column]);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   if (row < M && column < N) {\n      y[row] += alpha * value * x[column];\n   }\n}",
            "// TODO: implement this\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] += alpha * value * x[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   if (row < M && column < N) {\n      y[row] += alpha * value * x[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] += alpha * value * x[column];\n   }\n}",
            "// TODO: implement this\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   atomicAdd(&y[row], alpha * value * x[column]);\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * x[A[i].column];\n      }\n   }\n   y[row] = alpha * sum + beta * y[row];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] += alpha * value * x[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   atomicAdd(&y[row], alpha * value * x[column]);\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n\n   double sum = 0.0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * x[A[i].column];\n      }\n   }\n   y[row] = alpha * sum + beta * y[row];\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   if (row < M && column < N) {\n      y[row] = alpha * value * x[column] + beta * y[row];\n   }\n}",
            "// TODO: implement this\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] += alpha * value * x[column];\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= M) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * x[A[i].column];\n      }\n   }\n   y[row] = alpha * sum + beta * y[row];\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int chunk = M / size;\n   int start = rank * chunk;\n   int end = (rank + 1) * chunk;\n   if (rank == size - 1)\n      end = M;\n   std::vector<double> y_local(N, 0);\n   for (int i = start; i < end; i++) {\n      for (int j = 0; j < N; j++) {\n         y_local[j] += A[i].value * x[j];\n      }\n   }\n   std::vector<double> y_temp(N, 0);\n   MPI_Reduce(&y_local[0], &y_temp[0], N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < N; i++) {\n         y[i] = alpha * y_temp[i] + beta * y[i];\n      }\n   }\n}",
            "// TODO: Your code here\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<COOElement> A_rank;\n   std::vector<double> x_rank;\n   std::vector<double> y_rank;\n\n   for (size_t i = 0; i < A.size(); i++) {\n      if (A[i].row % size == rank) {\n         A_rank.push_back(A[i]);\n      }\n   }\n\n   for (size_t i = 0; i < x.size(); i++) {\n      if (i % size == rank) {\n         x_rank.push_back(x[i]);\n      }\n   }\n\n   for (size_t i = 0; i < y.size(); i++) {\n      if (i % size == rank) {\n         y_rank.push_back(y[i]);\n      }\n   }\n\n   std::vector<double> y_temp(N, 0.0);\n\n   for (size_t i = 0; i < A_rank.size(); i++) {\n      y_temp[A_rank[i].column] += A_rank[i].value * x_rank[A_rank[i].row];\n   }\n\n   for (size_t i = 0; i < y_temp.size(); i++) {\n      y_temp[i] = alpha * y_temp[i] + beta * y_rank[i];\n   }\n\n   if (rank == 0) {\n      for (size_t i = 0; i < y.size(); i++) {\n         y[i] = 0.0;\n      }\n   }\n\n   MPI_Reduce(y_temp.data(), y.data(), y.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n    // Hint: You may find the following functions useful\n    //       MPI_Reduce\n    //       MPI_Allreduce\n    //       MPI_Bcast\n    //       MPI_Scatter\n    //       MPI_Gather\n    //       MPI_Send\n    //       MPI_Recv\n    //       MPI_Sendrecv\n    //       MPI_Wait\n    //       MPI_Waitall\n    //       MPI_Request\n    //       MPI_Status\n    //       MPI_Comm_size\n    //       MPI_Comm_rank\n    //       MPI_Comm_split\n    //       MPI_Comm_free\n    //       MPI_Comm_dup\n    //       MPI_Comm_compare\n    //       MPI_Comm_create_group\n    //       MPI_Comm_group\n    //       MPI_Group_incl\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_range_excl\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //       MPI_Group_union\n    //       MPI_Group_intersection\n    //       MPI_Group_difference\n    //       MPI_Group_free\n    //       MPI_Group_compare\n    //       MPI_Group_translate_ranks\n    //       MPI_Group_range_incl\n    //       MPI_Group_range_excl\n    //",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //\n   //",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n   // Hint: You can use MPI_Reduce to sum the y vector on rank 0.\n   // Hint: You can use MPI_Bcast to broadcast the y vector to all ranks.\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n   // You may assume that MPI has already been initialized\n   // You may assume that A, x, and y have been allocated and have the correct size\n   // You may assume that alpha and beta are scalars\n   // You may assume that A is a sparse matrix stored in COO format\n   // You may assume that x and y are vectors\n   // You may assume that A is M x N\n   // You may assume that x and y are length N\n   // You may assume that A, x, and y are stored on all ranks\n   // You may assume that the result is stored in y on rank 0\n\n   // MPI_Init(NULL, NULL);\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> y_local(N);\n   std::vector<double> x_local(N);\n   std::vector<COOElement> A_local;\n   std::vector<double> y_global(N);\n\n   if (rank == 0) {\n      A_local = A;\n      x_local = x;\n   }\n\n   MPI_Bcast(&A_local[0], A_local.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&x_local[0], x_local.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   for (size_t i = 0; i < N; i++) {\n      y_local[i] = 0;\n   }\n\n   for (size_t i = 0; i < A_local.size(); i++) {\n      y_local[A_local[i].column] += A_local[i].value * x_local[A_local[i].row];\n   }\n\n   MPI_Reduce(&y_local[0], &y_global[0], N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (size_t i = 0; i < N; i++) {\n         y[i] = alpha * y_global[i] + beta * y[i];\n      }\n   }\n\n   // MPI_Finalize();\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   x and y are length N and A is M x N.\n   Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] = y[row] + alpha * value * x[column];\n   }\n}",
            "// TODO: implement this\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   atomicAdd(&y[row], alpha * value * x[column]);\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   if (row < M && column < N) {\n      y[row] = alpha * value * x[column] + beta * y[row];\n   }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   y[row] = alpha * value * x[column] + beta * y[row];\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      y[row] += alpha * value * x[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   if (row < M && column < N) {\n      y[row] = alpha * value * x[column] + beta * y[row];\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "44_sparse_la_spmv",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   x and y are length N and A is M x N.\n   Use MPI and OpenMP to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n   // You may use OpenMP and MPI\n   // You may assume that A, x, and y are all length M\n   // You may assume that A is a square matrix\n   // You may assume that A is stored in COO format\n   // You may assume that A is stored in row-major order\n   // You may assume that A is stored in compressed sparse row (CSR) format\n   // You may assume that A is stored in compressed sparse column (CSC) format\n   // You may assume that x and y are stored in column-major order\n   // You may assume that x and y are stored in row-major order\n   // You may assume that x and y are stored in Fortran order\n   // You may assume that x and y are stored in C order\n   // You may assume that x and y are stored in Fortran order\n   // You may assume that x and y are stored in C order\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in CSC format\n   // You may assume that A is stored in CSR format\n   // You may assume that A is stored in C",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n   // You may use OpenMP to parallelize the inner loop\n   // You may use MPI to parallelize the outer loop\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n    double *y_local = new double[N];\n    double *y_global = new double[N];\n    double *x_local = new double[N];\n    double *x_global = new double[N];\n\n    for (int i = 0; i < N; i++) {\n        y_local[i] = 0;\n        y_global[i] = 0;\n        x_local[i] = x[i];\n        x_global[i] = x[i];\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunk = M / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n\n    if (rank == 0) {\n        for (int i = 0; i < M; i++) {\n            for (int j = 0; j < N; j++) {\n                if (i >= start && i < end) {\n                    y_local[j] += A[i].value * x_global[A[i].column];\n                }\n            }\n        }\n    } else {\n        for (int i = 0; i < M; i++) {\n            if (i >= start && i < end) {\n                y_local[A[i].column] += A[i].value * x_global[A[i].column];\n            }\n        }\n    }\n\n    MPI_Reduce(y_local, y_global, N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < N; i++) {\n            y[i] = alpha * y_global[i] + beta * y[i];\n        }\n    }\n\n    delete[] y_local;\n    delete[] y_global;\n    delete[] x_local;\n    delete[] x_global;\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   // Hint: Use OpenMP to parallelize the inner loop\n   // Hint: Use MPI to parallelize the outer loop\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(Kokkos::View<const COOElement*> &A, Kokkos::View<COOElement*> &L, Kokkos::View<COOElement*> &U, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<COOElement> &L, std::vector<COOElement> &U, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   // Hint: You can use the following OpenMP directives:\n   // #pragma omp parallel for\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1)\n   // #pragma omp parallel for schedule(dynamic, 1)\n   // #pragma omp parallel for schedule(guided, 1)\n   // #pragma omp parallel for schedule(runtime)\n   // #pragma omp parallel for schedule(auto)\n   // #pragma omp parallel for schedule(static, 1",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   L.clear();\n   U.clear();\n   std::vector<double> L_values(N, 1);\n   std::vector<double> U_values(N, 0);\n   std::vector<double> U_diagonal(N, 0);\n   std::vector<double> L_diagonal(N, 0);\n   std::vector<double> L_values_temp(N, 0);\n   std::vector<double> U_values_temp(N, 0);\n   std::vector<double> L_diagonal_temp(N, 0);\n   std::vector<double> U_diagonal_temp(N, 0);\n   std::vector<double> L_values_temp2(N, 0);\n   std::vector<double> U_values_temp2(N, 0);\n   std::vector<double> L_diagonal_temp2(N, 0);\n   std::vector<double> U_diagonal_temp2(N, 0);\n   std::vector<double> L_values_temp3(N, 0);\n   std::vector<double> U_values_temp3(N, 0);\n   std::vector<double> L_diagonal_temp3(N, 0);\n   std::vector<double> U_diagonal_temp3(N, 0);\n   std::vector<double> L_values_temp4(N, 0);\n   std::vector<double> U_values_temp4(N, 0);\n   std::vector<double> L_diagonal_temp4(N, 0);\n   std::vector<double> U_diagonal_temp4(N, 0);\n   std::vector<double> L_values_temp5(N, 0);\n   std::vector<double> U_values_temp5(N, 0);\n   std::vector<double> L_diagonal_temp5(N, 0);\n   std::vector<double> U_diagonal_temp5(N, 0);\n   std::vector<double> L_values_temp6(N, 0);\n   std::vector<double> U_values_temp6(N, 0);\n   std::vector<double> L_diagonal_temp6(N, 0);\n   std::vector<double> U_diagonal_temp6(N, 0);\n   std::vector<double> L_values_temp7(N, 0);\n   std::vector<double> U_values_temp7(N, 0);\n   std::vector<double> L_diagonal_temp7(N, 0);\n   std::vector<double> U_diagonal_temp7(N, 0);\n   std::vector<double> L_values_temp8(N, 0);\n   std::vector<double> U_values_temp8(N, 0);\n   std::vector<double> L_diagonal_temp8(N, 0);\n   std::vector<double> U_diagonal_temp8(N, 0);\n   std::vector<double> L_values_temp9(N, 0);\n   std::vector<double> U_values_temp9(N, 0);\n   std::vector<double> L_diagonal_temp9(N, 0);\n   std::vector<double> U_diagonal_temp9(N, 0);\n   std::vector<double> L_values_temp10(N, 0);\n   std::vector<double> U_values_temp10(N, 0);\n   std::vector<double> L_diagonal_temp10(N, 0);\n   std::vector<double> U_diagonal_temp10(N, 0);\n   std::vector<double> L_values_temp11(N, 0);\n   std::vector<double> U_values_temp11(N, 0);\n   std::vector<double> L_diagonal_temp11(N, 0);\n   std::vector<double> U_diagonal_temp11(N, 0);\n   std",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   // Hint: You can use the following functions:\n   // - omp_get_thread_num() to get the thread id\n   // - omp_get_num_threads() to get the number of threads\n   // - omp_get_num_procs() to get the number of processors\n   // - omp_get_max_threads() to get the maximum number of threads\n   // - omp_get_thread_limit() to get the maximum number of threads that can be created\n   // - omp_get_dynamic() to get the dynamic threads setting\n   // - omp_set_dynamic(0) to disable dynamic threads\n   // - omp_set_num_threads(n) to set the number of threads\n   // - omp_in_parallel() to check if we are in parallel\n   // - omp_get_wtime() to get the current time\n   // - omp_get_wtick() to get the time tick\n   // - omp_set_nested(1) to enable nested parallelism\n   // - omp_get_nested() to get the nested parallelism setting\n   // - omp_set_schedule(omp_sched_type, chunk_size) to set the schedule\n   // - omp_get_schedule() to get the schedule\n   // - omp_get_cancellation() to get the cancellation setting\n   // - omp_set_cancel(0) to disable cancellation\n   // - omp_set_cancel(1) to enable cancellation\n   // - omp_test_cancel() to test if cancellation is enabled\n   // - omp_set_lock(omp_lock_t *) to set a lock\n   // - omp_unset_lock(omp_lock_t *) to unset a lock\n   // - omp_set_nest_lock(omp_nest_lock_t *) to set a nested lock\n   // - omp_unset_nest_lock(omp_nest_lock_t *) to unset a nested lock\n   // - omp_test_lock(omp_lock_t *) to test if a lock is set\n   // - omp_test_nest_lock(omp_nest_lock_t *) to test if a nested lock is set\n   // - omp_init_lock(omp_lock_t *) to initialize a lock\n   // - omp_init_nest_lock(omp_nest_lock_t *) to initialize a nested lock\n   // - omp_destroy_lock(omp_lock_t *) to destroy a lock\n   // - omp_destroy_nest_lock(omp_nest_lock_t *) to destroy a nested lock\n   // - omp_set_dynamic(0) to disable dynamic threads\n   // - omp_set_num_threads(n) to set the number of threads\n   // - omp_in_parallel() to check if we are in parallel\n   // - omp_get_wtime() to get the current time\n   // - omp_get_wtick() to get the time tick\n   // - omp_set_nested(1) to enable nested parallelism\n   // - omp_get_nested() to get the nested parallelism setting\n   // - omp_set_schedule(omp_sched_type, chunk_size) to set the schedule\n   // - omp_get_schedule() to get the schedule\n   // - omp_get_cancellation() to get the cancellation setting\n   // - omp_set_cancel(0) to disable cancellation\n   // - omp_set_cancel(1) to enable cancellation\n   // - omp_test_cancel() to test if cancellation is enabled\n   // - omp_set_lock(omp_lock_t *) to set a lock\n   // - omp_unset_lock(omp_lock_t *) to unset a lock\n   // - omp_set_nest_lock(omp_nest_lock_t *) to set a nested lock\n   // - omp_unset_nest_lock(omp_nest_lock_t *) to unset a nested lock\n   // - omp_test_lock(omp_lock_t *) to test if a lock is set\n   // - omp_test_nest_lock(omp_nest_lock_t *) to test if a nested lock is set\n   // - omp_init",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<COOElement> &L, std::vector<COOElement> &U, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n   // Hint: you can use the following functions:\n   // - std::find_if\n   // - std::find\n   // - std::sort\n   // - std::lower_bound\n   // - std::upper_bound\n   // - std::distance\n   // - std::copy\n   // - std::copy_if\n   // - std::transform\n   // - std::accumulate\n   // - std::inner_product\n   // - std::transform_reduce\n   // - std::for_each\n   // - std::for_each_n\n   // - std::count_if\n   // - std::count\n   // - std::all_of\n   // - std::any_of\n   // - std::none_of\n   // - std::find_end\n   // - std::find_first_of\n   // - std::adjacent_find\n   // - std::search\n   // - std::search_n\n   // - std::copy_n\n   // - std::copy_if_n\n   // - std::transform_exclusive_scan\n   // - std::transform_inclusive_scan\n   // - std::transform_reduce_exclusive_scan\n   // - std::transform_reduce_inclusive_scan\n   // - std::inner_product\n   // - std::partial_sum\n   // - std::adjacent_difference\n   // - std::iota\n   // - std::merge\n   // - std::inplace_merge\n   // - std::includes\n   // - std::set_union\n   // - std::set_intersection\n   // - std::set_difference\n   // - std::set_symmetric_difference\n   // - std::is_sorted\n   // - std::is_sorted_until\n   // - std::is_permutation\n   // - std::is_permutation_until\n   // - std::is_partitioned\n   // - std::is_partitioned_until\n   // - std::partition\n   // - std::partition_copy\n   // - std::stable_partition\n   // - std::nth_element\n   // - std::sort\n   // - std::stable_sort\n   // - std::partial_sort\n   // - std::partial_sort_copy\n   // - std::nth_element\n   // - std::partial_sort_copy\n   // - std::partial_sort_copy_if\n   // - std::is_heap\n   // - std::is_heap_until\n   // - std::is_heap_until\n   // - std::make_heap\n   // - std::push_heap\n   // - std::pop_heap\n   // - std::sort_heap\n   // - std::max_element\n   // - std::min_element\n   // - std::minmax_element\n   // - std::max_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::minmax_element\n   // - std::min",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   // Hint: Use the Gaussian elimination algorithm\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement the factorization\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, COOElement *L, size_t sizeL, COOElement *U, size_t sizeU, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n\n   // find the diagonal element\n   COOElement diag = {row, row, 0};\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == diag.row && A[i].column == diag.column) {\n         diag = A[i];\n         break;\n      }\n   }\n\n   // find the row of L\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column < row) {\n         L[i] = A[i];\n      }\n   }\n\n   // find the row of U\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column > row) {\n         U[i] = A[i];\n      }\n   }\n\n   // divide the row of U by the diagonal element\n   for (size_t i = 0; i < sizeU; i++) {\n      if (U[i].row == row) {\n         U[i].value /= diag.value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // Get the row and column of the element\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // Get the value of the element\n   double value = A[i].value;\n\n   // If the element is on the diagonal, set the value of L and U\n   if (row == column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = 1;\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n   // If the element is not on the diagonal, set the value of L and U\n   else {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value / U[column * N + column].value;\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row > a.column) {\n      l.value = a.value / U[a.column + a.column * N].value;\n      u.value = a.value;\n   } else {\n      l.value = a.value;\n      u.value = a.value / L[a.row + a.row * N].value;\n   }\n\n   L[i] = l;\n   U[i] = u;\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   COOElement a = A[i];\n   if (a.row == a.column) {\n      L[i] = a;\n      U[i] = a;\n   } else if (a.row < a.column) {\n      L[i] = a;\n   } else {\n      U[i] = a;\n   }\n   __syncthreads();\n   for (size_t j = 0; j < sizeL; j++) {\n      if (L[j].row == a.row && L[j].column == a.column) {\n         L[i].value = a.value / L[j].value;\n         break;\n      }\n   }\n   __syncthreads();\n   for (size_t j = 0; j < sizeU; j++) {\n      if (U[j].row == a.row && U[j].column == a.column) {\n         U[i].value = a.value / U[j].value;\n         break;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   if (A[i].row == A[i].column) {\n      L[i].row = A[i].row;\n      L[i].column = A[i].column;\n      L[i].value = 1;\n      U[i].row = A[i].row;\n      U[i].column = A[i].column;\n      U[i].value = A[i].value;\n   } else {\n      if (A[i].row < A[i].column) {\n         L[i].row = A[i].row;\n         L[i].column = A[i].column;\n         L[i].value = A[i].value;\n         U[i].row = A[i].row;\n         U[i].column = A[i].column;\n         U[i].value = 0;\n      } else {\n         L[i].row = A[i].row;\n         L[i].column = A[i].column;\n         L[i].value = 0;\n         U[i].row = A[i].row;\n         U[i].column = A[i].column;\n         U[i].value = A[i].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // Fill L and U\n   if (A[i].row == A[i].column) {\n      L[i] = A[i];\n      U[i] = A[i];\n   } else if (A[i].row < A[i].column) {\n      L[i] = A[i];\n   } else {\n      U[i] = A[i];\n   }\n\n   // Compute U\n   for (size_t j = 0; j < sizeA; j++) {\n      if (A[j].row == A[i].column) {\n         U[i].value -= A[j].value * L[i].value;\n      }\n   }\n\n   // Compute L\n   for (size_t j = 0; j < sizeA; j++) {\n      if (A[j].column == A[i].row) {\n         L[i].value /= U[j].value;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // Get the row and column of the element\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // Get the value of the element\n   double value = A[i].value;\n\n   // If the element is on the diagonal, then it is a diagonal element of L and U\n   if (row == column) {\n      // Store the diagonal element of L\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = 1.0;\n\n      // Store the diagonal element of U\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   } else {\n      // If the element is not on the diagonal, then it is a non-diagonal element of L and U\n\n      // Store the non-diagonal element of L\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value / U[column * N + column].value;\n\n      // Store the non-diagonal element of U\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid >= sizeA) return;\n\n   // Get the row and column of the element\n   size_t row = A[tid].row;\n   size_t column = A[tid].column;\n\n   // Get the value of the element\n   double value = A[tid].value;\n\n   // If the element is on the diagonal, set the value of L and U to 1\n   if (row == column) {\n      L[tid].row = row;\n      L[tid].column = column;\n      L[tid].value = 1;\n      U[tid].row = row;\n      U[tid].column = column;\n      U[tid].value = value;\n   }\n\n   // If the element is not on the diagonal, set the value of L and U\n   else {\n      L[tid].row = row;\n      L[tid].column = column;\n      L[tid].value = value / U[row * N + row].value;\n      U[tid].row = column;\n      U[tid].column = row;\n      U[tid].value = -value / L[row * N + row].value;\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      if (A[i].row == A[i].column) {\n         L[i].row = A[i].row;\n         L[i].column = A[i].column;\n         L[i].value = 1;\n         U[i].row = A[i].row;\n         U[i].column = A[i].column;\n         U[i].value = A[i].value;\n      } else if (A[i].row > A[i].column) {\n         L[i].row = A[i].row;\n         L[i].column = A[i].column;\n         L[i].value = A[i].value;\n         U[i].row = A[i].row;\n         U[i].column = A[i].column;\n         U[i].value = 0;\n      } else {\n         L[i].row = A[i].row;\n         L[i].column = A[i].column;\n         L[i].value = 0;\n         U[i].row = A[i].row;\n         U[i].column = A[i].column;\n         U[i].value = A[i].value;\n      }\n   }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // get row and column of element i\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // get value of element i\n   double value = A[i].value;\n\n   // if element is diagonal, set L and U to value\n   if (row == column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value;\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n\n   // if element is not diagonal, set L and U to value\n   if (row!= column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value;\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= sizeA) return;\n   COOElement a = A[tid];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row < a.column) {\n      l.value = a.value;\n   } else {\n      u.value = a.value;\n   }\n   L[tid] = l;\n   U[tid] = u;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   if (a.row == a.column) {\n      L[i].row = a.row;\n      L[i].column = a.column;\n      L[i].value = 1.0;\n      U[i].row = a.row;\n      U[i].column = a.column;\n      U[i].value = a.value;\n   } else if (a.row < a.column) {\n      L[i].row = a.row;\n      L[i].column = a.column;\n      L[i].value = a.value;\n   } else {\n      U[i].row = a.row;\n      U[i].column = a.column;\n      U[i].value = a.value;\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n\n   // Find the diagonal element of L\n   double diag = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column == row) {\n         diag = A[i].value;\n         break;\n      }\n   }\n\n   // Fill the diagonal element of L\n   L[row * N + row].row = row;\n   L[row * N + row].column = row;\n   L[row * N + row].value = 1;\n\n   // Fill the upper triangular matrix U\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column > row) {\n         U[row * N + A[i].column].row = row;\n         U[row * N + A[i].column].column = A[i].column;\n         U[row * N + A[i].column].value = A[i].value;\n      }\n   }\n\n   // Fill the lower triangular matrix L\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row > row && A[i].column == row) {\n         L[A[i].row * N + row].row = A[i].row;\n         L[A[i].row * N + row].column = row;\n         L[A[i].row * N + row].value = A[i].value / diag;\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   COOElement a = A[i];\n   if (a.row == a.column) {\n      L[i].row = a.row;\n      L[i].column = a.column;\n      L[i].value = 1;\n      U[i].row = a.row;\n      U[i].column = a.column;\n      U[i].value = a.value;\n   } else {\n      if (a.row < a.column) {\n         L[i].row = a.row;\n         L[i].column = a.column;\n         L[i].value = a.value;\n         U[i].row = a.row;\n         U[i].column = a.column;\n         U[i].value = 0;\n      } else {\n         L[i].row = a.row;\n         L[i].column = a.column;\n         L[i].value = 0;\n         U[i].row = a.row;\n         U[i].column = a.column;\n         U[i].value = a.value;\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n\n   // Find the diagonal element\n   size_t diagonal = -1;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column == row) {\n         diagonal = i;\n         break;\n      }\n   }\n   if (diagonal == -1) return;\n\n   // Find the elements below the diagonal\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row && A[i].column < row) {\n         sum += A[i].value * L[i].value;\n      }\n   }\n\n   // Store the result\n   L[diagonal].value = 1;\n   U[diagonal].value = A[diagonal].value - sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // Get the row and column of the element\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // Get the value of the element\n   double value = A[i].value;\n\n   // Compute the value of the element in L\n   if (row == column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = 1;\n   } else if (row < column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value / U[column * N + column].value;\n   }\n\n   // Compute the value of the element in U\n   if (row == column) {\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   } else if (row > column) {\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // Get the row and column of the element\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // Get the value of the element\n   double value = A[i].value;\n\n   // Initialize the L and U matrices\n   if (row == column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = 1;\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   } else if (row < column) {\n      L[i].row = row;\n      L[i].column = column;\n      L[i].value = value;\n   } else if (row > column) {\n      U[i].row = row;\n      U[i].column = column;\n      U[i].value = value;\n   }\n\n   // Wait for all threads to finish\n   __syncthreads();\n\n   // Perform the LU factorization\n   for (size_t j = 0; j < sizeL; j++) {\n      if (L[j].row == row && L[j].column == column) {\n         value = value / L[j].value;\n         U[i].value = value;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<COOElement> &L, std::vector<COOElement> &U, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, COOElement *L, size_t sizeL, COOElement *U, size_t sizeU, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row > a.column) {\n      l.value = a.value / U[a.column * N + a.column].value;\n      u.value = a.value;\n   } else {\n      l.value = a.value;\n      u.value = a.value / L[a.row * N + a.row].value;\n   }\n\n   L[i] = l;\n   U[i] = u;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // TODO: Fill in the kernel\n   // Hint: Use the following variables\n   // - A[i]\n   // - L[i]\n   // - U[i]\n   // - N\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // TODO: Implement the factorization\n   // Hint: You can use the atomicAdd function to update the values of L and U\n   // Hint: You can use the atomicMin function to update the values of L\n   // Hint: You can use the atomicMax function to update the values of U\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row > a.column) {\n      l.value = a.value / U[a.column * N + a.column].value;\n      u.value = a.value;\n   } else {\n      l.value = a.value;\n      u.value = a.value / L[a.row * N + a.row].value;\n   }\n\n   L[i] = l;\n   U[i] = u;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // TODO: Implement the kernel\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row < a.column) {\n      l.value = a.value;\n   } else {\n      u.value = a.value;\n   }\n\n   L[i] = l;\n   U[i] = u;\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   COOElement a = A[i];\n   COOElement l = {a.row, a.column, 0};\n   COOElement u = {a.row, a.column, 0};\n\n   if (a.row == a.column) {\n      l.value = 1;\n      u.value = a.value;\n   } else if (a.row < a.column) {\n      l.value = a.value;\n   } else {\n      u.value = a.value;\n   }\n\n   L[i] = l;\n   U[i] = u;\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   if (A[i].row == A[i].column) {\n      L[i].row = A[i].row;\n      L[i].column = A[i].column;\n      L[i].value = 1;\n      U[i].row = A[i].row;\n      U[i].column = A[i].column;\n      U[i].value = A[i].value;\n   } else if (A[i].row < A[i].column) {\n      L[i].row = A[i].row;\n      L[i].column = A[i].column;\n      L[i].value = A[i].value;\n      U[i].row = A[i].row;\n      U[i].column = A[i].column;\n      U[i].value = 0;\n   } else {\n      L[i].row = A[i].row;\n      L[i].column = A[i].column;\n      L[i].value = 0;\n      U[i].row = A[i].row;\n      U[i].column = A[i].column;\n      U[i].value = A[i].value;\n   }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_sparse_lu_decomp",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<COOElement> &L, std::vector<COOElement> &U, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n   // Hint: You can use the following functions:\n   // - omp_get_thread_num() to get the thread id\n   // - omp_get_num_threads() to get the number of threads\n   // - MPI_Comm_rank() to get the rank id\n   // - MPI_Comm_size() to get the number of ranks\n   // - MPI_Send() to send data to another rank\n   // - MPI_Recv() to receive data from another rank\n\n   // TODO: Implement this function\n   // Hint: You can use the following functions:\n   // - omp_get_thread_num() to get the thread id\n   // - omp_get_num_threads() to get the number of threads\n   // - MPI_Comm_rank() to get the rank id\n   // - MPI_Comm_size() to get the number of ranks\n   // - MPI_Send() to send data to another rank\n   // - MPI_Recv() to receive data from another rank\n\n}",
            "// TODO: Implement this function\n}",
            "}",
            "// TODO: Implement this function\n\n}",
            "}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n   // You can use the following functions:\n   // - MPI_Comm_rank()\n   // - MPI_Comm_size()\n   // - MPI_Send()\n   // - MPI_Recv()\n   // - MPI_Bcast()\n   // - MPI_Scatter()\n   // - MPI_Gather()\n   // - MPI_Allgather()\n   // - omp_get_thread_num()\n   // - omp_get_num_threads()\n   // - omp_get_max_threads()\n   // - omp_set_num_threads()\n   // - omp_in_parallel()\n   // - omp_get_num_procs()\n   // - omp_get_dynamic()\n   // - omp_set_dynamic()\n   // - omp_get_nested()\n   // - omp_set_nested()\n   // - omp_get_max_active_levels()\n   // - omp_set_max_active_levels()\n   // - omp_get_cancellation()\n   // - omp_set_cancellation()\n   // - omp_get_proc_bind()\n   // - omp_set_proc_bind()\n   // - omp_get_default_device()\n   // - omp_set_default_device()\n   // - omp_get_num_devices()\n   // - omp_get_device_num()\n   // - omp_get_num_teams()\n   // - omp_get_team_num()\n   // - omp_is_initial_device()\n   // - omp_get_initial_device()\n   // - omp_get_max_task_priority()\n   // - omp_get_num_places()\n   // - omp_get_place_num_procs()\n   // - omp_get_place_proc_ids()\n   // - omp_get_place_num()\n   // - omp_get_partition_num_places()\n   // - omp_get_partition_place_nums()\n   // - omp_set_default_device()\n   // - omp_get_default_device()\n   // - omp_get_num_devices()\n   // - omp_get_device_num()\n   // - omp_is_initial_device()\n   // - omp_get_initial_device()\n   // - omp_get_max_task_priority()\n   // - omp_in_parallel()\n   // - omp_get_num_threads()\n   // - omp_get_max_threads()\n   // - omp_get_thread_num()\n   // - omp_get_num_procs()\n   // - omp_get_dynamic()\n   // - omp_set_dynamic()\n   // - omp_get_nested()\n   // - omp_set_nested()\n   // - omp_get_max_active_levels()\n   // - omp_set_max_active_levels()\n   // - omp_get_cancellation()\n   // - omp_set_cancellation()\n   // - omp_get_proc_bind()\n   // - omp_set_proc_bind()\n   // - omp_get_default_device()\n   // - omp_set_default_device()\n   // - omp_get_num_devices()\n   // - omp_get_device_num()\n   // - omp_get_num_teams()\n   // - omp_get_team_num()\n   // - omp_is_initial_device()\n   // - omp_get_initial_device()\n   // - omp_get_max_task_priority()\n   // - omp_get_num_places()\n   // - omp_get_place_num_procs()\n   // - omp_get_place_proc_ids()\n   // - omp_get_place_num()\n   // - omp_get_partition_num_places()\n   // - omp_get_partition_place_nums()\n   // - omp_set_default_",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<Element*> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  Kokkos::parallel_for(\n    \"sparseAxpy\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      z[i].index = x[i].index;\n      z[i].value = alpha * x[i].value + y[i].value;\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<Element> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "// TODO: implement this function\n\t\n\t// You may use the following variables:\n\t// size_t n = x.size();\n\t// size_t m = y.size();\n\t// size_t i, j;\n\t// double x_value, y_value;\n\t\n\t// You may use the following OpenMP directives:\n\t// #pragma omp parallel for\n\t// #pragma omp parallel for private(i)\n\t// #pragma omp parallel for private(i, j)\n\t// #pragma omp parallel for private(i, j) shared(x, y, z)\n\t// #pragma omp parallel for private(i, j) shared(x, y, z) reduction(+:z)\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the following OpenMP clauses:\n\t// private(i)\n\t// private(i, j)\n\t// shared(x, y, z)\n\t// reduction(+:z)\n\t\n\t// You may use the following OpenMP environment variables:\n\t// OMP_NUM_THREADS\n\t\n\t// You may use the following OpenMP functions:\n\t// omp_get_thread_num()\n\t// omp_get_num_threads()\n\t\n\t// You may use the",
            "// TODO: implement this function\n\t\n}",
            "// TODO: implement this function\n\t// Hint: use the omp_get_thread_num() function to get the thread id\n\t// Hint: use the omp_get_num_threads() function to get the number of threads\n\t// Hint: use the omp_get_num_procs() function to get the number of processors\n\t// Hint: use the omp_get_max_threads() function to get the maximum number of threads\n\t// Hint: use the omp_get_dynamic() function to get the dynamic thread adjustment\n\t// Hint: use the omp_get_nested() function to get the nested parallelism\n\t// Hint: use the omp_in_parallel() function to check if the code is running in parallel\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_dynamic(0) function to disable dynamic thread adjustment\n\t// Hint: use the omp_set_nested(1) function to enable nested parallelism\n\t// Hint: use the omp_set_dynamic(1) function to enable dynamic thread adjustment\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the number of threads\n\t// Hint: use the omp_set_num_threads(n) function to set the",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tfor (size_t j = 0; j < y.size(); j++) {\n\t\t\tif (x[i].index == y[j].index) {\n\t\t\t\tz.push_back({x[i].index, alpha * x[i].value + y[j].value});\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: implement me\n  // Hint: you can use the following functions:\n  // - std::lower_bound(begin, end, value)\n  // - std::upper_bound(begin, end, value)\n  // - std::distance(begin, end)\n  // - std::min(a, b)\n  // - std::max(a, b)\n  // - std::sort(begin, end)\n  // - std::inplace_merge(begin, middle, end)\n  // - std::copy(begin, end, destination)\n  // - std::transform(begin, end, destination, function)\n  // - std::for_each(begin, end, function)\n  // - std::fill(begin, end, value)\n  // - std::copy_if(begin, end, destination, predicate)\n  // - std::count_if(begin, end, predicate)\n  // - std::find_if(begin, end, predicate)\n  // - std::find(begin, end, value)\n  // - std::remove_if(begin, end, predicate)\n  // - std::remove(begin, end, value)\n  // - std::unique(begin, end)\n  // - std::reverse(begin, end)\n  // - std::rotate(begin, middle, end)\n  // - std::swap(a, b)\n  // - std::swap_ranges(begin_a, end_a, begin_b)\n  // - std::is_sorted(begin, end)\n  // - std::is_sorted_until(begin, end)\n  // - std::is_permutation(begin_a, end_a, begin_b)\n  // - std::is_permutation_until(begin_a, end_a, begin_b)\n  // - std::equal(begin_a, end_a, begin_b)\n  // - std::equal_range(begin, end, value)\n  // - std::lexicographical_compare(begin_a, end_a, begin_b, end_b)\n  // - std::lexicographical_compare_three_way(begin_a, end_a, begin_b, end_b)\n  // - std::mismatch(begin_a, end_a, begin_b)\n  // - std::mismatch(begin_a, end_a, begin_b, end_b)\n  // - std::mismatch(begin_a, end_a, begin_b, end_b, predicate)\n  // - std::search(begin_a, end_a, begin_b, end_b)\n  // - std::search_n(begin_a, end_a, count, value)\n  // - std::search_n(begin_a, end_a, count, value, predicate)\n  // - std::find_end(begin_a, end_a, begin_b, end_b)\n  // - std::find_end(begin_a, end_a, begin_b, end_b, predicate)\n  // - std::adjacent_find(begin, end)\n  // - std::adjacent_find(begin, end, predicate)\n  // - std::count(begin, end, value)\n  // - std::count_if(begin, end, predicate)\n  // - std::all_of(begin, end, predicate)\n  // - std::any_of(begin, end, predicate)\n  // - std::none_of(begin, end, predicate)\n  // - std::for_each(begin, end, function)\n  // - std::for_each_n(begin, count, function)\n  // - std::transform(begin, end, destination, function)\n  // - std::transform(begin, end, destination, function, binary_operation)\n  // - std::accumulate(begin, end, value)\n  // - std::accumulate(begin, end, value, binary_operation)\n  // - std::inner_product(begin_a, end_a, begin_b, value)\n  // - std::inner_product(begin_a, end_a, begin_b, value, binary_operation_a, binary_operation_b)\n  // - std::reduce(begin, end, value)\n  // - std::reduce(begin, end, value,",
            "// TODO: Your code here\n\t\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    for (size_t j = 0; j < y.size(); ++j) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    z[i].index = x[i].index;\n    z[i].value = alpha * x[i].value + y[i].value;\n  }\n}",
            "// TODO\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n  // Hint: You can use the following functions:\n  // - std::lower_bound\n  // - std::upper_bound\n  // - std::set_intersection\n  // - std::set_union\n  // - std::set_difference\n  // - std::inplace_merge\n  // - std::merge\n  // - std::sort\n  // - std::stable_sort\n  // - std::is_sorted\n  // - std::is_sorted_until\n  // - std::is_permutation\n  // - std::is_permutation_until\n  // - std::is_heap\n  // - std::is_heap_until\n  // - std::is_heap_until\n  // - std::make_heap\n  // - std::push_heap\n  // - std::pop_heap\n  // - std::sort_heap\n  // - std::nth_element\n  // - std::partial_sort\n  // - std::partial_sort_copy\n  // - std::partial_sort_copy_if\n  // - std::nth_element\n  // - std::merge\n  // - std::inplace_merge\n  // - std::includes\n  // - std::set_union\n  // - std::set_intersection\n  // - std::set_difference\n  // - std::set_symmetric_difference\n  // - std::lexicographical_compare\n  // - std::lexicographical_compare_three_way\n  // - std::mismatch\n  // - std::is_permutation\n  // - std::is_permutation_until\n  // - std::is_permutation_until\n  // - std::is_heap\n  // - std::is_heap_until\n  // - std::is_heap_until\n  // - std::make_heap\n  // - std::push_heap\n  // - std::pop_heap\n  // - std::sort_heap\n  // - std::nth_element\n  // - std::partial_sort\n  // - std::partial_sort_copy\n  // - std::partial_sort_copy_if\n  // - std::nth_element\n  // - std::merge\n  // - std::inplace_merge\n  // - std::includes\n  // - std::set_union\n  // - std::set_intersection\n  // - std::set_difference\n  // - std::set_symmetric_difference\n  // - std::lexicographical_compare\n  // - std::lexicographical_compare_three_way\n  // - std::mismatch\n  // - std::is_permutation\n  // - std::is_permutation_until\n  // - std::is_permutation_until\n  // - std::is_heap\n  // - std::is_heap_until\n  // - std::is_heap_until\n  // - std::make_heap\n  // - std::push_heap\n  // - std::pop_heap\n  // - std::sort_heap\n  // - std::nth_element\n  // - std::partial_sort\n  // - std::partial_sort_copy\n  // - std::partial_sort_copy_if\n  // - std::nth_element\n  // - std::merge\n  // - std::inplace_merge\n  // - std::includes\n  // - std::set_union\n  // - std::set_intersection\n  // - std::set_difference\n  // - std::set_symmetric_difference\n  // - std::lexicographical_compare\n  // - std::lexicographical_compare_three_way\n  // - std::mismatch\n  // - std::is_permutation\n  // - std::is_permutation_until\n  // - std::is_permutation_until\n  // - std::is_heap\n  // - std::is_heap_until\n  // - std::is_heap_until\n  // - std::make_heap\n  // - std::push_heap\n  // - std::pop_heap\n  // - std::sort_heap\n  // - std::nth_element\n  // - std",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z[i].index = x[i].index;\n        z[i].value = alpha * x[i].value + y[j].value;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha*x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha*x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    for (int j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z[i].value = alpha * x[i].value + y[j].value;\n      }\n    }\n  }\n}",
            "// TODO: implement this function\n\t// Hint: you can use the following code to get the size of the vectors\n\t// size_t x_size = x.size();\n\t// size_t y_size = y.size();\n\t// size_t z_size = z.size();\n\t// Hint: you can use the following code to get the i-th element of the vectors\n\t// Element x_i = x[i];\n\t// Element y_i = y[i];\n\t// Element z_i = z[i];\n\t// Hint: you can use the following code to set the i-th element of the vectors\n\t// z[i] =...;\n\n\t// Hint: you can use the following code to get the number of threads\n\t// int nthreads = omp_get_num_threads();\n\t// Hint: you can use the following code to get the thread id\n\t// int tid = omp_get_thread_num();\n\n\t// Hint: you can use the following code to get the number of elements in the vector\n\t// size_t x_size = x.size();\n\t// size_t y_size = y.size();\n\t// size_t z_size = z.size();\n\n\t// Hint: you can use the following code to get the i-th element of the vectors\n\t// Element x_i = x[i];\n\t// Element y_i = y[i];\n\t// Element z_i = z[i];\n\n\t// Hint: you can use the following code to set the i-th element of the vectors\n\t// z[i] =...;\n\n\t// Hint: you can use the following code to get the number of threads\n\t// int nthreads = omp_get_num_threads();\n\n\t// Hint: you can use the following code to get the thread id\n\t// int tid = omp_get_thread_num();\n\n\t// Hint: you can use the following code to get the number of elements in the vector\n\t// size_t x_size = x.size();\n\t// size_t y_size = y.size();\n\t// size_t z_size = z.size();\n\n\t// Hint: you can use the following code to get the i-th element of the vectors\n\t// Element x_i = x[i];\n\t// Element y_i = y[i];\n\t// Element z_i = z[i];\n\n\t// Hint: you can use the following code to set the i-th element of the vectors\n\t// z[i] =...;\n\n\t// Hint: you can use the following code to get the number of threads\n\t// int nthreads = omp_get_num_threads();\n\n\t// Hint: you can use the following code to get the thread id\n\t// int tid = omp_get_thread_num();\n\n\t// Hint: you can use the following code to get the number of elements in the vector\n\t// size_t x_size = x.size();\n\t// size_t y_size = y.size();\n\t// size_t z_size = z.size();\n\n\t// Hint: you can use the following code to get the i-th element of the vectors\n\t// Element x_i = x[i];\n\t// Element y_i = y[i];\n\t// Element z_i = z[i];\n\n\t// Hint: you can use the following code to set the i-th element of the vectors\n\t// z[i] =...;\n\n\t// Hint: you can use the following code to get the number of threads\n\t// int nthreads = omp_get_num_threads();\n\n\t// Hint: you can use the following code to get the thread id\n\t// int tid = omp_get_thread_num();\n\n\t// Hint: you can use the following code to get the number of elements in the vector\n\t// size_t x_size = x.size();\n\t// size_t y_size = y.size();\n\t// size_t z_size = z.size();\n\n\t// Hint: you can use the following code to get the i-th element of the vectors\n\t// Element x_i = x[i];\n\t// Element y_i = y[i];\n\t// Element z_i = z[i];",
            "// TODO: Implement this function\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tfor (int j = 0; j < y.size(); j++) {\n\t\t\tif (x[i].index == y[j].index) {\n\t\t\t\tz[i].value = alpha * x[i].value + y[j].value;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Your code here\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    for (size_t j = 0; j < y.size(); j++) {\n      if (x[i].index == y[j].index) {\n        z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n      }\n    }\n  }\n}",
            "// TODO: implement this function\n\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "serial",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<Element> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  size_t i = 0;\n  size_t j = 0;\n  while (i < x_copy.size() && j < y_copy.size()) {\n    if (x_copy[i].index == y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value + y_copy[j].value});\n      i++;\n      j++;\n    } else if (x_copy[i].index < y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n      i++;\n    } else {\n      z.push_back({y_copy[j].index, y_copy[j].value});\n      j++;\n    }\n  }\n  while (i < x_copy.size()) {\n    z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n    i++;\n  }\n  while (j < y_copy.size()) {\n    z.push_back({y_copy[j].index, y_copy[j].value});\n    j++;\n  }\n}",
            "// TODO\n}",
            "// TODO\n  z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  size_t i = 0;\n  size_t j = 0;\n  while (i < x_copy.size() && j < y_copy.size()) {\n    if (x_copy[i].index == y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value + y_copy[j].value});\n      i++;\n      j++;\n    } else if (x_copy[i].index < y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n      i++;\n    } else {\n      z.push_back({y_copy[j].index, y_copy[j].value});\n      j++;\n    }\n  }\n  while (i < x_copy.size()) {\n    z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n    i++;\n  }\n  while (j < y_copy.size()) {\n    z.push_back({y_copy[j].index, y_copy[j].value});\n    j++;\n  }\n}",
            "// TODO: implement this function\n  // Hint: you can use the function std::lower_bound to find the insertion position of an element in a sorted vector\n  // Hint: you can use the function std::make_pair to create a pair\n  // Hint: you can use the function std::sort to sort a vector\n  // Hint: you can use the function std::unique to remove duplicates from a sorted vector\n  // Hint: you can use the function std::vector::insert to insert an element in a vector\n  // Hint: you can use the function std::vector::erase to remove an element from a vector\n  // Hint: you can use the function std::vector::begin to get an iterator to the beginning of a vector\n  // Hint: you can use the function std::vector::end to get an iterator to the end of a vector\n  // Hint: you can use the function std::vector::push_back to add an element to the end of a vector\n  // Hint: you can use the function std::vector::resize to resize a vector\n  // Hint: you can use the function std::vector::clear to clear a vector\n  // Hint: you can use the function std::vector::empty to check if a vector is empty\n  // Hint: you can use the function std::vector::size to get the size of a vector\n  // Hint: you can use the function std::vector::operator[] to access an element of a vector\n  // Hint: you can use the function std::vector::at to access an element of a vector\n  // Hint: you can use the function std::vector::assign to assign a vector\n  // Hint: you can use the function std::vector::swap to swap two vectors\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::shrink_to_fit to shrink the capacity of a vector\n  // Hint: you can use the function std::vector::resize to resize a vector\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::shrink_to_fit to shrink the capacity of a vector\n  // Hint: you can use the function std::vector::assign to assign a vector\n  // Hint: you can use the function std::vector::swap to swap two vectors\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::shrink_to_fit to shrink the capacity of a vector\n  // Hint: you can use the function std::vector::assign to assign a vector\n  // Hint: you can use the function std::vector::swap to swap two vectors\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::shrink_to_fit to shrink the capacity of a vector\n  // Hint: you can use the function std::vector::assign to assign a vector\n  // Hint: you can use the function std::vector::swap to swap two vectors\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::shrink_to_fit to shrink the capacity of a vector\n  // Hint: you can use the function std::vector::assign to assign a vector\n  // Hint: you can use the function std::vector::swap to swap two vectors\n  // Hint: you can use the function std::vector::reserve to reserve memory for a vector\n  // Hint: you can use the function std::vector::capacity to get the capacity of a vector\n  // Hint: you can use the function std::vector::sh",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::vector<Element> x_y_union;\n  std::set_union(x_copy.begin(), x_copy.end(), y_copy.begin(), y_copy.end(), std::back_inserter(x_y_union), [](Element const& a, Element const& b) { return a.index < b.index; });\n  for (auto const& e : x_y_union) {\n    if (e.index == x_copy.back().index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy.back().value});\n      break;\n    }\n    if (e.index == y_copy.back().index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy.back().value});\n      break;\n    }\n    if (e.index == x_copy.front().index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy.front().value});\n      continue;\n    }\n    if (e.index == y_copy.front().index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy.front().value});\n      continue;\n    }\n    if (e.index == x_copy[x_copy.size() / 2].index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy[y_copy.size() / 2].value});\n      continue;\n    }\n    if (e.index == y_copy[y_copy.size() / 2].index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy[x_copy.size() / 2].value});\n      continue;\n    }\n    if (e.index == x_copy.back().index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy.back().value});\n      break;\n    }\n    if (e.index == y_copy.back().index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy.back().value});\n      break;\n    }\n    if (e.index == x_copy.front().index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy.front().value});\n      continue;\n    }\n    if (e.index == y_copy.front().index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy.front().value});\n      continue;\n    }\n    if (e.index == x_copy[x_copy.size() / 2].index) {\n      z.push_back(Element{e.index, alpha * e.value + y_copy[y_copy.size() / 2].value});\n      continue;\n    }\n    if (e.index == y_copy[y_copy.size() / 2].index) {\n      z.push_back(Element{e.index, alpha * e.value + x_copy[x_copy.size() / 2].value});\n      continue;\n    }\n  }\n}",
            "// TODO\n}",
            "z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::vector<Element>::iterator x_it = x_copy.begin();\n  std::vector<Element>::iterator y_it = y_copy.begin();\n  while (x_it!= x_copy.end() && y_it!= y_copy.end()) {\n    if (x_it->index == y_it->index) {\n      z.push_back({x_it->index, alpha*x_it->value + y_it->value});\n      ++x_it;\n      ++y_it;\n    } else if (x_it->index < y_it->index) {\n      z.push_back(*x_it);\n      ++x_it;\n    } else {\n      z.push_back(*y_it);\n      ++y_it;\n    }\n  }\n  while (x_it!= x_copy.end()) {\n    z.push_back(*x_it);\n    ++x_it;\n  }\n  while (y_it!= y_copy.end()) {\n    z.push_back(*y_it);\n    ++y_it;\n  }\n}",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy(x);\n  std::vector<Element> y_copy(y);\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  size_t i = 0;\n  size_t j = 0;\n  while (i < x_copy.size() && j < y_copy.size()) {\n    if (x_copy[i].index == y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value + y_copy[j].value});\n      i++;\n      j++;\n    } else if (x_copy[i].index < y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n      i++;\n    } else {\n      z.push_back({y_copy[j].index, y_copy[j].value});\n      j++;\n    }\n  }\n  while (i < x_copy.size()) {\n    z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n    i++;\n  }\n  while (j < y_copy.size()) {\n    z.push_back({y_copy[j].index, y_copy[j].value});\n    j++;\n  }\n}",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> temp;\n  for(int i=0; i<x.size(); i++){\n    for(int j=0; j<y.size(); j++){\n      if(x[i].index == y[j].index){\n        temp.push_back(Element{x[i].index, alpha*x[i].value + y[j].value});\n      }\n    }\n  }\n  for(int i=0; i<y.size(); i++){\n    bool flag = true;\n    for(int j=0; j<temp.size(); j++){\n      if(y[i].index == temp[j].index){\n        flag = false;\n      }\n    }\n    if(flag){\n      temp.push_back(y[i]);\n    }\n  }\n  z = temp;\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element a, Element b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element a, Element b) { return a.index < b.index; });\n  size_t i = 0;\n  size_t j = 0;\n  while (i < x_copy.size() && j < y_copy.size()) {\n    if (x_copy[i].index == y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value + y_copy[j].value});\n      i++;\n      j++;\n    } else if (x_copy[i].index < y_copy[j].index) {\n      z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n      i++;\n    } else {\n      z.push_back({y_copy[j].index, y_copy[j].value});\n      j++;\n    }\n  }\n  while (i < x_copy.size()) {\n    z.push_back({x_copy[i].index, alpha * x_copy[i].value});\n    i++;\n  }\n  while (j < y_copy.size()) {\n    z.push_back({y_copy[j].index, y_copy[j].value});\n    j++;\n  }\n}",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy(x);\n  std::vector<Element> y_copy(y);\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) {\n    return a.index < b.index;\n  });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) {\n    return a.index < b.index;\n  });\n  std::vector<Element>::iterator x_it = x_copy.begin();\n  std::vector<Element>::iterator y_it = y_copy.begin();\n  while (x_it!= x_copy.end() && y_it!= y_copy.end()) {\n    if (x_it->index == y_it->index) {\n      z.push_back({x_it->index, alpha * x_it->value + y_it->value});\n      ++x_it;\n      ++y_it;\n    } else if (x_it->index < y_it->index) {\n      z.push_back({x_it->index, alpha * x_it->value});\n      ++x_it;\n    } else {\n      z.push_back({y_it->index, y_it->value});\n      ++y_it;\n    }\n  }\n  while (x_it!= x_copy.end()) {\n    z.push_back({x_it->index, alpha * x_it->value});\n    ++x_it;\n  }\n  while (y_it!= y_copy.end()) {\n    z.push_back({y_it->index, y_it->value});\n    ++y_it;\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n  std::vector<Element> z_temp;\n  std::vector<Element> x_temp;\n  std::vector<Element> y_temp;\n  for(int i=0; i<x.size(); i++){\n    x_temp.push_back(x[i]);\n  }\n  for(int i=0; i<y.size(); i++){\n    y_temp.push_back(y[i]);\n  }\n  for(int i=0; i<x_temp.size(); i++){\n    for(int j=0; j<y_temp.size(); j++){\n      if(x_temp[i].index == y_temp[j].index){\n        z_temp.push_back({x_temp[i].index, alpha*x_temp[i].value + y_temp[j].value});\n        x_temp.erase(x_temp.begin()+i);\n        y_temp.erase(y_temp.begin()+j);\n        i--;\n        j--;\n      }\n    }\n  }\n  for(int i=0; i<x_temp.size(); i++){\n    z_temp.push_back(x_temp[i]);\n  }\n  for(int i=0; i<y_temp.size(); i++){\n    z_temp.push_back(y_temp[i]);\n  }\n  z.clear();\n  for(int i=0; i<z_temp.size(); i++){\n    z.push_back(z_temp[i]);\n  }\n}",
            "// TODO: implement this function\n  // Hint: use the function std::lower_bound\n  // Hint: use the function std::upper_bound\n  // Hint: use the function std::inplace_merge\n  // Hint: use the function std::sort\n  // Hint: use the function std::unique\n  // Hint: use the function std::vector::insert\n  // Hint: use the function std::vector::erase\n  // Hint: use the function std::vector::resize\n  // Hint: use the function std::vector::reserve\n  // Hint: use the function std::vector::push_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std::vector::emplace_back\n  // Hint: use the function std",
            "// TODO: implement this function\n  z.clear();\n  for (auto const& e : x) {\n    auto it = std::find_if(y.begin(), y.end(), [&e](Element const& e2) { return e.index == e2.index; });\n    if (it!= y.end()) {\n      z.push_back({e.index, alpha * e.value + it->value});\n    } else {\n      z.push_back({e.index, alpha * e.value});\n    }\n  }\n  for (auto const& e : y) {\n    auto it = std::find_if(x.begin(), x.end(), [&e](Element const& e2) { return e.index == e2.index; });\n    if (it == x.end()) {\n      z.push_back({e.index, e.value});\n    }\n  }\n}",
            "// TODO: implement this function\n  z.clear();\n  std::vector<Element> x_copy = x;\n  std::vector<Element> y_copy = y;\n  std::sort(x_copy.begin(), x_copy.end(), [](Element a, Element b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element a, Element b) { return a.index < b.index; });\n  auto x_it = x_copy.begin();\n  auto y_it = y_copy.begin();\n  while (x_it!= x_copy.end() && y_it!= y_copy.end()) {\n    if (x_it->index == y_it->index) {\n      z.push_back(Element{x_it->index, alpha * x_it->value + y_it->value});\n      x_it++;\n      y_it++;\n    } else if (x_it->index < y_it->index) {\n      z.push_back(Element{x_it->index, alpha * x_it->value});\n      x_it++;\n    } else {\n      z.push_back(Element{y_it->index, y_it->value});\n      y_it++;\n    }\n  }\n  while (x_it!= x_copy.end()) {\n    z.push_back(Element{x_it->index, alpha * x_it->value});\n    x_it++;\n  }\n  while (y_it!= y_copy.end()) {\n    z.push_back(Element{y_it->index, y_it->value});\n    y_it++;\n  }\n}",
            "// TODO\n  z.clear();\n  for (auto const& x_elem : x) {\n    for (auto const& y_elem : y) {\n      if (x_elem.index == y_elem.index) {\n        z.push_back({x_elem.index, alpha * x_elem.value + y_elem.value});\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n  z.clear();\n  std::vector<Element> x_copy(x);\n  std::vector<Element> y_copy(y);\n  std::sort(x_copy.begin(), x_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  std::sort(y_copy.begin(), y_copy.end(), [](Element const& a, Element const& b) { return a.index < b.index; });\n  Element x_element, y_element;\n  size_t x_index = 0, y_index = 0;\n  while (x_index < x_copy.size() && y_index < y_copy.size()) {\n    x_element = x_copy[x_index];\n    y_element = y_copy[y_index];\n    if (x_element.index == y_element.index) {\n      z.push_back({x_element.index, alpha * x_element.value + y_element.value});\n      x_index++;\n      y_index++;\n    } else if (x_element.index < y_element.index) {\n      z.push_back({x_element.index, alpha * x_element.value});\n      x_index++;\n    } else {\n      z.push_back({y_element.index, y_element.value});\n      y_index++;\n    }\n  }\n  while (x_index < x_copy.size()) {\n    z.push_back({x_copy[x_index].index, alpha * x_copy[x_index].value});\n    x_index++;\n  }\n  while (y_index < y_copy.size()) {\n    z.push_back({y_copy[y_index].index, y_copy[y_index].value});\n    y_index++;\n  }\n}",
            "// TODO: implement this function\n  std::vector<Element> temp;\n  std::vector<Element> temp1;\n  std::vector<Element> temp2;\n  std::vector<Element> temp3;\n  std::vector<Element> temp4;\n  std::vector<Element> temp5;\n  std::vector<Element> temp6;\n  std::vector<Element> temp7;\n  std::vector<Element> temp8;\n  std::vector<Element> temp9;\n  std::vector<Element> temp10;\n  std::vector<Element> temp11;\n  std::vector<Element> temp12;\n  std::vector<Element> temp13;\n  std::vector<Element> temp14;\n  std::vector<Element> temp15;\n  std::vector<Element> temp16;\n  std::vector<Element> temp17;\n  std::vector<Element> temp18;\n  std::vector<Element> temp19;\n  std::vector<Element> temp20;\n  std::vector<Element> temp21;\n  std::vector<Element> temp22;\n  std::vector<Element> temp23;\n  std::vector<Element> temp24;\n  std::vector<Element> temp25;\n  std::vector<Element> temp26;\n  std::vector<Element> temp27;\n  std::vector<Element> temp28;\n  std::vector<Element> temp29;\n  std::vector<Element> temp30;\n  std::vector<Element> temp31;\n  std::vector<Element> temp32;\n  std::vector<Element> temp33;\n  std::vector<Element> temp34;\n  std::vector<Element> temp35;\n  std::vector<Element> temp36;\n  std::vector<Element> temp37;\n  std::vector<Element> temp38;\n  std::vector<Element> temp39;\n  std::vector<Element> temp40;\n  std::vector<Element> temp41;\n  std::vector<Element> temp42;\n  std::vector<Element> temp43;\n  std::vector<Element> temp44;\n  std::vector<Element> temp45;\n  std::vector<Element> temp46;\n  std::vector<Element> temp47;\n  std::vector<Element> temp48;\n  std::vector<Element> temp49;\n  std::vector<Element> temp50;\n  std::vector<Element> temp51;\n  std::vector<Element> temp52;\n  std::vector<Element> temp53;\n  std::vector<Element> temp54;\n  std::vector<Element> temp55;\n  std::vector<Element> temp56;\n  std::vector<Element> temp57;\n  std::vector<Element> temp58;\n  std::vector<Element> temp59;\n  std::vector<Element> temp60;\n  std::vector<Element> temp61;\n  std::vector<Element> temp62;\n  std::vector<Element> temp63;\n  std::vector<Element> temp64;\n  std::vector<Element> temp65;\n  std::vector<Element> temp66;\n  std::vector<Element> temp67;\n  std::vector<Element> temp68;\n  std::vector<Element> temp69;\n  std::vector<Element> temp70;\n  std::vector<Element> temp71;\n  std::vector<Element> temp72;\n  std::vector<Element> temp73;\n  std::vector<Element> temp74;\n  std::vector<Element> temp75;\n  std::vector<Element> temp76;\n  std::vector<Element> temp77;\n  std::vector<Element> temp78;\n  std::vector<Element> temp79;\n  std::vector<Element> temp80;\n  std::vector<Element> temp81;\n  std::vector<Element> temp82;\n  std::vector<Element> temp83;\n  std::vector<Element> temp84;\n  std::vector<Element"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "hip",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, Element *z, size_t Nx, size_t Ny, size_t Nz) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    if (xi.index < Nz && yi.index < Nz) {\n      z[xi.index].value += alpha * xi.value + yi.value;\n    }\n  }\n}",
            "// Get the index of the thread\n  size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  // Check if the thread is in bounds\n  if (index < Nz) {\n    // Initialize the value of the element\n    z[index].value = 0;\n    // Loop over all elements in x\n    for (size_t i = 0; i < Nx; i++) {\n      // Check if the current element in x is the same as the current element in z\n      if (x[i].index == index) {\n        // Add the value of the current element in x to the value of the current element in z\n        z[index].value += alpha * x[i].value;\n      }\n    }\n    // Loop over all elements in y\n    for (size_t i = 0; i < Ny; i++) {\n      // Check if the current element in y is the same as the current element in z\n      if (y[i].index == index) {\n        // Add the value of the current element in y to the value of the current element in z\n        z[index].value += y[i].value;\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x * blockDim.x + tid;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = gid; i < Nx; i += stride) {\n    size_t index = x[i].index;\n    double value = x[i].value + alpha * y[i].value;\n    z[index] = {index, value};\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < Nx || tid < Ny) {\n    Element x_element = x[tid];\n    Element y_element = y[tid];\n    if (x_element.index == y_element.index) {\n      z[tid].index = x_element.index;\n      z[tid].value = alpha * x_element.value + y_element.value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nz) return;\n  z[i].index = i;\n  z[i].value = alpha * x[i].value + y[i].value;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    Element zi = z[i];\n    if (xi.index == yi.index) {\n      zi.value = alpha * xi.value + yi.value;\n      z[i] = zi;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx && i >= Ny) return;\n\n  if (i < Nx) {\n    z[x[i].index] = alpha * x[i].value;\n  }\n  if (i < Ny) {\n    z[y[i].index] = alpha * y[i].value;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < Nx || tid < Ny) {\n    Element xi = x[tid];\n    Element yi = y[tid];\n    Element zi = z[tid];\n    if (xi.index == yi.index) {\n      zi.value = alpha * xi.value + yi.value;\n    }\n    z[tid] = zi;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nz) {\n    z[i].index = i;\n    z[i].value = alpha * x[i].value + y[i].value;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx) return;\n  size_t j = 0;\n  while (j < Ny && y[j].index < x[i].index) j++;\n  if (j < Ny && y[j].index == x[i].index) {\n    z[i].index = x[i].index;\n    z[i].value = alpha * x[i].value + y[j].value;\n  } else {\n    z[i].index = x[i].index;\n    z[i].value = alpha * x[i].value;\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + tid;\n  if (i < Nz) {\n    z[i].index = i;\n    z[i].value = 0;\n    for (size_t j = 0; j < Nx; j++) {\n      if (x[j].index == i) {\n        z[i].value += alpha * x[j].value;\n      }\n    }\n    for (size_t j = 0; j < Ny; j++) {\n      if (y[j].index == i) {\n        z[i].value += y[j].value;\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nz) return;\n  z[i].index = i;\n  z[i].value = alpha * x[i].value + y[i].value;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= Nx && tid >= Ny) return;\n\n  // Find the element in x and y that corresponds to the current thread.\n  // If the element is not found, the thread is not used.\n  Element x_element;\n  Element y_element;\n  if (tid < Nx) {\n    x_element = x[tid];\n  }\n  if (tid < Ny) {\n    y_element = y[tid];\n  }\n\n  // Find the element in z that corresponds to the current thread.\n  // If the element is not found, the thread is not used.\n  Element z_element;\n  if (tid < Nz) {\n    z_element = z[tid];\n  }\n\n  // Compute the value of the element in z.\n  // If the element is not found, the thread is not used.\n  if (x_element.index == y_element.index) {\n    z_element.value = alpha * x_element.value + y_element.value;\n  }\n\n  // Store the value of the element in z.\n  // If the element is not found, the thread is not used.\n  if (tid < Nz) {\n    z[tid] = z_element;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < Nx || tid < Ny) {\n    for (size_t i = 0; i < Nz; i++) {\n      if (x[i].index == tid) {\n        z[i].index = tid;\n        z[i].value = alpha * x[i].value + y[i].value;\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx && i >= Ny) return;\n  if (i < Nx) {\n    z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n  } else if (i < Ny) {\n    z[y[i].index] = alpha * y[i].value;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nz) return;\n  z[i].index = i;\n  z[i].value = alpha * x[i].value + y[i].value;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<Element> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_proc = n/size;\n  int n_rem = n%size;\n  int start = rank*n_per_proc;\n  int end = start + n_per_proc;\n  if (rank == size-1) {\n    end += n_rem;\n  }\n  std::vector<Element> x_proc(x.begin()+start, x.begin()+end);\n  std::vector<Element> y_proc(y.begin()+start, y.begin()+end);\n  std::vector<Element> z_proc(end-start);\n  for (int i = 0; i < x_proc.size(); i++) {\n    z_proc[i].index = x_proc[i].index;\n    z_proc[i].value = alpha*x_proc[i].value + y_proc[i].value;\n  }\n  std::vector<Element> z_all(n);\n  MPI_Gather(z_proc.data(), n_per_proc, MPI_DOUBLE_INT, z_all.data(), n_per_proc, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    z = z_all;\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  if (rank == 0) {\n    end += remainder;\n  }\n\n  std::vector<Element> local_x(x.begin() + start, x.begin() + end);\n  std::vector<Element> local_y(y.begin() + start, y.begin() + end);\n\n  std::vector<Element> local_z(local_x.size());\n\n  for (int i = 0; i < local_x.size(); i++) {\n    local_z[i].index = local_x[i].index;\n    local_z[i].value = alpha * local_x[i].value + local_y[i].value;\n  }\n\n  std::vector<Element> global_z(n);\n  MPI_Gather(&local_z[0], local_z.size(), MPI_DOUBLE_INT, &global_z[0], local_z.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    z = global_z;\n  }\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_rank = n/size;\n  int n_extra = n%size;\n  int start = rank*n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size-1) {\n    end += n_extra;\n  }\n  std::vector<Element> x_rank(x.begin()+start, x.begin()+end);\n  std::vector<Element> y_rank(y.begin()+start, y.begin()+end);\n  std::vector<Element> z_rank(end-start);\n  for (int i = 0; i < x_rank.size(); i++) {\n    z_rank[i].index = x_rank[i].index;\n    z_rank[i].value = alpha*x_rank[i].value + y_rank[i].value;\n  }\n  if (rank == 0) {\n    z.resize(n);\n  }\n  MPI_Gather(z_rank.data(), z_rank.size(), MPI_DOUBLE_INT, z.data(), z_rank.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); i++) {\n      for (size_t j = 0; j < y.size(); j++) {\n        if (x[i].index == y[j].index) {\n          z.push_back({x[i].index, alpha * x[i].value + y[j].value});\n          break;\n        }\n      }\n    }\n    for (size_t i = 0; i < y.size(); i++) {\n      bool flag = false;\n      for (size_t j = 0; j < x.size(); j++) {\n        if (x[j].index == y[i].index) {\n          flag = true;\n          break;\n        }\n      }\n      if (!flag) {\n        z.push_back({y[i].index, alpha * y[i].value});\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  // Hint: You can use the std::map class to store the result.\n  // Hint: You can use the std::map::find function to check if a key exists in the map.\n  // Hint: You can use the std::map::insert function to insert a new element in the map.\n  // Hint: You can use the std::map::at function to access an element in the map.\n  // Hint: You can use the std::map::erase function to remove an element from the map.\n  // Hint: You can use the std::map::begin function to get an iterator to the beginning of the map.\n  // Hint: You can use the std::map::end function to get an iterator to the end of the map.\n  // Hint: You can use the std::map::iterator::operator++ function to increment the iterator.\n  // Hint: You can use the std::map::iterator::operator* function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator== function to check if two iterators are equal.\n  // Hint: You can use the std::map::iterator::operator!= function to check if two iterators are not equal.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You can use the std::map::iterator::operator-> function to access the element pointed to by the iterator.\n  // Hint: You",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_per_rank = x.size()/size;\n  int num_extra = x.size()%size;\n  int start = rank*num_per_rank;\n  int end = (rank+1)*num_per_rank;\n  if (rank == size-1) {\n    end += num_extra;\n  }\n  std::vector<Element> x_rank(x.begin()+start, x.begin()+end);\n  std::vector<Element> y_rank(y.begin()+start, y.begin()+end);\n  std::vector<Element> z_rank(x.size());\n  for (int i = 0; i < x_rank.size(); i++) {\n    z_rank[i].index = x_rank[i].index;\n    z_rank[i].value = alpha*x_rank[i].value + y_rank[i].value;\n  }\n  MPI_Reduce(&z_rank[0], &z[0], x.size(), MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n  // You may assume that x and y are sorted by index.\n  // You may assume that x and y have no duplicate indices.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [0, n-1] for some n.\n  // You may assume that the indices in x and y are all unique.\n  // You may assume that the indices in x and y are all in the range [",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int m = y.size();\n  int k = n/size;\n  int r = n%size;\n  int start = rank*k + std::min(rank, r);\n  int end = (rank+1)*k + std::min(rank+1, r);\n  std::vector<Element> x_part(x.begin() + start, x.begin() + end);\n  std::vector<Element> y_part(y.begin() + start, y.begin() + end);\n  std::vector<Element> z_part(x_part.size());\n  for (int i = 0; i < x_part.size(); i++) {\n    z_part[i].index = x_part[i].index;\n    z_part[i].value = alpha * x_part[i].value + y_part[i].value;\n  }\n  if (rank == 0) {\n    z.resize(n);\n  }\n  MPI_Gather(z_part.data(), z_part.size(), MPI_DOUBLE_INT, z.data(), z_part.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "cuda",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, Element *z, size_t Nx, size_t Ny, size_t Nz) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    if (xi.index < Nz && yi.index < Nz) {\n      z[xi.index].value += alpha * xi.value + yi.value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    if (i < Nx && x[i].index < Nz) {\n      z[x[i].index] = alpha * x[i].value;\n    }\n    if (i < Ny && y[i].index < Nz) {\n      z[y[i].index] += y[i].value;\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < Nx || tid < Ny) {\n    Element x_elem = x[tid];\n    Element y_elem = y[tid];\n    if (x_elem.index == y_elem.index) {\n      z[tid].index = x_elem.index;\n      z[tid].value = alpha * x_elem.value + y_elem.value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx && i >= Ny) return;\n  if (i < Nx) {\n    z[x[i].index] += alpha * x[i].value;\n  }\n  if (i < Ny) {\n    z[y[i].index] += alpha * y[i].value;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element x_i = x[i];\n    Element y_i = y[i];\n    Element z_i;\n    z_i.index = x_i.index;\n    z_i.value = alpha * x_i.value + y_i.value;\n    z[i] = z_i;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx && i >= Ny) return;\n  if (i < Nx) {\n    z[x[i].index] += alpha * x[i].value;\n  }\n  if (i < Ny) {\n    z[y[i].index] += alpha * y[i].value;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    if (xi.index == yi.index) {\n      z[i].index = xi.index;\n      z[i].value = alpha * xi.value + yi.value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx) return;\n  size_t j = 0;\n  while (j < Ny && y[j].index < x[i].index) j++;\n  if (j < Ny && y[j].index == x[i].index) {\n    z[i].index = x[i].index;\n    z[i].value = alpha * x[i].value + y[j].value;\n  } else {\n    z[i].index = x[i].index;\n    z[i].value = alpha * x[i].value;\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= Nx && tid >= Ny) return;\n\n  size_t i = 0;\n  while (i < Nx && x[i].index < tid) ++i;\n  size_t j = 0;\n  while (j < Ny && y[j].index < tid) ++j;\n\n  if (i < Nx && x[i].index == tid) {\n    if (j < Ny && y[j].index == tid) {\n      z[tid].index = tid;\n      z[tid].value = alpha * x[i].value + y[j].value;\n    } else {\n      z[tid].index = tid;\n      z[tid].value = alpha * x[i].value;\n    }\n  } else {\n    if (j < Ny && y[j].index == tid) {\n      z[tid].index = tid;\n      z[tid].value = y[j].value;\n    } else {\n      z[tid].index = tid;\n      z[tid].value = 0;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (x[i].index!= z[j].index || y[i].index!= z[j].index)) {\n      j++;\n    }\n    if (j < Nz) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    size_t j = 0;\n    while (j < Nz && (z[j].index < x[i].index || z[j].index < y[i].index)) {\n      j++;\n    }\n    if (j < Nz && z[j].index == x[i].index) {\n      z[j].value = alpha * x[i].value + y[i].value;\n    } else {\n      z[j].index = x[i].index;\n      z[j].value = alpha * x[i].value + y[i].value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    if (xi.index < Nz && yi.index < Nz) {\n      z[xi.index] = {xi.index, alpha * xi.value + yi.value};\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element x_i = x[i];\n    Element y_i = y[i];\n    if (x_i.index == y_i.index) {\n      z[i].index = x_i.index;\n      z[i].value = alpha * x_i.value + y_i.value;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element xi = x[i];\n    Element yi = y[i];\n    Element zi = z[i];\n    if (xi.index == yi.index) {\n      zi.value = alpha * xi.value + yi.value;\n    }\n    z[i] = zi;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nx || i < Ny) {\n    Element x_i = x[i];\n    Element y_i = y[i];\n    if (x_i.index == y_i.index) {\n      z[i].index = x_i.index;\n      z[i].value = alpha * x_i.value + y_i.value;\n    } else if (x_i.index < y_i.index) {\n      z[i] = x_i;\n    } else {\n      z[i] = y_i;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= Nx && i >= Ny) return;\n  if (i < Nx) {\n    z[x[i].index] = alpha * x[i].value;\n  }\n  if (i < Ny) {\n    z[y[i].index] += alpha * y[i].value;\n  }\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < Nz) {\n    z[i].index = i;\n    z[i].value = alpha * x[i].value + y[i].value;\n  }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_axpy",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<Element> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_per_rank_remainder = num_elements % size;\n  int num_elements_per_rank_local = num_elements_per_rank + (rank < num_elements_per_rank_remainder);\n  int start_index = rank * num_elements_per_rank + (rank < num_elements_per_rank_remainder? rank : num_elements_per_rank_remainder);\n  int end_index = start_index + num_elements_per_rank_local;\n\n  std::vector<Element> x_local(num_elements_per_rank_local);\n  std::vector<Element> y_local(num_elements_per_rank_local);\n  std::vector<Element> z_local(num_elements_per_rank_local);\n\n  for (int i = 0; i < num_elements_per_rank_local; i++) {\n    x_local[i] = x[start_index + i];\n    y_local[i] = y[start_index + i];\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < num_elements_per_rank_local; i++) {\n    z_local[i].index = x_local[i].index;\n    z_local[i].value = alpha * x_local[i].value + y_local[i].value;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < num_elements_per_rank_local; i++) {\n      z[start_index + i] = z_local[i];\n    }\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_remainder = num_elements % size;\n  int num_elements_rank = num_elements_per_rank;\n  if (rank < num_elements_remainder) {\n    num_elements_rank++;\n  }\n  int start_index = rank * num_elements_per_rank;\n  if (rank < num_elements_remainder) {\n    start_index += rank;\n  }\n  int end_index = start_index + num_elements_rank;\n  std::vector<Element> local_x(num_elements_rank);\n  std::vector<Element> local_y(num_elements_rank);\n  std::vector<Element> local_z(num_elements_rank);\n  for (int i = 0; i < num_elements_rank; i++) {\n    local_x[i] = x[start_index + i];\n    local_y[i] = y[start_index + i];\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < num_elements_rank; i++) {\n    local_z[i].index = local_x[i].index;\n    local_z[i].value = alpha * local_x[i].value + local_y[i].value;\n  }\n  if (rank == 0) {\n    z.resize(num_elements);\n  }\n  MPI_Gather(local_z.data(), num_elements_rank, MPI_DOUBLE_INT, z.data(), num_elements_rank, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_threads = omp_get_max_threads();\n  int num_elements = x.size();\n  int num_elements_per_thread = num_elements / num_threads;\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_per_thread_per_rank = num_elements_per_thread / size;\n  int num_elements_per_thread_per_rank_remainder = num_elements_per_thread % size;\n  int num_elements_per_thread_per_rank_remainder_rank = rank;\n  int num_elements_per_thread_per_rank_remainder_rank_start = num_elements_per_thread_per_rank * rank;\n  int num_elements_per_thread_per_rank_remainder_rank_end = num_elements_per_thread_per_rank_remainder_rank_start + num_elements_per_thread_per_rank_remainder;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset = 0;\n  int num_elements_per_thread_per_rank_remainder_rank_end_offset = 0;\n  if (rank < num_elements_per_thread_per_rank_remainder) {\n    num_elements_per_thread_per_rank_remainder_rank_start_offset = rank;\n    num_elements_per_thread_per_rank_remainder_rank_end_offset = rank + 1;\n  } else if (rank >= num_elements_per_thread_per_rank_remainder) {\n    num_elements_per_thread_per_rank_remainder_rank_start_offset = num_elements_per_thread_per_rank_remainder;\n    num_elements_per_thread_per_rank_remainder_rank_end_offset = num_elements_per_thread_per_rank_remainder + (rank - num_elements_per_thread_per_rank_remainder);\n  }\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end = num_elements_per_thread_per_rank_remainder_rank_start_offset + num_elements_per_thread_per_rank_remainder_rank_end_offset;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset = num_elements_per_thread_per_rank_remainder_rank_start_offset_end - rank;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start = num_elements_per_thread_per_rank_remainder_rank_start_offset_end - num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_end = num_elements_per_thread_per_rank_remainder_rank_start_offset_end - num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset + 1;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start_offset = num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start - rank;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start_offset_end = num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start_offset + 1;\n  int num_elements_per_thread_per_rank_remainder_rank_start_offset_end_offset_start_offset_end_offset = num_elements_per_thread_per_rank_remainder_rank_start",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk = x.size()/size;\n  int start = rank*chunk;\n  int end = (rank+1)*chunk;\n  if(rank == size-1)\n    end = x.size();\n  std::vector<Element> local_x(x.begin()+start, x.begin()+end);\n  std::vector<Element> local_y(y.begin()+start, y.begin()+end);\n  std::vector<Element> local_z(y.begin()+start, y.begin()+end);\n  for(int i=0; i<local_x.size(); i++)\n    local_z[i].value = alpha*local_x[i].value + local_y[i].value;\n  if(rank == 0)\n    z = local_z;\n  else\n    MPI_Send(local_z.data(), local_z.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  if(rank!= 0)\n    MPI_Recv(z.data(), z.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_elements = x.size();\n  int num_elements_per_thread = num_elements / num_threads;\n  int num_elements_last_thread = num_elements - (num_elements_per_thread * (num_threads - 1));\n\n  std::vector<Element> z_local(num_elements_per_thread);\n\n  #pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    int start = thread_id * num_elements_per_thread;\n    int end = start + num_elements_per_thread;\n    if (thread_id == num_threads - 1) {\n      end = start + num_elements_last_thread;\n    }\n\n    for (int i = start; i < end; i++) {\n      z_local[i - start] = {x[i].index, alpha * x[i].value + y[i].value};\n    }\n\n    #pragma omp barrier\n\n    if (rank == 0) {\n      for (int i = 0; i < num_threads; i++) {\n        for (int j = 0; j < num_elements_per_thread; j++) {\n          z[i * num_elements_per_thread + j] = z_local[j];\n        }\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_rank = n/size;\n  int n_remainder = n%size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  int start = rank*n_per_rank + (rank < n_remainder? rank : n_remainder);\n  int end = start + n_local;\n  std::vector<Element> x_local(n_local), y_local(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local[i] = x[start + i];\n    y_local[i] = y[start + i];\n  }\n  std::vector<Element> z_local(n_local);\n  #pragma omp parallel for\n  for (int i = 0; i < n_local; i++) {\n    z_local[i].index = x_local[i].index;\n    z_local[i].value = alpha*x_local[i].value + y_local[i].value;\n  }\n  if (rank == 0) {\n    z.resize(n);\n  }\n  MPI_Reduce(z_local.data(), z.data(), n_local, MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int n_per_proc = n/size;\n  int n_rem = n%size;\n  int start = rank*n_per_proc;\n  int end = start + n_per_proc;\n  if(rank == size-1)\n    end += n_rem;\n  std::vector<Element> local_x(x.begin()+start, x.begin()+end);\n  std::vector<Element> local_y(y.begin()+start, y.begin()+end);\n  std::vector<Element> local_z(local_x.size());\n  #pragma omp parallel for\n  for(int i = 0; i < local_x.size(); i++)\n  {\n    local_z[i].index = local_x[i].index;\n    local_z[i].value = alpha*local_x[i].value + local_y[i].value;\n  }\n  if(rank == 0)\n  {\n    z.resize(n);\n    for(int i = 0; i < local_z.size(); i++)\n    {\n      z[local_z[i].index] = local_z[i].value;\n    }\n  }\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_elements = x.size();\n  int num_per_thread = num_elements / num_threads;\n  int num_remainder = num_elements % num_threads;\n\n  int start = rank * num_per_thread;\n  int end = start + num_per_thread;\n  if (rank == size - 1) {\n    end += num_remainder;\n  }\n\n  std::vector<Element> local_z(num_per_thread);\n  for (int i = start; i < end; i++) {\n    local_z[i - start].index = x[i].index;\n    local_z[i - start].value = alpha * x[i].value + y[i].value;\n  }\n\n  MPI_Reduce(&local_z[0], &z[0], num_per_thread, MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n\n  int num_elements_per_thread = x.size() / num_threads;\n  int num_elements_remainder = x.size() % num_threads;\n\n  int num_elements_per_thread_rank = num_elements_per_thread;\n  if (rank < num_elements_remainder) {\n    num_elements_per_thread_rank++;\n  }\n\n  int start_index = rank * num_elements_per_thread + std::min(rank, num_elements_remainder);\n  int end_index = start_index + num_elements_per_thread_rank;\n\n  std::vector<Element> local_z;\n  for (int i = start_index; i < end_index; i++) {\n    Element element;\n    element.index = x[i].index;\n    element.value = alpha * x[i].value + y[i].value;\n    local_z.push_back(element);\n  }\n\n  std::vector<Element> global_z;\n  MPI_Reduce(&local_z[0], &global_z[0], local_z.size(), MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    z = global_z;\n  }\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_elements = x.size();\n  int elements_per_thread = num_elements / num_threads;\n  int remainder = num_elements % num_threads;\n\n  std::vector<Element> local_x(elements_per_thread);\n  std::vector<Element> local_y(elements_per_thread);\n  std::vector<Element> local_z(elements_per_thread);\n\n  if (rank == 0) {\n    z.resize(num_elements);\n  }\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_id = omp_get_thread_num();\n    int start_index = thread_id * elements_per_thread;\n    int end_index = start_index + elements_per_thread;\n    if (thread_id == num_threads - 1) {\n      end_index += remainder;\n    }\n\n    for (int i = start_index; i < end_index; i++) {\n      local_x[i - start_index] = x[i];\n      local_y[i - start_index] = y[i];\n    }\n\n    #pragma omp barrier\n\n    for (int i = 0; i < elements_per_thread; i++) {\n      if (local_x[i].index == local_y[i].index) {\n        local_z[i].index = local_x[i].index;\n        local_z[i].value = alpha * local_x[i].value + local_y[i].value;\n      }\n    }\n\n    #pragma omp barrier\n\n    if (rank == 0) {\n      for (int i = 0; i < elements_per_thread; i++) {\n        z[start_index + i] = local_z[i];\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_elements_per_thread = x.size() / num_threads;\n  int num_elements_remainder = x.size() % num_threads;\n\n  std::vector<Element> local_z(num_elements_per_thread + (rank < num_elements_remainder));\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_id = omp_get_thread_num();\n    int start_index = thread_id * num_elements_per_thread + std::min(thread_id, num_elements_remainder);\n    int end_index = start_index + num_elements_per_thread + (thread_id < num_elements_remainder);\n\n    for (int i = start_index; i < end_index; i++) {\n      local_z[i - start_index].index = x[i].index;\n      local_z[i - start_index].value = alpha * x[i].value + y[i].value;\n    }\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Reduce(local_z.data(), z.data(), x.size(), MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_remainder;\n  }\n  std::vector<Element> local_z(end - start);\n  for (int i = start; i < end; i++) {\n    local_z[i - start].index = x[i].index;\n    local_z[i - start].value = alpha * x[i].value + y[i].value;\n  }\n  std::vector<Element> global_z(n);\n  MPI_Reduce(&local_z[0], &global_z[0], n, MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    z = global_z;\n  }\n}",
            "// TODO: implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n\n  std::vector<Element> local_x(chunk + (rank < remainder? 1 : 0));\n  std::vector<Element> local_y(chunk + (rank < remainder? 1 : 0));\n  std::vector<Element> local_z(chunk + (rank < remainder? 1 : 0));\n\n  MPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, local_y.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_z[i].index = local_x[i].index;\n    local_z[i].value = alpha * local_x[i].value + local_y[i].value;\n  }\n\n  MPI_Gather(local_z.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, z.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = x.size()/size;\n  int remainder = x.size()%size;\n\n  int start = rank*chunk;\n  int end = start + chunk;\n  if (rank == size-1) {\n    end += remainder;\n  }\n\n  std::vector<Element> x_rank(x.begin()+start, x.begin()+end);\n  std::vector<Element> y_rank(y.begin()+start, y.begin()+end);\n  std::vector<Element> z_rank(z.begin()+start, z.begin()+end);\n\n  #pragma omp parallel for\n  for (int i = 0; i < x_rank.size(); i++) {\n    z_rank[i].value = alpha*x_rank[i].value + y_rank[i].value;\n  }\n\n  MPI_Reduce(z_rank.data(), z.data(), z.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_threads = omp_get_max_threads();\n  int num_elements = x.size();\n  int num_elements_per_thread = num_elements / num_threads;\n  int num_elements_remainder = num_elements % num_threads;\n  int start_index = rank * num_elements_per_thread;\n  int end_index = start_index + num_elements_per_thread;\n  if (rank == 0) {\n    end_index += num_elements_remainder;\n  }\n  std::vector<Element> local_x(x.begin() + start_index, x.begin() + end_index);\n  std::vector<Element> local_y(y.begin() + start_index, y.begin() + end_index);\n  std::vector<Element> local_z(end_index - start_index);\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_z[i].index = local_x[i].index;\n    local_z[i].value = alpha * local_x[i].value + local_y[i].value;\n  }\n  MPI_Reduce(local_z.data(), z.data(), z.size(), MPI_DOUBLE_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(Kokkos::View<const COOElement*> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < N; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < N; j++) {\n         for (size_t k = 0; k < A.size(); k++) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * x[j];\n            }\n         }\n      }\n      x[i] = (b[i] - sum) / A[i].value;\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   // Hint: Use OpenMP to parallelize the loop over the rows of A\n   // Hint: Use OpenMP to parallelize the loop over the columns of A\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of b\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use OpenMP to parallelize the loop over the elements of x\n   // Hint: Use Open",
            "// TODO: Implement this function\n   // Hint: Use OpenMP to parallelize the loop over the rows of A\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (size_t i = 0; i < N; ++i) {\n      double sum = 0;\n      for (size_t j = 0; j < N; ++j) {\n         for (size_t k = 0; k < A.size(); ++k) {\n            if (A[k].row == i && A[k].column == j) {\n               sum += A[k].value * b[j];\n            }\n         }\n      }\n      x[i] = sum;\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "std::vector<double> y(N, 0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < N; ++i) {\n      for (auto const& element : A) {\n         if (element.row == i) {\n            y[i] += element.value * b[element.column];\n         }\n      }\n   }\n   #pragma omp parallel for\n   for (size_t i = 0; i < N; ++i) {\n      double sum = 0;\n      for (auto const& element : A) {\n         if (element.column == i) {\n            sum += element.value * y[element.row];\n         }\n      }\n      x[i] = (sum - y[i]) / A[i].value;\n   }\n}",
            "// TODO: Implement this function\n   // Hint: Use OpenMP to parallelize the computation\n   // Hint: Use the following formula to compute the value of x_i:\n   // x_i = (b_i - sum_{j=0}^{i-1} A_{i,j} x_j - sum_{j=i+1}^{N-1} A_{i,j} x_j) / A_{i,i}\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   // TODO: implement the kernel\n}",
            "// TODO: implement the kernel\n}",
            "// TODO: Implement this function\n}",
            "// Get the thread index\n   size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid >= sizeA) return;\n\n   // Get the element\n   COOElement e = A[tid];\n\n   // Compute the value\n   x[e.row] += e.value * b[e.column];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "// TODO: Implement the kernel\n}",
            "// TODO: implement\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n\n   // initialize x with zeros\n   x[row] = 0;\n\n   // iterate over all elements in the row\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         // add the element to the current row\n         x[row] += A[i].value * b[A[i].column];\n      }\n   }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; i++) {\n      if (A[i].row == row) {\n         sum += A[i].value * x[A[i].column];\n      }\n   }\n   x[row] = (b[row] - sum) / A[row * N + row];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "// TODO: Implement the kernel\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row >= N) return;\n   double sum = 0;\n   for (size_t i = 0; i < sizeA; ++i) {\n      if (A[i].row == row) {\n         sum += A[i].value * x[A[i].column];\n      }\n   }\n   x[row] = (b[row] - sum) / A[row * N + row];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   // TODO: implement the kernel\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   x[row] += value * b[column];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   x[row] += value * b[column];\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   // get the row and column of the element\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n\n   // get the value of the element\n   double value = A[i].value;\n\n   // compute the value of the element in the solution vector\n   x[row] += value * b[column];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n   x[row] += value * b[column];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] = x[row] + value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= sizeA) return;\n\n   size_t row = A[i].row;\n   size_t column = A[i].column;\n   double value = A[i].value;\n\n   // TODO: implement the kernel\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < sizeA) {\n      size_t row = A[i].row;\n      size_t column = A[i].column;\n      double value = A[i].value;\n      x[row] += value * b[column];\n   }\n}"
        ]
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "42_sparse_la_sparse_solve",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n\n}",
            "// TODO: Your code here\n   //\n   // Hint:\n   // - Use MPI_Reduce to collect the results from all ranks\n   // - Use OpenMP to parallelize the computation on each rank\n   // - Use MPI_Bcast to broadcast the result to all ranks\n   // - Use MPI_Scatter to distribute the matrix A to all ranks\n   // - Use MPI_Gather to collect the results from all ranks\n   // - Use MPI_Scatterv to distribute the vector b to all ranks\n   // - Use MPI_Gatherv to collect the results from all ranks\n   // - Use MPI_Allreduce to collect the results from all ranks\n   // - Use OpenMP to parallelize the computation on each rank\n\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Fill in the body of this function\n    // Use OpenMP to count in parallel\n    // Use the atomic increment operator to update the bins\n\n}",
            "// TODO: Fill in the code to count the number of pixels with each grayscale intensity\n    //       Store the results in bins\n    //       Use OpenMP to count in parallel\n    //       You may use the following code as a starting point\n    //       (but you don't have to use it)\n    //\n    //       #pragma omp parallel for\n    //       for (size_t i = 0; i < image.size(); ++i) {\n    //         bins[image[i]]++;\n    //       }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n      bins[image[i]]++;\n    }\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: implement\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for(int i=0; i<image.size(); i++){\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement this function\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: implement this function\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: implement this function\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: implement\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = image.size();\n    int n_per_rank = n/size;\n    int n_rem = n%size;\n    int n_start = rank*n_per_rank;\n    int n_end = n_start + n_per_rank;\n    if (rank == 0) {\n        n_end += n_rem;\n    }\n    std::vector<int> local_image(n_per_rank);\n    std::copy(image.begin()+n_start, image.begin()+n_end, local_image.begin());\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n    for (int i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int image_size = image.size();\n  int image_size_per_rank = image_size / size;\n  int image_size_remainder = image_size % size;\n\n  std::vector<int> image_per_rank(image_size_per_rank);\n  std::vector<int> image_remainder(image_size_remainder);\n\n  MPI_Scatter(image.data(), image_size_per_rank, MPI_INT, image_per_rank.data(), image_size_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < image_size_per_rank; i++) {\n      bins[image_per_rank[i]]++;\n    }\n  } else {\n    MPI_Scatter(image.data(), image_size_per_rank, MPI_INT, image_remainder.data(), image_size_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < image_size_per_rank; i++) {\n      bins[image_per_rank[i]]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < image_size_remainder; i++) {\n      bins[image_remainder[i]]++;\n    }\n  }\n\n  MPI_Gather(bins.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int local_size = image.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank + 1) * local_size;\n    if (rank == size - 1) {\n        local_end = image.size();\n    }\n    std::array<size_t, 256> local_bins;\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n    for (int i = local_start; i < local_end; i++) {\n        local_bins[image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = image.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int n_per_proc_rank = n_per_proc + (rank < n_rem? 1 : 0);\n    int start = rank * n_per_proc + std::min(rank, n_rem);\n    int end = start + n_per_proc_rank;\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n    for (int i = start; i < end; i++) {\n        local_bins[image[i]]++;\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Status status;\n            MPI_Recv(&local_bins, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += local_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int n = image.size();\n   int n_per_proc = n / size;\n   int n_rem = n % size;\n   int n_start = rank * n_per_proc;\n   int n_end = n_start + n_per_proc;\n   if (rank == size - 1) {\n     n_end += n_rem;\n   }\n   std::array<size_t, 256> local_bins;\n   for (int i = 0; i < 256; i++) {\n     local_bins[i] = 0;\n   }\n   for (int i = n_start; i < n_end; i++) {\n     local_bins[image[i]]++;\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = image.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n\n    std::vector<int> local_image(n_per_rank);\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            local_image[i] = image[i];\n        }\n    } else {\n        for (int i = 0; i < n_per_rank; i++) {\n            local_image[i] = image[n_per_rank * (rank - 1) + i];\n        }\n    }\n\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n    for (int i = 0; i < n_per_rank; i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    std::array<size_t, 256> global_bins;\n    global_bins.fill(0);\n    MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_rem; i++) {\n            global_bins[image[n_per_rank * size + i]]++;\n        }\n    }\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = image.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank + 1) * local_size;\n    if (rank == size - 1) {\n        local_end = image.size();\n    }\n    std::vector<int> local_image(image.begin() + local_start, image.begin() + local_end);\n\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n    for (int i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<int> local_image;\n   int local_size = image.size()/size;\n   int local_start = rank*local_size;\n   int local_end = local_start + local_size;\n   if (rank == size - 1) {\n     local_end = image.size();\n   }\n   for (int i = local_start; i < local_end; i++) {\n     local_image.push_back(image[i]);\n   }\n   std::array<size_t, 256> local_bins;\n   for (int i = 0; i < 256; i++) {\n     local_bins[i] = 0;\n   }\n   for (int i = 0; i < local_image.size(); i++) {\n     local_bins[local_image[i]]++;\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<int> local_image(image.size()/size);\n   std::array<size_t, 256> local_bins;\n   std::fill(local_bins.begin(), local_bins.end(), 0);\n\n   MPI_Scatter(image.data(), local_image.size(), MPI_INT, local_image.data(), local_image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   for (int i = 0; i < local_image.size(); i++) {\n     local_bins[local_image[i]]++;\n   }\n\n   MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = image.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n    if (rank == size - 1) {\n        end += n_rem;\n    }\n    std::array<size_t, 256> local_bins;\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n    for (int i = start; i < end; i++) {\n        local_bins[image[i]]++;\n    }\n    std::array<size_t, 256> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int image_size = image.size();\n  int image_size_per_rank = image_size / size;\n  int image_size_remainder = image_size % size;\n  int image_start = rank * image_size_per_rank;\n  int image_end = image_start + image_size_per_rank;\n  if (rank == 0) {\n    image_end += image_size_remainder;\n  }\n  std::vector<int> image_rank(image_size_per_rank);\n  MPI_Scatter(&image[0], image_size_per_rank, MPI_INT, &image_rank[0], image_size_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n  std::array<size_t, 256> bins_rank;\n  for (int i = 0; i < 256; i++) {\n    bins_rank[i] = 0;\n  }\n  for (int i = 0; i < image_size_per_rank; i++) {\n    bins_rank[image_rank[i]]++;\n  }\n  MPI_Gather(&bins_rank[0], 256, MPI_UNSIGNED_LONG, &bins[0], 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n }",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int local_size = image.size() / size;\n  int local_start = rank * local_size;\n  int local_end = local_start + local_size;\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n  for (int i = local_start; i < local_end; i++) {\n    local_bins[image[i]]++;\n  }\n  std::array<size_t, 256> global_bins;\n  global_bins.fill(0);\n  MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "// TODO\n }",
            "// TODO: implement this function\n }",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int image_size = image.size();\n    int image_size_per_rank = image_size / size;\n    int image_size_remainder = image_size % size;\n    int image_size_per_rank_plus_one = image_size_per_rank + 1;\n    int image_size_per_rank_plus_one_remainder = image_size_per_rank_plus_one % size;\n    int image_size_per_rank_plus_one_remainder_rank = image_size_per_rank_plus_one_remainder * rank;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one = image_size_per_rank_plus_one_remainder * (rank + 1);\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last = image_size_per_rank_plus_one_remainder_rank_plus_one - 1;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank = image_size_per_rank_plus_one_remainder_rank_plus_one_last % size;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one = image_size_per_rank_plus_one_remainder_rank_plus_one_last % size;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank + image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last % size;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank + image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank % size;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank_plus_one = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank + image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank_plus_one;\n    int image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_rank = image_size_per_rank_plus_one_remainder_rank_plus_one_last_rank_plus_one_last_rank_plus_one_last_",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = image.size() / size;\n    int local_start = rank * local_size;\n    int local_end = local_start + local_size;\n\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n\n    for (int i = local_start; i < local_end; i++) {\n        local_bins[image[i]]++;\n    }\n\n    std::array<size_t, 256> global_bins;\n    global_bins.fill(0);\n\n    MPI_Reduce(&local_bins, &global_bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int n = image.size();\n   int n_per_rank = n/size;\n   int n_rem = n%size;\n   int n_per_rank_plus = n_per_rank + 1;\n   int n_per_rank_minus = n_per_rank - 1;\n   int n_per_rank_plus_rem = n_per_rank_plus + n_rem;\n   int n_per_rank_minus_rem = n_per_rank_minus + n_rem;\n   int n_per_rank_plus_rem_start = rank*n_per_rank_plus_rem;\n   int n_per_rank_minus_rem_start = rank*n_per_rank_minus_rem;\n   int n_per_rank_plus_rem_end = n_per_rank_plus_rem_start + n_per_rank_plus_rem;\n   int n_per_rank_minus_rem_end = n_per_rank_minus_rem_start + n_per_rank_minus_rem;\n   std::vector<int> image_rank;\n   if (rank == 0) {\n     image_rank.resize(n_per_rank_plus_rem_end);\n     for (int i = 0; i < n_per_rank_plus_rem_end; i++) {\n       image_rank[i] = image[i];\n     }\n   } else if (rank == size-1) {\n     image_rank.resize(n_per_rank_minus_rem_end);\n     for (int i = 0; i < n_per_rank_minus_rem_end; i++) {\n       image_rank[i] = image[n_per_rank_minus_rem_start + i];\n     }\n   } else {\n     image_rank.resize(n_per_rank_plus_rem);\n     for (int i = 0; i < n_per_rank_plus_rem; i++) {\n       image_rank[i] = image[n_per_rank_plus_rem_start + i];\n     }\n   }\n   std::array<size_t, 256> bins_rank;\n   for (int i = 0; i < 256; i++) {\n     bins_rank[i] = 0;\n   }\n   #pragma omp parallel for\n   for (int i = 0; i < image_rank.size(); i++) {\n     bins_rank[image_rank[i]]++;\n   }\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = bins_rank[i];\n     }\n     for (int i = 1; i < size; i++) {\n       MPI_Recv(&bins_rank, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for (int j = 0; j < 256; j++) {\n         bins[j] += bins_rank[j];\n       }\n     }\n   } else {\n     MPI_Send(&bins_rank, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO: Your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int n = image.size();\n   int n_per_rank = n / size;\n   int n_remain = n % size;\n   int n_local = n_per_rank + (rank < n_remain? 1 : 0);\n   std::vector<int> image_local(n_local);\n   MPI_Scatter(image.data(), n_local, MPI_INT, image_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n   std::array<size_t, 256> bins_local;\n   bins_local.fill(0);\n   #pragma omp parallel for\n   for (int i = 0; i < n_local; i++) {\n     bins_local[image_local[i]]++;\n   }\n   MPI_Reduce(bins_local.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the image\n    // Hint: Use MPI to distribute the image among the ranks\n    // Hint: Use MPI_Reduce to combine the results from each rank\n\n    // TODO: Implement this function\n    // H",
            "// TODO: Implement this function\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int n = image.size();\n   int n_per_rank = n / size;\n   int n_remainder = n % size;\n   int n_local = n_per_rank + (rank < n_remainder);\n   std::vector<int> image_local(n_local);\n   MPI_Scatter(image.data(), n_local, MPI_INT, image_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n   std::array<size_t, 256> bins_local;\n   bins_local.fill(0);\n   #pragma omp parallel for\n   for (int i = 0; i < n_local; i++) {\n     bins_local[image_local[i]]++;\n   }\n   MPI_Reduce(bins_local.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = image.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  std::vector<int> local_image;\n  if (rank == 0) {\n    local_image = std::vector<int>(image.begin(), image.begin() + n_per_rank + n_remainder);\n  } else {\n    local_image = std::vector<int>(image.begin() + n_per_rank * rank + n_remainder * (rank - 1),\n                                   image.begin() + n_per_rank * (rank + 1) + n_remainder * rank);\n  }\n\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_image.size(); i++) {\n    local_bins[local_image[i]]++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = image.size() / size;\n  int local_start = rank * local_size;\n  int local_end = local_start + local_size;\n\n  std::vector<int> local_image(local_size);\n  std::copy(image.begin() + local_start, image.begin() + local_end, local_image.begin());\n\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_image.size(); i++) {\n    local_bins[local_image[i]]++;\n  }\n\n  if (rank == 0) {\n    std::array<size_t, 256> global_bins;\n    global_bins.fill(0);\n    for (int i = 0; i < size; i++) {\n      MPI_Recv(&local_bins, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 256; j++) {\n        global_bins[j] += local_bins[j];\n      }\n    }\n    bins = global_bins;\n  } else {\n    MPI_Send(&local_bins, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = image.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int n_proc = rank < n_rem? rank * (n_per_proc + 1) : rank * n_per_proc + n_rem;\n    int n_proc_next = (rank + 1) < n_rem? (rank + 1) * (n_per_proc + 1) : (rank + 1) * n_per_proc + n_rem;\n    int n_local = n_proc_next - n_proc;\n    std::vector<int> local_image(n_local);\n    for (int i = 0; i < n_local; i++) {\n        local_image[i] = image[n_proc + i];\n    }\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; i++) {\n        local_bins[local_image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = image.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n    int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n    std::vector<int> image_local(n_local);\n    MPI_Scatter(image.data(), n_local, MPI_INT, image_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n    std::array<size_t, 256> bins_local;\n    bins_local.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; ++i) {\n        bins_local[image_local[i]]++;\n    }\n    MPI_Reduce(bins_local.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n }",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_threads = omp_get_max_threads();\n  int num_per_thread = image.size() / num_threads;\n  int num_remainder = image.size() % num_threads;\n  std::vector<int> local_image;\n  if (rank == 0) {\n    for (int i = 0; i < num_threads; i++) {\n      if (i < num_remainder) {\n        local_image.insert(local_image.end(), image.begin() + i * (num_per_thread + 1), image.begin() + (i + 1) * (num_per_thread + 1));\n      } else {\n        local_image.insert(local_image.end(), image.begin() + i * num_per_thread + num_remainder, image.begin() + (i + 1) * num_per_thread + num_remainder);\n      }\n    }\n  } else {\n    for (int i = 0; i < num_threads; i++) {\n      if (i < num_remainder) {\n        local_image.insert(local_image.end(), image.begin() + i * (num_per_thread + 1) + rank * num_per_thread, image.begin() + (i + 1) * (num_per_thread + 1) + rank * num_per_thread);\n      } else {\n        local_image.insert(local_image.end(), image.begin() + i * num_per_thread + num_remainder + rank * num_per_thread, image.begin() + (i + 1) * num_per_thread + num_remainder + rank * num_per_thread);\n      }\n    }\n  }\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n  #pragma omp parallel for\n  for (int i = 0; i < local_image.size(); i++) {\n    local_bins[local_image[i]]++;\n  }\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      for (int j = 1; j < size; j++) {\n        MPI_Recv(&bins[i], 1, MPI_UNSIGNED_LONG, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n    }\n  } else {\n    for (int i = 0; i < 256; i++) {\n      MPI_Send(&local_bins[i], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n}",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the pixels\n    // Hint: Use MPI to distribute the work over the ranks\n    // Hint: Use MPI_Reduce to combine the results from all ranks\n    // Hint: Use MPI_Bcast to broadcast the results from rank 0\n    // Hint: Use MPI_Comm_rank to get the rank of the current process\n    // Hint: Use MPI_Comm_size to get the number of processes\n    // Hint: Use MPI_Scatter to distribute the image to the ranks\n    // Hint: Use MPI_Gather to collect the results from the ranks\n    // Hint: Use MPI_Send and MPI_Recv to send and receive the results from the ranks\n    // Hint: Use MPI_Request to send and receive the results from the ranks\n    // Hint: Use MPI_Wait to wait for the results from the ranks\n    // Hint: Use MPI_Waitall to wait for the results from the ranks\n    // Hint: Use MPI_Allreduce to combine the results from all ranks\n    // Hint: Use MPI_Allgather to collect the results from the ranks\n    // Hint: Use MPI_Allgatherv to collect the results from the ranks\n    // Hint: Use MPI_Alltoall to collect the results from the ranks\n    // Hint: Use MPI_Alltoallv to collect the results from the ranks\n    // Hint: Use MPI_Alltoallw to collect the results from the ranks\n    // Hint: Use MPI_Reduce_scatter to combine the results from all ranks\n    // Hint: Use MPI_Reduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Scan to combine the results from all ranks\n    // Hint: Use MPI_Exscan to combine the results from all ranks\n    // Hint: Use MPI_Ireduce to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Iscan to combine the results from all ranks\n    // Hint: Use MPI_Iexscan to combine the results from all ranks\n    // Hint: Use MPI_Iallreduce to combine the results from all ranks\n    // Hint: Use MPI_Iallgather to collect the results from the ranks\n    // Hint: Use MPI_Iallgatherv to collect the results from the ranks\n    // Hint: Use MPI_Ialltoall to collect the results from the ranks\n    // Hint: Use MPI_Ialltoallv to collect the results from the ranks\n    // Hint: Use MPI_Ialltoallw to collect the results from the ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks\n    // Hint: Use MPI_Ireduce_scatter_block to combine the results from all ranks",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = image.size() / size;\n  int local_start = rank * local_size;\n  int local_end = local_start + local_size;\n\n  std::vector<int> local_image(local_size);\n  for (int i = 0; i < local_size; i++) {\n    local_image[i] = image[local_start + i];\n  }\n\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_size; i++) {\n    local_bins[local_image[i]]++;\n  }\n\n  std::array<size_t, 256> global_bins;\n  global_bins.fill(0);\n\n  MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = image.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    std::vector<int> image_rank(n_per_rank);\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&image[i * n_per_rank], n_per_rank, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Recv(&image_rank[0], n_per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            bins[image[i]]++;\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&image_rank[0], n_per_rank, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n_per_rank; j++) {\n                bins[image_rank[j]]++;\n            }\n        }\n    }\n    else {\n        for (int i = 0; i < n_per_rank; i++) {\n            bins[image_rank[i]]++;\n        }\n        MPI_Send(&image_rank[0], n_per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunkSize = image.size() / size;\n  int remainder = image.size() % size;\n\n  std::vector<int> myImage;\n  if (rank == 0) {\n    myImage = std::vector<int>(image.begin(), image.begin() + chunkSize + remainder);\n  } else {\n    myImage = std::vector<int>(image.begin() + rank * chunkSize, image.begin() + (rank + 1) * chunkSize);\n  }\n\n  std::array<size_t, 256> myBins;\n  myBins.fill(0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < myImage.size(); i++) {\n    myBins[myImage[i]]++;\n  }\n\n  MPI_Reduce(myBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_threads = omp_get_max_threads();\n  int num_pixels = image.size();\n  int num_pixels_per_thread = num_pixels / num_threads;\n  int num_pixels_remainder = num_pixels % num_threads;\n\n  std::vector<int> local_image(num_pixels_per_thread);\n  std::array<size_t, 256> local_bins;\n\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      local_bins[i] = 0;\n    }\n  }\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_id = omp_get_thread_num();\n    int start_index = thread_id * num_pixels_per_thread;\n    int end_index = start_index + num_pixels_per_thread;\n\n    if (thread_id == num_threads - 1) {\n      end_index += num_pixels_remainder;\n    }\n\n    for (int i = start_index; i < end_index; i++) {\n      local_image[i - start_index] = image[i];\n    }\n\n    for (int i = 0; i < 256; i++) {\n      local_bins[i] = 0;\n    }\n\n    for (int i = 0; i < num_pixels_per_thread; i++) {\n      local_bins[local_image[i]]++;\n    }\n\n    if (rank == 0) {\n      for (int i = 0; i < 256; i++) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i].size() > 0) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a'] += 1;\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: your code here\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    // TODO: Your code here\n    // You may use the following variables:\n    // - `s`\n    // - `bins`\n    // - `omp_get_thread_num()`\n    // - `omp_get_num_threads()`\n\n    //",
            "// TODO: Your code here\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: Implement this function\n    //...\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char first_letter = s[i][0];\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            bins[first_letter - 'a']++;\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            char c = str[0];\n            if (c >= 'a' && c <= 'z') {\n                bins[c - 'a']++;\n            }\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.length() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.length() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            auto letter = str[0];\n            if (letter >= 'a' && letter <= 'z') {\n                bins[letter - 'a']++;\n            }\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.length() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& word : s) {\n        if (word.size() > 0) {\n            bins[word[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "hip",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int firstLetter = tolower(s[idx][0]);\n    atomicAdd(&bins[firstLetter - 'a'], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int c = tolower(s[tid][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        char first = tolower(s[i][0]);\n        atomicAdd(&bins[first - 'a'], 1);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int firstLetter = tolower(s[tid][0]);\n    atomicAdd(&bins[firstLetter - 'a'], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    char c = s[idx][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = tolower(s[tid][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int firstLetter = tolower(s[tid][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char first = tolower(s[tid][0]);\n        if (first >= 'a' && first <= 'z') {\n            atomicAdd(&bins[first - 'a'], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    char c = s[idx][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        char c = tolower(s[i][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        char c = tolower(s[i][0]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "cuda",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        char c = s[idx][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    if (bid < N) {\n        char c = s[bid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int firstLetter = tolower(s[i][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int firstLetter = tolower(s[tid][0]);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        char first = tolower(s[idx][0]);\n        if (first >= 'a' && first <= 'z') {\n            atomicAdd(&bins[first - 'a'], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int firstLetter = tolower(s[idx][0]);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        char c = tolower(s[idx][0]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int letter = tolower(s[idx][0]) - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "// TODO\n}",
            "int idx = threadIdx.x;\n    if (idx < N) {\n        char first = s[idx][0];\n        if (first >= 'a' && first <= 'z') {\n            atomicAdd(&bins[first - 'a'], 1);\n        }\n    }\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<std::string> local_s;\n    if (rank == 0) {\n        local_s = s;\n    }\n\n    MPI_Bcast(&local_s[0], local_s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_s.size(); i++) {\n        char c = local_s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            local_bins[c - 'a']++;\n        }\n    }\n\n    std::array<size_t, 26> global_bins;\n    global_bins.fill(0);\n\n    MPI_Reduce(&local_bins[0], &global_bins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = s.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n\n    if (rank == 0) {\n        end += n_remainder;\n    }\n\n    std::array<size_t, 26> local_bins;\n    for (int i = 0; i < 26; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = start; i < end; i++) {\n        local_bins[s[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int num_threads, rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (auto const& str : s) {\n            bins[str[0] - 'a']++;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> temp;\n            MPI_Recv(temp.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += temp[j];\n            }\n        }\n    } else {\n        std::array<size_t, 26> temp;\n        std::fill(temp.begin(), temp.end(), 0);\n        for (auto const& str : s) {\n            temp[str[0] - 'a']++;\n        }\n        MPI_Send(temp.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<std::string> local_s;\n    if (rank == 0) {\n        local_s = s;\n    }\n\n    MPI_Bcast(&local_s.size(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&local_s[0], local_s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_s.size(); ++i) {\n        local_bins[local_s[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(&local_bins[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    int num_per_thread = s.size() / num_threads;\n    int num_extra = s.size() % num_threads;\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int start = thread_id * num_per_thread;\n        int end = start + num_per_thread;\n        if (thread_id == num_threads - 1) {\n            end += num_extra;\n        }\n\n        for (int i = start; i < end; i++) {\n            char first_letter = s[i][0];\n            if (first_letter >= 'a' && first_letter <= 'z') {\n                local_bins[first_letter - 'a']++;\n            }\n        }\n    }\n\n    std::array<size_t, 26> global_bins;\n    global_bins.fill(0);\n\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement me!\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int bin = x[i] / 10;\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Fill in the code here\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement\n}",
            "// TODO: your code here\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(v / 10);\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(v / 10);\n        ++bins[bin];\n    }\n}",
            "for (auto const& v : x) {\n        int bin = static_cast<int>(v / 10);\n        if (bin < 0) {\n            bin = 0;\n        } else if (bin >= 10) {\n            bin = 9;\n        }\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        auto bin = static_cast<size_t>(std::floor(v / 10));\n        ++bins[bin];\n    }\n}",
            "for (auto const& v : x) {\n        auto bin = static_cast<size_t>(std::floor(v / 10.0));\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        int bin = std::floor(v / 10);\n        bins[bin]++;\n    }\n}",
            "for (double value : x) {\n        int bin = static_cast<int>(value / 10);\n        if (bin < 0) bin = 0;\n        if (bin > 9) bin = 9;\n        ++bins[bin];\n    }\n}",
            "for (auto const& val : x) {\n        auto bin = static_cast<size_t>(std::floor(val / 10.0));\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        int bin = static_cast<int>(v / 10);\n        if (bin >= 0 && bin < 10) {\n            ++bins[bin];\n        }\n    }\n}",
            "for (auto const& value : x) {\n        auto bin = static_cast<size_t>(value / 10);\n        if (bin < 10) {\n            ++bins[bin];\n        }\n    }\n}",
            "for (auto const& v : x) {\n        auto bin = static_cast<size_t>(std::floor(v / 10));\n        ++bins[bin];\n    }\n}",
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(v / 10);\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(std::floor(v / 10.0));\n        ++bins[bin];\n    }\n}",
            "for (auto const& val : x) {\n        auto bin = static_cast<size_t>(std::floor(val / 10.0));\n        bins[bin]++;\n    }\n}",
            "for (auto const& val : x) {\n        auto bin = static_cast<size_t>(std::floor(val / 10));\n        ++bins[bin];\n    }\n}",
            "for (auto const& value : x) {\n        auto bin = static_cast<size_t>(value / 10);\n        if (bin < 10) {\n            ++bins[bin];\n        }\n    }\n}",
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(std::floor(v / 10));\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        auto bin = static_cast<size_t>(v / 10);\n        bins[bin]++;\n    }\n}",
            "for (auto const& v : x) {\n        int bin = static_cast<int>(v / 10);\n        if (bin >= 0 && bin < 10) {\n            ++bins[bin];\n        }\n    }\n}",
            "for (auto const& v : x) {\n        auto const bin = static_cast<size_t>(v / 10);\n        ++bins[bin];\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "hip",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n}",
            "// TODO: Implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int bin = (int)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = (size_t)x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t bin = (size_t) (x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = floor(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = (size_t)x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nt = gridDim.x * blockDim.x;\n  int i = bid * nt + tid;\n  if (i < N) {\n    int bin = (int)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = (size_t) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  size_t bin = (size_t)x[tid] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        int bin = (int) (x[tid] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: your code here\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "cuda",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = (size_t) (x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int bin = (int)x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this kernel\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = (int)x[tid] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nt = blockDim.x;\n  int nb = gridDim.x;\n\n  // TODO: Implement this function\n  __syncthreads();\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int bin = (int) (x[idx] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Implement this kernel\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: your code here\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over the vector x\n    // Hint: Use the modulo operator to determine the bin for each element\n    // Hint: Use the floor function to get the integer part of a number\n    // Hint: Use the if-else statement to increment the correct bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint: Use the atomic keyword to increment the bin\n    // Hint:",
            "// TODO\n}",
            "// TODO: Implement this function\n    bins = {0, 0, 0, 0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double fraction = x[i] - floor(x[i]);\n        if (fraction < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fraction < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fraction < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use the `omp_get_thread_num()` function to determine the thread ID\n    // Hint: Use the `omp_get_num_threads()` function to determine the number of threads\n    // Hint: Use the `omp_get_num_procs()` function to determine the number of processors\n    // Hint: Use the `omp_get_max_threads()` function to determine the maximum number of threads\n    // Hint: Use the `omp_get_dynamic()` function to determine whether dynamic thread adjustment is enabled\n    // Hint: Use the `omp_get_nested()` function to determine whether nested parallelism is enabled\n    // Hint: Use the `omp_in_parallel()` function to determine whether the code is currently executing in parallel\n    // Hint: Use the `omp_set_dynamic()` function to enable/disable dynamic thread adjustment\n    // Hint: Use the `omp_set_nested()` function to enable/disable nested parallelism\n    // Hint: Use the `omp_set_num_threads()` function to set the number of threads\n    // Hint: Use the `omp_get_num_threads()` function to determine the number of threads\n    // Hint: Use the `omp_get_thread_num()` function to determine the thread ID\n    // Hint: Use the `omp_get_max_threads()` function to determine the maximum number of threads\n    // Hint: Use the `omp_get_num_procs()` function to determine the number of processors\n    // Hint: Use the `omp_get_dynamic()` function to determine whether dynamic thread adjustment is enabled\n    // Hint: Use the `omp_get_nested()` function to determine whether nested parallelism is enabled\n    // Hint: Use the `omp_in_parallel()` function to determine whether the code is currently executing in parallel\n    // Hint: Use the `omp_set_dynamic()` function to enable/disable dynamic thread adjustment\n    // Hint: Use the `omp_set_nested()` function to enable/disable nested parallelism\n    // Hint: Use the `omp_set_num_threads()` function to set the number of threads\n    // Hint: Use the `omp_get_num_threads()` function to determine the number of threads\n    // Hint: Use the `omp_get_thread_num()` function to determine the thread ID\n    // Hint: Use the `omp_get_max_threads()` function to determine the maximum number of threads\n    // Hint: Use the `omp_get_num_procs()` function to determine the number of processors\n    // Hint: Use the `omp_get_dynamic()` function to determine whether dynamic thread adjustment is enabled\n    // Hint: Use the `omp_get_nested()` function to determine whether nested parallelism is enabled\n    // Hint: Use the `omp_in_parallel()` function to determine whether the code is currently executing in parallel\n    // Hint: Use the `omp_set_dynamic()` function to enable/disable dynamic thread adjustment\n    // Hint: Use the `omp_set_nested()` function to enable/disable nested parallelism\n    // Hint: Use the `omp_set_num_threads()` function to set the number of threads\n    // Hint: Use the `omp_get_num_threads()` function to determine the number of threads\n    // Hint: Use the `omp_get_thread_num()` function to determine the thread ID\n    // Hint: Use the `omp_get_max_threads()` function to determine the maximum number of threads\n    // Hint: Use the `omp_get_num_procs()` function to determine the number of processors\n    // Hint: Use the `omp_get_dynamic()` function to determine whether dynamic thread adjustment is enabled\n    // Hint: Use the `omp_get_nested()` function to determine whether nested parallelism is enabled\n    // Hint: Use the `omp_in_parallel()` function to determine whether the code is currently executing in parallel\n    // Hint: Use the `omp_set_dynamic()` function to enable/disable dynamic thread adjustment\n    // Hint: Use the `omp_set_nested()` function to enable/disable nested parallelism\n    // Hint: Use the `omp_set_num_threads()` function to set the number of threads\n    // Hint: Use the `omp_get_num_threads()` function to determine the number of threads\n    // Hint: Use the `omp_get_thread_",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running\n    // Hint: Use the `omp_get_thread_num()` function to determine which thread is running\n    // Hint: Use the `omp_get_num_procs()` function to determine how many processors are available\n    // Hint: Use the `omp_get_num_threads()` function to determine how many threads are running",
            "// TODO: Fill in the code to compute the bins\n    // Hint: You can use the std::lower_bound function to find the indices of the\n    //       elements in x that are greater than or equal to a given value.\n    //       std::lower_bound(x.begin(), x.end(), 0.25) returns an iterator to the\n    //       first element in x that is greater than or equal to 0.25.\n    //       std::lower_bound(x.begin(), x.end(), 0.75) returns an iterator to the\n    //       first element in x that is greater than or equal to 0.75.\n    //       std::distance(x.begin(), it) returns the number of elements in x\n    //       between the beginning of x and the iterator it.\n    //       std::fill(bins.begin(), bins.end(), 0) sets all elements of bins to 0.\n\n    // TODO: Use OpenMP to parallelize the loop over the elements of x\n    // Hint: You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads.\n    //       You can use the omp_get_thread_num() function to get the thread number\n    //       and the omp_get_num_threads() function to get the number of threads",
            "// TODO: Implement this function\n    // Hint: Use the omp_get_thread_num() function to get the thread number\n    // Hint: Use the omp_get_num_threads() function to get the number of threads\n    // Hint: Use the omp_get_num_procs() function to get the number of processors\n    // Hint: Use the omp_get_max_threads() function to get the maximum number of threads\n    // Hint: Use the omp_get_dynamic() function to get the dynamic thread adjustment\n    // Hint: Use the omp_get_nested() function to get the nested parallelism\n    // Hint: Use the omp_in_parallel() function to check if the code is running in parallel\n    // Hint: Use the omp_set_dynamic() function to set the dynamic thread adjustment\n    // Hint: Use the omp_set_nested() function to set the nested parallelism\n    // Hint: Use the omp_set_num_threads() function to set the number of threads\n    // Hint: Use the omp_set_schedule() function to set the schedule type\n    // Hint: Use the omp_get_schedule() function to get the schedule type\n    // Hint: Use the omp_get_thread_limit() function to get the thread limit\n    // Hint: Use the omp_set_thread_limit() function to set the thread limit\n    // Hint: Use the omp_get_wtick() function to get the time in seconds of a clock tick\n    // Hint: Use the omp_get_wtime() function to get the wall clock time in seconds\n    // Hint: Use the omp_get_cancellation() function to get the cancellation status\n    // Hint: Use the omp_set_cancel() function to set the cancellation status\n    // Hint: Use the omp_test_cancel() function to test the cancellation status\n    // Hint: Use the omp_cancellation_point() function to set a cancellation point\n    // Hint: Use the omp_get_thread_num() function to get the thread number\n    // Hint: Use the omp_get_num_threads() function to get the number of threads\n    // Hint: Use the omp_get_num_procs() function to get the number of processors\n    // Hint: Use the omp_get_max_threads() function to get the maximum number of threads\n    // Hint: Use the omp_get_dynamic() function to get the dynamic thread adjustment\n    // Hint: Use the omp_get_nested() function to get the nested parallelism\n    // Hint: Use the omp_in_parallel() function to check if the code is running in parallel\n    // Hint: Use the omp_set_dynamic() function to set the dynamic thread adjustment\n    // Hint: Use the omp_set_nested() function to set the nested parallelism\n    // Hint: Use the omp_set_num_threads() function to set the number of threads\n    // Hint: Use the omp_set_schedule() function to set the schedule type\n    // Hint: Use the omp_get_schedule() function to get the schedule type\n    // Hint: Use the omp_get_thread_limit() function to get the thread limit\n    // Hint: Use the omp_set_thread_limit() function to set the thread limit\n    // Hint: Use the omp_get_wtick() function to get the time in seconds of a clock tick\n    // Hint: Use the omp_get_wtime() function to get the wall clock time in seconds\n    // Hint: Use the omp_get_cancellation() function to get the cancellation status\n    // Hint: Use the omp_set_cancel() function to set the cancellation status\n    // Hint: Use the omp_test_cancel() function to test the cancellation status\n    // Hint: Use the omp_cancellation_point() function to set a cancellation point\n    // Hint: Use the omp_get_thread_num() function to get the thread number\n    // Hint: Use the omp_get_num_threads() function to get the number of threads\n    // Hint: Use the omp_get_num_procs() function to get the number of processors\n    // H",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (frac < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (frac < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    } else {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: implement\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (frac < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (frac < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "for (auto i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& i : x) {\n        if (i >= 0 && i < 0.25) {\n            bins[0]++;\n        } else if (i >= 0.25 && i < 0.5) {\n            bins[1]++;\n        } else if (i >= 0.5 && i < 0.75) {\n            bins[2]++;\n        } else if (i >= 0.75 && i < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto const& i : x) {\n    if (i >= 0 && i < 0.25) {\n      bins[0]++;\n    } else if (i >= 0.25 && i < 0.5) {\n      bins[1]++;\n    } else if (i >= 0.5 && i < 0.75) {\n      bins[2]++;\n    } else if (i >= 0.75 && i < 1) {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto const& i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        } else if (i < 0.5) {\n            bins[1]++;\n        } else if (i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "for (auto const& xi : x) {\n        auto const q = std::floor(xi * 4);\n        bins[q]++;\n    }\n}",
            "// TODO: Implement this function\n}",
            "for (auto const& i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        } else if (i < 0.5) {\n            bins[1]++;\n        } else if (i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto i : x) {\n    if (i < 0.25) {\n      bins[0]++;\n    } else if (i < 0.5) {\n      bins[1]++;\n    } else if (i < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto const& val : x) {\n        if (val < 0.25) {\n            bins[0]++;\n        } else if (val < 0.5) {\n            bins[1]++;\n        } else if (val < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        } else if (i < 0.5) {\n            bins[1]++;\n        } else if (i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        } else if (i < 0.5) {\n            bins[1]++;\n        } else if (i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& i : x) {\n    if (i < 0.25) {\n      bins[0]++;\n    } else if (i < 0.5) {\n      bins[1]++;\n    } else if (i < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double xi = x[i];\n    int bin = 0;\n    if (xi >= 0 && xi < 0.25)\n      bin = 0;\n    else if (xi >= 0.25 && xi < 0.5)\n      bin = 1;\n    else if (xi >= 0.5 && xi < 0.75)\n      bin = 2;\n    else if (xi >= 0.75 && xi < 1)\n      bin = 3;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (frac < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (frac < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double frac = x[tid] - floor(x[tid]);\n    if (frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double xi = x[i];\n    size_t bin = 0;\n    if (xi < 0.25) {\n      bin = 0;\n    } else if (xi < 0.5) {\n      bin = 1;\n    } else if (xi < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double frac = x[tid] - floor(x[tid]);\n    if (frac < 0.25) bins[0]++;\n    else if (frac < 0.5) bins[1]++;\n    else if (frac < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "// TODO: Implement\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = 0;\n    if (x[tid] < 0.25) {\n      bin = 0;\n    } else if (x[tid] < 0.5) {\n      bin = 1;\n    } else if (x[tid] < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double xi = x[i];\n        double frac = xi - floor(xi);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = 0;\n    if (x[tid] < 0.25) {\n      bin = 0;\n    } else if (x[tid] < 0.5) {\n      bin = 1;\n    } else if (x[tid] < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO\n\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double xi = x[i];\n    if (xi < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (xi < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (xi < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int q = (int) (x[i] * 4);\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (frac < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (frac < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double xi = x[i];\n        int q = (xi - floor(xi)) / 0.25;\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    std::vector<double> x_rank(n_per_rank);\n    std::vector<size_t> bins_rank(4);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            x_rank[i] = x[i];\n        }\n    } else {\n        MPI_Recv(&x_rank[0], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_per_rank; i++) {\n            if (x_rank[i] < 0.25) {\n                bins_rank[0]++;\n            } else if (x_rank[i] < 0.5) {\n                bins_rank[1]++;\n            } else if (x_rank[i] < 0.75) {\n                bins_rank[2]++;\n            } else {\n                bins_rank[3]++;\n            }\n        }\n    } else {\n        for (int i = 0; i < n_per_rank; i++) {\n            if (x_rank[i] < 0.25) {\n                bins_rank[0]++;\n            } else if (x_rank[i] < 0.5) {\n                bins_rank[1]++;\n            } else if (x_rank[i] < 0.75) {\n                bins_rank[2]++;\n            } else {\n                bins_rank[3]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins_rank[0], 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += bins_rank[j];\n            }\n        }\n    } else {\n        MPI_Send(&bins_rank[0], 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// Your code here\n}",
            "// TODO: implement this function\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: count the number of points in each quadrant\n   // Hint: use the modulus operator (%) to determine the quadrant\n   // Hint: use the floor function (std::floor) to convert a double to an integer\n   // Hint: use the OpenMP library to parallelize the loop\n   // Hint: use the omp_get_thread_num() function to get the thread number\n   // Hint: use the omp_get_num_threads() function to get the number of threads\n   // Hint: use the omp_get_num_procs() function to get the number of processors\n   // Hint: use the omp_get_max_threads() function to get the maximum number of threads\n   // Hint: use the omp_get_dynamic() function to get the dynamic thread adjustment\n   // Hint: use the omp_get_nested() function to get the nested parallelism\n   // Hint: use the omp_get_cancellation() function to get the cancellation\n   // Hint: use the omp_get_thread_limit() function to get the thread limit\n   // Hint: use the omp_in_parallel() function to check if the code is running in parallel\n   // Hint: use the omp_set_num_threads() function to set the number of threads\n   // Hint: use the omp_set_dynamic() function to set the dynamic thread adjustment\n   // Hint: use the omp_set_nested() function to set the nested parallelism\n   // Hint: use the omp_set_cancellation() function to set the cancellation\n   // Hint: use the omp_set_thread_limit() function to set the thread limit\n   // Hint: use the omp_set_schedule() function to set the schedule type\n   // Hint: use the omp_set_max_active_levels() function to set the maximum active parallelism level\n   // Hint: use the omp_get_schedule() function to get the schedule type\n   // Hint: use the omp_get_max_active_levels() function to get the maximum active parallelism level\n   // Hint: use the omp_get_proc_bind() function to get the process binding\n   // Hint: use the omp_set_proc_bind() function to set the process binding\n   // Hint: use the omp_get_num_places() function to get the number of places\n   // Hint: use the omp_get_place_num_procs() function to get the number of processors in a place\n   // Hint: use the omp_get_place_proc_ids() function to get the processor ids in a place\n   // Hint: use the omp_get_place_num() function to get the place number\n   // Hint: use the omp_get_partition_num_places() function to get the number of places in a partition\n   // Hint: use the omp_get_partition_place_nums() function to get the place numbers in a partition\n   // Hint: use the omp_get_default_device() function to get the default device\n   // Hint: use the omp_set_default_device() function to set the default device\n   // Hint: use the omp_get_num_devices() function to get the number of devices\n   // Hint: use the omp_get_device_num() function to get the device number\n   // Hint: use the omp_get_num_teams() function to get the number of teams\n   // Hint: use the omp_get_team_num() function to get the team number\n   // Hint: use the omp_is_initial_device() function to check if the device is the initial device\n   // Hint: use the omp_get_initial_device() function to get the initial device\n   // Hint: use the omp_get_max_task_priority() function to get the maximum task priority\n   // Hint: use the omp_get_num_tasks() function to get the number of tasks\n   // Hint: use the omp_get_task_id() function to get the task id\n   // Hint: use the omp_get_task_num() function to get the task number\n   // Hint: use the omp_get_thread_limit() function to get the thread limit",
            "// TODO: implement this function\n}",
            "// TODO: Fill in the code to count the number of points in each quadrant\n   // Hint: You can use the `omp_get_thread_num()` function to get the thread ID\n   //       and use it to index the `bins` array\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: implement\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      int q = (points[i].x > 0) + (points[i].y > 0) * 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = 0;\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            q = 0;\n         } else {\n            q = 1;\n         }\n      } else {\n         if (points[i].y > 0) {\n            q = 2;\n         } else {\n            q = 3;\n         }\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   size_t bin = 0;\n   if (points[i].x > 0) {\n      if (points[i].y > 0) bin = 0;\n      else bin = 1;\n   } else {\n      if (points[i].y > 0) bin = 2;\n      else bin = 3;\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) return;\n   size_t bin = 0;\n   if (points[i].x > 0) bin += 1;\n   if (points[i].y > 0) bin += 2;\n   atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      Point p = points[idx];\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x >= 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x >= 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = 0;\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            q = 0;\n         } else {\n            q = 1;\n         }\n      } else {\n         if (points[i].y > 0) {\n            q = 2;\n         } else {\n            q = 3;\n         }\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   size_t bin = 0;\n   if (points[i].x > 0) bin += 1;\n   if (points[i].y > 0) bin += 2;\n\n   atomicAdd(&bins[bin], 1);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n   Point p = points[tid];\n   if (p.x >= 0 && p.y >= 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (p.x >= 0 && p.y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   Point p = points[i];\n   if (p.x >= 0 && p.y >= 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (p.x >= 0 && p.y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) return;\n   size_t bin = 0;\n   if (points[i].x > 0) bin += 1;\n   if (points[i].y > 0) bin += 2;\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   size_t bin = 0;\n   if (points[i].x > 0) bin += 1;\n   if (points[i].y > 0) bin += 2;\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (tid < N) {\n      if (points[tid].x >= 0 && points[tid].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0 && points[tid].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[tid].x >= 0 && points[tid].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   Point p = points[idx];\n   if (p.x >= 0 && p.y >= 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (p.x >= 0 && p.y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      size_t bin = 0;\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            bin = 0;\n         } else {\n            bin = 1;\n         }\n      } else {\n         if (points[i].y > 0) {\n            bin = 2;\n         } else {\n            bin = 3;\n         }\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: your code here\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n\n   Point p = points[idx];\n   int q = 0;\n   if (p.x >= 0 && p.y >= 0) q = 0;\n   else if (p.x < 0 && p.y >= 0) q = 1;\n   else if (p.x < 0 && p.y < 0) q = 2;\n   else if (p.x >= 0 && p.y < 0) q = 3;\n   atomicAdd(&bins[q], 1);\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = (points[i].x >= 0) + (points[i].y >= 0) * 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int q = 0;\n        if (points[idx].x > 0) {\n            q += 1;\n        }\n        if (points[idx].y > 0) {\n            q += 2;\n        }\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      int q = 0;\n      if (points[idx].x > 0) {\n         q += 1;\n      }\n      if (points[idx].y > 0) {\n         q += 2;\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n\n   if (points[idx].x >= 0 && points[idx].y >= 0) bins[0]++;\n   else if (points[idx].x < 0 && points[idx].y >= 0) bins[1]++;\n   else if (points[idx].x < 0 && points[idx].y < 0) bins[2]++;\n   else if (points[idx].x >= 0 && points[idx].y < 0) bins[3]++;\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n\n   int x = points[idx].x;\n   int y = points[idx].y;\n   int q = 0;\n   if (x > 0 && y > 0) q = 1;\n   if (x < 0 && y > 0) q = 2;\n   if (x < 0 && y < 0) q = 3;\n   atomicAdd(&bins[q], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = 0;\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            q = 0;\n         } else {\n            q = 1;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            q = 2;\n         } else {\n            q = 3;\n         }\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = (points[i].x > 0) + (points[i].y > 0) * 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n\n   int q = 0;\n   if (points[idx].x > 0) q += 1;\n   if (points[idx].y > 0) q += 2;\n\n   atomicAdd(&bins[q], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   int q = 0;\n   if (points[idx].x > 0) q += 1;\n   if (points[idx].y > 0) q += 2;\n   atomicAdd(&bins[q], 1);\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   Point p = points[idx];\n   if (p.x >= 0 && p.y >= 0) bins[0]++;\n   else if (p.x < 0 && p.y >= 0) bins[1]++;\n   else if (p.x < 0 && p.y < 0) bins[2]++;\n   else if (p.x >= 0 && p.y < 0) bins[3]++;\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: your code here\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, std::vector<const int*> const& y, int val) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Your code here\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "int x_contains = 0;\n  int y_contains = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lsum) {\n      if (x(i) == val) {\n        lsum += 1;\n      }\n    },\n    x_contains);\n  for (auto y_vec : y) {\n    int y_vec_contains = 0;\n    Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, y_vec->extent(0)),\n      KOKKOS_LAMBDA(int i, int& lsum) {\n        if ((*y_vec)(i) == val) {\n          lsum += 1;\n        }\n      },\n      y_vec_contains);\n    y_contains += y_vec_contains;\n  }\n  return x_contains ^ y_contains;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "int x_contains = 0;\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int& lsum) {\n      if (x(i) == val) {\n        lsum += 1;\n      }\n    },\n    x_contains\n  );\n\n  int y_contains = 0;\n  for (auto y_vec : y) {\n    Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, y_vec->extent(0)),\n      KOKKOS_LAMBDA(int i, int& lsum) {\n        if ((*y_vec)(i) == val) {\n          lsum += 1;\n        }\n      },\n      y_contains\n    );\n  }\n\n  return x_contains ^ y_contains;\n}",
            "// TODO: Implement this function\n  return false;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            #pragma omp critical\n            found =!found;\n        }\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            #pragma omp critical\n            found =!found;\n        }\n    }\n    return found;\n}",
            "bool result = false;\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp task\n            {\n                for (int i = 0; i < x.size(); ++i) {\n                    if (x[i] == val) {\n                        result = true;\n                    }\n                }\n            }\n\n            #pragma omp task\n            {\n                for (int i = 0; i < y.size(); ++i) {\n                    if (y[i] == val) {\n                        result = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return result;\n}",
            "bool contains = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == val) {\n      contains =!contains;\n    }\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < y.size(); i++) {\n    if (y[i] == val) {\n      contains =!contains;\n    }\n  }\n  return contains;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        bool localResult = false;\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp for\n        for (int i = 0; i < y.size(); i++) {\n            if (y[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp critical\n        result = result || localResult;\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel\n  {\n    bool localResult = false;\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp for\n    for (int i = 0; i < y.size(); i++) {\n      if (y[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp critical\n    {\n      result = result || localResult;\n    }\n  }\n  return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        bool local_result = false;\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == val) {\n                local_result = true;\n            }\n        }\n        #pragma omp for\n        for (int i = 0; i < y.size(); i++) {\n            if (y[i] == val) {\n                local_result =!local_result;\n            }\n        }\n        #pragma omp critical\n        {\n            result = result || local_result;\n        }\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel\n  {\n    bool localResult = false;\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp for\n    for (int i = 0; i < y.size(); i++) {\n      if (y[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp critical\n    {\n      result = result || localResult;\n    }\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel\n  {\n    bool local_result = false;\n    #pragma omp for\n    for (int i = 0; i < x.size(); ++i) {\n      if (x[i] == val) {\n        local_result =!local_result;\n      }\n    }\n    #pragma omp for\n    for (int i = 0; i < y.size(); ++i) {\n      if (y[i] == val) {\n        local_result =!local_result;\n      }\n    }\n    #pragma omp critical\n    result = result || local_result;\n  }\n  return result;\n}",
            "bool xContains = false;\n  bool yContains = false;\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == val) {\n        xContains = true;\n      }\n    }\n\n    #pragma omp for\n    for (int i = 0; i < y.size(); i++) {\n      if (y[i] == val) {\n        yContains = true;\n      }\n    }\n  }\n\n  return (xContains &&!yContains) || (!xContains && yContains);\n}",
            "bool found = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] == val) {\n      #pragma omp critical\n      found =!found;\n    }\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < y.size(); ++i) {\n    if (y[i] == val) {\n      #pragma omp critical\n      found =!found;\n    }\n  }\n  return found;\n}",
            "bool result = false;\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            int n = x.size();\n            int m = y.size();\n            int i = 0;\n            int j = 0;\n            while (i < n && j < m) {\n                if (x[i] < y[j]) {\n                    if (x[i] == val) {\n                        result = true;\n                    }\n                    i++;\n                } else if (y[j] < x[i]) {\n                    if (y[j] == val) {\n                        result = true;\n                    }\n                    j++;\n                } else {\n                    i++;\n                    j++;\n                }\n            }\n            while (i < n) {\n                if (x[i] == val) {\n                    result = true;\n                }\n                i++;\n            }\n            while (j < m) {\n                if (y[j] == val) {\n                    result = true;\n                }\n                j++;\n            }\n        }\n    }\n\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        bool localResult = false;\n        #pragma omp for\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp for\n        for (int i = 0; i < y.size(); ++i) {\n            if (y[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp critical\n        result = result || localResult;\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp task\n            {\n                for (int i : x) {\n                    if (i == val) {\n                        result = true;\n                    }\n                }\n            }\n            #pragma omp task\n            {\n                for (int i : y) {\n                    if (i == val) {\n                        result =!result;\n                    }\n                }\n            }\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            int nthreads = omp_get_num_threads();\n            std::cout << \"Using \" << nthreads << \" threads.\\n\";\n        }\n        #pragma omp for reduction(^:result)\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] == val) {\n                result =!result;\n            }\n        }\n        #pragma omp for reduction(^:result)\n        for (int i = 0; i < y.size(); ++i) {\n            if (y[i] == val) {\n                result =!result;\n            }\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        bool localResult = false;\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp for\n        for (int i = 0; i < y.size(); i++) {\n            if (y[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp critical\n        result = result || localResult;\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            int nthreads = omp_get_num_threads();\n            int tid = omp_get_thread_num();\n            int n = x.size();\n            int m = y.size();\n            int chunk = (n + m) / nthreads;\n            int start = tid * chunk;\n            int end = (tid + 1) * chunk;\n            if (end > n) end = n;\n            if (end > m) end = m;\n            for (int i = start; i < end; i++) {\n                if (i < n) {\n                    if (x[i] == val) {\n                        result = true;\n                        break;\n                    }\n                }\n                if (i < m) {\n                    if (y[i] == val) {\n                        result = true;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            #pragma omp critical\n            result =!result;\n        }\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            #pragma omp critical\n            result =!result;\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        bool localResult = false;\n        #pragma omp for\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] == val) {\n                localResult = true;\n            }\n        }\n        #pragma omp for\n        for (int i = 0; i < y.size(); ++i) {\n            if (y[i] == val) {\n                localResult =!localResult;\n            }\n        }\n        #pragma omp critical\n        {\n            result = result || localResult;\n        }\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel\n  {\n    bool localResult = false;\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp for\n    for (int i = 0; i < y.size(); i++) {\n      if (y[i] == val) {\n        localResult =!localResult;\n      }\n    }\n    #pragma omp critical\n    {\n      result = result || localResult;\n    }\n  }\n  return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            int nthreads = omp_get_num_threads();\n            std::vector<int> x_thread(nthreads);\n            std::vector<int> y_thread(nthreads);\n            #pragma omp for\n            for (int i = 0; i < x.size(); i++) {\n                int tid = omp_get_thread_num();\n                if (x[i] == val) {\n                    x_thread[tid] = 1;\n                }\n                if (y[i] == val) {\n                    y_thread[tid] = 1;\n                }\n            }\n            for (int i = 0; i < nthreads; i++) {\n                if (x_thread[i] == 1 && y_thread[i] == 0) {\n                    result = true;\n                    break;\n                }\n                if (x_thread[i] == 0 && y_thread[i] == 1) {\n                    result = true;\n                    break;\n                }\n            }\n        }\n    }\n    return result;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "serial",
        "prompt": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "int x_count = 0;\n    int y_count = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            x_count++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            y_count++;\n        }\n    }\n    if (x_count == 0 && y_count == 1) {\n        return true;\n    } else if (x_count == 1 && y_count == 0) {\n        return true;\n    } else {\n        return false;\n    }\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "bool xContains = std::find(x.begin(), x.end(), val)!= x.end();\n    bool yContains = std::find(y.begin(), y.end(), val)!= y.end();\n    return xContains!= yContains;\n}",
            "int xCount = 0;\n    int yCount = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            xCount++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            yCount++;\n        }\n    }\n    if (xCount == 0 && yCount == 1) {\n        return true;\n    } else if (xCount == 1 && yCount == 0) {\n        return true;\n    } else {\n        return false;\n    }\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "int xCount = 0;\n    int yCount = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            xCount++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            yCount++;\n        }\n    }\n    if (xCount == 0 && yCount == 1) {\n        return true;\n    } else if (xCount == 1 && yCount == 0) {\n        return true;\n    } else {\n        return false;\n    }\n}",
            "// TODO: implement this function\n    return false;\n}",
            "bool xContains = std::find(x.begin(), x.end(), val)!= x.end();\n    bool yContains = std::find(y.begin(), y.end(), val)!= y.end();\n    return xContains!= yContains;\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "return (std::find(x.begin(), x.end(), val)!= x.end()) ^ (std::find(y.begin(), y.end(), val)!= y.end());\n}",
            "bool xContains = std::find(x.begin(), x.end(), val)!= x.end();\n    bool yContains = std::find(y.begin(), y.end(), val)!= y.end();\n    return xContains!= yContains;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "hip",
        "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == val ^ y[tid] == val) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val) {\n            *found =!(*found);\n        }\n        if (y[tid] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val) {\n            *found =!(*found);\n        }\n        if (y[tid] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found = true;\n        }\n        if (y[i] == val) {\n            *found = false;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == val || y[i] == val) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val ^ y[tid] == val) {\n            *found = true;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val || y[tid] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val) {\n      *found =!(*found);\n    }\n    if (y[tid] == val) {\n      *found =!(*found);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val ^ y[tid] == val) {\n            *found = true;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val ^ y[tid] == val) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val || y[tid] == val) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!*found;\n        }\n        if (y[i] == val) {\n            *found =!*found;\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == val) {\n            *found =!(*found);\n        }\n        if (y[tid] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == val || y[i] == val) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val ^ y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val) {\n      *found =!(*found);\n    }\n    if (y[tid] == val) {\n      *found =!(*found);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val) {\n      *found =!(*found);\n    }\n    if (y[tid] == val) {\n      *found =!(*found);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == val) {\n      *found = true;\n    }\n    if (y[tid] == val) {\n      *found = false;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int x_size = x.size();\n    int y_size = y.size();\n\n    int x_start = rank * x_size / size;\n    int x_end = (rank + 1) * x_size / size;\n\n    int y_start = rank * y_size / size;\n    int y_end = (rank + 1) * y_size / size;\n\n    bool result = false;\n\n    for (int i = x_start; i < x_end; i++) {\n        if (x[i] == val) {\n            result =!result;\n        }\n    }\n\n    for (int i = y_start; i < y_end; i++) {\n        if (y[i] == val) {\n            result =!result;\n        }\n    }\n\n    int result_all;\n    MPI_Reduce(&result, &result_all, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_all;\n    }\n\n    return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n\n  return false;\n}",
            "// TODO: replace this line with your code\n  return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return false;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement this function\n    return false;\n}",
            "// TODO: implement\n    return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  bool result = false;\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  bool result = false;\n  return result;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    // Hint: You may want to use MPI_Reduce\n    // Hint: You may want to use MPI_LOR\n    // Hint: You may want to use MPI_BOR\n    // Hint: You may want to use MPI_BXOR\n    // Hint: You may want to use MPI_IN_PLACE\n    // Hint: You may want to use MPI_Comm_split\n    // Hint: You may want to use MPI_Comm_rank\n    // Hint: You may want to use MPI_Comm_size\n    // Hint: You may want to use MPI_Comm_free\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_COMM_TYPE_SHARED\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI_Comm_split_type\n    // Hint: You may want to use MPI",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement this function\n    return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n  bool result = false;\n  return result;\n}",
            "// TODO: implement\n    return false;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // Hint: You can use MPI_Reduce to implement this function\n  // Hint: You can use MPI_BOR to implement the XOR operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  // Hint: You can use MPI_LXOR to implement the XOR operation\n  // Hint: You can use MPI_LAND to implement the AND operation\n  // Hint: You can use MPI_LOR to implement the OR operation\n  //",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement\n    return false;\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int x_size = x.size();\n    int y_size = y.size();\n\n    int x_start = rank * x_size / size;\n    int x_end = (rank + 1) * x_size / size;\n    int y_start = rank * y_size / size;\n    int y_end = (rank + 1) * y_size / size;\n\n    bool result = false;\n\n    for (int i = x_start; i < x_end; i++) {\n        if (x[i] == val) {\n            result =!result;\n        }\n    }\n\n    for (int i = y_start; i < y_end; i++) {\n        if (y[i] == val) {\n            result =!result;\n        }\n    }\n\n    return result;\n}",
            "// TODO: implement this\n  return false;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement this function\n\n    return false;\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement\n    return false;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "cuda",
        "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            if (x[i]!= y[i]) {\n                *found = true;\n            }\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bool x_contains = x[i] == val;\n    bool y_contains = y[i] == val;\n    *found = x_contains ^ y_contains;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == val || y[i] == val) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found = true;\n        }\n        if (y[i] == val) {\n            *found = false;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found = true;\n        }\n        if (y[i] == val) {\n            *found = false;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found =!(*found);\n        }\n        if (y[i] == val) {\n            *found =!(*found);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] == val || y[idx] == val) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == val || y[i] == val) {\n      *found = true;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if ((x[i] == val)!= (y[i] == val)) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found = true;\n        }\n        if (y[i] == val) {\n            *found = false;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val || y[i] == val) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == val) {\n            *found = true;\n        }\n        if (y[i] == val) {\n            *found = false;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_xor_contains",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  return false;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  bool result = false;\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == val) {\n        result = true;\n        break;\n      }\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  if (result) {\n    return result;\n  }\n  if (rank == 0) {\n    for (int i = 0; i < y.size(); i++) {\n      if (y[i] == val) {\n        result = true;\n        break;\n      }\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  bool result = false;\n  if (rank == 0) {\n    // TODO: use MPI and OpenMP to compute the result\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return false;\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int x_size = x.size();\n  int y_size = y.size();\n  int x_chunk = x_size / size;\n  int y_chunk = y_size / size;\n  int x_rem = x_size % size;\n  int y_rem = y_size % size;\n  int x_start = rank * x_chunk + std::min(rank, x_rem);\n  int x_end = (rank + 1) * x_chunk + std::min(rank + 1, x_rem);\n  int y_start = rank * y_chunk + std::min(rank, y_rem);\n  int y_end = (rank + 1) * y_chunk + std::min(rank + 1, y_rem);\n  std::vector<int> x_chunk_vec(x.begin() + x_start, x.begin() + x_end);\n  std::vector<int> y_chunk_vec(y.begin() + y_start, y.begin() + y_end);\n  int x_chunk_size = x_chunk_vec.size();\n  int y_chunk_size = y_chunk_vec.size();\n  int x_chunk_rem = x_chunk_size % size;\n  int y_chunk_rem = y_chunk_size % size;\n  int x_chunk_start = rank * x_chunk_size / size + std::min(rank, x_chunk_rem);\n  int x_chunk_end = (rank + 1) * x_chunk_size / size + std::min(rank + 1, x_chunk_rem);\n  int y_chunk_start = rank * y_chunk_size / size + std::min(rank, y_chunk_rem);\n  int y_chunk_end = (rank + 1) * y_chunk_size / size + std::min(rank + 1, y_chunk_rem);\n  std::vector<int> x_chunk_vec_chunk(x_chunk_vec.begin() + x_chunk_start, x_chunk_vec.begin() + x_chunk_end);\n  std::vector<int> y_chunk_vec_chunk(y_chunk_vec.begin() + y_chunk_start, y_chunk_vec.begin() + y_chunk_end);\n  int x_chunk_vec_chunk_size = x_chunk_vec_chunk.size();\n  int y_chunk_vec_chunk_size = y_chunk_vec_chunk.size();\n  int x_chunk_vec_chunk_rem = x_chunk_vec_chunk_size % size;\n  int y_chunk_vec_chunk_rem = y_chunk_vec_chunk_size % size;\n  int x_chunk_vec_chunk_start = rank * x_chunk_vec_chunk_size / size + std::min(rank, x_chunk_vec_chunk_rem);\n  int x_chunk_vec_chunk_end = (rank + 1) * x_chunk_vec_chunk_size / size + std::min(rank + 1, x_chunk_vec_chunk_rem);\n  int y_chunk_vec_chunk_start = rank * y_chunk_vec_chunk_size / size + std::min(rank, y_chunk_vec_chunk_rem);\n  int y_chunk_vec_chunk_end = (rank + 1) * y_chunk_vec_chunk_size / size + std::min(rank + 1, y_chunk_vec_chunk_rem);\n  std::vector<int> x_chunk_vec_chunk_chunk(x_chunk_vec_chunk.begin() + x_chunk_vec_chunk_start, x_chunk_vec_chunk.begin() + x_chunk_vec_chunk_end);\n  std::vector<int> y_chunk_vec_chunk_chunk(y_chunk_vec_chunk.begin() + y_chunk_vec_chunk_start, y_chunk_vec_chunk.begin() + y_chunk_vec_chunk_end);\n  int x_chunk_vec_chunk",
            "bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<int> x_local(x.size() / size);\n  std::vector<int> y_local(y.size() / size);\n  int x_start = rank * x_local.size();\n  int y_start = rank * y_local.size();\n  for (int i = 0; i < x_local.size(); i++) {\n    x_local[i] = x[x_start + i];\n  }\n  for (int i = 0; i < y_local.size(); i++) {\n    y_local[i] = y[y_start + i];\n  }\n  int x_result = 0;\n  int y_result = 0;\n  #pragma omp parallel for reduction(+:x_result)\n  for (int i = 0; i < x_local.size(); i++) {\n    if (x_local[i] == val) {\n      x_result++;\n    }\n  }\n  #pragma omp parallel for reduction(+:y_result)\n  for (int i = 0; i < y_local.size(); i++) {\n    if (y_local[i] == val) {\n      y_result++;\n    }\n  }\n  int result_local = 0;\n  if (x_result == 0 && y_result == 1) {\n    result_local = 1;\n  } else if (x_result == 1 && y_result == 0) {\n    result_local = 1;\n  }\n  int result_global;\n  MPI_Reduce(&result_local, &result_global, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    if (result_global == 0) {\n      result = false;\n    } else if (result_global == 1) {\n      result = true;\n    } else {\n      result = false;\n    }\n  }\n  return result;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    return false;\n}",
            "// TODO: your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<int> x_part(x.size() / size);\n    std::vector<int> y_part(y.size() / size);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * x.size() / size], x.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&y[i * y.size() / size], y.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(&x_part[0], x.size() / size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&y_part[0], y.size() / size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x_part[0], x.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&y_part[0], y.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == val) {\n                result = true;\n                break;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < y.size(); i++) {\n            if (y[i] == val) {\n                result = false;\n                break;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < x_part.size(); i++) {\n            if (x_part[i] == val) {\n                result = true;\n                break;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < y_part.size(); i++) {\n            if (y_part[i] == val) {\n                result = false;\n                break;\n            }\n        }\n    }\n\n    return result;\n}",
            "// TODO: implement this\n  return false;\n}",
            "bool result = false;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_max_threads();\n  int num_per_thread = x.size() / num_threads;\n  int num_remainder = x.size() % num_threads;\n\n  std::vector<int> x_local(num_per_thread + (rank < num_remainder? 1 : 0));\n  std::vector<int> y_local(num_per_thread + (rank < num_remainder? 1 : 0));\n\n  if (rank < num_remainder) {\n    x_local = std::vector<int>(x.begin() + rank * (num_per_thread + 1),\n                               x.begin() + (rank + 1) * (num_per_thread + 1));\n    y_local = std::vector<int>(y.begin() + rank * (num_per_thread + 1),\n                               y.begin() + (rank + 1) * (num_per_thread + 1));\n  } else {\n    x_local = std::vector<int>(x.begin() + rank * num_per_thread,\n                               x.begin() + (rank + 1) * num_per_thread);\n    y_local = std::vector<int>(y.begin() + rank * num_per_thread,\n                               y.begin() + (rank + 1) * num_per_thread);\n  }\n\n  int num_local = x_local.size();\n\n  for (int i = 0; i < num_local; i++) {\n    if (x_local[i] == val) {\n      result = true;\n      break;\n    }\n  }\n\n  for (int i = 0; i < num_local; i++) {\n    if (y_local[i] == val) {\n      result =!result;\n      break;\n    }\n  }\n\n  bool result_global = false;\n  MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n  return result_global;\n}",
            "bool result = false;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  // You may assume that x and y are the same length\n  // You may assume that x and y are sorted\n  // You may assume that x and y are distinct\n  // You may assume that val is in either x or y\n  // You may assume that val is not in both x and y\n  // You may assume that val is not in neither x nor y\n\n  return result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int x_size = x.size();\n  int y_size = y.size();\n  int x_chunk = x_size / size;\n  int y_chunk = y_size / size;\n  int x_start = rank * x_chunk;\n  int y_start = rank * y_chunk;\n  int x_end = (rank == size - 1)? x_size : (rank + 1) * x_chunk;\n  int y_end = (rank == size - 1)? y_size : (rank + 1) * y_chunk;\n  bool result = false;\n  #pragma omp parallel for\n  for (int i = x_start; i < x_end; i++) {\n    if (x[i] == val) {\n      result = true;\n    }\n  }\n  #pragma omp parallel for\n  for (int i = y_start; i < y_end; i++) {\n    if (y[i] == val) {\n      result =!result;\n    }\n  }\n  bool result_all;\n  MPI_Allreduce(&result, &result_all, 1, MPI_C_BOOL, MPI_LOR, MPI_COMM_WORLD);\n  return result_all;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return false;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int x_size = x.size();\n    int y_size = y.size();\n\n    int x_start = rank * x_size / size;\n    int y_start = rank * y_size / size;\n\n    int x_end = (rank + 1) * x_size / size;\n    int y_end = (rank + 1) * y_size / size;\n\n    bool result = false;\n\n    #pragma omp parallel for\n    for (int i = x_start; i < x_end; i++) {\n        if (x[i] == val) {\n            result =!result;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = y_start; i < y_end; i++) {\n        if (y[i] == val) {\n            result =!result;\n        }\n    }\n\n    bool result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return result_global;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return false;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    return false;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    bool result = false;\n\n    return result;\n}",
            "bool result = false;\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return result;\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<int> x_chunk(x.begin() + start, x.begin() + end);\n  std::vector<int> y_chunk(y.begin() + start, y.begin() + end);\n  int x_count = 0;\n  int y_count = 0;\n  #pragma omp parallel for reduction(+:x_count)\n  for (int i = 0; i < x_chunk.size(); i++) {\n    if (x_chunk[i] == val) {\n      x_count++;\n    }\n  }\n  #pragma omp parallel for reduction(+:y_count)\n  for (int i = 0; i < y_chunk.size(); i++) {\n    if (y_chunk[i] == val) {\n      y_count++;\n    }\n  }\n  if (x_count == 1 && y_count == 0) {\n    result = true;\n  } else if (x_count == 0 && y_count == 1) {\n    result = true;\n  } else {\n    result = false;\n  }\n  int result_all;\n  MPI_Allreduce(&result, &result_all, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);\n  return result_all;\n}",
            "bool result = false;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    std::vector<int> x_chunk(x.begin() + start, x.begin() + end);\n    std::vector<int> y_chunk(y.begin() + start, y.begin() + end);\n    #pragma omp parallel for\n    for (int i = 0; i < x_chunk.size(); i++) {\n        if (x_chunk[i] == val) {\n            result = true;\n        }\n        if (y_chunk[i] == val) {\n            result = false;\n        }\n    }\n    int result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n    return result_global;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the first even number in the vector x.\n   Use Kokkos to parallelize the search. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(Kokkos::View<const int*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::MemberType<PolicyType>;\n\n  // Create a Kokkos view to hold the result\n  Kokkos::View<int*, ExecutionSpace> result(\"result\", 1);\n\n  // Create a Kokkos parallel_for to search for the first even number\n  Kokkos::parallel_for(\n    \"findFirstEven\",\n    PolicyType(0, x.extent(0)),\n    KOKKOS_LAMBDA(const MemberType& i) {\n      if (x(i) % 2 == 0) {\n        // The first even number is found, so set the result and break out of the loop\n        result(0) = i;\n        Kokkos::atomic_compare_exchange(&result(0), i, i);\n      }\n    });\n\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n\n  // Return the result\n  return result(0);\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Fill in the body of this function.\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Policy = Kokkos::RangePolicy<ExecutionSpace>;\n  using Member = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // The lambda function that will be executed by each thread.\n  auto functor = KOKKOS_LAMBDA(const Member& teamMember) {\n    const int i = teamMember.league_rank();\n    const int n = x.extent(0);\n    if (i < n) {\n      if (x(i) % 2 == 0) {\n        teamMember.team_reduce(Kokkos::Min<int>(i));\n      }\n    }\n  };\n\n  // Execute the functor on the Kokkos device.\n  int result = -1;\n  Kokkos::parallel_reduce(Policy(0, x.extent(0)), functor, Kokkos::Min<int>(result));\n  return result;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = typename PolicyType::member_type;\n\n  // Create a Kokkos view to store the result\n  Kokkos::View<int*, ExecutionSpace> result(\"result\", 1);\n\n  // Set the initial value of the result to -1\n  Kokkos::deep_copy(result, -1);\n\n  // Create a Kokkos parallel_for to search for the first even number\n  Kokkos::parallel_for(\n      \"findFirstEven\",\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(const MemberType& i, int& result) {\n        if (x(i) % 2 == 0) {\n          result = i;\n        }\n      },\n      result);\n\n  // Copy the result back to the host\n  int result_host = -1;\n  Kokkos::deep_copy(result_host, result);\n\n  return result_host;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // Create a Kokkos view to hold the result\n  Kokkos::View<int*, ExecutionSpace> result(\"result\", 1);\n\n  // Launch a parallel Kokkos kernel\n  Kokkos::parallel_for(\n      \"findFirstEven\",\n      PolicyType(0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        // Check if x[i] is even\n        if (x(i) % 2 == 0) {\n          // If so, store the index in result\n          Kokkos::atomic_store(&result(0), i);\n        }\n      });\n\n  // Wait for the kernel to finish\n  Kokkos::fence();\n\n  // Return the result\n  return result(0);\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            if (i < firstEven) {\n                firstEven = i;\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                if (i < firstEven) {\n                    firstEven = i;\n                }\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            if (i < firstEven) {\n                firstEven = i;\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t firstEven = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            firstEven = i;\n            break;\n        }\n    }\n    return firstEven;\n}",
            "size_t n = x.size();\n  size_t firstEven = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] % 2 == 0) {\n      firstEven = i;\n      break;\n    }\n  }\n  return firstEven;\n}",
            "size_t result = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            result = i;\n            break;\n        }\n    }\n    return result;\n}",
            "size_t result = 0;\n\n  #pragma omp parallel for reduction(min:result)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      result = i;\n    }\n  }\n\n  return result;\n}",
            "size_t result = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                if (i < firstEven) {\n                    firstEven = i;\n                }\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                if (i < firstEven) {\n                    firstEven = i;\n                }\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n  size_t result = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] % 2 == 0) {\n      result = i;\n      break;\n    }\n  }\n\n  return result;\n}",
            "// TODO: Your code here\n\n  return 0;\n}",
            "size_t firstEven = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      firstEven = i;\n      break;\n    }\n  }\n  return firstEven;\n}",
            "size_t result = 0;\n  #pragma omp parallel for reduction(min: result)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      result = i;\n    }\n  }\n  return result;\n}",
            "size_t n = x.size();\n    size_t i;\n\n    #pragma omp parallel for shared(x) private(i)\n    for (i = 0; i < n; i++) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                return i;\n            }\n        }\n    }\n\n    return n;\n}",
            "size_t result = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            result = i;\n            break;\n        }\n    }\n    return result;\n}",
            "size_t firstEven = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      #pragma omp critical\n      {\n        if (i < firstEven) {\n          firstEven = i;\n        }\n      }\n    }\n  }\n\n  return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            if (i < firstEven) {\n                firstEven = i;\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = n;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                if (i < firstEven) {\n                    firstEven = i;\n                }\n            }\n        }\n    }\n\n    return firstEven;\n}",
            "size_t n = x.size();\n    size_t firstEven = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] % 2 == 0) {\n            firstEven = i;\n            break;\n        }\n    }\n\n    return firstEven;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "size_t index = 0;\n    for (auto const& i : x) {\n        if (i % 2 == 0) {\n            return index;\n        }\n        index++;\n    }\n    return -1;\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "size_t index = 0;\n    for (auto i = x.begin(); i!= x.end(); ++i) {\n        if (*i % 2 == 0) {\n            return index;\n        }\n        index++;\n    }\n    return -1;\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "hip",
        "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use AMD HIP to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && x[index] % 2 == 0) {\n        *firstEvenIndex = index;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N && x[idx] % 2 == 0) {\n    *firstEvenIndex = idx;\n  }\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            *firstEvenIndex = idx;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            *firstEvenIndex = tid;\n            return;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] % 2 == 0) {\n      *firstEvenIndex = tid;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            *firstEvenIndex = tid;\n            return;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N && x[i] % 2 == 0) {\n    *firstEvenIndex = i;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        *firstEvenIndex = tid;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t firstEven = 0;\n    if (rank == 0) {\n        for (size_t i = 0; i < x.size(); i++) {\n            if (x[i] % 2 == 0) {\n                firstEven = i;\n                break;\n            }\n        }\n    }\n\n    // TODO: use MPI to find the first even number in x\n\n    return firstEven;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int start = rank * x.size() / size;\n    int end = (rank + 1) * x.size() / size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n\n    return -1;\n}",
            "// TODO: Implement this function\n    return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  return 0;\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "cuda",
        "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use CUDA to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n        return;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] % 2 == 0) {\n            *firstEvenIndex = index;\n            return;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < N; i += stride) {\n        if (x[i] % 2 == 0) {\n            *firstEvenIndex = i;\n            return;\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && x[index] % 2 == 0) {\n        *firstEvenIndex = index;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n        return;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && x[index] % 2 == 0) {\n        *firstEvenIndex = index;\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N && x[idx] % 2 == 0) {\n        *firstEvenIndex = idx;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] % 2 == 0) {\n        *firstEvenIndex = i;\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "34_search_find_the_first_even_number",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI and OpenMP to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  int n = x.size();\n  int n_per_proc = n/size;\n  int n_left = n%size;\n  int start = rank*n_per_proc + std::min(rank, n_left);\n  int end = (rank+1)*n_per_proc + std::min(rank+1, n_left);\n  int result = -1;\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2 == 0) {\n      result = i;\n      break;\n    }\n  }\n\n  int result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return result_global;\n}",
            "size_t n = x.size();\n    size_t rank = 0;\n    size_t size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t start = rank * n / size;\n    size_t end = (rank + 1) * n / size;\n\n    size_t result = 0;\n    for (size_t i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            result = i;\n            break;\n        }\n    }\n\n    size_t result_global = 0;\n    MPI_Reduce(&result, &result_global, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    return result_global;\n}",
            "// TODO: Your code here\n}",
            "size_t n = x.size();\n    size_t firstEven = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    return firstEven;\n}",
            "size_t result = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n\n  return result;\n}",
            "size_t result = 0;\n\n    // TODO: Your code here\n\n    return result;\n}",
            "size_t result = 0;\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunkSize = x.size() / size;\n    int chunkStart = rank * chunkSize;\n    int chunkEnd = (rank + 1) * chunkSize;\n    if (rank == size - 1) {\n        chunkEnd = x.size();\n    }\n\n    int firstEven = -1;\n    for (int i = chunkStart; i < chunkEnd; i++) {\n        if (x[i] % 2 == 0) {\n            firstEven = i;\n            break;\n        }\n    }\n\n    MPI_Reduce(&firstEven, &result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "size_t n = x.size();\n  size_t firstEven = n;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = (rank == size - 1)? n : start + chunk;\n  int firstEvenLocal = n;\n  #pragma omp parallel for reduction(min:firstEvenLocal)\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2 == 0) {\n      firstEvenLocal = i;\n      break;\n    }\n  }\n  int firstEvenGlobal = firstEvenLocal;\n  MPI_Allreduce(&firstEvenLocal, &firstEvenGlobal, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return firstEvenGlobal;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO\n  return 0;\n}",
            "size_t result = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    return result;\n}",
            "size_t result = 0;\n\n  // TODO: Your code here\n\n  return result;\n}",
            "size_t n = x.size();\n  size_t firstEven = n;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunkSize = n / size;\n  int remainder = n % size;\n  int start = rank * chunkSize;\n  int end = start + chunkSize;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2 == 0) {\n      #pragma omp critical\n      {\n        if (i < firstEven) {\n          firstEven = i;\n        }\n      }\n    }\n  }\n  int firstEvenGlobal;\n  MPI_Reduce(&firstEven, &firstEvenGlobal, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return firstEvenGlobal;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_extra = n % size;\n\n  int start = n_per_proc * rank;\n  int end = start + n_per_proc;\n  if (rank < n_extra) {\n    end++;\n  }\n\n  int result = -1;\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    if (x[i] % 2 == 0) {\n      result = i;\n      break;\n    }\n  }\n\n  int result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return result_global;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = omp_get_max_threads();\n    int num_threads_per_rank = num_threads / size;\n\n    int start = rank * num_threads_per_rank;\n    int end = start + num_threads_per_rank;\n\n    int result = -1;\n\n    #pragma omp parallel for num_threads(num_threads_per_rank)\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            result = i;\n            break;\n        }\n    }\n\n    int result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    return result_global;\n}",
            "size_t result = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n\n  return result;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "size_t n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    return 0;\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// Your code here\n}",
            "// TODO: Fill in the body of this function\n   return 0;\n}",
            "// Your code here\n   return 0;\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: implement this function\n   return 0;\n}",
            "// Your code here\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t index = 0;\n   for (auto const& book : books) {\n      if (book.pages < 100) {\n         index = books.size() - 1;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t last_short_book = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   return last_short_book;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n\tfor (size_t i = 0; i < books.size(); i++) {\n\t\tif (books[i].pages < 100) {\n\t\t\tindex = i;\n\t\t}\n\t}\n\treturn index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n\tfor (auto i = 0; i < books.size(); i++) {\n\t\tif (books[i].pages < 100) {\n\t\t\tindex = i;\n\t\t}\n\t}\n\treturn index;\n}",
            "size_t index = 0;\n\tfor (size_t i = 0; i < books.size(); i++) {\n\t\tif (books[i].pages < 100) {\n\t\t\tindex = i;\n\t\t}\n\t}\n\treturn index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (auto i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "for (int i = books.size() - 1; i >= 0; i--) {\n\t\tif (books[i].pages < 100) {\n\t\t\treturn i;\n\t\t}\n\t}\n\treturn -1;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}",
            "size_t index = 0;\n\tfor (size_t i = 0; i < books.size(); i++) {\n\t\tif (books[i].pages < 100) {\n\t\t\tindex = i;\n\t\t}\n\t}\n\treturn index;\n}",
            "size_t last_short_book = 0;\n\tfor (size_t i = 0; i < books.size(); i++) {\n\t\tif (books[i].pages < 100) {\n\t\t\tlast_short_book = i;\n\t\t}\n\t}\n\treturn last_short_book;\n}",
            "size_t index = 0;\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         index = i;\n      }\n   }\n   return index;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "hip",
        "prompt": "struct Book {\n   char* title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use AMD HIP to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N) {\n      if (books[index].pages < 100) {\n         *lastShortBookIndex = index;\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && books[i].pages < 100) {\n\t\t*lastShortBookIndex = i;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N && books[i].pages < 100) {\n      *lastShortBookIndex = i;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N && books[i].pages < 100) {\n      *lastShortBookIndex = i;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      if (books[idx].pages < 100) {\n         atomicMin(lastShortBookIndex, idx);\n      }\n   }\n}",
            "int index = threadIdx.x;\n   int stride = blockDim.x;\n\n   for (int i = index; i < N; i += stride) {\n      if (books[i].pages < 100) {\n         *lastShortBookIndex = i;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      if (books[idx].pages < 100) {\n         *lastShortBookIndex = idx;\n      }\n   }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N && books[index].pages < 100) {\n      *lastShortBookIndex = index;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      if (books[idx].pages < 100) {\n         *lastShortBookIndex = idx;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      if (books[i].pages < 100) {\n         *lastShortBookIndex = i;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N && books[index].pages < 100) {\n      *lastShortBookIndex = index;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N && books[i].pages < 100) {\n      *lastShortBookIndex = i;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        *lastShortBookIndex = index;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N && books[i].pages < 100) {\n      *lastShortBookIndex = i;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: implement\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "// Your code here\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int last_short_book = -1;\n   int short_book = -1;\n   int short_book_rank = -1;\n   int short_book_rank_count = 0;\n   int short_book_rank_count_total = 0;\n   int short_book_rank_count_total_max = 0;\n   int short_book_rank_count_total_min = 0;\n   int short_book_rank_count_total_sum = 0;\n   int short_book_rank_count_total_avg = 0;\n   int short_book_rank_count_total_median = 0;\n   int short_book_rank_count_total_mode = 0;\n   int short_book_rank_count_total_mode_count = 0;\n   int short_book_rank_count_total_mode_count_max = 0;\n   int short_book_rank_count_total_mode_count_min = 0;\n   int short_book_rank_count_total_mode_count_sum = 0;\n   int short_book_rank_count_total_mode_count_avg = 0;\n   int short_book_rank_count_total_mode_count_median = 0;\n   int short_book_rank_count_total_mode_count_mode = 0;\n   int short_book_rank_count_total_mode_count_mode_count = 0;\n   int short_book_rank_count_total_mode_count_mode_count_max = 0;\n   int short_book_rank_count_total_mode_count_mode_count_min = 0;\n   int short_book_rank_count_total_mode_count_mode_count_sum = 0;\n   int short_book_rank_count_total_mode_count_mode_count_avg = 0;\n   int short_book_rank_count_total_mode_count_mode_count_median = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_max = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_min = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_sum = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_avg = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_median = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_max = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_min = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_sum = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_avg = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_median = 0;\n   int short_book_rank_count_total_mode_count_mode_count_mode_count_mode_count_mode = 0;\n   int short_book_rank_count_total_mode_count_",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int start = rank * (books.size() / size);\n   int end = (rank + 1) * (books.size() / size);\n   if (rank == size - 1)\n      end = books.size();\n   int last = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100)\n         last = i;\n   }\n   int result;\n   MPI_Reduce(&last, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   return result;\n}",
            "// TODO\n   return 0;\n}",
            "// TODO: implement\n   return 0;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "// Your code here\n}",
            "// TODO: Your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int count = books.size();\n   int count_per_process = count / size;\n   int count_remainder = count % size;\n   int start = rank * count_per_process;\n   int end = start + count_per_process;\n   if (rank == 0) {\n      end += count_remainder;\n   }\n   int last_index = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last_index = i;\n      }\n   }\n   int last_index_global;\n   MPI_Reduce(&last_index, &last_index_global, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      return last_index_global;\n   }\n   return -1;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int last_short_book = -1;\n   if (rank == 0) {\n      for (int i = 0; i < books.size(); i++) {\n         if (books[i].pages < 100) {\n            last_short_book = i;\n         }\n      }\n   }\n\n   MPI_Bcast(&last_short_book, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   return last_short_book;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int last_short_book = -1;\n   int short_book = -1;\n   int short_book_rank = -1;\n   int short_book_count = 0;\n\n   // TODO: Implement this function\n\n   if (rank == 0) {\n      // TODO: Gather the results from all ranks and return the index of the last short book\n   }\n\n   return last_short_book;\n}",
            "// Your code here\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int start = rank * (books.size() / size);\n   int end = (rank + 1) * (books.size() / size);\n   int last = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last = i;\n      }\n   }\n   int result;\n   MPI_Reduce(&last, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   return result;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "cuda",
        "prompt": "struct Book {\n   char* title;\n   int pages;\n};\n\n/* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int index = threadIdx.x;\n   if (index < N && books[index].pages < 100) {\n      *lastShortBookIndex = index;\n   }\n}",
            "int idx = threadIdx.x;\n\tif (idx < N && books[idx].pages < 100) {\n\t\t*lastShortBookIndex = idx;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && books[i].pages < 100) {\n\t\t*lastShortBookIndex = i;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && books[i].pages < 100) {\n\t\t*lastShortBookIndex = i;\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N && books[idx].pages < 100) {\n\t\t*lastShortBookIndex = idx;\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && books[i].pages < 100) {\n        *lastShortBookIndex = i;\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N) {\n      if (books[index].pages < 100) {\n         *lastShortBookIndex = index;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (books[idx].pages < 100) {\n            *lastShortBookIndex = idx;\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N && books[idx].pages < 100) {\n\t\t*lastShortBookIndex = idx;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N && books[i].pages < 100) {\n      *lastShortBookIndex = i;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N && books[idx].pages < 100) {\n      *lastShortBookIndex = idx;\n   }\n}",
            "// TODO: Implement this function\n\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N && books[idx].pages < 100) {\n\t\t*lastShortBookIndex = idx;\n\t}\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (index < N && books[index].pages < 100) {\n\t\t*lastShortBookIndex = index;\n\t}\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (index < N && books[index].pages < 100) {\n\t\t*lastShortBookIndex = index;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && books[i].pages < 100) {\n\t\t*lastShortBookIndex = i;\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tif (books[idx].pages < 100) {\n\t\t\t*lastShortBookIndex = idx;\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && books[i].pages < 100) {\n\t\t*lastShortBookIndex = i;\n\t}\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "31_search_search_for_last_struct_by_key",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI and OpenMP to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t result = 0;\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int chunk_size = books.size() / size;\n   int start = rank * chunk_size;\n   int end = (rank + 1) * chunk_size;\n   if (rank == size - 1) {\n      end = books.size();\n   }\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   int result_from_all_ranks[size];\n   MPI_Gather(&result, 1, MPI_INT, result_from_all_ranks, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < size; i++) {\n         if (result_from_all_ranks[i] > result) {\n            result = result_from_all_ranks[i];\n         }\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   int size = books.size();\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_threads;\n   #pragma omp parallel\n   {\n      num_threads = omp_get_num_threads();\n   }\n   int chunk = size / num_threads;\n   int start = rank * chunk;\n   int end = (rank + 1) * chunk;\n   if (rank == num_threads - 1) {\n      end = size;\n   }\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n   int result;\n   MPI_Reduce(&last_short_book, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   return result;\n}",
            "size_t result = 0;\n   #pragma omp parallel\n   {\n      size_t my_result = 0;\n      #pragma omp for\n      for (size_t i = 0; i < books.size(); ++i) {\n         if (books[i].pages < 100) {\n            my_result = i;\n         }\n      }\n      #pragma omp critical\n      {\n         if (my_result > result) {\n            result = my_result;\n         }\n      }\n   }\n   return result;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t result = 0;\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int chunk = books.size() / size;\n   int start = rank * chunk;\n   int end = (rank + 1) * chunk;\n   if (rank == size - 1) {\n      end = books.size();\n   }\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "// TODO: Implement this function\n   return 0;\n}",
            "// TODO: Your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int i, j, k, l;\n   int start, end;\n   int last_book = 0;\n   int last_book_rank = 0;\n   int last_book_rank_global = 0;\n   int last_book_rank_global_temp = 0;\n   int last_book_rank_global_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = 0;\n   int last_book_rank_global_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         {\n            result = i;\n         }\n      }\n   }\n   return result;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int start = rank * books.size() / size;\n   int end = (rank + 1) * books.size() / size;\n\n   int last_short_book = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n\n   int last_short_book_global;\n   MPI_Reduce(&last_short_book, &last_short_book_global, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return last_short_book_global;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int num_books = books.size();\n   int num_books_per_rank = num_books / size;\n   int num_books_remainder = num_books % size;\n   int start = rank * num_books_per_rank;\n   int end = start + num_books_per_rank;\n   if (rank == size - 1) {\n      end += num_books_remainder;\n   }\n\n   int last_book_index = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last_book_index = i;\n      }\n   }\n\n   int last_book_index_global;\n   MPI_Reduce(&last_book_index, &last_book_index_global, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return last_book_index_global;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int num_books_per_rank = books.size() / size;\n   int num_books_remaining = books.size() % size;\n   int start = rank * num_books_per_rank;\n   int end = start + num_books_per_rank;\n   if (rank == size - 1) {\n      end += num_books_remaining;\n   }\n\n   int last_short_book_index = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n\n   int last_short_book_index_global;\n   MPI_Reduce(&last_short_book_index, &last_short_book_index_global, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return last_short_book_index_global;\n}",
            "// TODO\n   return 0;\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int chunk = books.size() / size;\n   int start = rank * chunk;\n   int end = (rank == size - 1)? books.size() : start + chunk;\n   int last = -1;\n   #pragma omp parallel for\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last = i;\n      }\n   }\n   int result;\n   MPI_Reduce(&last, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   return result;\n}",
            "// TODO: Your code here\n   return 0;\n}",
            "size_t last_short_book = 0;\n   int num_threads = omp_get_max_threads();\n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int num_books_per_thread = books.size() / num_threads;\n   int start_index = rank * num_books_per_thread;\n   int end_index = (rank + 1) * num_books_per_thread;\n   if (rank == size - 1) {\n      end_index = books.size();\n   }\n   int last_short_book_rank = 0;\n   int last_short_book_index = 0;\n   for (int i = start_index; i < end_index; i++) {\n      if (books[i].pages < 100) {\n         last_short_book_rank = rank;\n         last_short_book_index = i;\n      }\n   }\n   MPI_Reduce(&last_short_book_rank, &last_short_book, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&last_short_book_index, &last_short_book, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   return last_short_book;\n}",
            "size_t result = 0;\n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int chunk = books.size() / size;\n   int start = rank * chunk;\n   int end = (rank + 1) * chunk;\n   if (rank == size - 1) {\n      end = books.size();\n   }\n   #pragma omp parallel for\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n   return result;\n}",
            "size_t result = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); i++) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         {\n            result = i;\n         }\n      }\n   }\n   return result;\n}",
            "size_t last_short_book = 0;\n   int num_procs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int num_books = books.size();\n   int num_books_per_proc = num_books / num_procs;\n   int num_books_remainder = num_books % num_procs;\n\n   int start_index = rank * num_books_per_proc + std::min(rank, num_books_remainder);\n   int end_index = (rank + 1) * num_books_per_proc + std::min(rank + 1, num_books_remainder);\n\n   int num_books_to_search = end_index - start_index;\n\n   int num_threads = omp_get_max_threads();\n   int num_books_per_thread = num_books_to_search / num_threads;\n   int num_books_remainder_per_thread = num_books_to_search % num_threads;\n\n   int start_index_per_thread = start_index + rank * num_books_per_thread + std::min(rank, num_books_remainder_per_thread);\n   int end_index_per_thread = start_index_per_thread + num_books_per_thread + (rank < num_books_remainder_per_thread);\n\n   int num_books_to_search_per_thread = end_index_per_thread - start_index_per_thread;\n\n   #pragma omp parallel for num_threads(num_threads)\n   for (int i = start_index_per_thread; i < end_index_per_thread; i++) {\n      if (books[i].pages < 100) {\n         last_short_book = i;\n      }\n   }\n\n   int last_short_book_rank = 0;\n   MPI_Reduce(&last_short_book, &last_short_book_rank, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return last_short_book_rank;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(Kokkos::View<const double*> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename ExecutionSpace::device_type;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // Define a lambda that will be called for each thread in the parallel loop\n  auto closestToPi = KOKKOS_LAMBDA(const MemberType& teamMember) {\n    // Get the index of the thread in the parallel loop\n    const size_t i = teamMember.league_rank();\n\n    // Get the value of PI\n    const double pi = M_PI;\n\n    // Get the value of x\n    const double x_i = x(i);\n\n    // Get the absolute difference between x_i and pi\n    const double diff = std::abs(x_i - pi);\n\n    // Store the index of the value in x that is closest to pi\n    Kokkos::single(Kokkos::PerThread(teamMember), [&]() {\n      if (diff < bestDiff) {\n        bestDiff = diff;\n        bestIndex = i;\n      }\n    });\n  };\n\n  // Set the bestDiff to a large value\n  double bestDiff = std::numeric_limits<double>::max();\n\n  // Set the bestIndex to an invalid value\n  size_t bestIndex = std::numeric_limits<size_t>::max();\n\n  // Create a parallel loop\n  Kokkos::parallel_for(\n      Kokkos::TeamPolicy<ExecutionSpace>(x.extent(0), Kokkos::AUTO),\n      closestToPi);\n\n  // Return the best index\n  return bestIndex;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename ExecutionSpace::device_type;\n  using MemorySpace = typename ExecutionSpace::memory_space;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  // Create a parallel_for lambda to search for the closest value to PI.\n  // The lambda will be executed in parallel.\n  auto searchForPi = KOKKOS_LAMBDA(const MemberType& teamMember) {\n    // Get the index of this thread.\n    const int i = teamMember.league_rank();\n\n    // Get the value at this index.\n    const double x_i = x(i);\n\n    // Get the absolute value of the difference between x_i and PI.\n    const double diff = std::abs(x_i - M_PI);\n\n    // Get the index of the value in the vector x that is closest to PI.\n    // Use an atomic operation to ensure that only one thread writes to the\n    // result variable.\n    Kokkos::atomic_compare_exchange<MemorySpace>(&result, i, i,\n                                                 Kokkos::MemoryOrder::acq_rel,\n                                                 Kokkos::MemoryOrder::acq_rel);\n  };\n\n  // Create a parallel_for policy to execute the lambda in parallel.\n  // The policy will execute the lambda in parallel across all threads.\n  Kokkos::parallel_for(\n      \"Search for PI\",\n      Kokkos::TeamPolicy<ExecutionSpace>(x.extent(0), Kokkos::AUTO),\n      searchForPi);\n\n  // Synchronize the device.\n  Kokkos::fence();\n\n  // Return the index of the value in the vector x that is closest to PI.\n  return result;\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t index = 0;\n  double min = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min) {\n      min = diff;\n      index = i;\n    }\n  }\n  return index;\n}",
            "size_t closest = 0;\n  double min_diff = std::abs(x[0] - M_PI);\n  for (size_t i = 1; i < x.size(); ++i) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min_diff) {\n      min_diff = diff;\n      closest = i;\n    }\n  }\n  return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n\n    return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double min = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n  double minDiff = std::numeric_limits<double>::max();\n  for (size_t i = 0; i < x.size(); ++i) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < minDiff) {\n      minDiff = diff;\n      closest = i;\n    }\n  }\n  return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n\n    return closest;\n}",
            "// TODO: Implement this function\n    return 0;\n}",
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double minDiff = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n  double min = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min) {\n      min = diff;\n      closest = i;\n    }\n  }\n\n  return closest;\n}",
            "size_t closest = 0;\n    double min = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double min = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            closest = i;\n        }\n    }\n\n    return closest;\n}",
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n\n    return closest;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "size_t closest = 0;\n    double min = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            closest = i;\n        }\n    }\n\n    return closest;\n}",
            "// TODO: Your code here\n    size_t closest = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "size_t closest = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "double min = std::numeric_limits<double>::max();\n  size_t index = 0;\n  for (size_t i = 0; i < x.size(); i++) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min) {\n      min = diff;\n      index = i;\n    }\n  }\n  return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (std::abs(x[i] - M_PI) < min) {\n            min = std::abs(x[i] - M_PI);\n            index = i;\n        }\n    }\n    return index;\n}",
            "double pi = M_PI;\n    double min = abs(x[0] - pi);\n    size_t index = 0;\n    for (size_t i = 1; i < x.size(); ++i) {\n        double temp = abs(x[i] - pi);\n        if (temp < min) {\n            min = temp;\n            index = i;\n        }\n    }\n    return index;\n}",
            "size_t index = 0;\n    double min_diff = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min_diff = std::numeric_limits<double>::max();\n    size_t min_index = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = i;\n        }\n    }\n    return min_index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "size_t index = 0;\n    double min = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n  size_t index = 0;\n  for (size_t i = 0; i < x.size(); i++) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min) {\n      min = diff;\n      index = i;\n    }\n  }\n  return index;\n}",
            "double min_diff = std::numeric_limits<double>::max();\n    size_t min_index = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = i;\n        }\n    }\n    return min_index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (std::abs(x[i] - M_PI) < min) {\n            min = std::abs(x[i] - M_PI);\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (std::abs(x[i] - M_PI) < min) {\n            min = std::abs(x[i] - M_PI);\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "size_t index = 0;\n  double min = std::numeric_limits<double>::max();\n  for (size_t i = 0; i < x.size(); i++) {\n    double diff = std::abs(x[i] - M_PI);\n    if (diff < min) {\n      min = diff;\n      index = i;\n    }\n  }\n  return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (std::abs(x[i] - M_PI) < min) {\n            min = std::abs(x[i] - M_PI);\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = std::numeric_limits<double>::max();\n    size_t index = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min) {\n            min = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
            "double min = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "double pi = M_PI;\n    double min = std::abs(x[0] - pi);\n    size_t index = 0;\n    for (size_t i = 1; i < x.size(); i++) {\n        if (std::abs(x[i] - pi) < min) {\n            min = std::abs(x[i] - pi);\n            index = i;\n        }\n    }\n    return index;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "hip",
        "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n      *closestToPiIndex = i;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double diff = abs(x[i] - M_PI);\n        if (i == 0 || diff < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO: Implement this function\n  // 1. Find the index of the value in the vector x that is closest to the math constant PI.\n  // 2. Store the index in closestToPiIndex.\n  // 3. Use M_PI for the value of PI.\n  // 4. Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n  // 5. Example:\n  //\n  // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n  // output: 1\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double diff = abs(x[tid] - M_PI);\n        if (tid == 0 || diff < x[*closestToPiIndex]) {\n            *closestToPiIndex = tid;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  double minDiff = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n    return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO\n    return 0;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "// TODO\n    return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n  return 0;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "cuda",
        "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search in parallel. The kernel is launched with at least N threads.\n    // Example:\n\n    // input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n    // output: 1\n\n    // Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n    // Use M_PI for the value of PI.\n    // Use CUDA to search",
            "// TODO: implement this function\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        double diff = fabs(x[i] - M_PI);\n        if (diff < fabs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double diff = abs(x[index] - M_PI);\n        if (index == 0 || diff < *closestToPiIndex) {\n            *closestToPiIndex = diff;\n        }\n    }\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double diff = abs(x[i] - M_PI);\n        if (i == 0 || diff < *closestToPiIndex) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (abs(x[i] - M_PI) < abs(x[*closestToPiIndex] - M_PI)) {\n            *closestToPiIndex = i;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double diff = abs(x[i] - M_PI);\n        if (i == 0 || diff < *closestToPiIndex) {\n            *closestToPiIndex = i;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "33_search_find_the_closest_number_to_pi",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    return 0;\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    return 0;\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO\n}",
            "size_t closest = 0;\n    double min_diff = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest = i;\n        }\n    }\n    return closest;\n}",
            "// TODO\n    return 0;\n}",
            "// TODO\n    return 0;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: Your code here\n    return 0;\n}",
            "// TODO: Implement this function\n    return 0;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}",
            "// TODO: Implement this function\n  return false;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n    return false;\n}",
            "bool found = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      found = true;\n    }\n  }\n  return found;\n}",
            "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n            break;\n        }\n    }\n    return found;\n}",
            "bool result = false;\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == target) {\n        result = true;\n        #pragma omp cancel for\n      }\n    }\n  }\n  return result;\n}",
            "bool found = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      found = true;\n    }\n  }\n  return found;\n}",
            "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n            break;\n        }\n    }\n    return found;\n}",
            "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n            break;\n        }\n    }\n    return found;\n}",
            "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n            break;\n        }\n    }\n    return found;\n}",
            "// TODO: Implement this function\n  bool found = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      found = true;\n      break;\n    }\n  }\n  return found;\n}",
            "bool result = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            result = true;\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            result = true;\n        }\n    }\n    return result;\n}",
            "bool result = false;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            for (int i = 0; i < x.size(); i++) {\n                if (x[i] == target) {\n                    result = true;\n                    break;\n                }\n            }\n        }\n    }\n    return result;\n}",
            "bool result = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n      result = true;\n    }\n  }\n  return result;\n}",
            "// TODO: implement this function\n  bool result = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      result = true;\n    }\n  }\n  return result;\n}",
            "// TODO: implement this function\n  bool result = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  return result;\n}",
            "bool found = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      found = true;\n    }\n  }\n  return found;\n}",
            "bool result = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  return result;\n}",
            "bool result = false;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      result = true;\n    }\n  }\n  return result;\n}",
            "bool found = false;\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == target) {\n                found = true;\n                #pragma omp cancel for\n            }\n        }\n    }\n    return found;\n}",
            "bool result = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    return result;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "serial",
        "prompt": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (auto const& i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO: implement this function\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "// TODO: implement this function\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "// TODO: implement this function\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "return std::find(x.begin(), x.end(), target)!= x.end();\n}",
            "return std::find(x.begin(), x.end(), target)!= x.end();\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "return std::find(x.begin(), x.end(), target)!= x.end();\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      return true;\n    }\n  }\n  return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n      return true;\n    }\n  }\n  return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "hip",
        "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] == target) {\n        *found = true;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "// TODO: implement this\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  bool found = false;\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      found = true;\n      break;\n    }\n  }\n  bool result;\n  MPI_Reduce(&found, &result, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_extra = n % size;\n\n    bool result = false;\n    if (rank == 0) {\n        for (int i = 0; i < n_per_proc; i++) {\n            if (x[i] == target) {\n                result = true;\n                break;\n            }\n        }\n    } else {\n        for (int i = n_per_proc * (rank - 1) + n_extra; i < n_per_proc * rank + n_extra; i++) {\n            if (x[i] == target) {\n                result = true;\n                break;\n            }\n        }\n    }\n\n    bool result_all;\n    MPI_Reduce(&result, &result_all, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return result_all;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  bool result = false;\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == target) {\n        result = true;\n        break;\n      }\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: implement\n    return false;\n}",
            "// TODO: implement this function\n  return false;\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  bool result = false;\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == target) {\n        result = true;\n        break;\n      }\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int count = x.size();\n  int chunk = count / size;\n  int remainder = count % size;\n  int start = rank * chunk;\n  int end = (rank == size - 1)? count : start + chunk;\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  int result_all;\n  MPI_Allreduce(&result, &result_all, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);\n  return result_all;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    bool result = false;\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] == target) {\n                result = true;\n                break;\n            }\n        }\n    }\n    MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int start = rank * (x.size() / size);\n  int end = (rank + 1) * (x.size() / size);\n  if (rank == size - 1)\n    end = x.size();\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  bool result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n  if (rank == 0)\n    return result_global;\n  else\n    return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size();\n  int local_target = target;\n  int local_result = 0;\n  int global_result = 0;\n\n  if (rank == 0) {\n    for (int i = 0; i < local_size; i++) {\n      if (x[i] == local_target) {\n        local_result = 1;\n        break;\n      }\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    if (global_result == 0) {\n      return false;\n    } else {\n      return true;\n    }\n  }\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  bool result = false;\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  bool result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n  return result_global;\n}",
            "// TODO\n    return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  bool result = false;\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == target) {\n        result = true;\n        break;\n      }\n    }\n  }\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int count = 0;\n    int count_sum = 0;\n    int count_sum_all;\n    int start = rank * x.size() / size;\n    int end = (rank + 1) * x.size() / size;\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &count_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        count_sum_all = count_sum;\n    }\n    MPI_Bcast(&count_sum_all, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (count_sum_all > 0) {\n        return true;\n    } else {\n        return false;\n    }\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * x.size() / size;\n    int end = (rank + 1) * x.size() / size;\n    bool result = false;\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    int result_all;\n    MPI_Allreduce(&result, &result_all, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);\n    return result_all;\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "cuda",
        "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "// TODO: Implement this\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] == target) {\n        *found = true;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N && x[i] == target) {\n        *found = true;\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] == target) {\n      *found = true;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] == target) {\n            *found = true;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "32_search_check_if_array_contains_value",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "bool result = false;\n\n  // TODO: Implement this function\n\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n\n  return false;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n    int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n    std::vector<int> x_local(n_local);\n    MPI_Scatter(x.data(), n_local, MPI_INT, x_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n\n    bool result = false;\n    if (rank == 0) {\n        result = std::find(x_local.begin(), x_local.end(), target)!= x_local.end();\n    }\n\n    MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n\n    return result;\n}",
            "bool result = false;\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    std::vector<int> my_x(x.begin() + start, x.begin() + end);\n    #pragma omp parallel for\n    for (int i = 0; i < my_x.size(); i++) {\n        if (my_x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    int result_all;\n    MPI_Allreduce(&result, &result_all, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);\n    return result_all;\n}",
            "bool result = false;\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    std::vector<int> my_x(x.begin() + start, x.begin() + end);\n    int my_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < my_x.size(); i++) {\n        if (my_x[i] == target) {\n            my_result = 1;\n            break;\n        }\n    }\n    int global_result = 0;\n    MPI_Reduce(&my_result, &global_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        result = global_result > 0;\n    }\n    return result;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    std::vector<int> x_rank(x.begin() + start, x.begin() + end);\n    int num_threads = omp_get_max_threads();\n    int chunk_thread = x_rank.size() / num_threads;\n    int remainder_thread = x_rank.size() % num_threads;\n    #pragma omp parallel for num_threads(num_threads)\n    for (int i = 0; i < num_threads; i++) {\n        int start_thread = i * chunk_thread;\n        int end_thread = start_thread + chunk_thread;\n        if (i == num_threads - 1) {\n            end_thread += remainder_thread;\n        }\n        for (int j = start_thread; j < end_thread; j++) {\n            if (x_rank[j] == target) {\n                result = true;\n            }\n        }\n    }\n    int result_all;\n    MPI_Allreduce(&result, &result_all, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD);\n    return result_all;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    bool result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        return result_global;\n    }\n    return false;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    bool result = false;\n    int start = rank * x.size() / size;\n    int end = (rank + 1) * x.size() / size;\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    bool final_result;\n    MPI_Reduce(&result, &final_result, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    return final_result;\n}",
            "// TODO\n    return false;\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  bool result = false;\n\n  if (rank == 0) {\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = x.size() / num_threads;\n    int remainder = x.size() % num_threads;\n\n    std::vector<bool> results(num_threads, false);\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n      int thread_id = omp_get_thread_num();\n      int start = thread_id * num_per_thread;\n      int end = start + num_per_thread;\n      if (thread_id == num_threads - 1) {\n        end += remainder;\n      }\n      for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n          results[thread_id] = true;\n          break;\n        }\n      }\n    }\n\n    for (int i = 0; i < num_threads; i++) {\n      result = result || results[i];\n    }\n  }\n\n  MPI_Bcast(&result, 1, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    bool result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    return result_global;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n  if (rank == size - 1) {\n    end = n;\n  }\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      result = true;\n      break;\n    }\n  }\n  bool result_global;\n  MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n  return result_global;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1)\n        end = x.size();\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    bool result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    if (rank == 0)\n        return result_global;\n    else\n        return false;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    int result_all;\n    MPI_Reduce(&result, &result_all, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n    return result_all;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_rank + std::min(rank, n_rem);\n    int end = (rank + 1) * n_per_rank + std::min(rank + 1, n_rem);\n    std::vector<int> x_rank(x.begin() + start, x.begin() + end);\n\n    bool result = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x_rank.size(); i++) {\n        if (x_rank[i] == target) {\n            result = true;\n            break;\n        }\n    }\n\n    bool result_global;\n    MPI_Allreduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, MPI_COMM_WORLD);\n    return result_global;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int count = 0;\n    int count_sum = 0;\n    int count_sum_all = 0;\n    int chunk = x.size() / size;\n    int chunk_remain = x.size() % size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == 0) {\n        start += chunk_remain;\n        end += chunk_remain;\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &count_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        count_sum_all = count_sum;\n    }\n    MPI_Bcast(&count_sum_all, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (count_sum_all > 0) {\n        return true;\n    } else {\n        return false;\n    }\n}",
            "// TODO: Your code here\n  bool result = false;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int start = rank * chunk;\n  int end = (rank == size - 1)? x.size() : start + chunk;\n  std::vector<int> local_x(x.begin() + start, x.begin() + end);\n  int local_result = 0;\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    if (local_x[i] == target) {\n      local_result = 1;\n      break;\n    }\n  }\n  int global_result = 0;\n  MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    result = (global_result == 0)? false : true;\n  }\n  return result;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    bool flag = false;\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = x.size() / num_threads;\n    int num_left = x.size() % num_threads;\n    int start = rank * num_per_thread + std::min(rank, num_left);\n    int end = (rank + 1) * num_per_thread + std::min(rank + 1, num_left);\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            flag = true;\n            break;\n        }\n    }\n    int result;\n    MPI_Reduce(&flag, &result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n    return result;\n}",
            "// TODO: Your code here\n    bool result = false;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = x.size();\n    }\n    for (int i = start; i < end; i++) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n    bool result_global;\n    MPI_Reduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    return result_global;\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(Kokkos::View<Kokkos::complex<double>*> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "fft(x);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&](int i) {\n    x(i) /= x.extent(0);\n  });\n}",
            "/*\n   * TODO:\n   * 1. Compute the inverse fourier transform of x in-place.\n   * 2. Use Kokkos to compute in parallel.\n   * 3. Assume Kokkos has already been initialized.\n   */\n  fft(x);\n  const int n = x.extent(0);\n  const double scale = 1.0 / n;\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n), [&](int i) {\n    x(i) *= scale;\n  });\n}",
            "/* TODO: implement */\n  fft(x);\n  for (int i = 0; i < x.extent(0); i++) {\n    x(i) /= x.extent(0);\n  }\n}",
            "/*\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n                         x(i) = std::conj(x(i));\n                       });\n  */\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n                         x(i) = Kokkos::complex<double>(x(i).real(), -x(i).imag());\n                       });\n  fft(x);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n                         x(i) = Kokkos::complex<double>(x(i).real()/x.extent(0), x(i).imag()/x.extent(0));\n                       });\n}",
            "/*\n   * TODO:\n   * 1. call fft on x\n   * 2. divide each element of x by the number of elements in x\n   */\n  fft(x);\n  Kokkos::parallel_for(\n    \"ifft\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) /= x.extent(0);\n    }\n  );\n}",
            "fft(x);\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x(i) = x(i) / x.extent(0);\n  });\n}",
            "/* compute the forward transform */\n  fft(x);\n\n  /* compute the inverse transform */\n  const int N = x.extent(0);\n  const double scale = 1.0 / N;\n  Kokkos::parallel_for(\n    \"ifft\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, N),\n    KOKKOS_LAMBDA(int i) {\n      x(i) *= scale;\n    }\n  );\n}",
            "/*\n   * TODO:\n   *\n   * 1. Compute the inverse fourier transform of x in-place.\n   * 2. Use Kokkos to compute in parallel.\n   * 3. Assume Kokkos has already been initialized.\n   * 4. Use the fft function you wrote above.\n   *\n   * Hint:\n   *\n   * 1. You can use the Kokkos::parallel_for function to parallelize a loop.\n   * 2. You can use the Kokkos::complex<double> type.\n   * 3. You can use the Kokkos::complex<double>::conj() function to compute the complex conjugate.\n   * 4. You can use the Kokkos::complex<double>::operator/() function to divide two complex numbers.\n   * 5. You can use the Kokkos::complex<double>::operator*() function to multiply two complex numbers.\n   * 6. You can use the Kokkos::complex<double>::operator+=() function to add two complex numbers.\n   * 7. You can use the Kokkos::complex<double>::operator-=() function to subtract two complex numbers.\n   * 8. You can use the Kokkos::complex<double>::operator==() function to check if two complex numbers are equal.\n   * 9. You can use the Kokkos::complex<double>::operator!=() function to check if two complex numbers are not equal.\n   * 10. You can use the Kokkos::complex<double>::operator()() function to access the real and imaginary parts of a complex number.\n   * 11. You can use the Kokkos::complex<double>::real() function to access the real part of a complex number.\n   * 12. You can use the Kokkos::complex<double>::imag() function to access the imaginary part of a complex number.\n   * 13. You can use the Kokkos::complex<double>::assign() function to assign a complex number.\n   * 14. You can use the Kokkos::complex<double>::zero() function to set a complex number to zero.\n   * 15. You can use the Kokkos::complex<double>::identity() function to set a complex number to one.\n   * 16. You can use the Kokkos::complex<double>::conj() function to compute the complex conjugate of a complex number.\n   * 17. You can use the Kokkos::complex<double>::operator/() function to divide two complex numbers.\n   * 18. You can use the Kokkos::complex<double>::operator*() function to multiply two complex numbers.\n   * 19. You can use the Kokkos::complex<double>::operator+=() function to add two complex numbers.\n   * 20. You can use the Kokkos::complex<double>::operator-=() function to subtract two complex numbers.\n   * 21. You can use the Kokkos::complex<double>::operator==() function to check if two complex numbers are equal.\n   * 22. You can use the Kokkos::complex<double>::operator!=() function to check if two complex numbers are not equal.\n   * 23. You can use the Kokkos::complex<double>::operator()() function to access the real and imaginary parts of a complex number.\n   * 24. You can use the Kokkos::complex<double>::real() function to access the real part of a complex number.\n   * 25. You can use the Kokkos::complex<double>::imag() function to access the imaginary part of a complex number.\n   * 26. You can use the Kokkos::complex<double>::assign() function to assign a complex number.\n   * 27. You can use the Kokkos::complex<double>::zero() function to set a complex number to zero.\n   * 28. You can use the Kokkos::complex<double>::identity() function to set a complex number to one.\n   * 29. You can use the Kokkos::complex<double>::conj() function to compute the complex conjugate of a complex number.\n   * 30. You can use the Kokkos::complex<double>::operator/() function to divide two",
            "/* Compute the inverse fourier transform of x in-place.\n     Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n     Example:\n     \n     input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n     output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n  */\n  /* Your code here */\n  fft(x);\n  for (int i = 0; i < x.extent(0); i++) {\n    x(i) = x(i) / x.extent(0);\n  }\n}",
            "/*\n   * TODO:\n   * 1. Compute the inverse fourier transform of x in-place.\n   * 2. Use Kokkos to compute in parallel.\n   * 3. Assume Kokkos has already been initialized.\n   */\n  fft(x);\n  for (int i = 0; i < x.extent(0); i++) {\n    x(i) = x(i) / x.extent(0);\n  }\n}",
            "/* TODO: Your code here */\n  fft(x);\n  Kokkos::parallel_for(\n    \"ifft\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      x(i) /= x.extent(0);\n    }\n  );\n}",
            "/*\n   * TODO:\n   * 1. Compute the inverse fourier transform of x in-place.\n   * 2. Use Kokkos to compute in parallel.\n   * 3. Assume Kokkos has already been initialized.\n   */\n  fft(x);\n  for (int i = 0; i < x.extent(0); i++) {\n    x(i) /= x.extent(0);\n  }\n}",
            "/* TODO: Your code here */\n  fft(x);\n  Kokkos::parallel_for(\n    \"ifft\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) /= x.extent(0);\n    }\n  );\n}",
            "/* TODO: implement ifft using Kokkos */\n  fft(x);\n  Kokkos::parallel_for(\n    \"ifft\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      x(i) /= x.extent(0);\n    }\n  );\n}",
            "/* TODO: Your code here */\n  fft(x);\n  Kokkos::parallel_for(x.extent(0), [&](int i) {\n    x(i) = x(i) / x.extent(0);\n  });\n}",
            "/* TODO: Your code here */\n  fft(x);\n  Kokkos::parallel_for(x.extent(0), [=](int i) {\n    x(i) /= x.extent(0);\n  });\n}",
            "/* TODO: your code here */\n  fft(x);\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x(i) = x(i) / x.extent(0);\n  });\n}",
            "/*\n    Your code here.\n  */\n}",
            "/*\n   * TODO:\n   *\n   * 1. call fft on x\n   * 2. scale x by 1/N\n   * 3. call fft on x\n   */\n\n  fft(x);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) /= x.extent(0);\n    }\n  );\n  fft(x);\n}",
            "/*\n   * Your code here.\n   */\n  fft(x);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&](const int i) {\n    x(i) /= x.extent(0);\n  });\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "/* TODO: Your code here */\n  int n = x.size();\n  for (int i = 0; i < n; i++) {\n    x[i] = std::conj(x[i]);\n  }\n  fft(x);\n  for (int i = 0; i < n; i++) {\n    x[i] = x[i] / n;\n  }\n}",
            "/* TODO: implement ifft */\n    // 1. compute fft\n    fft(x);\n    // 2. divide by length\n    double length = x.size();\n    for (int i = 0; i < length; i++) {\n        x[i] /= length;\n    }\n}",
            "/* TODO: Your code here */\n    // 1. Compute the FFT of x\n    fft(x);\n    // 2. Divide each element by the length of x\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        x[i] = x[i] / n;\n    }\n}",
            "/* TODO: implement ifft */\n    int n = x.size();\n    int n2 = n / 2;\n    int n4 = n / 4;\n    int n8 = n / 8;\n    int n16 = n / 16;\n    int n32 = n / 32;\n    int n64 = n / 64;\n    int n128 = n / 128;\n    int n256 = n / 256;\n    int n512 = n / 512;\n    int n1024 = n / 1024;\n    int n2048 = n / 2048;\n    int n4096 = n / 4096;\n    int n8192 = n / 8192;\n    int n16384 = n / 16384;\n    int n32768 = n / 32768;\n    int n65536 = n / 65536;\n    int n131072 = n / 131072;\n    int n262144 = n / 262144;\n    int n524288 = n / 524288;\n    int n1048576 = n / 1048576;\n    int n2097152 = n / 2097152;\n    int n4194304 = n / 4194304;\n    int n8388608 = n / 8388608;\n    int n16777216 = n / 16777216;\n    int n33554432 = n / 33554432;\n    int n67108864 = n / 67108864;\n    int n134217728 = n / 134217728;\n    int n268435456 = n / 268435456;\n    int n536870912 = n / 536870912;\n    int n1073741824 = n / 1073741824;\n    int n2147483648 = n / 2147483648;\n    int n4294967296 = n / 4294967296;\n    int n8589934592 = n / 8589934592;\n    int n17179869184 = n / 17179869184;\n    int n34359738368 = n / 34359738368;\n    int n68719476736 = n / 68719476736;\n    int n137438953472 = n / 137438953472;\n    int n274877906944 = n / 274877906944;\n    int n549755813888 = n / 549755813888;\n    int n1099511627776 = n / 1099511627776;\n    int n2199023255552 = n / 2199023255552;\n    int n4398046511104 = n / 4398046511104;\n    int n8796093022208 = n / 8796093022208;\n    int n1",
            "/* TODO: Your code here */\n  // 1. compute the FFT\n  fft(x);\n  // 2. scale the result\n  int n = x.size();\n  double scale = 1.0 / n;\n  for (int i = 0; i < n; i++) {\n    x[i] *= scale;\n  }\n}",
            "/* TODO: Your code here */\n    int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> u(n);\n    std::vector<std::complex<double>> v(n);\n    std::vector<std::complex<double>> t(n);\n    std::vector<std::complex<double>> s(n);\n    std::vector<std::complex<double>> r(n);\n    std::vector<std::complex<double>> q(n);\n    std::vector<std::complex<double>> p(n);\n    std::vector<std::complex<double>> o(n);\n    std::vector<std::complex<double>> nn(n);\n    std::vector<std::complex<double>> mm(n);\n    std::vector<std::complex<double>> ll(n);\n    std::vector<std::complex<double>> kk(n);\n    std::vector<std::complex<double>> jj(n);\n    std::vector<std::complex<double>> ii(n);\n    std::vector<std::complex<double>> hh(n);\n    std::vector<std::complex<double>> gg(n);\n    std::vector<std::complex<double>> ff(n);\n    std::vector<std::complex<double>> ee(n);\n    std::vector<std::complex<double>> dd(n);\n    std::vector<std::complex<double>> cc(n);\n    std::vector<std::complex<double>> bb(n);\n    std::vector<std::complex<double>> aa(n);\n    std::vector<std::complex<double>> zz(n);\n    std::vector<std::complex<double>> yy(n);\n    std::vector<std::complex<double>> xx(n);\n    std::vector<std::complex<double>> ww(n);\n    std::vector<std::complex<double>> vv(n);\n    std::vector<std::complex<double>> uu(n);\n    std::vector<std::complex<double>> tt(n);\n    std::vector<std::complex<double>> ss(n);\n    std::vector<std::complex<double>> rr(n);\n    std::vector<std::complex<double>> pp(n);\n    std::vector<std::complex<double>> oo(n);\n    std::vector<std::complex<double>> nn2(n);\n    std::vector<std::complex<double>> mm2(n);\n    std::vector<std::complex<double>> ll2(n);\n    std::vector<std::complex<double>> kk2(n);\n    std::vector<std::complex<double>> jj2(n);\n    std::vector<std::complex<double>> ii2(n);\n    std::vector<std::complex<double>> hh2(n);\n    std::vector<std::complex<double>> gg2(n);\n    std::vector<std::complex<double>> ff2(n);\n    std::vector<std::complex<double>> ee2(n);\n    std::vector<std::complex<double>> dd2(n);\n    std::vector<std::complex<double>> cc2(n);\n    std::vector<std::complex<double>> bb2(n);\n    std::vector<std::complex<double>> aa2(n);\n    std::vector<std::complex<double>> zz2(n);\n    std::vector<std::complex<double>> yy2(n);\n    std::vector<std::complex<double>> xx2(n);\n    std::vector<std::complex<double>> ww2(n);\n    std::vector<std::complex<double>> vv2(n);\n    std::vector<std::complex<double>> uu2(n);\n    std::vector<std::complex<double>> tt2(n);\n    std::vector<std::complex<double>> ss2(n);\n    std::vector<std::complex<double>> rr2(n);\n    std::vector<std::complex<double>> pp2",
            "/* TODO: Your code here */\n    // 1. Compute the FFT of x\n    fft(x);\n    // 2. Compute the inverse FFT of x\n    // 3. Scale the result by 1/N\n    int N = x.size();\n    for (int i = 0; i < N; i++) {\n        x[i] /= N;\n    }\n}",
            "/* TODO: Your code here */\n    // 1. compute the FFT of x\n    fft(x);\n    // 2. scale the result by 1/N\n    double scale = 1.0 / x.size();\n    for (int i = 0; i < x.size(); i++) {\n        x[i] *= scale;\n    }\n}",
            "/* TODO: implement ifft */\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n    fft(x);\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}",
            "/* TODO: Your code here */\n    int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> u(n);\n    std::vector<std::complex<double>> v(n);\n    std::vector<std::complex<double>> t(n);\n    std::vector<std::complex<double>> s(n);\n    std::vector<std::complex<double>> r(n);\n    std::vector<std::complex<double>> q(n);\n    std::vector<std::complex<double>> p(n);\n    std::vector<std::complex<double>> o(n);\n    std::vector<std::complex<double>> nn(n);\n    std::vector<std::complex<double>> mm(n);\n    std::vector<std::complex<double>> ll(n);\n    std::vector<std::complex<double>> kk(n);\n    std::vector<std::complex<double>> jj(n);\n    std::vector<std::complex<double>> ii(n);\n    std::vector<std::complex<double>> hh(n);\n    std::vector<std::complex<double>> gg(n);\n    std::vector<std::complex<double>> ff(n);\n    std::vector<std::complex<double>> ee(n);\n    std::vector<std::complex<double>> dd(n);\n    std::vector<std::complex<double>> cc(n);\n    std::vector<std::complex<double>> bb(n);\n    std::vector<std::complex<double>> aa(n);\n    std::vector<std::complex<double>> a(n);\n    std::vector<std::complex<double>> b(n);\n    std::vector<std::complex<double>> c(n);\n    std::vector<std::complex<double>> d(n);\n    std::vector<std::complex<double>> e(n);\n    std::vector<std::complex<double>> f(n);\n    std::vector<std::complex<double>> g(n);\n    std::vector<std::complex<double>> h(n);\n    std::vector<std::complex<double>> i(n);\n    std::vector<std::complex<double>> j(n);\n    std::vector<std::complex<double>> k(n);\n    std::vector<std::complex<double>> l(n);\n    std::vector<std::complex<double>> m(n);\n    std::vector<std::complex<double>> nn1(n);\n    std::vector<std::complex<double>> mm1(n);\n    std::vector<std::complex<double>> ll1(n);\n    std::vector<std::complex<double>> kk1(n);\n    std::vector<std::complex<double>> jj1(n);\n    std::vector<std::complex<double>> ii1(n);\n    std::vector<std::complex<double>> hh1(n);\n    std::vector<std::complex<double>> gg1(n);\n    std::vector<std::complex<double>> ff1(n);\n    std::vector<std::complex<double>> ee1(n);\n    std::vector<std::complex<double>> dd1(n);\n    std::vector<std::complex<double>> cc1(n);\n    std::vector<std::complex<double>> bb1(n);\n    std::vector<std::complex<double>> aa1(n);\n    std::vector<std::complex<double>> a1(n);\n    std::vector<std::complex<double>> b1(n);\n    std::vector<std::complex<double>> c1(n);\n    std::vector<std::complex<double>> d1(n);\n    std::vector<std::complex<double>> e1(n);\n    std::vector<std::complex<double>> f1(n);\n    std::vector<std::complex<double>> g1(n);\n    std::vector<std::complex<double>> h1(n);\n    std::vector<std::complex<double>>",
            "/* TODO: your code here */\n    int n = x.size();\n    std::vector<std::complex<double>> x_copy(n);\n    std::copy(x.begin(), x.end(), x_copy.begin());\n    fft(x_copy);\n    for (int i = 0; i < n; i++) {\n        x[i] = x_copy[i] / n;\n    }\n}",
            "/* Your code here */\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n    fft(x);\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n    double scale = 1.0 / n;\n    for (int i = 0; i < n; i++) {\n        x[i] *= scale;\n    }\n}",
            "/* TODO: implement ifft in-place */\n    int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    for (int i = 0; i < n; i++) {\n        y[i] = std::conj(x[i]);\n    }\n    fft(y);\n    for (int i = 0; i < n; i++) {\n        z[i] = y[i] / n;\n    }\n    x = z;\n}",
            "/* TODO: implement ifft */\n  int n = x.size();\n  int n2 = n/2;\n  int n4 = n/4;\n  int n8 = n/8;\n  int n16 = n/16;\n  int n32 = n/32;\n  int n64 = n/64;\n  int n128 = n/128;\n  int n256 = n/256;\n  int n512 = n/512;\n  int n1024 = n/1024;\n  int n2048 = n/2048;\n  int n4096 = n/4096;\n  int n8192 = n/8192;\n  int n16384 = n/16384;\n  int n32768 = n/32768;\n  int n65536 = n/65536;\n  int n131072 = n/131072;\n  int n262144 = n/262144;\n  int n524288 = n/524288;\n  int n1048576 = n/1048576;\n  int n2097152 = n/2097152;\n  int n4194304 = n/4194304;\n  int n8388608 = n/8388608;\n  int n16777216 = n/16777216;\n  int n33554432 = n/33554432;\n  int n67108864 = n/67108864;\n  int n134217728 = n/134217728;\n  int n268435456 = n/268435456;\n  int n536870912 = n/536870912;\n  int n1073741824 = n/1073741824;\n  int n2147483648 = n/2147483648;\n  int n4294967296 = n/4294967296;\n  int n8589934592 = n/8589934592;\n  int n17179869184 = n/17179869184;\n  int n34359738368 = n/34359738368;\n  int n68719476736 = n/68719476736;\n  int n137438953472 = n/137438953472;\n  int n274877906944 = n/274877906944;\n  int n549755813888 = n/549755813888;\n  int n1099511627776 = n/1099511627776;\n  int n2199023255552 = n/2199023255552;\n  int n4398046511104 = n/4398046511104;\n  int n8796093022208 = n/8796093022208;\n  int n17592186044416 = n/17592186044416;\n  int n35184372",
            "/* TODO: Your code here */\n    int n = x.size();\n    std::vector<std::complex<double>> x_copy(x);\n    fft(x);\n    for (int i = 0; i < n; i++) {\n        x[i] /= n;\n    }\n    std::vector<std::complex<double>> x_copy2(x);\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n    fft(x);\n    for (int i = 0; i < n; i++) {\n        x[i] /= n;\n    }\n    for (int i = 0; i < n; i++) {\n        x[i] = x_copy[i] * x[i];\n    }\n}",
            "/* TODO: implement ifft */\n    // fft(x);\n    // for (int i = 0; i < x.size(); i++) {\n    //     x[i] /= x.size();\n    // }\n    // return;\n    int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> u(n);\n    std::vector<std::complex<double>> v(n);\n    std::vector<std::complex<double>> t(n);\n    std::vector<std::complex<double>> s(n);\n    std::vector<std::complex<double>> r(n);\n    std::vector<std::complex<double>> q(n);\n    std::vector<std::complex<double>> p(n);\n    std::vector<std::complex<double>> o(n);\n    std::vector<std::complex<double>> nn(n);\n    std::vector<std::complex<double>> mm(n);\n    std::vector<std::complex<double>> ll(n);\n    std::vector<std::complex<double>> kk(n);\n    std::vector<std::complex<double>> jj(n);\n    std::vector<std::complex<double>> ii(n);\n    std::vector<std::complex<double>> hh(n);\n    std::vector<std::complex<double>> gg(n);\n    std::vector<std::complex<double>> ff(n);\n    std::vector<std::complex<double>> ee(n);\n    std::vector<std::complex<double>> dd(n);\n    std::vector<std::complex<double>> cc(n);\n    std::vector<std::complex<double>> bb(n);\n    std::vector<std::complex<double>> aa(n);\n    std::vector<std::complex<double>> a(n);\n    std::vector<std::complex<double>> b(n);\n    std::vector<std::complex<double>> c(n);\n    std::vector<std::complex<double>> d(n);\n    std::vector<std::complex<double>> e(n);\n    std::vector<std::complex<double>> f(n);\n    std::vector<std::complex<double>> g(n);\n    std::vector<std::complex<double>> h(n);\n    std::vector<std::complex<double>> i(n);\n    std::vector<std::complex<double>> j(n);\n    std::vector<std::complex<double>> k(n);\n    std::vector<std::complex<double>> l(n);\n    std::vector<std::complex<double>> m(n);\n    std::vector<std::complex<double>> nn1(n);\n    std::vector<std::complex<double>> mm1(n);\n    std::vector<std::complex<double>> ll1(n);\n    std::vector<std::complex<double>> kk1(n);\n    std::vector<std::complex<double>> jj1(n);\n    std::vector<std::complex<double>> ii1(n);\n    std::vector<std::complex<double>> hh1(n);\n    std::vector<std::complex<double>> gg1(n);\n    std::vector<std::complex<double>> ff1(n);\n    std::vector<std::complex<double>> ee1(n);\n    std::vector<std::complex<double>> dd1(n);\n    std::vector<std::complex<double>> cc1(n);\n    std::vector<std::complex<double>> bb1(n);\n    std::vector<std::complex<double>> aa1(n);\n    std::vector<std::complex<double>> a1(n);\n    std::vector<std::complex<double>> b1(n);\n    std::vector<std::complex<double>> c1(n);\n    std::vector<std::complex<double>> d1(n);\n    std::vector<std::complex<double>> e1(n);\n    std::vector<std::complex<double>>",
            "/* TODO: Your code here */\n    // 1. compute fft\n    fft(x);\n    // 2. divide by length\n    double length = x.size();\n    for (auto &i : x) {\n        i /= length;\n    }\n}",
            "/* TODO: Your code here */\n    int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> t(n);\n    std::vector<std::complex<double>> u(n);\n    std::vector<std::complex<double>> v(n);\n    std::vector<std::complex<double>> x_temp(n);\n    std::vector<std::complex<double>> y_temp(n);\n    std::vector<std::complex<double>> z_temp(n);\n    std::vector<std::complex<double>> w_temp(n);\n    std::vector<std::complex<double>> t_temp(n);\n    std::vector<std::complex<double>> u_temp(n);\n    std::vector<std::complex<double>> v_temp(n);\n    std::vector<std::complex<double>> x_temp2(n);\n    std::vector<std::complex<double>> y_temp2(n);\n    std::vector<std::complex<double>> z_temp2(n);\n    std::vector<std::complex<double>> w_temp2(n);\n    std::vector<std::complex<double>> t_temp2(n);\n    std::vector<std::complex<double>> u_temp2(n);\n    std::vector<std::complex<double>> v_temp2(n);\n    std::vector<std::complex<double>> x_temp3(n);\n    std::vector<std::complex<double>> y_temp3(n);\n    std::vector<std::complex<double>> z_temp3(n);\n    std::vector<std::complex<double>> w_temp3(n);\n    std::vector<std::complex<double>> t_temp3(n);\n    std::vector<std::complex<double>> u_temp3(n);\n    std::vector<std::complex<double>> v_temp3(n);\n    std::vector<std::complex<double>> x_temp4(n);\n    std::vector<std::complex<double>> y_temp4(n);\n    std::vector<std::complex<double>> z_temp4(n);\n    std::vector<std::complex<double>> w_temp4(n);\n    std::vector<std::complex<double>> t_temp4(n);\n    std::vector<std::complex<double>> u_temp4(n);\n    std::vector<std::complex<double>> v_temp4(n);\n    std::vector<std::complex<double>> x_temp5(n);\n    std::vector<std::complex<double>> y_temp5(n);\n    std::vector<std::complex<double>> z_temp5(n);\n    std::vector<std::complex<double>> w_temp5(n);\n    std::vector<std::complex<double>> t_temp5(n);\n    std::vector<std::complex<double>> u_temp5(n);\n    std::vector<std::complex<double>> v_temp5(n);\n    std::vector<std::complex<double>> x_temp6(n);\n    std::vector<std::complex<double>> y_temp6(n);\n    std::vector<std::complex<double>> z_temp6(n);\n    std::vector<std::complex<double>> w_temp6(n);\n    std::vector<std::complex<double>> t_temp6(n);\n    std::vector<std::complex<double>> u_temp6(n);\n    std::vector<std::complex<double>> v_temp6(n);\n    std::vector<std::complex<double>> x_temp7(n);\n    std::vector<std::complex<double>> y_temp7(n);\n    std::vector<std::complex<double>> z_temp7(n);\n    std::vector<std::complex<double>> w_temp7(n);\n    std::vector<std::complex<double>> t_temp7(n);\n    std::vector<std::complex<double>> u_temp7(n);\n    std::vector",
            "/* TODO: implement ifft */\n  int n = x.size();\n  int n2 = n/2;\n  int n4 = n2/2;\n  int n8 = n4/2;\n  int n16 = n8/2;\n  int n32 = n16/2;\n  int n64 = n32/2;\n  int n128 = n64/2;\n  int n256 = n128/2;\n  int n512 = n256/2;\n  int n1024 = n512/2;\n  int n2048 = n1024/2;\n  int n4096 = n2048/2;\n  int n8192 = n4096/2;\n  int n16384 = n8192/2;\n  int n32768 = n16384/2;\n  int n65536 = n32768/2;\n  int n131072 = n65536/2;\n  int n262144 = n131072/2;\n  int n524288 = n262144/2;\n  int n1048576 = n524288/2;\n  int n2097152 = n1048576/2;\n  int n4194304 = n2097152/2;\n  int n8388608 = n4194304/2;\n  int n16777216 = n8388608/2;\n  int n33554432 = n16777216/2;\n  int n67108864 = n33554432/2;\n  int n134217728 = n67108864/2;\n  int n268435456 = n134217728/2;\n  int n536870912 = n268435456/2;\n  int n1073741824 = n536870912/2;\n  int n2147483648 = n1073741824/2;\n  int n4294967296 = n2147483648/2;\n  int n8589934592 = n4294967296/2;\n  int n17179869184 = n8589934592/2;\n  int n34359738368 = n17179869184/2;\n  int n68719476736 = n34359738368/2;\n  int n137438953472 = n68719476736/2;\n  int n274877906944 = n137438953472/2;\n  int n549755813888 = n274877906944/2;\n  int n1099511627776 = n549755813888/2;\n  int n2199023255552 = n1099511627776/2;\n  int n4398046511104 = n2199023255552/2;\n  int n8796093022208 = n4398046511104/2;\n  int n17592186044416",
            "/* TODO: implement ifft */\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n    fft(x);\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]) / n;\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "serial",
        "prompt": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "/* compute the forward fft */\n  fft(x);\n  /* compute the inverse fft */\n  for (auto &c : x) {\n    c /= x.size();\n  }\n}",
            "fft(x);\n    for (auto &v : x) {\n        v /= x.size();\n    }\n}",
            "/* TODO: implement ifft */\n  /*\n   * 1. compute fft\n   * 2. divide by N\n   * 3. conjugate\n   */\n  fft(x);\n  for (auto &i : x) {\n    i /= x.size();\n    i = std::conj(i);\n  }\n}",
            "fft(x);\n  for (auto &x_i : x) {\n    x_i /= x.size();\n  }\n}",
            "/* Compute the forward fourier transform */\n  fft(x);\n\n  /* Compute the inverse fourier transform */\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = std::conj(x[i]) / x.size();\n  }\n}",
            "/* compute the forward transform */\n  fft(x);\n  /* compute the inverse transform */\n  for (auto &c : x) {\n    c /= x.size();\n  }\n}",
            "/* compute the forward transform */\n  fft(x);\n\n  /* divide by the number of elements */\n  for (int i = 0; i < x.size(); i++) {\n    x[i] /= x.size();\n  }\n}",
            "/*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  fft(y);\n  for (size_t i = 0; i < x.size(); i++) {\n    x[i] = y[i] / x.size();\n  }\n  */\n  /*\n  // alternative implementation using fft\n  std::vector<std::complex<double>> y(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    y[i] = x[i] / x.size();\n  }\n  f",
            "fft(x);\n  for (auto &c : x) {\n    c /= x.size();\n  }\n}",
            "/*\n   * TODO:\n   * 1. compute the inverse fourier transform of x in-place\n   * 2. make sure that the result is normalized\n   * 3. make sure that the result is in the correct order\n   */\n  fft(x);\n  for (auto &c : x) {\n    c /= x.size();\n  }\n  std::reverse(x.begin(), x.end());\n}",
            "std::reverse(x.begin(), x.end());\n    fft(x);\n    for (auto &c : x) {\n        c /= x.size();\n    }\n}",
            "fft(x);\n  for (auto &v : x) {\n    v /= x.size();\n  }\n}",
            "/* compute the forward fourier transform */\n  fft(x);\n\n  /* compute the inverse fourier transform */\n  for (auto &v : x) {\n    v /= x.size();\n  }\n}",
            "/*\n   * TODO:\n   * 1. Compute the inverse fourier transform of x in-place.\n   * 2. Use the fft function you wrote above.\n   * 3. Divide the result by the number of elements in x.\n   */\n  fft(x);\n  for (int i = 0; i < x.size(); i++) {\n    x[i] /= x.size();\n  }\n}",
            "/* compute the forward transform */\n  fft(x);\n\n  /* divide by the number of elements */\n  for (auto &v : x) {\n    v /= x.size();\n  }\n}",
            "/* compute the forward fourier transform */\n    fft(x);\n\n    /* scale the result */\n    for (auto &c : x) {\n        c /= x.size();\n    }\n}",
            "fft(x);\n  for (auto &c : x) {\n    c /= x.size();\n  }\n}",
            "/* compute forward fft */\n  fft(x);\n\n  /* compute inverse fft */\n  for (auto &v : x) {\n    v /= x.size();\n  }\n}",
            "std::reverse(x.begin(), x.end());\n    fft(x);\n    for (auto &c : x) {\n        c /= x.size();\n    }\n}",
            "/* compute forward fft */\n    fft(x);\n\n    /* compute inverse fft */\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= x.size();\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "hip",
        "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(hipDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    // Compute the inverse fourier transform of x in-place.\n    // Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n    // Example:\n    //\n    // input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n    // output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n\n    // TODO: Implement the inverse fourier transform of x in-place.\n    // You may use the following functions:\n    //\n    // hipDeviceSynchronize()\n    // hipGetDeviceCount()\n    // hipGetDevice()\n    // hipSetDevice()\n    // hipMalloc()\n    // hipFree()\n    // hipMemcpy()\n    // hipMemcpyDeviceToHost()\n    // hipMemcpyHostToDevice()\n    // hipMemset()\n    // hipLaunchKernelGGL()\n    // hipDeviceSynchronize()\n    // hipGetLastError()\n    // hipfftPlan1d()\n    // hipfftDestroy()\n    // hipfftExecD2Z()\n    // hipfftExecZ2D()\n    // hipfftMakePlan1d()\n    // hipfftSetAutoAllocation()\n    // hipfftSetWorkArea()\n    // hipfftGetSize1d()\n    // hipfftGetSizeMany()\n    // hipfftGetSize()\n    // hipfftGetVersion()\n    // hipfftGetPlanSize()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGetPlan()\n    // hipfftGetId()\n    // hipfftGetType()\n    // hipfftGetWorkSize()\n    // hipfftGetAllocation()\n    // hipfftGetVersion()\n    // hipfftGetPlanMany()\n    // hipfftGet",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    x[i] = hipCmul(x[i], hipConj(x[(N - i) % N]));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    if (i == 0) {\n        x[0] = make_hipDoubleComplex(0.5 * hipCabsf(x[0]), 0);\n        return;\n    }\n    size_t j = N - i;\n    hipDoubleComplex xj = x[j];\n    x[j] = make_hipDoubleComplex(0, 0);\n    x[i] = make_hipDoubleComplex(hipCabsf(x[i]), 0);\n    if (i < N / 2) {\n        x[i] = hipCadd(x[i], hipConjf(xj));\n        x[j] = hipCsub(x[j], hipConjf(x[i]));\n    } else {\n        x[i] = hipCsub(x[i], hipConjf(xj));\n        x[j] = hipCadd(x[j], hipConjf(x[i]));\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // Bit-reversed addressing permutation\n    size_t j = reverseBits(tid, log2(N));\n\n    // Butterfly\n    size_t k = N / 2;\n    while (k > 0) {\n        size_t l = j & (k - 1);\n        size_t i = j - l;\n        size_t m = i + k;\n        if (m >= N) m -= N;\n\n        hipDoubleComplex z = x[m];\n        x[m] = x[i] - z;\n        x[i] += z;\n\n        j = i;\n        k >>= 1;\n    }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n    double arg = -2 * M_PI * tid / N;\n    hipDoubleComplex z = make_hipDoubleComplex(cos(arg), sin(arg));\n    hipDoubleComplex temp = x[tid];\n    x[tid] = hipCmul(temp, z);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  x[i] = hipCexp(hipDoubleComplex(-2.0 * M_PI * i / N, 0.0) * x[i]);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = N - i;\n    if (i > j) {\n        hipDoubleComplex tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = N - i;\n    if (i > j) {\n        hipDoubleComplex tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n    double theta = 2 * M_PI * tid / N;\n    double w = cos(theta);\n    double w_c = sin(theta);\n    hipDoubleComplex tmp = x[tid];\n    x[tid] = x[tid] + x[N - tid];\n    x[N - tid] = tmp - w * (x[N - tid]);\n    for (size_t s = 2; s <= N; s *= 2) {\n        size_t m = s / 2;\n        size_t offset = tid & (s - 1);\n        if (offset >= m) {\n            size_t j = tid - offset + m;\n            hipDoubleComplex z = x[j] * hipConj(w_c) - x[tid] * hipConj(w);\n            x[j] = x[tid] * hipConj(w_c) + x[j] * hipConj(w);\n            x[tid] = z;\n        }\n        w = w * w - w_c * w_c;\n        w_c = w_c * (w_c + 2 * w);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    if (i == 0) {\n        x[0] = make_hipDoubleComplex(0.5, 0);\n        return;\n    }\n    double theta = 2 * M_PI * i / N;\n    double sn = sin(theta);\n    double cs = cos(theta);\n    double2 c = make_hipDoubleComplex(cs, sn);\n    double2 s = make_hipDoubleComplex(-sn, cs);\n    double2 xi = x[i];\n    double2 xip1 = x[(i + 1) % N];\n    double2 xi_new = make_hipDoubleComplex(\n        xi.x * c.x - xi.y * s.x,\n        xi.x * s.x + xi.y * c.x\n    );\n    double2 xip1_new = make_hipDoubleComplex(\n        xip1.x * c.x - xip1.y * s.x,\n        xip1.x * s.x + xip1.y * c.x\n    );\n    x[i] = xi_new;\n    x[(i + 1) % N] = xip1_new;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  double theta = 2 * M_PI * i / N;\n  double s = sin(theta);\n  double c = cos(theta);\n  hipDoubleComplex w = make_hipDoubleComplex(c, s);\n  hipDoubleComplex t = x[i];\n  x[i] = x[0];\n  for (size_t j = 1; j < N; j++) {\n    size_t k = (j * i) % N;\n    hipDoubleComplex u = w * x[k];\n    x[k] = t - u;\n    t = t + u;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    // Compute the inverse FFT of x\n    //...\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    x[i] = hipCexp(hipDoubleComplex(-2.0 * M_PI * i / N, 0.0) * x[i]);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    x[i] = hipCexp(hipDoubleComplex(-2 * M_PI * i / N, 0) * x[i]);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    hipDoubleComplex temp = x[i];\n    x[i] = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t j = 0; j < N; j++) {\n        double angle = -2.0 * M_PI * i * j / N;\n        hipDoubleComplex value = make_hipDoubleComplex(cos(angle), sin(angle));\n        x[i] = hipCadd(x[i], hipCmul(value, temp));\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    double theta = 2 * M_PI * i / N;\n    double w = cos(theta);\n    double w_conj = cos(theta);\n    double x_real = x[i].x;\n    double x_imag = x[i].y;\n    double x_real_new = (x_real + x_imag) / 2;\n    double x_imag_new = (x_imag - x_real) / 2;\n    x[i].x = x_real_new * w - x_imag_new * w_conj;\n    x[i].y = x_imag_new * w + x_real_new * w_conj;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  if (i == 0) {\n    x[0] = make_hipDoubleComplex(0.5, 0);\n  } else {\n    double theta = 2 * M_PI * i / N;\n    double sn = sin(theta);\n    double cs = cos(theta);\n    hipDoubleComplex z = make_hipDoubleComplex(sn, cs);\n    x[i] = make_hipDoubleComplex(sn, -cs);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    if (i == 0) {\n        x[0] = make_hipDoubleComplex(0.5, 0);\n        return;\n    }\n    size_t n = N >> 1;\n    if (i > n) {\n        x[i] = make_hipDoubleComplex(0, 0);\n        return;\n    }\n    double angle = -2 * M_PI * i / N;\n    double s = sin(angle);\n    double c = cos(angle);\n    hipDoubleComplex z = x[i];\n    hipDoubleComplex w = make_hipDoubleComplex(c, s);\n    hipDoubleComplex u = x[n - i];\n    hipDoubleComplex t = make_hipDoubleComplex(c, -s);\n    x[i] = z + w * u;\n    x[n - i] = z - w * u;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = hipCexp(hipDoubleComplex(-2.0 * M_PI * tid / N, 0.0) * x[tid]);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    // Compute the inverse fourier transform of x in-place.\n    // Use AMD HIP to compute in parallel.\n    // The kernel is launched with at least N threads.\n    // Example:\n    //\n    // input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n    // output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n    //\n    // The input is assumed to be a real-valued signal.\n    // The output is a complex-valued signal.\n    // The output is normalized to be real-valued.\n    //\n    // The input is assumed to be in the time domain.\n    // The output is in the frequency domain.\n    //\n    // The input is assumed to be in the following order:\n    // [x(0), x(1), x(2),..., x(N-1)]\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(N/2), X(1), X(N/2-1), X(2), X(N/2-2),..., X(N/2)]\n    // where X(k) = X(N-k), for k = 0,..., N/2\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(1),..., X(N/2), X(N/2-1),..., X(1)]\n    // where X(k) = X(N-k), for k = 0,..., N/2\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(N/4), X(N/2), X(3N/4), X(1), X(N/4-1), X(N/2-1), X(3N/4-1),..., X(N/4)]\n    // where X(k) = X(N-k), for k = 0,..., N/4\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(1),..., X(N/4), X(N/4-1),..., X(1)]\n    // where X(k) = X(N-k), for k = 0,..., N/4\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(N/8), X(N/4), X(3N/8), X(1), X(N/8-1), X(N/4-1), X(3N/8-1),..., X(N/8)]\n    // where X(k) = X(N-k), for k = 0,..., N/8\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(1),..., X(N/8), X(N/8-1),..., X(1)]\n    // where X(k) = X(N-k), for k = 0,..., N/8\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(N/16), X(N/8), X(3N/16), X(1), X(N/16-1), X(N/8-1), X(3N/16-1),..., X(N/16)]\n    // where X(k) = X(N-k), for k = 0,..., N/16\n    //\n    // The output is assumed to be in the following order:\n    // [X(0), X(1),..., X(N/16),"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement ifft\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  int n = x.size();\n  int n_local = n / size;\n  int n_local_start = rank * n_local;\n  int n_local_end = n_local_start + n_local;\n  std::vector<std::complex<double>> x_local(n_local);\n  for (int i = n_local_start; i < n_local_end; i++) {\n    x_local[i - n_local_start] = x[i];\n  }\n  fft(x_local);\n  std::vector<std::complex<double>> x_local_inv(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local_inv[i] = std::conj(x_local[i]) / n;\n  }\n  fft(x_local_inv);\n  for (int i = 0; i < n_local; i++) {\n    x[i + n_local_start] = x_local_inv[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_local; i++) {\n      x[i] = x_local_inv[i];\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /*\n     * TODO:\n     * 1. Compute the inverse fourier transform of x in-place.\n     * 2. Use MPI to compute in parallel.\n     * 3. Assume MPI has already been initialized.\n     * 4. Every rank has a complete copy of x.\n     * 5. The final result is stored on rank 0.\n     */\n\n    // TODO: your code here\n    fft(x);\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] / x.size();\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // compute the inverse fft on each rank\n  fft(x);\n\n  // gather the results from all ranks to rank 0\n  std::vector<std::complex<double>> result(x.size());\n  MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // divide by the number of ranks to get the true inverse fft\n  if (rank == 0) {\n    for (int i = 0; i < result.size(); i++) {\n      result[i] /= size;\n    }\n  }\n\n  // broadcast the result to all ranks\n  MPI_Bcast(result.data(), result.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // copy the result back to x\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = result[i];\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* compute the inverse fourier transform of x on rank 0 */\n    if (rank == 0) {\n        fft(x);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] = std::conj(x[i]) / x.size();\n        }\n    }\n\n    /* broadcast the result to all other ranks */\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /*\n    TODO:\n    - Compute the inverse fourier transform of x in-place.\n    - Use MPI to compute in parallel.\n    - Assume MPI has already been initialized.\n    - Every rank has a complete copy of x.\n    - The final result is stored on rank 0.\n  */\n\n  // TODO: Your code here\n  if (rank == 0) {\n    fft(x);\n    for (auto& i : x) {\n      i /= x.size();\n    }\n  }\n\n  MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: your code here */\n  if (rank == 0) {\n    fft(x);\n  }\n  MPI_Bcast(&x[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  for (int i = 0; i < x.size(); i++) {\n    x[i] /= x.size();\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // compute the inverse fourier transform on rank 0\n    if (rank == 0) {\n        fft(x);\n        for (auto &c : x) {\n            c /= x.size();\n        }\n    }\n\n    // broadcast the result to all other ranks\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* compute the inverse fourier transform of x on rank 0 */\n    if (rank == 0) {\n        fft(x);\n        for (auto &z : x) {\n            z /= x.size();\n        }\n    }\n\n    /* broadcast the result to all other ranks */\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: implement ifft in-place using MPI */\n    if (rank == 0) {\n        fft(x);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] = x[i] / x.size();\n        }\n    } else {\n        fft(x);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[0], x.size(), MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        fft(x);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] /= x.size();\n        }\n    } else {\n        fft(x);\n    }\n\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // compute the inverse fourier transform of x in-place\n    //...\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* compute the inverse fft on each rank */\n    fft(x);\n\n    /* gather the results on rank 0 */\n    if (rank == 0) {\n        std::vector<std::complex<double>> x_all(size * x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, x_all.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = x_all;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n\n    /* compute the inverse fft on rank 0 */\n    if (rank == 0) {\n        fft(x);\n    }\n\n    /* broadcast the result to all ranks */\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  fft(x);\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] / x.size();\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: Your code here */\n    // 1. \u5206\u914d\u4efb\u52a1\n    // 2. \u6536\u96c6\u7ed3\u679c\n    // 3. \u5408\u5e76\u7ed3\u679c\n\n    // 1. \u5206\u914d\u4efb\u52a1\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n    int n_local = n_per_rank + (rank < n_extra? 1 : 0);\n    std::vector<std::complex<double>> x_local(n_local);\n    MPI_Scatter(x.data(), n_local, MPI_DOUBLE_COMPLEX, x_local.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // 2. \u6536\u96c6\u7ed3\u679c\n    fft(x_local);\n    std::vector<std::complex<double>> x_local_result(n_local);\n    MPI_Gather(x_local.data(), n_local, MPI_DOUBLE_COMPLEX, x_local_result.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // 3. \u5408\u5e76\u7ed3\u679c\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_local_result[i];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: your code here */\n  // 1. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft\n  fft(x);\n\n  // 2. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21ifft\n  // 2.1 \u5148\u628ax\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230y\n  std::vector<std::complex<double>> y(x.begin(), x.begin() + x.size() / 2);\n  // 2.2 \u5bf9y\u505aifft\n  fft(y);\n  // 2.3 \u628ay\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230x\u7684\u540e\u534a\u90e8\u5206\n  std::copy(y.begin(), y.begin() + y.size() / 2, x.begin() + x.size() / 2);\n\n  // 3. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft\n  fft(x);\n\n  // 4. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21ifft\n  // 4.1 \u5148\u628ax\u7684\u540e\u534a\u90e8\u5206\u590d\u5236\u5230y\n  std::copy(x.begin() + x.size() / 2, x.end(), y.begin());\n  // 4.2 \u5bf9y\u505aifft\n  fft(y);\n  // 4.3 \u628ay\u7684\u540e\u534a\u90e8\u5206\u590d\u5236\u5230x\u7684\u524d\u534a\u90e8\u5206\n  std::copy(y.begin() + y.size() / 2, y.end(), x.begin());\n\n  // 5. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft\n  fft(x);\n\n  // 6. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21ifft\n  // 6.1 \u5148\u628ax\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230y\n  std::copy(x.begin(), x.begin() + x.size() / 2, y.begin());\n  // 6.2 \u5bf9y\u505aifft\n  fft(y);\n  // 6.3 \u628ay\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230x\u7684\u524d\u534a\u90e8\u5206\n  std::copy(y.begin(), y.begin() + y.size() / 2, x.begin());\n\n  // 7. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft\n  fft(x);\n\n  // 8. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21ifft\n  // 8.1 \u5148\u628ax\u7684\u540e\u534a\u90e8\u5206\u590d\u5236\u5230y\n  std::copy(x.begin() + x.size() / 2, x.end(), y.begin());\n  // 8.2 \u5bf9y\u505aifft\n  fft(y);\n  // 8.3 \u628ay\u7684\u540e\u534a\u90e8\u5206\u590d\u5236\u5230x\u7684\u540e\u534a\u90e8\u5206\n  std::copy(y.begin() + y.size() / 2, y.end(), x.begin() + x.size() / 2);\n\n  // 9. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft\n  fft(x);\n\n  // 10. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21ifft\n  // 10.1 \u5148\u628ax\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230y\n  std::copy(x.begin(), x.begin() + x.size() / 2, y.begin());\n  // 10.2 \u5bf9y\u505aifft\n  fft(y);\n  // 10.3 \u628ay\u7684\u524d\u534a\u90e8\u5206\u590d\u5236\u5230x\u7684\u524d\u534a\u90e8\u5206\n  std::copy(y.begin(), y.begin() + y.size() / 2, x.begin());\n\n  // 11. \u5bf9\u4e8e\u6bcf\u4e2arank\uff0c\u90fd\u8981\u505a\u4e00\u6b21fft",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  //",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();\n  // }\n\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = std::conj(x[i]);\n  // }\n  // fft(x);\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] /= x.size();",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: your code here */\n    fft(x);\n    for (auto &i : x) {\n        i /= size;\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(cuDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex z = x[i];\n    cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n    for (size_t j = 0; j < N; j++) {\n        cuDoubleComplex u = x[j];\n        cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n        w = cuCadd(w, t);\n    }\n    x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + tid;\n  size_t j = N / 2 + i;\n  if (i < N / 2) {\n    cuDoubleComplex x_i = x[i];\n    cuDoubleComplex x_j = x[j];\n    x[i] = cuCadd(x_i, x_j);\n    x[j] = cuCsub(x_i, x_j);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex z = x[i];\n    cuDoubleComplex z_conj = make_cuDoubleComplex(cuCreal(z), -cuCimag(z));\n    cuDoubleComplex sum = make_cuDoubleComplex(0, 0);\n    for (size_t j = 0; j < N; j++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * i * j / N), -sin(2 * M_PI * i * j / N));\n        sum = cuCadd(sum, cuCmul(w, x[j]));\n    }\n    x[i] = cuCmul(make_cuDoubleComplex(1.0 / N, 0), cuCadd(z, z_conj));\n    x[i] = cuCadd(x[i], sum);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex x_i = x[i];\n    cuDoubleComplex x_N_minus_i = x[N - i];\n    cuDoubleComplex x_i_conj = make_cuDoubleComplex(cuCreal(x_i), -cuCimag(x_i));\n    cuDoubleComplex x_N_minus_i_conj = make_cuDoubleComplex(cuCreal(x_N_minus_i), -cuCimag(x_N_minus_i));\n    cuDoubleComplex x_i_plus_x_N_minus_i = cuCadd(x_i, x_N_minus_i);\n    cuDoubleComplex x_i_minus_x_N_minus_i = cuCsub(x_i, x_N_minus_i);\n    cuDoubleComplex x_i_plus_x_N_minus_i_conj = cuCadd(x_i_conj, x_N_minus_i_conj);\n    cuDoubleComplex x_i_minus_x_N_minus_i_conj = cuCsub(x_i_conj, x_N_minus_i_conj);\n    x[i] = cuCdiv(cuCadd(x_i_plus_x_N_minus_i, x_i_plus_x_N_minus_i_conj), make_cuDoubleComplex(2.0, 0.0));\n    x[N - i] = cuCdiv(cuCsub(x_i_minus_x_N_minus_i, x_i_minus_x_N_minus_i_conj), make_cuDoubleComplex(2.0, 0.0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex xi = x[i];\n  cuDoubleComplex yi = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * i * j / N), sin(2 * M_PI * i * j / N));\n    yi = cuCadd(yi, cuCmul(x[j], w));\n  }\n  x[i] = yi;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex tmp = x[i];\n    cuDoubleComplex tmp2 = x[N - i];\n    cuDoubleComplex tmp3 = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex tmp4 = make_cuDoubleComplex(0, 0);\n    if (i > 0) {\n        tmp3 = make_cuDoubleComplex(cos(2 * M_PI * i / N), sin(2 * M_PI * i / N));\n        tmp4 = make_cuDoubleComplex(cos(2 * M_PI * (N - i) / N), sin(2 * M_PI * (N - i) / N));\n    }\n    x[i] = cuCdiv(tmp + tmp2, make_cuDoubleComplex(2, 0));\n    x[i] = cuCmul(x[i], tmp3);\n    x[N - i] = cuCdiv(tmp - tmp2, make_cuDoubleComplex(2, 0));\n    x[N - i] = cuCmul(x[N - i], tmp4);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex x_i = x[i];\n    cuDoubleComplex x_i_conj = make_cuDoubleComplex(x_i.x, -x_i.y);\n    cuDoubleComplex x_i_conj_mul_x_i = cuCmul(x_i_conj, x_i);\n    cuDoubleComplex x_i_conj_mul_x_i_div_N = cuCdiv(x_i_conj_mul_x_i, make_cuDoubleComplex(N, 0));\n    x[i] = cuCsqrt(x_i_conj_mul_x_i_div_N);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n    cuDoubleComplex *x_ = x;\n    cuDoubleComplex *x_end = x + N;\n    cuDoubleComplex *x_start = x + N / 2;\n    cuDoubleComplex *x_end_start = x + N / 2;\n    cuDoubleComplex *x_start_start = x;\n    cuDoubleComplex *x_end_end = x + N;\n    cuDoubleComplex *x_start_end = x + N / 2;\n    cuDoubleComplex *x_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end_end_end = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_end_end_start = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_end_end_end_start = x + N / 2;\n    cuDoubleComplex *x_end_end_end_end_end_end_end_end_end_end = x + N;\n    cuDoubleComplex *x_start_end_end_end_end_end_",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex tmp = x[i];\n    x[i] = cuCmul(tmp, cuCdiv(make_cuDoubleComplex(1.0, 0.0), make_cuDoubleComplex(N, 0.0)));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex xi = x[i];\n    cuDoubleComplex xip = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xim = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi2 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi2p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi2m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi3 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi3p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi3m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi4 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi4p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi4m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi5 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi5p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi5m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi6 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi6p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi6m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi7 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi7p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi7m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi8 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi8p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi8m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi9 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi9p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi9m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi10 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi10p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi10m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi11 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi11p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi11m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi12 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi12p = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi12m = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi13 = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex xi13p",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex xi = x[i];\n    cuDoubleComplex x_ = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex x_conj = make_cuDoubleComplex(0, 0);\n    for (size_t j = 0; j < N; j++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(-2 * M_PI * i * j / N), sin(-2 * M_PI * i * j / N));\n        x_ += x[j] * w;\n        x_conj += conj(x[j]) * w;\n    }\n    x[i] = x_ / N;\n    x[i + N / 2] = x_conj / N;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex z = x[i];\n  cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n  for (size_t j = 0; j < N; j++) {\n    cuDoubleComplex u = x[j];\n    cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n    w = cuCadd(w, t);\n  }\n  x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t tid = threadIdx.x;\n    size_t nthreads = blockDim.x;\n    size_t nblocks = gridDim.x;\n    size_t i, j;\n    cuDoubleComplex c, s, t;\n    cuDoubleComplex *X = x;\n    cuDoubleComplex *Y = x + N/2;\n    cuDoubleComplex *Z = x + N;\n    cuDoubleComplex *W = x + 3*N/2;\n    cuDoubleComplex *U = x + 2*N;\n    cuDoubleComplex *V = x + 5*N/2;\n    cuDoubleComplex *WW = x + 3*N;\n    cuDoubleComplex *UU = x + 4*N;\n    cuDoubleComplex *VV = x + 7*N/2;\n    cuDoubleComplex *ZZ = x + N/2;\n    cuDoubleComplex *YY = x + 3*N/2;\n    cuDoubleComplex *XX = x + 5*N/2;\n    cuDoubleComplex *WWW = x + 3*N/2;\n    cuDoubleComplex *UUU = x + 4*N/2;\n    cuDoubleComplex *VVV = x + 7*N/2;\n    cuDoubleComplex *ZZZ = x + 5*N/2;\n    cuDoubleComplex *YYY = x + 7*N/2;\n    cuDoubleComplex *XXX = x + 9*N/2;\n    cuDoubleComplex *WWWW = x + 5*N/2;\n    cuDoubleComplex *UUUU = x + 6*N/2;\n    cuDoubleComplex *VVVV = x + 11*N/2;\n    cuDoubleComplex *ZZZZ = x + 7*N/2;\n    cuDoubleComplex *YYYY = x + 13*N/2;\n    cuDoubleComplex *XXXX = x + 15*N/2;\n    cuDoubleComplex *WWWWW = x + 9*N/2;\n    cuDoubleComplex *UUUUU = x + 10*N/2;\n    cuDoubleComplex *VVVVV = x + 17*N/2;\n    cuDoubleComplex *ZZZZZ = x + 11*N/2;\n    cuDoubleComplex *YYYYY = x + 19*N/2;\n    cuDoubleComplex *XXXXX = x + 21*N/2;\n    cuDoubleComplex *WWWWWW = x + 13*N/2;\n    cuDoubleComplex *UUUUUU = x + 14*N/2;\n    cuDoubleComplex *VVVVVV = x + 25*N/2;\n    cuDoubleComplex *ZZZZZZ = x + 15*N/2;\n    cuDoubleComplex *YYYYYY = x + 27*N/2;\n    cuDoubleComplex *XXXXXX = x + 29*N/2;\n    cuDoubleComplex *WWWWWWW = x + 17*N/2;\n    cuDoubleComplex *UUUUUUU = x + 18*N/2;\n    cuDoubleComplex *VVVVVVV = x + 33*N/2;\n    cuDoubleComplex *ZZZZZZZ = x + 19*N/2;\n    cuDoubleComplex *YYYYYYY = x + 35*N/2;\n    cuDoubleComplex *XXXXXXX = x + 37*N/2;\n    cuDoubleComplex *WWWWWWWW = x + 21*N/2;\n    cuDoubleComplex *UUUUUUUU = x + 22*N/2;\n    cuDoubleComplex *VVVVVVVV = x + 41*N/2;\n    cuDoubleComplex *ZZZZZZZZ = x + 23*N/2;\n    cuDoubleComplex *YYYYYYYY",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex z = x[i];\n    cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n    for (size_t j = 0; j < N; j++) {\n        cuDoubleComplex u = x[j];\n        cuDoubleComplex t = cuCmul(u, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * j / N)));\n        w = cuCadd(w, t);\n    }\n    x[i] = cuCdiv(w, make_cuDoubleComplex(N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    cuDoubleComplex tmp = x[i];\n    x[i] = cuCmul(x[i], cuCdiv(make_cuDoubleComplex(1.0, 0.0), make_cuDoubleComplex(N, 0.0)));\n    for (size_t j = 1; j < N; j++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * i * j / N), -sin(2 * M_PI * i * j / N));\n        x[i] = cuCadd(x[i], cuCmul(x[N - j], w));\n    }\n    x[i] = cuCmul(x[i], make_cuDoubleComplex(1.0 / N, 0.0));\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement ifft\n  // Hint: you may want to use fft to compute the inverse transform\n  // Hint: you may want to use MPI_Scatter and MPI_Gather to distribute the work\n  // Hint: you may want to use OpenMP to parallelize the work\n  // Hint: you may want to use std::complex<double>::operator/ to divide complex numbers\n  // Hint: you may want to use std::complex<double>::operator* to multiply complex numbers\n  // Hint: you may want to use std::complex<double>::operator+ to add complex numbers\n  // Hint: you may want to use std::complex<double>::operator- to subtract complex numbers\n  // Hint: you may want to use std::complex<double>::real() to get the real part of a complex number\n  // Hint: you may want to use std::complex<double>::imag() to get the imaginary part of a complex number\n  // Hint: you may want to use std::complex<double>::conj() to get the complex conjugate of a complex number\n  // Hint: you may want to use std::complex<double>::operator== to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator!= to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator= to assign complex numbers\n  // Hint: you may want to use std::complex<double>::operator+= to add complex numbers\n  // Hint: you may want to use std::complex<double>::operator-= to subtract complex numbers\n  // Hint: you may want to use std::complex<double>::operator*= to multiply complex numbers\n  // Hint: you may want to use std::complex<double>::operator/= to divide complex numbers\n  // Hint: you may want to use std::complex<double>::operator== to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator!= to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator= to assign complex numbers\n  // Hint: you may want to use std::complex<double>::operator+= to add complex numbers\n  // Hint: you may want to use std::complex<double>::operator-= to subtract complex numbers\n  // Hint: you may want to use std::complex<double>::operator*= to multiply complex numbers\n  // Hint: you may want to use std::complex<double>::operator/= to divide complex numbers\n  // Hint: you may want to use std::complex<double>::operator== to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator!= to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator= to assign complex numbers\n  // Hint: you may want to use std::complex<double>::operator+= to add complex numbers\n  // Hint: you may want to use std::complex<double>::operator-= to subtract complex numbers\n  // Hint: you may want to use std::complex<double>::operator*= to multiply complex numbers\n  // Hint: you may want to use std::complex<double>::operator/= to divide complex numbers\n  // Hint: you may want to use std::complex<double>::operator== to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator!= to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator= to assign complex numbers\n  // Hint: you may want to use std::complex<double>::operator+= to add complex numbers\n  // Hint: you may want to use std::complex<double>::operator-= to subtract complex numbers\n  // Hint: you may want to use std::complex<double>::operator*= to multiply complex numbers\n  // Hint: you may want to use std::complex<double>::operator/= to divide complex numbers\n  // Hint: you may want to use std::complex<double>::operator== to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator!= to compare complex numbers\n  // Hint: you may want to use std::complex<double>::operator= to assign",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  if (rank == 0) {\n    fft(x);\n    for (auto &i : x) {\n      i /= x.size();\n    }\n  } else {\n    std::vector<std::complex<double>> x_rank(x.size() / size);\n    MPI_Scatter(x.data(), x_rank.size(), MPI_DOUBLE_COMPLEX, x_rank.data(), x_rank.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    fft(x_rank);\n    for (auto &i : x_rank) {\n      i /= x.size();\n    }\n    MPI_Gather(x_rank.data(), x_rank.size(), MPI_DOUBLE_COMPLEX, x.data(), x_rank.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: compute the inverse fourier transform of x in-place\n  // Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n  // Every rank has a complete copy of x. The final result is stored on rank 0.\n  // Example:\n  //\n  // input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n  // output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO: implement\n  // TODO:",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  // 1. split the vector into chunks\n  // 2. compute the fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute the inverse fft on rank 0\n  // 5. scatter the results to all ranks\n\n  // 1. split the vector into chunks\n  int chunk_size = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk_size;\n  int end = start + chunk_size;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<std::complex<double>> chunk(x.begin() + start, x.begin() + end);\n\n  // 2. compute the fft on each chunk\n  fft(chunk);\n\n  // 3. gather the results on rank 0\n  std::vector<std::complex<double>> all_chunks;\n  if (rank == 0) {\n    all_chunks.resize(x.size());\n  }\n  MPI_Gather(chunk.data(), chunk.size(), MPI_DOUBLE_COMPLEX, all_chunks.data(), chunk.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // 4. compute the inverse fft on rank 0\n  if (rank == 0) {\n    fft(all_chunks);\n  }\n\n  // 5. scatter the results to all ranks\n  MPI_Scatter(all_chunks.data(), chunk.size(), MPI_DOUBLE_COMPLEX, chunk.data(), chunk.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // 6. copy the results back to x\n  std::copy(chunk.begin(), chunk.end(), x.begin() + start);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: Your code here */\n    if (rank == 0) {\n        fft(x);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] /= x.size();\n        }\n    } else {\n        std::vector<std::complex<double>> x_local(x.size() / size);\n        for (int i = 0; i < x_local.size(); i++) {\n            x_local[i] = x[rank * x_local.size() + i];\n        }\n        fft(x_local);\n        for (int i = 0; i < x_local.size(); i++) {\n            x_local[i] /= x.size();\n        }\n        MPI_Send(&x_local[0], x_local.size(), MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    }\n    if (rank!= 0) {\n        MPI_Recv(&x[0], x.size() / size, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* compute the inverse fourier transform of x in-place */\n  fft(x);\n\n  /* compute the inverse of x in-place */\n  for (auto &elem : x) {\n    elem = std::conj(elem) / x.size();\n  }\n\n  /* compute the inverse fourier transform of x in-place */\n  fft(x);\n\n  /* gather the results from all ranks to rank 0 */\n  if (rank == 0) {\n    std::vector<std::complex<double>> recv_buf(x.size() * size);\n    MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recv_buf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    std::copy(recv_buf.begin(), recv_buf.end(), x.begin());\n  } else {\n    MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: your code here\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n    int n_local = n_per_rank + (rank < n_extra? 1 : 0);\n    std::vector<std::complex<double>> x_local(n_local);\n    std::vector<std::complex<double>> x_local_fft(n_local);\n    std::vector<std::complex<double>> x_local_ifft(n_local);\n    std::vector<std::complex<double>> x_local_fft_ifft(n_local);\n    std::vector<std::complex<double>> x_local_fft_ifft_fft(n_local);\n\n    // copy local data\n    for (int i = 0; i < n_local; i++) {\n        x_local[i] = x[rank * n_per_rank + i];\n    }\n\n    // fft\n    fft(x_local);\n    for (int i = 0; i < n_local; i++) {\n        x_local_fft[i] = x_local[i];\n    }\n\n    // ifft\n    fft(x_local_fft);\n    for (int i = 0; i < n_local; i++) {\n        x_local_ifft[i] = x_local_fft[i];\n    }\n\n    // fft\n    fft(x_local_ifft);\n    for (int i = 0; i < n_local; i++) {\n        x_local_fft_ifft[i] = x_local_ifft[i];\n    }\n\n    // ifft\n    fft(x_local_fft_ifft);\n    for (int i = 0; i < n_local; i++) {\n        x_local_fft_ifft_fft[i] = x_local_fft_ifft[i];\n    }\n\n    // copy back\n    for (int i = 0; i < n_local; i++) {\n        x[rank * n_per_rank + i] = x_local_fft_ifft_fft[i];\n    }\n\n    // gather\n    if (rank == 0) {\n        std::vector<std::complex<double>> x_gathered(n);\n        MPI_Gather(x.data(), n_local, MPI_DOUBLE_COMPLEX, x_gathered.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < n; i++) {\n            x[i] = x_gathered[i];\n        }\n    } else {\n        MPI_Gather(x.data(), n_local, MPI_DOUBLE_COMPLEX, NULL, n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: your code here\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n    std::vector<std::complex<double>> x_local(n_local);\n    std::vector<std::complex<double>> x_local_fft(n_local);\n    std::vector<std::complex<double>> x_local_ifft(n_local);\n\n    MPI_Scatter(x.data(), n_local, MPI_DOUBLE_COMPLEX, x_local.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    fft(x_local);\n\n    MPI_Gather(x_local.data(), n_local, MPI_DOUBLE_COMPLEX, x_local_fft.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_local_fft[i] / n;\n        }\n    }\n\n    MPI_Bcast(x.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    fft(x);\n\n    MPI_Gather(x.data(), n_local, MPI_DOUBLE_COMPLEX, x_local_ifft.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_local_ifft[i] / n;\n        }\n    }\n\n    MPI_Bcast(x.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use MPI_Bcast to broadcast the final result\n\n  // TODO: your code here\n  // Hint: use MPI_Scatter and MPI_Gather to distribute the input data\n  // Hint: use OpenMP to parallelize the FFT\n  // Hint: use MPI_Reduce to combine the results\n  // Hint: use M",
            "/* TODO: Your code here */\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    std::vector<std::complex<double>> x_rank(n_per_rank + n_extra);\n    std::vector<std::complex<double>> x_rank_fft(n_per_rank + n_extra);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x_rank[i] = x[i];\n        }\n    }\n\n    MPI_Scatter(x_rank.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, x_rank_fft.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    fft(x_rank_fft);\n\n    std::vector<std::complex<double>> x_rank_fft_inv(n_per_rank + n_extra);\n\n    for (int i = 0; i < n_per_rank + n_extra; i++) {\n        x_rank_fft_inv[i] = std::conj(x_rank_fft[i]) / n;\n    }\n\n    fft(x_rank_fft_inv);\n\n    MPI_Gather(x_rank_fft_inv.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, x_rank.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_rank[i];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on each chunk\n  // 3. gather the results on rank 0\n  // 4. compute ifft on rank 0\n  // 5. scatter the results to all ranks\n\n  // TODO:\n  // 1. split x into chunks\n  // 2. compute fft on",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: Your code here */\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n    int n_local = n_per_rank + (rank < n_rem? 1 : 0);\n    std::vector<std::complex<double>> x_local(n_local);\n    std::vector<std::complex<double>> x_local_fft(n_local);\n    std::vector<std::complex<double>> x_local_ifft(n_local);\n\n    // copy local data\n    for (int i = 0; i < n_local; i++) {\n        x_local[i] = x[rank * n_per_rank + std::min(i, n_rem)];\n    }\n\n    // fft\n    fft(x_local_fft);\n\n    // ifft\n    for (int i = 0; i < n_local; i++) {\n        x_local_ifft[i] = std::conj(x_local_fft[i]) / n_local;\n    }\n\n    // gather\n    std::vector<std::complex<double>> x_gathered(n);\n    MPI_Gather(x_local_ifft.data(), n_local, MPI_DOUBLE_COMPLEX, x_gathered.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // copy back\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = x_gathered[i];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  std::vector<std::complex<double>> x_local(n_local);\n  std::vector<std::complex<double>> x_local_fft(n_local);\n  std::vector<std::complex<double>> x_local_ifft(n_local);\n  std::vector<std::complex<double>> x_local_ifft_fft(n_local);\n\n  // Copy local data\n  for (int i = 0; i < n_local; i++) {\n    x_local[i] = x[rank * n_per_rank + i];\n  }\n\n  // FFT\n  fft(x_local);\n\n  // Scatter\n  std::vector<std::complex<double>> x_local_fft_scattered(n_local);\n  MPI_Scatter(x_local_fft.data(), n_local, MPI_DOUBLE_COMPLEX, x_local_fft_scattered.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // Inverse FFT\n  for (int i = 0; i < n_local; i++) {\n    x_local_ifft[i] = std::conj(x_local_fft_scattered[i]) / n_local;\n  }\n\n  // Gather\n  MPI_Gather(x_local_ifft.data(), n_local, MPI_DOUBLE_COMPLEX, x_local_ifft_fft.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // FFT\n  fft(x_local_ifft_fft);\n\n  // Copy back to x\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = x_local_ifft_fft[i];\n    }\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  int n_local_start = rank * n_per_rank + std::min(rank, n_remainder);\n  int n_local_end = n_local_start + n_local;\n\n  std::vector<std::complex<double>> x_local(n_local);\n  for (int i = n_local_start; i < n_local_end; i++) {\n    x_local[i - n_local_start] = x[i];\n  }\n\n  fft(x_local);\n\n  std::vector<std::complex<double>> x_local_conj(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local_conj[i] = std::conj(x_local[i]);\n  }\n\n  std::vector<std::complex<double>> x_local_conj_fft(n_local);\n  fft(x_local_conj_fft);\n\n  std::vector<std::complex<double>> x_local_conj_fft_scaled(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local_conj_fft_scaled[i] = x_local_conj_fft[i] / n;\n  }\n\n  std::vector<std::complex<double>> x_local_conj_fft_scaled_inv(n_local);\n  fft(x_local_conj_fft_scaled_inv);\n\n  std::vector<std::complex<double>> x_local_inv(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local_inv[i] = x_local_conj_fft_scaled_inv[i] / n;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_local; i++) {\n      x[n_local_start + i] = x_local_inv[i];\n    }\n  }\n\n  MPI_Gather(x_local_inv.data(), n_local, MPI_DOUBLE_COMPLEX, x.data(), n_local, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] /= n;\n    }\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  std::vector<std::complex<double>> x_local(n_per_rank + n_extra);\n  std::vector<std::complex<double>> x_local_fft(n_per_rank + n_extra);\n\n  /* copy data to local vector */\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank + n_extra; i++) {\n      x_local[i] = x[i];\n    }\n  }\n\n  /* send data to other ranks */\n  MPI_Scatter(x_local.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX,\n              x_local_fft.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, 0,\n              MPI_COMM_WORLD);\n\n  /* compute fft in parallel */\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_rank + n_extra; i++) {\n    fft(x_local_fft);\n  }\n\n  /* gather results from other ranks */\n  MPI_Gather(x_local_fft.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX,\n             x.data(), n_per_rank + n_extra, MPI_DOUBLE_COMPLEX, 0,\n             MPI_COMM_WORLD);\n\n  /* copy results to x if this is rank 0 */\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank + n_extra; i++) {\n      x[i] = x_local_fft[i];\n    }\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  // Hint: you may want to use MPI_Scatter and MPI_Gather\n  // Hint: you may want to use OpenMP\n  // Hint: you may want to use fft\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* TODO: Your code here */\n  // 1. \u5c06\u6570\u636e\u5206\u5757\n  // 2. \u6bcf\u4e2a\u8fdb\u7a0b\u8ba1\u7b97\u81ea\u5df1\u7684\u5757\n  // 3. \u5c06\u7ed3\u679c\u5408\u5e76\n\n  // 1. \u5c06\u6570\u636e\u5206\u5757\n  int block_size = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * block_size;\n  int end = start + block_size;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<std::complex<double>> local_x(x.begin() + start, x.begin() + end);\n\n  // 2. \u6bcf\u4e2a\u8fdb\u7a0b\u8ba1\u7b97\u81ea\u5df1\u7684\u5757\n  fft(local_x);\n\n  // 3. \u5c06\u7ed3\u679c\u5408\u5e76\n  std::vector<std::complex<double>> result(x.size());\n  MPI_Gather(local_x.data(), block_size, MPI_DOUBLE_COMPLEX, result.data(), block_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      x[i] = result[i];\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: Your code here */\n    // 1. \u5148\u628ax\u5206\u6210size\u4efd\uff0c\u6bcf\u4efdsize/size\u4e2a\u6570\n    // 2. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 3. \u6bcf\u4e2arank\u90fd\u8fdb\u884cifft\n    // 4. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 5. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cifft\n    // 6. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 7. \u6bcf\u4e2arank\u90fd\u8fdb\u884cifft\n    // 8. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 9. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 10. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 11. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 12. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 13. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 14. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 15. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 16. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 17. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 18. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 19. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 20. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 21. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 22. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 23. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 24. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 25. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 26. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 27. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 28. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 29. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 30. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 31. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 32. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 33. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 34. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 35. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 36. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 37. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 38. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 39. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 40. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 41. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 42. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 43. \u6bcf\u4e2arank\u90fd\u8fdb\u884cfft\n    // 44. \u6bcf\u4e2arank\u90fd\u628a\u81ea\u5df1\u7684\u7ed3\u679c\u53d1\u7ed9rank 0\n    // 45. rank 0 \u6536\u96c6\u6240\u6709\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884cfft\n    // 46. \u628a\u7ed3\u679c\u53d1\u56de\u53bb\n    // 4",
            "/* TODO: your code here */\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_local = n / size;\n  int n_local_start = rank * n_local;\n  int n_local_end = n_local_start + n_local;\n\n  std::vector<std::complex<double>> x_local(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local[i] = x[n_local_start + i];\n  }\n\n  fft(x_local);\n\n  std::vector<std::complex<double>> x_local_inv(n_local);\n  for (int i = 0; i < n_local; i++) {\n    x_local_inv[i] = std::conj(x_local[i]) / n;\n  }\n\n  fft(x_local_inv);\n\n  for (int i = 0; i < n_local; i++) {\n    x[n_local_start + i] = x_local_inv[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x[i * n_local], n_local, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&x[0], n_local, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /* TODO: your code here */\n    if (rank == 0) {\n        fft(x);\n        for (int i = 0; i < x.size(); i++) {\n            x[i] /= x.size();\n        }\n    }\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "int n = x.extent(0);\n  int m = 1;\n  while (m < n) {\n    int mh = m / 2;\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, mh), KOKKOS_LAMBDA(const int& i) {\n      Kokkos::complex<double> tau = Kokkos::complex<double>(0, -2 * M_PI * i / n);\n      for (int j = i; j < n; j += m) {\n        int k = j + mh;\n        Kokkos::complex<double> z = x(j) - x(k);\n        x(j) = x(j) + x(k);\n        x(k) = z * tau;\n      }\n    });\n    m = m * 2;\n  }\n  if (n > 1) {\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, n / 2), KOKKOS_LAMBDA(const int& i) {\n      int j = n - i - 1;\n      Kokkos::complex<double> t = x(j);\n      x(j) = x(i);\n      x(i) = t;\n    });\n  }\n}",
            "const int N = x.extent(0);\n  const int N2 = N / 2;\n  const double PI = 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679;\n  const double TWO_PI = 2.0 * PI;\n\n  // Compute the FFT in-place\n  for (int n = 1; n <= N2; ++n) {\n    double theta = TWO_PI * n / N;\n    Kokkos::complex<double> w(cos(theta), -sin(theta));\n    for (int i = 0; i < N; i += n) {\n      Kokkos::complex<double> t = w * x(i + N2);\n      x(i + N2) = x(i) - t;\n      x(i) += t;\n    }\n  }\n\n  // Compute the inverse FFT in-place\n  for (int n = N2; n >= 1; --n) {\n    double theta = TWO_PI * n / N;\n    Kokkos::complex<double> w(cos(theta), -sin(theta));\n    for (int i = 0; i < N; i += n) {\n      Kokkos::complex<double> t = w * x(i + N2);\n      x(i + N2) = x(i) - t;\n      x(i) += t;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "const int N = x.extent(0);\n  const int M = N/2;\n  const double pi = 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679;\n  const double theta = 2.0*pi/N;\n  const Kokkos::complex<double> w(1.0, 0.0);\n  const Kokkos::complex<double> w_N(1.0, 0.0);\n  const Kokkos::complex<double> w_M(cos(theta), sin(theta));\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M),\n                       KOKKOS_LAMBDA(const int& i) {\n    Kokkos::complex<double> t = x(i);\n    x(i) = x(i+M) + t;\n    x(i+M) = x(i+M) - t;\n  });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M),\n                       KOKKOS_LAMBDA(const int& i) {\n    Kokkos::complex<double> t = x(i);\n    x(i) = t + w*x(i+M);\n    x(i+M) = t - w*x(i+M);\n  });\n\n  for (int s = 2; s <= M; s *= 2) {\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M/s),\n                         KOKKOS_LAMBDA(const int& i) {\n      Kokkos::complex<double> w_s(cos(theta*i), sin(theta*i));\n      for (int j = 0; j < s/2; j++) {\n        Kokkos::complex<double> t = w_s*x(i*s + j + M);\n        x(i*s + j + M) = x(i*s + j) - t;\n        x(i*s + j) = x(i*s + j) + t;\n      }\n    });\n  }\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M),\n                       KOKKOS_LAMBDA(const int& i) {\n    Kokkos::complex<double> t = x(i);\n    x(i) = t + w_N*x(i+M);\n    x(i+M) = t - w_N*x(i+M);\n  });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M),\n                       KOKKOS_LAMBDA(const int& i) {\n    Kokkos::complex<double> t = x(i);\n    x(i) = x(i+M) + t;\n    x(i+M) = x(i+M) - t;\n  });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, M),\n                       KOKKOS_LAMBDA(const int& i) {\n    Kokkos::complex<double> t = x(i);\n    x(i) = t + w_M*x(i+M);\n    x(i+M) = t - w_M*x(i+M);\n  });\n}",
            "// TODO: Implement this function\n}",
            "const int n = x.extent(0);\n  if (n == 1) {\n    return;\n  }\n  Kokkos::View<Kokkos::complex<double>*> x_even(\"x_even\", n/2);\n  Kokkos::View<Kokkos::complex<double>*> x_odd(\"x_odd\", n/2);\n  Kokkos::parallel_for(n/2, KOKKOS_LAMBDA(const int i) {\n    x_even(i) = x(2*i);\n    x_odd(i) = x(2*i+1);\n  });\n  fft(x_even);\n  fft(x_odd);\n  Kokkos::parallel_for(n/2, KOKKOS_LAMBDA(const int i) {\n    Kokkos::complex<double> t = x_even(i) + Kokkos::exp(-Kokkos::complex<double>(0, 2*M_PI*i/n))*x_odd(i);\n    x(i) = t + x_odd(i);\n    x(i+n/2) = t - x_odd(i);\n  });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int n = x.extent(0);\n  int m = 0;\n  while (m < n) {\n    int i = 0;\n    while (i < n) {\n      int j = 0;\n      while (j < m) {\n        Kokkos::complex<double> t = x(i + j);\n        x(i + j) = x(i + j + m) + t;\n        x(i + j + m) = x(i + j) - t;\n        j += m;\n      }\n      i += 2 * m;\n    }\n    m *= 2;\n  }\n  if (n > 1) {\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n / 2),\n                         [&](int i) {\n                           Kokkos::complex<double> t = x(i);\n                           x(i) = x(i + n / 2) + t;\n                           x(i + n / 2) = x(i) - t;\n                         });\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n  int m = 0;\n  while (1 << m < n) m++;\n  int N = 1 << m;\n  for (int i = 0; i < n; i++) {\n    int j = 0;\n    for (int k = 0; k < m; k++) {\n      j = (j << 1) | ((i >> k) & 1);\n    }\n    if (j > i) {\n      std::swap(x[i], x[j]);\n    }\n  }\n  for (int l = 1; l <= m; l++) {\n    int ml = 1 << l;\n    int mlp = ml >> 1;\n    double al = 2 * M_PI / ml;\n    std::complex<double> wl(cos(al), sin(al));\n    for (int j = 0; j < mlp; j++) {\n      std::complex<double> w(1, 0);\n      for (int k = 0; k < n; k += ml) {\n        std::complex<double> t = w * x[k + j + mlp];\n        x[k + j + mlp] = x[k + j] - t;\n        x[k + j] += t;\n        w = w * wl;\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  int m = 0;\n  while (n > 1) {\n    int n2 = n / 2;\n    int k = 0;\n    for (int i = 0; i < n; i += 2) {\n      if (i < k) {\n        std::swap(x[i], x[k]);\n        std::swap(x[i + 1], x[k + 1]);\n      }\n      k += n2;\n    }\n    m = n2;\n    while (m >= 2 && m < n) {\n      double theta = 2 * M_PI / m;\n      double wtemp = std::sin(0.5 * theta);\n      double wpr = -2.0 * wtemp * wtemp;\n      double wpi = std::sin(theta);\n      double wr = 1.0;\n      double wi = 0.0;\n      for (int i = 0; i < m; i++) {\n        int i1 = i + m;\n        std::complex<double> tempr(wr, wi);\n        std::complex<double> tempi(wr, wi);\n        std::complex<double> temp = x[i1] * tempr - x[i] * tempi;\n        x[i] = x[i] * tempr + x[i1] * tempi;\n        x[i1] = temp;\n        wtemp = wr;\n        wr = wtemp * wpr - wi * wpi + wr;\n        wi = wi * wpr + wtemp * wpi + wi;\n      }\n      m = m * 2;\n    }\n    n = n / 2;\n  }\n}",
            "int n = x.size();\n  int m = 0;\n  while (n > 1) {\n    int n2 = n / 2;\n    int k = 0;\n    for (int i = 0; i < n; i += 2) {\n      if (i < k) {\n        std::swap(x[i], x[k]);\n        std::swap(x[i + 1], x[k + 1]);\n      }\n      k += n2;\n    }\n    m = n2;\n    n = n2;\n  }\n  for (int i = 0; i < m; i++) {\n    int j = 0;\n    for (int k = 0; k < m; k++) {\n      double t_real = x[j].real() - x[j + m].real();\n      double t_imag = x[j].imag() + x[j + m].imag();\n      x[j].real(x[j].real() + x[j + m].real());\n      x[j].imag(x[j].imag() - x[j + m].imag());\n      x[j + m].real(t_real);\n      x[j + m].imag(t_imag);\n      j++;\n    }\n  }\n}",
            "int n = x.size();\n  int m = 1;\n  while (m < n) {\n    int m2 = m << 1;\n    #pragma omp parallel for\n    for (int i = 0; i < n; i += m2) {\n      for (int j = 0; j < m; ++j) {\n        int k = j + m;\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * j / n) * x[i + k];\n        x[i + k] = x[i + j] - t;\n        x[i + j] += t;\n      }\n    }\n    m = m2;\n  }\n}",
            "int n = x.size();\n    int m = 0;\n    while (n > 1) {\n        int n1 = n / 2;\n        int n2 = n - n1;\n        std::vector<std::complex<double>> x1(n1);\n        std::vector<std::complex<double>> x2(n2);\n        #pragma omp parallel for\n        for (int i = 0; i < n1; i++) {\n            x1[i] = x[i];\n        }\n        #pragma omp parallel for\n        for (int i = 0; i < n2; i++) {\n            x2[i] = x[i + n1];\n        }\n        fft(x1);\n        fft(x2);\n        #pragma omp parallel for\n        for (int i = 0; i < n1; i++) {\n            std::complex<double> t = x1[i];\n            x1[i] = t + x2[i];\n            x2[i] = t - x2[i];\n        }\n        #pragma omp parallel for\n        for (int i = 0; i < n1; i++) {\n            x[i] = x1[i];\n        }\n        #pragma omp parallel for\n        for (int i = 0; i < n2; i++) {\n            x[i + n1] = x2[i];\n        }\n        n = n2;\n        m++;\n    }\n    if (m & 1) {\n        #pragma omp parallel for\n        for (int i = 0; i < n; i++) {\n            x[i] *= -1;\n        }\n    }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n\n  // Split the array into two halves\n  std::vector<std::complex<double>> x1(x.begin(), x.begin() + n / 2);\n  std::vector<std::complex<double>> x2(x.begin() + n / 2, x.end());\n\n  // Recursively compute the fourier transform of each half\n  fft(x1);\n  fft(x2);\n\n  // Combine the two halves\n  for (int i = 0; i < n / 2; ++i) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * x2[i];\n    x[i] = x1[i] + t;\n    x[i + n / 2] = x1[i] - t;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  int m = log2(n);\n  for (int i = 0; i < n; i++) {\n    int j = 0;\n    for (int k = 0; k < m; k++) {\n      j = (j << 1) | ((i >> k) & 1);\n    }\n    if (j > i) {\n      std::swap(x[i], x[j]);\n    }\n  }\n  for (int k = 1; k <= m; k++) {\n    int n1 = 1 << (k - 1);\n    int n2 = 1 << k;\n    double theta = 2 * M_PI / n2;\n    std::complex<double> w(1, 0);\n    for (int j = 0; j < n1; j++) {\n      for (int i = j; i < n; i += n2) {\n        int i1 = i + n1;\n        std::complex<double> t = w * x[i1];\n        x[i1] = x[i] - t;\n        x[i] = x[i] + t;\n      }\n      w = w * std::complex<double>(cos(theta), sin(theta));\n    }\n  }\n}",
            "int n = x.size();\n    int m = 0;\n    while (m < n) {\n        int k = 0;\n        for (int i = 0; i < n; i++) {\n            if (i < k) {\n                std::swap(x[i], x[k]);\n            }\n            int t = n / 2;\n            while (k >= t) {\n                k -= t;\n                t /= 2;\n            }\n            if (k >= t) {\n                k -= t;\n            }\n            k += t;\n        }\n        m *= 2;\n    }\n    for (int l = 2; l <= n; l *= 2) {\n        int m = l / 2;\n        double theta = 2 * M_PI / l;\n        std::complex<double> wm(cos(theta), -sin(theta));\n        for (int j = 0; j < m; j++) {\n            std::complex<double> w(1, 0);\n            for (int k = j; k < n; k += l) {\n                int i = k + m;\n                std::complex<double> t = w * x[i];\n                x[i] = x[k] - t;\n                x[k] = x[k] + t;\n                w = w * wm;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  int m = n / 2;\n  std::vector<std::complex<double>> even(m);\n  std::vector<std::complex<double>> odd(m);\n  for (int i = 0; i < m; i++) {\n    even[i] = x[2 * i];\n    odd[i] = x[2 * i + 1];\n  }\n  fft(even);\n  fft(odd);\n  for (int i = 0; i < m; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n    x[i] = even[i] + t;\n    x[i + m] = even[i] - t;\n  }\n}",
            "int n = x.size();\n  int m = 0;\n  while (1 << m < n) m++;\n  int N = 1 << m;\n  if (N!= n) {\n    std::cerr << \"FFT size must be a power of 2\" << std::endl;\n    exit(1);\n  }\n  for (int i = 0; i < n; i++) {\n    int j = 0;\n    for (int k = 0; k < m; k++) {\n      j = (j << 1) | ((i >> k) & 1);\n    }\n    if (j > i) {\n      std::swap(x[i], x[j]);\n    }\n  }\n  for (int l = 2; l <= n; l <<= 1) {\n    double theta = 2 * M_PI / l;\n    #pragma omp parallel for\n    for (int k = 0; k < n; k += l) {\n      std::complex<double> w(1, 0);\n      for (int j = 0; j < l / 2; j++) {\n        std::complex<double> t = w * x[k + j + l / 2];\n        x[k + j + l / 2] = x[k + j] - t;\n        x[k + j] = x[k + j] + t;\n        w = w * std::complex<double>(cos(theta), sin(theta));\n      }\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "int n = x.size();\n  int n2 = n / 2;\n  int n4 = n / 4;\n  int n8 = n / 8;\n  int n16 = n / 16;\n  int n32 = n / 32;\n  int n64 = n / 64;\n  int n128 = n / 128;\n  int n256 = n / 256;\n  int n512 = n / 512;\n  int n1024 = n / 1024;\n  int n2048 = n / 2048;\n  int n4096 = n / 4096;\n  int n8192 = n / 8192;\n  int n16384 = n / 16384;\n  int n32768 = n / 32768;\n  int n65536 = n / 65536;\n  int n131072 = n / 131072;\n  int n262144 = n / 262144;\n  int n524288 = n / 524288;\n  int n1048576 = n / 1048576;\n  int n2097152 = n / 2097152;\n  int n4194304 = n / 4194304;\n  int n8388608 = n / 8388608;\n  int n16777216 = n / 16777216;\n  int n33554432 = n / 33554432;\n  int n67108864 = n / 67108864;\n  int n134217728 = n / 134217728;\n  int n268435456 = n / 268435456;\n  int n536870912 = n / 536870912;\n  int n1073741824 = n / 1073741824;\n  int n2147483648 = n / 2147483648;\n  int n4294967296 = n / 4294967296;\n  int n8589934592 = n / 8589934592;\n  int n17179869184 = n / 17179869184;\n  int n34359738368 = n / 34359738368;\n  int n68719476736 = n / 68719476736;\n  int n137438953472 = n / 137438953472;\n  int n274877906944 = n / 274877906944;\n  int n549755813888 = n / 549755813888;\n  int n1099511627776 = n / 1099511627776;\n  int n2199023255552 = n / 2199023255552;\n  int n4398046511104 = n / 4398046511104;\n  int n8796093022208 = n / 8796093022208;\n  int n1759218604",
            "int n = x.size();\n  int m = 0;\n  while (1 << m < n) m++;\n  int N = 1 << m;\n  int M = N / 2;\n  std::vector<std::complex<double>> x_even(M);\n  std::vector<std::complex<double>> x_odd(M);\n  for (int i = 0; i < M; i++) {\n    x_even[i] = x[2 * i];\n    x_odd[i] = x[2 * i + 1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < M; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / N) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i + M] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n    int m = 0;\n    while (1 << m < n) m++;\n    for (int i = 0; i < n; i++) {\n        int j = 0;\n        for (int k = 0; k < m; k++) {\n            j = (j << 1) | ((i >> k) & 1);\n        }\n        if (j > i) {\n            std::swap(x[i], x[j]);\n        }\n    }\n    for (int l = 1; l < n; l <<= 1) {\n        double theta = 2 * M_PI / l;\n        #pragma omp parallel for\n        for (int k = 0; k < l; k++) {\n            std::complex<double> w(cos(k * theta), sin(k * theta));\n            for (int j = 0; j < n / l; j++) {\n                std::complex<double> t = w * x[j * l + k];\n                x[j * l + k] = x[j * l] - t;\n                x[j * l] = x[j * l] + t;\n            }\n        }\n    }\n}",
            "int n = x.size();\n  int m = 0;\n  while (1 << m < n) m++;\n  int m2 = m / 2;\n  int n2 = 1 << m2;\n  int n4 = 1 << m4;\n  int n8 = 1 << m8;\n  int n16 = 1 << m16;\n\n  // Bit-reversal permutation\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int j = 0;\n    for (int k = 0; k < m; k++) {\n      int bit = (i >> k) & 1;\n      j = (j << 1) | bit;\n    }\n    if (i < j) {\n      std::swap(x[i], x[j]);\n    }\n  }\n\n  // Cooley-Tukey decimation-in-time radix-2 FFT\n  for (int s = 1; s < n; s *= 2) {\n    #pragma omp parallel for\n    for (int k = 0; k < n; k += 2 * s) {\n      for (int j = 0; j < s; j++) {\n        int i = j + k;\n        int i2 = i + s;\n        std::complex<double> t = x[i2] * std::exp(-2 * M_PI * i * i2 / n);\n        x[i2] = x[i] - t;\n        x[i] = x[i] + t;\n      }\n    }\n  }\n}",
            "int n = x.size();\n  if (n == 1) return;\n  std::vector<std::complex<double>> x_even(n/2), x_odd(n/2);\n  for (int i = 0; i < n/2; i++) {\n    x_even[i] = x[2*i];\n    x_odd[i] = x[2*i+1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i + n/2] = x_even[i] - t;\n  }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int n = x.size();\n  if (n == 1) return;\n  std::vector<std::complex<double>> x_even = std::vector<std::complex<double>>(n/2);\n  std::vector<std::complex<double>> x_odd = std::vector<std::complex<double>>(n/2);\n  for (int i = 0; i < n/2; i++) {\n    x_even[i] = x[2*i];\n    x_odd[i] = x[2*i+1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i+n/2] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n\n  std::vector<std::complex<double>> even = std::vector<std::complex<double>>(n / 2);\n  std::vector<std::complex<double>> odd = std::vector<std::complex<double>>(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    even[i] = x[2 * i];\n    odd[i] = x[2 * i + 1];\n  }\n\n  fft(even);\n  fft(odd);\n\n  for (int i = 0; i < n / 2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n    x[i] = even[i] + t;\n    x[i + n / 2] = even[i] - t;\n  }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  std::vector<std::complex<double>> even(n / 2);\n  std::vector<std::complex<double>> odd(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    even[i] = x[2 * i];\n    odd[i] = x[2 * i + 1];\n  }\n  fft(even);\n  fft(odd);\n  for (int i = 0; i < n / 2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n    x[i] = even[i] + t;\n    x[i + n / 2] = even[i] - t;\n  }\n}",
            "int n = x.size();\n    if (n == 1) return;\n    std::vector<std::complex<double>> x_even = std::vector<std::complex<double>>(n/2);\n    std::vector<std::complex<double>> x_odd = std::vector<std::complex<double>>(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n    fft(x_even);\n    fft(x_odd);\n    for (int i = 0; i < n/2; i++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * x_odd[i];\n        x[i] = x_even[i] + t;\n        x[i+n/2] = x_even[i] - t;\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        return;\n    }\n    std::vector<std::complex<double>> even(n / 2);\n    std::vector<std::complex<double>> odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        even[i] = x[2 * i];\n        odd[i] = x[2 * i + 1];\n    }\n    fft(even);\n    fft(odd);\n    for (int i = 0; i < n / 2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n        x[i] = even[i] + t;\n        x[i + n / 2] = even[i] - t;\n    }\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    if (n == 1) return;\n    std::vector<std::complex<double>> even, odd;\n    for (int i = 0; i < n / 2; i++) {\n        even.push_back(x[2 * i]);\n        odd.push_back(x[2 * i + 1]);\n    }\n    fft(even);\n    fft(odd);\n    for (int i = 0; i < n / 2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * PI * i / n) * odd[i];\n        x[i] = even[i] + t;\n        x[i + n / 2] = even[i] - t;\n    }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  std::vector<std::complex<double>> x_even(n / 2);\n  std::vector<std::complex<double>> x_odd(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    x_even[i] = x[2 * i];\n    x_odd[i] = x[2 * i + 1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n / 2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i + n / 2] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  std::vector<std::complex<double>> x_even(n / 2);\n  std::vector<std::complex<double>> x_odd(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    x_even[i] = x[2 * i];\n    x_odd[i] = x[2 * i + 1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n / 2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i + n / 2] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n    if (n == 1) return;\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n    fft(x_even);\n    fft(x_odd);\n    for (int i = 0; i < n/2; i++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * x_odd[i];\n        x[i] = x_even[i] + t;\n        x[i + n/2] = x_even[i] - t;\n    }\n}",
            "// TODO: implement this\n    int n = x.size();\n    if (n == 1) return;\n    std::vector<std::complex<double>> even = std::vector<std::complex<double>>(n/2);\n    std::vector<std::complex<double>> odd = std::vector<std::complex<double>>(n/2);\n    for (int i = 0; i < n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i+1];\n    }\n    fft(even);\n    fft(odd);\n    for (int i = 0; i < n/2; i++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*i/n)*odd[i];\n        x[i] = even[i] + t;\n        x[i+n/2] = even[i] - t;\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        return;\n    }\n    std::vector<std::complex<double>> even(n / 2);\n    std::vector<std::complex<double>> odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        even[i] = x[2 * i];\n        odd[i] = x[2 * i + 1];\n    }\n    fft(even);\n    fft(odd);\n    for (int i = 0; i < n / 2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n        x[i] = even[i] + t;\n        x[i + n / 2] = even[i] - t;\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    fft(x_even);\n    fft(x_odd);\n    for (int i = 0; i < n / 2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * x_odd[i];\n        x[i] = x_even[i] + t;\n        x[i + n / 2] = x_even[i] - t;\n    }\n}",
            "int n = x.size();\n  if (n == 1) return;\n  std::vector<std::complex<double>> x_even(n/2);\n  std::vector<std::complex<double>> x_odd(n/2);\n  for (int i = 0; i < n/2; i++) {\n    x_even[i] = x[2*i];\n    x_odd[i] = x[2*i+1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i + n/2] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  std::vector<std::complex<double>> even(n / 2);\n  std::vector<std::complex<double>> odd(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    even[i] = x[2 * i];\n    odd[i] = x[2 * i + 1];\n  }\n  fft(even);\n  fft(odd);\n  for (int i = 0; i < n / 2; i++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n    x[i] = even[i] + t;\n    x[i + n / 2] = even[i] - t;\n  }\n}",
            "// TODO: implement this\n}",
            "// TODO: implement this function\n  int n = x.size();\n  if (n == 1) return;\n  std::vector<std::complex<double>> x_even(n/2);\n  std::vector<std::complex<double>> x_odd(n/2);\n  for (int i = 0; i < n/2; i++) {\n    x_even[i] = x[2*i];\n    x_odd[i] = x[2*i+1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2*M_PI*i/n)*x_odd[i];\n    x[i] = x_even[i] + t;\n    x[i+n/2] = x_even[i] - t;\n  }\n}",
            "int n = x.size();\n  if (n == 1) return;\n  std::vector<std::complex<double>> even = std::vector<std::complex<double>>(n/2);\n  std::vector<std::complex<double>> odd = std::vector<std::complex<double>>(n/2);\n  for (int i = 0; i < n/2; i++) {\n    even[i] = x[2*i];\n    odd[i] = x[2*i+1];\n  }\n  fft(even);\n  fft(odd);\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * odd[i];\n    x[i] = even[i] + t;\n    x[i+n/2] = even[i] - t;\n  }\n}",
            "// TODO: implement this\n}",
            "int n = x.size();\n  if (n == 1) {\n    return;\n  }\n  std::vector<std::complex<double>> x_even = std::vector<std::complex<double>>(n / 2);\n  std::vector<std::complex<double>> x_odd = std::vector<std::complex<double>>(n / 2);\n  for (int i = 0; i < n / 2; i++) {\n    x_even[i] = x[2 * i];\n    x_odd[i] = x[2 * i + 1];\n  }\n  fft(x_even);\n  fft(x_odd);\n  std::complex<double> w = std::exp(-2 * M_PI * I / n);\n  std::complex<double> wn = std::complex<double>(1, 0);\n  for (int i = 0; i < n / 2; i++) {\n    x[i] = x_even[i] + wn * x_odd[i];\n    x[i + n / 2] = x_even[i] - wn * x_odd[i];\n    wn *= w;\n  }\n}",
            "// TODO: implement this function\n    int n = x.size();\n    if (n == 1) {\n        return;\n    }\n    std::vector<std::complex<double>> even(n / 2);\n    std::vector<std::complex<double>> odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        even[i] = x[2 * i];\n        odd[i] = x[2 * i + 1];\n    }\n    fft(even);\n    fft(odd);\n    for (int i = 0; i < n / 2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n        x[i] = even[i] + t;\n        x[i + n / 2] = even[i] - t;\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "hip",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(hipDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  size_t half = N / 2;\n  size_t i = tid;\n  size_t j = 0;\n  hipDoubleComplex temp;\n\n  // Bit-reversed addressing permutation: swap adjacent quads\n  j = ((i & 0xaaaaaaaa) >> 1) | ((i & 0x55555555) << 1);\n  x[i] = x[j];\n  // Swap quads\n  j = ((i & 0xc0c0c0c0) >> 6) | ((i & 0x30303030) << 2);\n  x[i] = x[j];\n  // Swap octets\n  j = ((i & 0xf0f0f0f0) >> 4) | ((i & 0x0f0f0f0f) << 4);\n  x[i] = x[j];\n  // Swap pairs\n  j = ((i & 0xff00ff00) >> 8) | ((i & 0x00ff00ff) << 8);\n  x[i] = x[j];\n\n  // In-place complex-to-complex FFT\n  for (size_t n = 2; n <= N; n <<= 1) {\n    size_t m = n >> 1;\n    hipDoubleComplex wm = make_hipDoubleComplex(cos(-M_PI / m), sin(-M_PI / m));\n    for (size_t k = 0; k < half; k += n) {\n      hipDoubleComplex t = x[k + m];\n      x[k + m] = hipCmul(x[k], wm);\n      x[k] = hipCadd(x[k], t);\n    }\n  }\n\n  // Normalize\n  x[0] = hipCmul(x[0], make_hipDoubleComplex(1.0 / N, 0));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = i;\n  size_t k = 0;\n  size_t m = N;\n  while (m > 1) {\n    k = k << 1;\n    m = m >> 1;\n    if (j & m) {\n      j = j & (m - 1);\n      j = j + k;\n    }\n  }\n  if (i < j) {\n    hipDoubleComplex tmp = x[i];\n    x[i] = x[j];\n    x[j] = tmp;\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  size_t half = 1;\n  while (half < N) {\n    size_t k = tid * 2 * half;\n    size_t k1 = k + half;\n    hipDoubleComplex z = x[k1];\n    x[k1] = x[k] - z;\n    x[k] += z;\n    half *= 2;\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  for (size_t i = tid; i < N; i += stride) {\n    size_t j = (i & (N >> 1)) * 2;\n    size_t k = (i & ~(N >> 1)) * 2;\n    hipDoubleComplex a = x[j];\n    hipDoubleComplex b = x[k];\n    x[j] = a + b;\n    x[k] = a - b;\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  size_t i = tid;\n  size_t j = 0;\n  size_t k = N / 2;\n  size_t l = 1;\n  size_t m = N / 2;\n  size_t n = 1;\n\n  while (m > 1) {\n    if (j >= k) {\n      j = j - k;\n      k = k / 2;\n      l = 2 * l;\n    }\n    if (i >= l) {\n      i = i - l;\n      m = m / 2;\n      n = 2 * n;\n    }\n    if (j < m) {\n      size_t o = i + j * n;\n      size_t p = o + m * n;\n      hipDoubleComplex t = x[o];\n      x[o] = x[p];\n      x[p] = t;\n    }\n    j = j + k;\n  }\n\n  for (size_t s = 2; s <= N; s *= 2) {\n    size_t r = s / 2;\n    size_t u = 2 * r;\n    for (size_t q = 0; q < r; q++) {\n      hipDoubleComplex t = x[q + r];\n      hipDoubleComplex w = x[q];\n      hipDoubleComplex u1 = hipCmul(w, hipMakeDouble2(cos(2 * M_PI * q / u), -sin(2 * M_PI * q / u)));\n      hipDoubleComplex u2 = hipCmul(t, hipMakeDouble2(cos(2 * M_PI * q / u), sin(2 * M_PI * q / u)));\n      x[q] = hipCadd(w, t);\n      x[q + r] = hipCsub(u1, u2);\n    }\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n  size_t n = N << 1;\n  size_t m = n >> 1;\n  size_t k = 0;\n  for (size_t i = 0; i < n; i++) {\n    if (i < tid) {\n      k = tid;\n      tid = i;\n      i = k;\n    }\n    k = m + tid - i;\n    if (k < n) {\n      hipDoubleComplex z = x[k];\n      x[k] = x[i] - z;\n      x[i] = x[i] + z;\n    }\n    m >>= 1;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  // Use a bit reversal to map the input to the output.\n  size_t j = bit_reverse(i, log2(N));\n  if (i < j) {\n    hipDoubleComplex tmp = x[i];\n    x[i] = x[j];\n    x[j] = tmp;\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t stride = hipBlockDim_x * hipGridDim_x;\n    size_t half = 1;\n    while (half < N) {\n        size_t pos = 2 * tid * half;\n        size_t step = stride * half;\n        for (size_t i = 0; i < half; i++) {\n            hipDoubleComplex a = x[pos];\n            hipDoubleComplex b = x[pos + half];\n            hipDoubleComplex t = hipCmul(a - b, hipMakeDouble2(0.0, -1.0));\n            x[pos] = a + b;\n            x[pos + half] = t;\n            pos += step;\n        }\n        half *= 2;\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n\n  size_t i = tid;\n  size_t j = 0;\n  for (size_t k = 0; k < N; k++) {\n    size_t m = N >> 1;\n    while (j >= m) {\n      j -= m;\n      m >>= 1;\n    }\n    j += m;\n\n    if (i < j) {\n      hipDoubleComplex temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n    __syncthreads();\n  }\n\n  size_t k = 1;\n  while (k < N) {\n    size_t m = k << 1;\n    size_t l = m << 1;\n    size_t a = 0;\n    size_t b = 0;\n    for (size_t j = 0; j < k; j++) {\n      a = j << l;\n      b = j << m;\n      for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n        size_t c = a + b;\n        hipDoubleComplex t = x[i + c];\n        hipDoubleComplex u = x[i + c + k];\n        x[i + c] = t + u;\n        x[i + c + k] = t - u;\n      }\n      __syncthreads();\n    }\n    k = l;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  // Bit-reversal permutation\n  size_t j = 0;\n  for (size_t k = 0; k < log2(N); k++) {\n    j = 2 * j + (i & 1);\n    i >>= 1;\n  }\n\n  if (i > j) {\n    hipDoubleComplex tmp = x[i];\n    x[i] = x[j];\n    x[j] = tmp;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  size_t j = 0;\n  for (size_t k = N / 2; k > 0; k /= 2) {\n    size_t l = j ^ k;\n    hipDoubleComplex t = x[i ^ l];\n    if (j > i) x[i ^ l] = x[i];\n    x[i] = t;\n    j = (j << 1) | (i & 1);\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid >= N) return;\n\n  // Bit-reversed addressing permutation\n  size_t j = reverseBits(tid, log2(N));\n  if (j > tid) {\n    hipDoubleComplex tmp = x[tid];\n    x[tid] = x[j];\n    x[j] = tmp;\n  }\n\n  // Cooley-Tukey decimation-in-time radix-2 FFT\n  for (size_t k = 2; k <= N; k *= 2) {\n    size_t halfsize = k / 2;\n    size_t j1 = j % (2 * halfsize);\n    size_t j2 = j1 + halfsize;\n    if (j1 >= halfsize) j1 = j2;\n    j2 = (j + halfsize) % N;\n    if (j2 >= halfsize) j2 = j2 - halfsize;\n    hipDoubleComplex z = x[j2] * exp(make_hipDoubleComplex(0, -2 * PI * j1 / N));\n    x[j2] = x[j1] - z;\n    x[j1] = x[j1] + z;\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  size_t half = N / 2;\n\n  // Do the bit reversal\n  size_t i = tid;\n  size_t j = 0;\n  for (size_t k = 0; k < N; k++) {\n    size_t bit = N >> 1;\n    while (j >= bit) {\n      j -= bit;\n      bit >>= 1;\n    }\n    j += bit;\n    if (i < j) {\n      hipDoubleComplex tmp = x[i];\n      x[i] = x[j];\n      x[j] = tmp;\n    }\n    i += stride;\n  }\n\n  // Compute the FFT\n  for (size_t l = 2; l <= N; l <<= 1) {\n    size_t le = l >> 1;\n    size_t le2 = le >> 1;\n    hipDoubleComplex u{1.0, 0.0};\n    for (size_t j = 0; j < le2; j++) {\n      hipDoubleComplex z = x[tid + le2 + j];\n      x[tid + le2 + j] = x[tid + j] - z;\n      x[tid + j] += z;\n      z = u * z;\n      x[tid + j] = x[tid + j] + z;\n      x[tid + le2 + j] = x[tid + le2 + j] - z;\n      u = {u.x * u.x - u.y * u.y, 2.0 * u.x * u.y};\n    }\n    tid += le;\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid >= N) return;\n\n  // bit-reversed addressing\n  size_t i = hip_bit_reversed(tid, log2(N));\n\n  // input is a real signal, so only store the real part\n  double x_real = x[i].x;\n\n  // output is a complex signal, so store both the real and imaginary parts\n  double x_imag = 0.0;\n\n  // perform the fft\n  for (size_t n = 0; n < N; n++) {\n    // calculate the phase factor\n    double angle = -2.0 * M_PI * i * n / N;\n\n    // apply the phase factor to the input\n    double complex y = x_real * cos(angle) + x_imag * sin(angle);\n\n    // store the output\n    x[i].x = hip_real(y);\n    x[i].y = hip_imag(y);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n  size_t j = (N >> 1) + (i & (N >> 1));\n  hipDoubleComplex xj = x[j];\n  x[j] = x[i] - xj;\n  x[i] = x[i] + xj;\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n\n    // bit-reversed addressing\n    size_t i = hip_bit_reversed(tid, log2(N));\n\n    // compute the FFT\n    for (size_t n = 2; n <= N; n <<= 1) {\n        size_t m = n >> 1;\n        for (size_t j = 0; j < n; j += m) {\n            size_t k = j + m;\n            hipDoubleComplex z = x[i + k];\n            x[i + k] = x[i] - z;\n            x[i] += z;\n        }\n        __syncthreads();\n        if (tid >= n) continue;\n        i <<= 1;\n        hipDoubleComplex z = x[i + k];\n        x[i + k] = x[i] - z;\n        x[i] += z;\n        __syncthreads();\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n\n  // bit-reversed addressing\n  size_t j = 0;\n  for (size_t k = N/2; k > 0; k /= 2) {\n    j ^= k & tid;\n  }\n\n  // swap\n  if (j > tid) {\n    hipDoubleComplex temp = x[tid];\n    x[tid] = x[j];\n    x[j] = temp;\n  }\n\n  // butterfly\n  for (size_t l = 1; l < N; l <<= 1) {\n    size_t m = l << 1;\n    double theta = -2.0 * M_PI / m;\n    hipDoubleComplex wm = make_hipDoubleComplex(cos(theta), sin(theta));\n    for (size_t k = 0; k < l; k++) {\n      size_t i = tid + k * m;\n      size_t j = i + l;\n      hipDoubleComplex t = wm * x[j];\n      x[j] = x[i] - t;\n      x[i] = x[i] + t;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = i;\n  size_t k = 0;\n  while (j > 0) {\n    k += j & 1;\n    j >>= 1;\n  }\n  size_t l = 1;\n  while (l < N) {\n    size_t m = l << 1;\n    size_t n = m + l;\n    for (size_t p = 0; p < l; p++) {\n      hipDoubleComplex z = x[i + p + l];\n      hipDoubleComplex w = x[i + p];\n      x[i + p] = w + z;\n      x[i + p + l] = w - z;\n    }\n    if (k & 1) {\n      hipDoubleComplex z = x[i];\n      hipDoubleComplex w = make_hipDoubleComplex(hipCos(PI * k / N), -hipSin(PI * k / N)) * x[i + l];\n      x[i] = z + w;\n      x[i + l] = z - w;\n    }\n    i += m;\n    k++;\n    l = n;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n    size_t i = tid;\n    size_t j = 0;\n    for (size_t k = 0; k < N; k++) {\n        size_t l = N / 2;\n        while (j >= l) {\n            j -= l;\n            l /= 2;\n        }\n        j += l;\n        if (i < j) {\n            hipDoubleComplex tmp = x[i];\n            x[i] = x[j];\n            x[j] = tmp;\n        }\n        size_t m = N / 2;\n        while (m >= 2 && j >= m) {\n            j -= m;\n            m /= 2;\n        }\n        j += m;\n    }\n    for (size_t k = 0; k < N; k++) {\n        size_t l = 1;\n        while (l < N / 2) {\n            size_t m = 2 * l;\n            if (j >= m) {\n                j -= m;\n                l = m;\n            } else {\n                break;\n            }\n        }\n        size_t i1 = i + l;\n        if (i1 >= N) continue;\n        hipDoubleComplex z = x[i1];\n        hipDoubleComplex w = x[i] - z;\n        x[i] = x[i] + z;\n        z = w * hipConj(x[j]);\n        x[i1] = x[i] - z;\n        x[i] = x[i] + z;\n    }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid >= N) return;\n  size_t i = tid;\n  size_t j = 0;\n  for (size_t k = 0; k < N; k++) {\n    size_t bit = N >> 1;\n    while (j >= bit) {\n      j -= bit;\n      bit >>= 1;\n    }\n    j += bit;\n    if (i < j) {\n      hipDoubleComplex temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n    __syncthreads();\n    if (N >= 2 && (i & (N >> 1))) {\n      x[i] = -x[i];\n    }\n    __syncthreads();\n  }\n  for (size_t k = 2; k <= N; k <<= 1) {\n    size_t length = k >> 1;\n    hipDoubleComplex wn = {cos(2 * M_PI / k), sin(2 * M_PI / k)};\n    for (size_t j = 0; j < k; j += 2 * length) {\n      for (size_t i = j; i < j + length; i++) {\n        hipDoubleComplex z = x[i + length] * wn;\n        x[i + length] = x[i] - z;\n        x[i] = x[i] + z;\n      }\n      __syncthreads();\n    }\n    if (tid >= k) return;\n  }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int logn = 0;\n    while ((1 << logn) < n) {\n        logn++;\n    }\n\n    int n2 = 1 << logn;\n    int n4 = n2 / 2;\n    int n8 = n4 / 2;\n\n    std::vector<std::complex<double>> x2(n2);\n    std::vector<std::complex<double>> x4(n4);\n    std::vector<std::complex<double>> x8(n8);\n\n    // copy x to x2\n    for (int i = 0; i < n; i++) {\n        x2[i] = x[i];\n    }\n\n    // stage 1: butterfly\n    for (int i = 0; i < logn; i++) {\n        int stride = 1 << i;\n        for (int j = 0; j < n2; j += stride) {\n            for (int k = 0; k < stride / 2; k++) {\n                std::complex<double> t = x2[j + k] - x2[j + k + stride / 2];\n                x2[j + k] = x2[j + k] + x2[j + k + stride / 2];\n                x2[j + k + stride / 2] = t;\n            }\n        }\n    }\n\n    // stage 2: bit reverse\n    for (int i = 0; i < n2; i++) {\n        int j = 0;\n        int k = i;\n        for (int l = 0; l < logn; l++) {\n            j = (j << 1) | (k & 1);\n            k >>= 1;\n        }\n        x4[j] = x2[i];\n    }\n\n    // stage 3: butterfly\n    for (int i = 0; i < logn; i++) {\n        int stride = 1 << i;\n        for (int j = 0; j < n4; j += stride) {\n            for (int k = 0; k < stride / 2; k++) {\n                std::complex<double> t = x4[j + k] - x4[j + k + stride / 2];\n                x4[j + k] = x4[j + k] + x4[j + k + stride / 2];\n                x4[j + k + stride / 2] = t;\n            }\n        }\n    }\n\n    // stage 4: bit reverse\n    for (int i = 0; i < n4; i++) {\n        int j = 0;\n        int k = i;\n        for (int l = 0; l < logn; l++) {\n            j = (j << 1) | (k & 1);\n            k >>= 1;\n        }\n        x8[j] = x4[i];\n    }\n\n    // stage 5: butterfly\n    for (int i = 0; i < logn; i++) {\n        int stride = 1 << i;\n        for (int j = 0; j < n8; j += stride) {\n            for (int k = 0; k < stride / 2; k++) {\n                std::complex<double> t = x8[j + k] - x8[j + k + stride / 2];\n                x8[j + k] = x8[j + k] + x8[j + k + stride / 2];\n                x8[j + k + stride / 2] = t;\n            }\n        }\n    }\n\n    // stage 6: bit reverse\n    for (int i = 0; i < n8; i++) {\n        int j = 0;\n        int k = i;\n        for (int l = 0; l < logn; l++) {\n            j = (j << 1) | (k & 1);",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_half = n / 2;\n  int n_quarter = n_half / 2;\n  int n_half_quarter = n_quarter / 2;\n  int n_half_quarter_quarter = n_half_quarter / 2;\n  int n_half_quarter_quarter_quarter = n_half_quarter_quarter / 2;\n  int n_half_quarter_quarter_quarter_quarter = n_half_quarter_quarter_quarter / 2;\n  int n_half_quarter_quarter_quarter_quarter_quarter = n_half_quarter_quarter_quarter_quarter / 2;\n\n  // 1. split into 2^10 parts\n  std::vector<std::complex<double>> x_1(n_half_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_2(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_3(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_4(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_5(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_6(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_7(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_8(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_9(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_10(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_11(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_12(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_13(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_14(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_15(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_16(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_17(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_18(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_19(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<double>> x_20(n_half_quarter_quarter_quarter_quarter_quarter_quarter);\n  std::vector<std::complex<",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int logn = std::ceil(std::log2(n));\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    // Compute the FFT of each rank's local data\n    std::vector<std::complex<double>> local_fft(n_per_rank);\n    for (int i = 0; i < n_per_rank; i++) {\n        local_fft[i] = x[rank * n_per_rank + i];\n    }\n    fft_inplace(local_fft);\n\n    // Gather the results of the FFTs from all ranks\n    std::vector<std::complex<double>> global_fft(n);\n    MPI_Gather(local_fft.data(), n_per_rank, MPI_DOUBLE_COMPLEX, global_fft.data(), n_per_rank, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Compute the FFT of the global data\n    if (rank == 0) {\n        fft_inplace(global_fft);\n    }\n\n    // Scatter the results of the FFT back to the ranks\n    MPI_Scatter(global_fft.data(), n_per_rank, MPI_DOUBLE_COMPLEX, local_fft.data(), n_per_rank, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Store the results on each rank\n    for (int i = 0; i < n_per_rank; i++) {\n        x[rank * n_per_rank + i] = local_fft[i];\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int logn = 0;\n  while ((1 << logn) < n) {\n    ++logn;\n  }\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  // Compute the local FFT\n  std::vector<std::complex<double>> x_local(n_per_rank);\n  std::copy(x.begin() + rank * n_per_rank, x.begin() + (rank + 1) * n_per_rank, x_local.begin());\n  fft_local(x_local);\n\n  // Gather the results from all ranks\n  std::vector<std::complex<double>> x_all(n);\n  MPI_Gather(x_local.data(), n_per_rank, MPI_DOUBLE_COMPLEX, x_all.data(), n_per_rank, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // Copy the results back to x\n  if (rank == 0) {\n    std::copy(x_all.begin(), x_all.end(), x.begin());\n  }\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement the FFT in-place in x.\n  //       Use MPI_Send, MPI_Recv, and MPI_Bcast to communicate.\n  //       Use MPI_Reduce to sum the results.\n  //       Use MPI_Scatter to distribute the data to the ranks.\n  //       Use MPI_Gather to collect the results.\n  //       Use MPI_Allreduce to sum the results.\n  //       Use MPI_Scatterv and MPI_Gatherv to distribute and collect the data.\n  //       Use MPI_Alltoall to distribute the data to the ranks.\n  //       Use MPI_Alltoallv to distribute the data to the ranks.\n  //       Use MPI_Alltoallw to distribute the data to the ranks.\n  //       Use MPI_Reduce_scatter to sum the results.\n  //       Use MPI_Reduce_scatter_block to sum the results.\n  //       Use MPI_Scan to sum the results.\n  //       Use MPI_Exscan to sum the results.\n  //       Use MPI_Ireduce to sum the results.\n  //       Use MPI_Ireduce_scatter to sum the results.\n  //       Use MPI_Ireduce_scatter_block to sum the results.\n  //       Use MPI_Iscan to sum the results.\n  //       Use MPI_Iexscan to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Scan_block to sum the results.\n  //       Use MPI_Iscan_block to sum the results.\n  //       Use MPI_Exscan_block to sum the results.\n  //       Use MPI_Iexscan_block to sum the results.\n  //       Use MPI_Reduce_scatter_block to sum the results.\n  //       Use MPI_Ireduce_scatter_block to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Scan_block to sum the results.\n  //       Use MPI_Iscan_block to sum the results.\n  //       Use MPI_Exscan_block to sum the results.\n  //       Use MPI_Iexscan_block to sum the results.\n  //       Use MPI_Reduce_scatter_block to sum the results.\n  //       Use MPI_Ireduce_scatter_block to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Scan_block to sum the results.\n  //       Use MPI_Iscan_block to sum the results.\n  //       Use MPI_Exscan_block to sum the results.\n  //       Use MPI_Iexscan_block to sum the results.\n  //       Use MPI_Reduce_scatter_block to sum the results.\n  //       Use MPI_Ireduce_scatter_block to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Reduce_local to sum the results.\n  //       Use MPI_Ireduce_local to sum the results.\n  //       Use MPI_Scan_block to sum the results.\n  //",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_root = 1;\n    while (n_root < n) {\n        int n_half = n_root;\n        n_root <<= 1;\n\n        for (int i = 0; i < n; i += n_root) {\n            for (int j = 0; j < n_half; j++) {\n                int k = i + j + n_half;\n                std::complex<double> temp = x[i + j] - x[k];\n                x[i + j] += x[k];\n                x[k] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] /= n;\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(cuDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = 0;\n    for (size_t m = N; m!= 1; m >>= 1) {\n        size_t l = m >> 1;\n        cuDoubleComplex z = x[i + l];\n        if (j & m) {\n            x[i] -= z;\n            x[i + l] = cuCadd(x[i], z);\n        } else {\n            x[i] += z;\n            x[i + l] = cuCsub(x[i], z);\n        }\n        j++;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex xi = x[i];\n  cuDoubleComplex xi_conj = make_cuDoubleComplex(cuCreal(xi), -cuCimag(xi));\n  x[i] = xi_conj;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        size_t j = (i & (N - 1)) * 2;\n        cuDoubleComplex a = x[j];\n        cuDoubleComplex b = x[j + 1];\n        cuDoubleComplex c = make_cuDoubleComplex(a.x + b.x, a.y + b.y);\n        cuDoubleComplex d = make_cuDoubleComplex(a.x - b.x, a.y - b.y);\n        cuDoubleComplex e = make_cuDoubleComplex(d.x * cuCos(M_PI * i / N) - d.y * cuSin(M_PI * i / N),\n                                                 d.x * cuSin(M_PI * i / N) + d.y * cuCos(M_PI * i / N));\n        x[j] = c;\n        x[j + 1] = cuConj(e);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = i;\n    size_t k = 0;\n    size_t m = N;\n    while (m > 1) {\n        k = k << 1;\n        m = m >> 1;\n        if (j >= m) {\n            j = j - m;\n        }\n        j = j + k;\n    }\n    if (i < j) {\n        cuDoubleComplex tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n    }\n    __syncthreads();\n    size_t l = 1;\n    while (l < N) {\n        size_t m = l << 1;\n        size_t n = l;\n        while (n < N) {\n            size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n            size_t j = i + l;\n            size_t k = i + m;\n            cuDoubleComplex t = x[j];\n            cuDoubleComplex u = x[k];\n            cuDoubleComplex w = cuCmul(u, make_cuDoubleComplex(0.0, -1.0));\n            x[j] = cuCadd(x[i], w);\n            x[k] = cuCsub(x[i], w);\n            __syncthreads();\n            i = i + m;\n            j = i + l;\n            k = i + m;\n            t = x[j];\n            u = x[k];\n            w = cuCmul(u, make_cuDoubleComplex(0.0, -1.0));\n            x[j] = cuCadd(t, w);\n            x[k] = cuCsub(t, w);\n            __syncthreads();\n        }\n        l = m;\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    cuDoubleComplex *X = x;\n    cuDoubleComplex *Y = x + N/2;\n    cuDoubleComplex *Z = x + N;\n    cuDoubleComplex c, d;\n    cuDoubleComplex u, v, w;\n    cuDoubleComplex t;\n    cuDoubleComplex s = make_cuDoubleComplex(0, -2.0*M_PI/N);\n    cuDoubleComplex r = make_cuDoubleComplex(1, 0);\n    cuDoubleComplex zero = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex one = make_cuDoubleComplex(1, 0);\n    cuDoubleComplex two = make_cuDoubleComplex(2, 0);\n    cuDoubleComplex four = make_cuDoubleComplex(4, 0);\n    cuDoubleComplex eight = make_cuDoubleComplex(8, 0);\n\n    for (size_t k = 0; k < N/2; k += stride) {\n        size_t j = tid + k;\n        if (j >= N/2) continue;\n        c = X[j];\n        d = Y[j];\n        u = c;\n        v = cuCmul(cuCmul(d, r), s);\n        w = cuCmul(cuCmul(d, r), s);\n        w = cuCmul(w, two);\n        w = cuCsub(w, four);\n        w = cuCmul(w, cuCdiv(one, eight));\n        w = cuCadd(w, cuCmul(u, two));\n        w = cuCadd(w, v);\n        t = cuCadd(u, v);\n        t = cuCadd(t, w);\n        X[j] = t;\n        t = cuCsub(u, v);\n        t = cuCadd(t, w);\n        Y[j] = t;\n        t = cuCadd(u, v);\n        t = cuCsub(t, w);\n        Z[j] = t;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    // TODO: Compute the fourier transform of x in-place.\n    //       Use CUDA to compute in parallel.\n    //       The kernel is launched with at least N threads.\n    //       Use the cuDoubleComplex type to represent complex numbers.\n    //       Use the cuCadd() and cuCmul() functions to add and multiply complex numbers.\n    //       Use the cuCreal() and cuCimag() functions to get the real and imaginary parts of a complex number.\n    //       Use the cuCabs() function to get the absolute value of a complex number.\n    //       Use the cuCdiv() function to divide two complex numbers.\n    //       Use the cuCsqrt() function to get the square root of a complex number.\n    //       Use the cuCexp() function to get the exponential of a complex number.\n    //       Use the cuClog() function to get the logarithm of a complex number.\n    //       Use the cuCsin() function to get the sine of a complex number.\n    //       Use the cuCcos() function to get the cosine of a complex number.\n    //       Use the cuCpow() function to get the power of a complex number.\n    //       Use the cuCdivf() function to divide two complex numbers.\n    //       Use the cuCmulf() function to multiply two complex numbers.\n    //       Use the cuCaddf() function to add two complex numbers.\n    //       Use the cuCsubf() function to subtract two complex numbers.\n    //       Use the cuCabsf() function to get the absolute value of a complex number.\n    //       Use the cuCsqrtf() function to get the square root of a complex number.\n    //       Use the cuCexpf() function to get the exponential of a complex number.\n    //       Use the cuClogf() function to get the logarithm of a complex number.\n    //       Use the cuCsinf() function to get the sine of a complex number.\n    //       Use the cuCcosf() function to get the cosine of a complex number.\n    //       Use the cuCpowf() function to get the power of a complex number.\n    //       Use the cuCdiv() function to divide two complex numbers.\n    //       Use the cuCmul() function to multiply two complex numbers.\n    //       Use the cuCadd() function to add two complex numbers.\n    //       Use the cuCsub() function to subtract two complex numbers.\n    //       Use the cuCabsf() function to get the absolute value of a complex number.\n    //       Use the cuCsqrtf() function to get the square root of a complex number.\n    //       Use the cuCexpf() function to get the exponential of a complex number.\n    //       Use the cuClogf() function to get the logarithm of a complex number.\n    //       Use the cuCsinf() function to get the sine of a complex number.\n    //       Use the cuCcosf() function to get the cosine of a complex number.\n    //       Use the cuCpowf() function to get the power of a complex number.\n    //       Use the cuCdiv() function to divide two complex numbers.\n    //       Use the cuCmul() function to multiply two complex numbers.\n    //       Use the cuCadd() function to add two complex numbers.\n    //       Use the cuCsub() function to subtract two complex numbers.\n    //       Use the cuCabsf() function to get the absolute value of a complex number.\n    //       Use the cuCsqrtf() function to get the square root of a complex number.\n    //       Use the cuCexpf() function to get the exponential of a complex number.\n    //       Use the cuClogf() function to get the logarithm of a complex number.\n    //       Use the cuCsinf() function to get the sine of a complex number.\n    //       Use the cuCcosf() function to get the cosine of a complex number.\n    //       Use the cuCpowf() function to get the power of a complex number.\n    //       Use the cuCdiv() function to divide two complex numbers.\n    //       Use the cuCmul() function to multiply two complex numbers.\n    //       Use the cuCadd() function to",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = 0;\n  for (size_t k = N / 2; k > 0; k /= 2) {\n    size_t l = j ^ k;\n    cuDoubleComplex z = x[l];\n    x[l] = x[j];\n    x[j] = cuCadd(x[j], z);\n    j = (j & (k - 1)) + k;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = 0;\n    for (size_t k = N / 2; k > 0; k /= 2) {\n        size_t l = j ^ k;\n        cuDoubleComplex t = x[i ^ l];\n        if (j > i) x[i ^ l] = x[i];\n        x[i] = cuCadd(x[i], t);\n        j = (j << 1) | (i & k);\n    }\n}",
            "// TODO: implement the FFT kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = N / 2;\n    while (j > 0) {\n        cuDoubleComplex z = x[i + j];\n        x[i + j] = x[i] - z;\n        x[i] += z;\n        j /= 2;\n        z = cuCmul(z, make_cuDoubleComplex(0, -1.0 / j));\n        x[i + j] = cuCadd(x[i + j], z);\n        x[i + j] = cuCsub(x[i + j], z);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = 0;\n  for (size_t m = N; m!= 1; m >>= 1) {\n    size_t l = m >> 1;\n    cuDoubleComplex z = x[j + l];\n    x[j + l] = x[j] - z;\n    x[j] += z;\n    j = (j << 1) + (i & (m >> 1));\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t i = tid;\n  size_t j = 0;\n  size_t k = N / 2;\n  size_t n = N;\n\n  // Bit reversal\n  while (n > 1) {\n    size_t bit = n / 2;\n    if (i & bit) {\n      j ^= bit;\n    }\n    i &= (bit - 1);\n    i ^= j;\n    n = bit;\n  }\n\n  // Reordering\n  size_t l = 0;\n  while (l < k) {\n    size_t lo = l;\n    size_t hi = l + k;\n    if (i < hi) {\n      cuDoubleComplex tmp = x[lo];\n      x[lo] = x[hi];\n      x[hi] = tmp;\n    }\n    l += n;\n    n >>= 1;\n  }\n\n  // FFT\n  size_t m = 1;\n  while (m < N) {\n    size_t l = 0;\n    while (l < N) {\n      size_t lo = l;\n      size_t hi = l + m;\n      cuDoubleComplex z = x[hi];\n      cuDoubleComplex w = x[lo];\n      cuDoubleComplex t = cuCmul(z, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * i * l / N)));\n      x[lo] = cuCadd(w, t);\n      x[hi] = cuCsub(w, t);\n      l += m * 2;\n    }\n    m *= 2;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement the FFT in-place\n    //\n    // Hint:\n    //   - Use the cuDoubleComplex functions from the CUDA math API\n    //   - Use the cuCadd, cuCmul, cuCsub, cuCdiv functions from the CUDA math API\n    //   - Use the cuCreal, cuCimag functions from the CUDA math API\n    //   - Use the cuCsqrt function from the CUDA math API\n    //   - Use the cuCexp function from the CUDA math API\n    //   - Use the cuClog function from the CUDA math API\n    //   - Use the cuCsin function from the CUDA math API\n    //   - Use the cuCcos function from the CUDA math API\n    //   - Use the cuCabs function from the CUDA math API\n    //   - Use the cuCdivf function from the CUDA math API\n    //   - Use the cuCmulf function from the CUDA math API\n    //   - Use the cuCaddf function from the CUDA math API\n    //   - Use the cuCsubf function from the CUDA math API\n    //   - Use the cuCsqrtf function from the CUDA math API\n    //   - Use the cuCexpf function from the CUDA math API\n    //   - Use the cuClogf function from the CUDA math API\n    //   - Use the cuCsinf function from the CUDA math API\n    //   - Use the cuCcosf function from the CUDA math API\n    //   - Use the cuCabsf function from the CUDA math API\n    //   - Use the cuCdivf function from the CUDA math API\n    //   - Use the cuCmulf function from the CUDA math API\n    //   - Use the cuCaddf function from the CUDA math API\n    //   - Use the cuCsubf function from the CUDA math API\n    //   - Use the cuCsqrtf function from the CUDA math API\n    //   - Use the cuCexpf function from the CUDA math API\n    //   - Use the cuClogf function from the CUDA math API\n    //   - Use the cuCsinf function from the CUDA math API\n    //   - Use the cuCcosf function from the CUDA math API\n    //   - Use the cuCabsf function from the CUDA math API\n    //   - Use the cuCdivf function from the CUDA math API\n    //   - Use the cuCmulf function from the CUDA math API\n    //   - Use the cuCaddf function from the CUDA math API\n    //   - Use the cuCsubf function from the CUDA math API\n    //   - Use the cuCsqrtf function from the CUDA math API\n    //   - Use the cuCexpf function from the CUDA math API\n    //   - Use the cuClogf function from the CUDA math API\n    //   - Use the cuCsinf function from the CUDA math API\n    //   - Use the cuCcosf function from the CUDA math API\n    //   - Use the cuCabsf function from the CUDA math API\n    //   - Use the cuCdivf function from the CUDA math API\n    //   - Use the cuCmulf function from the CUDA math API\n    //   - Use the cuCaddf function from the CUDA math API\n    //   - Use the cuCsubf function from the CUDA math API\n    //   - Use the cuCsqrtf function from the CUDA math API\n    //   - Use the cuCexpf function from the CUDA math API\n    //   - Use the cuClogf function from the CUDA math API\n    //   - Use the cuCsinf function from the CUDA math API\n    //   - Use the cuCcosf function from the CUDA math API\n    //   - Use the cuCabsf function from the CUDA math API\n    //   - Use the cuCdivf function from the CUDA math API\n    //   - Use the cuCmulf function from the CUDA math API\n    //   - Use the cuCaddf function from the CUDA math API\n    //   - Use",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = 0;\n    for (size_t k = N / 2; k > 0; k /= 2) {\n        size_t l = j ^ k;\n        cuDoubleComplex z = x[l];\n        x[l] = x[j];\n        x[j] = cuCadd(x[j], z);\n        j = j ^ k;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = 0;\n  for (size_t m = N; m!= 1; m >>= 1) {\n    size_t l = m >> 1;\n    cuDoubleComplex z = x[i + l];\n    if (j & m) {\n      x[i] -= z;\n      x[i + l] = cuCadd(x[i], z);\n    } else {\n      x[i] += z;\n      x[i + l] = cuCsub(x[i], z);\n    }\n    j++;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  cuDoubleComplex xi = x[i];\n  cuDoubleComplex xj = x[N - i];\n  x[i] = cuCadd(xi, xj);\n  x[N - i] = cuCsub(xi, xj);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  size_t j = 0;\n  for (size_t m = N; m!= 1; m >>= 1) {\n    size_t l = m >> 1;\n    cuDoubleComplex z = x[i];\n    if (i & m) {\n      cuDoubleComplex w = x[i ^ l];\n      x[i] = cuCadd(z, w);\n      x[i ^ l] = cuCsub(z, w);\n    }\n    j++;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t j = i;\n    size_t k = 0;\n    while (j > 0) {\n        k += j & 1;\n        j >>= 1;\n    }\n    size_t l = 1 << k;\n    size_t m = l << 1;\n    size_t n = N >> k;\n    for (size_t s = 1; s < n; s <<= 1) {\n        size_t t = m >> s;\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * i / m), sin(2 * M_PI * i / m));\n        for (size_t u = 0; u < s; u++) {\n            size_t a = i ^ t ^ (u << (k + 1));\n            cuDoubleComplex z = x[a];\n            cuDoubleComplex y = x[i] * w;\n            x[a] = y + z;\n            x[i] = y - z;\n        }\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    cuDoubleComplex c = make_cuDoubleComplex(0, 0);\n    for (size_t i = tid; i < N; i += stride) {\n        for (size_t j = 0; j < N; j++) {\n            cuDoubleComplex z = make_cuDoubleComplex(0, 0);\n            if (i == j) {\n                z = make_cuDoubleComplex(1, 0);\n            } else if (i > j) {\n                z = make_cuDoubleComplex(0, -1);\n            } else {\n                z = make_cuDoubleComplex(0, 1);\n            }\n            cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * i * j / N), sin(2 * M_PI * i * j / N));\n            c += x[j] * cuCmul(z, w);\n        }\n        x[i] = c;\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_fft_conjugate",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n\n  std::vector<std::complex<double>> x_local(n_local);\n  std::vector<std::complex<double>> x_local_out(n_local);\n\n  MPI_Scatter(x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE_COMPLEX,\n              x_local.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE_COMPLEX,\n              0, MPI_COMM_WORLD);\n\n  // Compute the FFT of x_local in-place\n  //...\n\n  MPI_Gather(x_local.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE_COMPLEX,\n             x.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE_COMPLEX,\n             0, MPI_COMM_WORLD);\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank == size - 1) {\n    end += n_extra;\n  }\n  // TODO: Implement the FFT in-place in x.\n  // You may assume that the input is a power of 2.\n  // You may assume that the input is already padded with zeros.\n  // You may assume that the input is stored in a contiguous array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored in a complex array.\n  // You may assume that the input is stored",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int logn = log2(n);\n  int nthreads = omp_get_max_threads();\n\n  // TODO: Implement this function\n}",
            "// TODO: your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  // Compute the FFT of each rank's chunk of x\n  std::vector<std::complex<double>> x_local(n_per_rank);\n  std::copy(x.begin() + rank * n_per_rank, x.begin() + (rank + 1) * n_per_rank, x_local.begin());\n  fft_inplace(x_local);\n\n  // Gather the results from each rank\n  std::vector<std::complex<double>> x_all(n);\n  MPI_Gather(x_local.data(), n_per_rank, MPI_DOUBLE_COMPLEX, x_all.data(), n_per_rank, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // Compute the FFT of the results\n  if (rank == 0) {\n    fft_inplace(x_all);\n  }\n\n  // Broadcast the results to all ranks\n  MPI_Bcast(x_all.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // Copy the results back to x\n  std::copy(x_all.begin(), x_all.end(), x.begin());\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(Kokkos::View<Kokkos::complex<double>*> const& x, Kokkos::View<double*> &r, Kokkos::View<double*> &i) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a Kokkos::complex array to store the results\n  Kokkos::View<Kokkos::complex<double>*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the twiddle factors\n  Kokkos::View<Kokkos::complex<double>*> twiddle(\"twiddle\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the bit reversed indices\n  Kokkos::View<Kokkos::complex<double>*> bit_reversed(\"bit_reversed\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the FFT\n  Kokkos::View<Kokkos::complex<double>*> fft_result(\"fft_result\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result(\"ifft_result\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_2(\"ifft_result_2\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_3(\"ifft_result_3\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_4(\"ifft_result_4\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_5(\"ifft_result_5\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_6(\"ifft_result_6\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_7(\"ifft_result_7\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_8(\"ifft_result_8\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_9(\"ifft_result_9\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_10(\"ifft_result_10\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_11(\"ifft_result_11\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_12(\"ifft_result_12\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the IFFT\n  Kokkos::View<Kokkos::complex<double>*> ifft_result_13(\"",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "Kokkos::View<double*> x_real(\"x_real\", x.size());\n  Kokkos::View<double*> x_imag(\"x_imag\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_real(i) = Kokkos::real(x(i));\n    x_imag(i) = Kokkos::imag(x(i));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_real(i) = x_real(i) * 2.0;\n    x_imag(i) = x_imag(i) * 2.0;\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(x_real(i), x_imag(i));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::complex<double>(Kokkos::real(x(i)), -Kokkos::imag(x(i)));\n  });\n  Kokk",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a Kokkos::complex array to store the results\n  Kokkos::View<Kokkos::complex<double>*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the twiddle factors\n  Kokkos::View<Kokkos::complex<double>*> twiddle(\"twiddle\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the bit reversed indices\n  Kokkos::View<Kokkos::complex<double>*> bit_reversed(\"bit_reversed\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp(\"tmp\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp2(\"tmp2\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp3(\"tmp3\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp4(\"tmp4\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp5(\"tmp5\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp6(\"tmp6\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp7(\"tmp7\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp8(\"tmp8\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp9(\"tmp9\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp10(\"tmp10\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp11(\"tmp11\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp12(\"tmp12\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp13(\"tmp13\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp14(\"tmp14\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp15(\"tmp15\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*> tmp16(\"tmp16\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the temporary results\n  Kokkos::View<Kokkos::complex<double>*>",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement FFT\n}",
            "// TODO: Implement this function\n}",
            "// Create a Kokkos::complex array to store the results\n  Kokkos::View<Kokkos::complex<double>*> y(\"y\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the twiddle factors\n  Kokkos::View<Kokkos::complex<double>*> twiddle(\"twiddle\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the bit reversed indices\n  Kokkos::View<Kokkos::complex<double>*> bit_reversed(\"bit_reversed\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the first pass\n  Kokkos::View<Kokkos::complex<double>*> first_pass(\"first_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the second pass\n  Kokkos::View<Kokkos::complex<double>*> second_pass(\"second_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the third pass\n  Kokkos::View<Kokkos::complex<double>*> third_pass(\"third_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the fourth pass\n  Kokkos::View<Kokkos::complex<double>*> fourth_pass(\"fourth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the fifth pass\n  Kokkos::View<Kokkos::complex<double>*> fifth_pass(\"fifth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the sixth pass\n  Kokkos::View<Kokkos::complex<double>*> sixth_pass(\"sixth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the seventh pass\n  Kokkos::View<Kokkos::complex<double>*> seventh_pass(\"seventh_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the eighth pass\n  Kokkos::View<Kokkos::complex<double>*> eighth_pass(\"eighth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the ninth pass\n  Kokkos::View<Kokkos::complex<double>*> ninth_pass(\"ninth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the tenth pass\n  Kokkos::View<Kokkos::complex<double>*> tenth_pass(\"tenth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the eleventh pass\n  Kokkos::View<Kokkos::complex<double>*> eleventh_pass(\"eleventh_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the twelfth pass\n  Kokkos::View<Kokkos::complex<double>*> twelfth_pass(\"twelfth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the thirteenth pass\n  Kokkos::View<Kokkos::complex<double>*> thirteenth_pass(\"thirteenth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the fourteenth pass\n  Kokkos::View<Kokkos::complex<double>*> fourteenth_pass(\"fourteenth_pass\", x.extent(0));\n\n  // Create a Kokkos::complex array to store the results of the fifteenth pass\n  Kokkos::View<Kokkos::complex<double>*> fifteenth"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    // Split x into even and odd elements\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n\n    // Recursively compute the fourier transform of x_even and x_odd\n    std::vector<double> r_even(n/2);\n    std::vector<double> i_even(n/2);\n    std::vector<double> r_odd(n/2);\n    std::vector<double> i_odd(n/2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n\n    // Combine the results\n    for (int k = 0; k < n/2; k++) {\n        double t_real = r_even[k] + std::cos(2*M_PI*k/n)*r_odd[k] - std::sin(2*M_PI*k/n)*i_odd[k];\n        double t_imag = i_even[k] + std::sin(2*M_PI*k/n)*r_odd[k] + std::cos(2*M_PI*k/n)*i_odd[k];\n        r[k] = t_real;\n        i[k] = t_imag;\n        r[k+n/2] = t_real;\n        i[k+n/2] = -t_imag;\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n\n    std::vector<double> r_even(n/2);\n    std::vector<double> i_even(n/2);\n    std::vector<double> r_odd(n/2);\n    std::vector<double> i_odd(n/2);\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            fft(x_even, r_even, i_even);\n        }\n        #pragma omp section\n        {\n            fft(x_odd, r_odd, i_odd);\n        }\n    }\n\n    for (int k = 0; k < n/2; k++) {\n        double t = std::polar(1.0, -2*M_PI*k/n)*r_odd[k];\n        r[k] = r_even[k] + t.real();\n        r[k+n/2] = r_even[k] - t.real();\n        i[k] = i_even[k] + t.imag();\n        i[k+n/2] = -i_even[k] + t.imag();\n    }\n}",
            "int n = x.size();\n  int logn = std::log2(n);\n\n  std::vector<std::complex<double>> even(n/2);\n  std::vector<std::complex<double>> odd(n/2);\n\n  #pragma omp parallel for\n  for (int k = 0; k < n/2; k++) {\n    even[k] = x[2*k];\n    odd[k] = x[2*k+1];\n  }\n\n  fft(even, r, i);\n  fft(odd, r, i);\n\n  #pragma omp parallel for\n  for (int k = 0; k < n/2; k++) {\n    std::complex<double> t = std::polar(1.0, -2 * M_PI * k / n) * odd[k];\n    r[k] = r[k] + t.real();\n    i[k] = i[k] + t.imag();\n    r[k+n/2] = r[k] - t.real();\n    i[k+n/2] = i[k] - t.imag();\n  }\n}",
            "int n = x.size();\n    int n2 = n / 2;\n    std::vector<std::complex<double>> x_even(n2);\n    std::vector<std::complex<double>> x_odd(n2);\n    std::vector<std::complex<double>> y_even(n2);\n    std::vector<std::complex<double>> y_odd(n2);\n    std::vector<std::complex<double>> z_even(n2);\n    std::vector<std::complex<double>> z_odd(n2);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n\n    fft(x_even, y_even, z_even);\n    fft(x_odd, y_odd, z_odd);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n2; i++) {\n        std::complex<double> y = y_even[i];\n        std::complex<double> z = z_even[i];\n        std::complex<double> w = std::polar(1.0, -2 * M_PI * i / n) * z_odd[i];\n        r[i] = y.real() + w.real();\n        r[i + n2] = y.real() - w.real();\n        i[i] = y.imag() + w.imag();\n        i[i + n2] = -y.imag() + w.imag();\n    }\n}",
            "int n = x.size();\n    int n2 = n / 2;\n    std::vector<std::complex<double>> x_even(n2);\n    std::vector<std::complex<double>> x_odd(n2);\n    std::vector<std::complex<double>> y_even(n2);\n    std::vector<std::complex<double>> y_odd(n2);\n\n    for (int i = 0; i < n2; ++i) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n\n    fft(x_even, r, i);\n    fft(x_odd, r, i);\n\n    for (int k = 0; k < n2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / n) * x_odd[k];\n        y_even[k] = x_even[k] + t;\n        y_odd[k] = x_even[k] - t;\n    }\n\n    for (int i = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            r[i / 2] = y_even[i / 2].real();\n            i[i / 2] = y_even[i / 2].imag();\n        } else {\n            r[i / 2] = y_odd[i / 2].real();\n            i[i / 2] = y_odd[i / 2].imag();\n        }\n    }\n}",
            "int n = x.size();\n    int n2 = n/2;\n    int n4 = n/4;\n    std::vector<std::complex<double>> x2(n2);\n    std::vector<std::complex<double>> x4(n4);\n    std::vector<std::complex<double>> x8(n4);\n    std::vector<std::complex<double>> x16(n4);\n    std::vector<std::complex<double>> x32(n4);\n    std::vector<std::complex<double>> x64(n4);\n    std::vector<std::complex<double>> x128(n4);\n    std::vector<std::complex<double>> x256(n4);\n    std::vector<std::complex<double>> x512(n4);\n    std::vector<std::complex<double>> x1024(n4);\n    std::vector<std::complex<double>> x2048(n4);\n    std::vector<std::complex<double>> x4096(n4);\n    std::vector<std::complex<double>> x8192(n4);\n    std::vector<std::complex<double>> x16384(n4);\n    std::vector<std::complex<double>> x32768(n4);\n    std::vector<std::complex<double>> x65536(n4);\n    std::vector<std::complex<double>> x131072(n4);\n    std::vector<std::complex<double>> x262144(n4);\n    std::vector<std::complex<double>> x524288(n4);\n    std::vector<std::complex<double>> x1048576(n4);\n    std::vector<std::complex<double>> x2097152(n4);\n    std::vector<std::complex<double>> x4194304(n4);\n    std::vector<std::complex<double>> x8388608(n4);\n    std::vector<std::complex<double>> x16777216(n4);\n    std::vector<std::complex<double>> x33554432(n4);\n    std::vector<std::complex<double>> x67108864(n4);\n    std::vector<std::complex<double>> x134217728(n4);\n    std::vector<std::complex<double>> x268435456(n4);\n    std::vector<std::complex<double>> x536870912(n4);\n    std::vector<std::complex<double>> x1073741824(n4);\n    std::vector<std::complex<double>> x2147483648(n4);\n    std::vector<std::complex<double>> x4294967296(n4);\n    std::vector<std::complex<double>> x8589934592(n4);\n    std::vector<std::complex<double>> x17179869184(n4);\n    std::vector<std::complex<double>> x34359738368(n4);\n    std::vector<std::complex<double>> x68719476736(n4);\n    std::vector<std::complex<double>> x137438953472(n4);\n    std::vector<std::complex<double>> x274877906944(n4);\n    std::vector<std::complex<double>> x549755813888(n4);\n    std::vector<std::complex<double>> x1099511627776(n4);\n    std::vector<std::complex<double>> x2199023255552(n4);\n    std::vector<std::complex<double>> x4398",
            "int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        double angle = 2 * M_PI * k / n;\n        std::complex<double> w(cos(angle), sin(angle));\n        y[k] = x[k];\n        z[k] = w * x[k];\n    }\n\n    fft(y, r, i);\n    fft(z, r, i);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        double angle = 2 * M_PI * k / n;\n        std::complex<double> w(cos(angle), sin(angle));\n        y[k] = y[k] + w * z[k];\n    }\n\n    fft(y, r, i);\n}",
            "int n = x.size();\n    int m = log2(n);\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n\n    // bit-reversal permutation\n    for (int i = 0; i < n; i++) {\n        int j = 0;\n        int k = i;\n        for (int l = 0; l < m; l++) {\n            j = 2 * j + (k & 1);\n            k >>= 1;\n        }\n        y[j] = x[i];\n    }\n\n    // Cooley-Tukey FFT\n    for (int l = 1; l <= m; l++) {\n        int k = 1 << l;\n        double theta = 2 * M_PI / k;\n        std::complex<double> wm(cos(theta), -sin(theta));\n        for (int j = 0; j < n / (2 * k); j++) {\n            std::complex<double> w(1, 0);\n            for (int i = j; i < n; i += 2 * k) {\n                z[i] = y[i] + w * y[i + k];\n                z[i + k] = y[i] - w * y[i + k];\n                w *= wm;\n            }\n        }\n        std::swap(y, z);\n    }\n\n    // store results\n    r.resize(n);\n    i.resize(n);\n    for (int i = 0; i < n; i++) {\n        r[i] = y[i].real();\n        i[i] = y[i].imag();\n    }\n}",
            "int n = x.size();\n    int m = 1;\n    while (m < n) {\n        int k = 0;\n        #pragma omp parallel for\n        for (int j = 0; j < m; ++j) {\n            std::complex<double> wm = std::exp(std::complex<double>(0, -2 * M_PI * k / m));\n            for (int i = j; i < n; i += 2 * m) {\n                std::complex<double> t = x[i + m] * wm;\n                x[i + m] = x[i] - t;\n                x[i] = x[i] + t;\n            }\n            ++k;\n        }\n        m *= 2;\n    }\n    r.resize(n);\n    i.resize(n);\n    for (int i = 0; i < n; ++i) {\n        r[i] = x[i].real();\n        i[i] = x[i].imag();\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> u(n);\n\n    // copy input to output\n    for (int k = 0; k < n; ++k) {\n        y[k] = x[k];\n    }\n\n    // bit reversal\n    for (int k = 0; k < n; ++k) {\n        int j = 0;\n        int m = n;\n        while (m > 1) {\n            j = j * 2 + (k & 1);\n            k = k >> 1;\n            m = m >> 1;\n        }\n        z[j] = y[k];\n    }\n\n    // Danielson-Lanczos\n    for (int m = 1; m < n; m = m + m) {\n        double theta = 2 * M_PI / m;\n        std::complex<double> wm(cos(theta), -sin(theta));\n        for (int j = 0; j < m; ++j) {\n            w[j] = 1;\n        }\n        for (int k = 0; k < n; k = k + m + m) {\n            for (int j = 0; j < m; ++j) {\n                u[j] = w[j] * z[k + j + m];\n            }\n            for (int j = 0; j < m; ++j) {\n                int j1 = j + m;\n                z[k + j] = z[k + j] + u[j];\n                z[k + j1] = wm * (z[k + j] - u[j]);\n                w[j] = w[j1] * wm;\n            }\n        }\n    }\n\n    // copy results\n    for (int k = 0; k < n; ++k) {\n        r[k] = z[k].real();\n        i[k] = z[k].imag();\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        double theta = 2 * M_PI * k / n;\n        std::complex<double> w(cos(theta), sin(theta));\n        std::complex<double> xk = x[k];\n        std::complex<double> yk = xk * w;\n        y[k] = yk;\n    }\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        std::complex<double> yk = y[k];\n        std::complex<double> zk = yk + y[(n - k) % n];\n        z[k] = zk;\n    }\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        std::complex<double> zk = z[k];\n        std::complex<double> rk = zk / 2;\n        r[k] = rk.real();\n        i[k] = rk.imag();\n    }\n}",
            "int n = x.size();\n    int n2 = n/2;\n    std::vector<std::complex<double>> x_even(n2);\n    std::vector<std::complex<double>> x_odd(n2);\n    std::vector<std::complex<double>> y_even(n2);\n    std::vector<std::complex<double>> y_odd(n2);\n\n    for (int i = 0; i < n2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n\n    fft(x_even, r, i);\n    fft(x_odd, r, i);\n\n    for (int i = 0; i < n2; i++) {\n        y_even[i] = x_even[i] + std::polar(1.0, -2*M_PI*i/n)*x_odd[i];\n        y_odd[i] = x_even[i] - std::polar(1.0, -2*M_PI*i/n)*x_odd[i];\n    }\n\n    for (int i = 0; i < n; i++) {\n        if (i % 2 == 0) {\n            r[i/2] = y_even[i/2].real();\n            i[i/2] = y_even[i/2].imag();\n        } else {\n            r[i/2] = y_odd[i/2].real();\n            i[i/2] = y_odd[i/2].imag();\n        }\n    }\n}",
            "// TODO: Implement this function\n    // You may use the following functions:\n    // - std::exp(std::complex<double> z)\n    // - std::pow(std::complex<double> z, int n)\n    // - std::conj(std::complex<double> z)\n    // - std::norm(std::complex<double> z)\n    // - std::arg(std::complex<double> z)\n    // - std::polar(double r, double theta)\n    // - std::abs(std::complex<double> z)\n    // - std::sqrt(std::complex<double> z)\n    // - std::log(std::complex<double> z)\n    // - std::log10(std::complex<double> z)\n    // - std::cos(std::complex<double> z)\n    // - std::sin(std::complex<double> z)\n    // - std::tan(std::complex<double> z)\n    // - std::acos(std::complex<double> z)\n    // - std::asin(std::complex<double> z)\n    // - std::atan(std::complex<double> z)\n    // - std::cosh(std::complex<double> z)\n    // - std::sinh(std::complex<double> z)\n    // - std::tanh(std::complex<double> z)\n    // - std::acosh(std::complex<double> z)\n    // - std::asinh(std::complex<double> z)\n    // - std::atanh(std::complex<double> z)\n    // - std::isfinite(std::complex<double> z)\n    // - std::isinf(std::complex<double> z)\n    // - std::isnan(std::complex<double> z)\n    // - std::isnormal(std::complex<double> z)\n    // - std::signbit(std::complex<double> z)\n    // - std::fpclassify(std::complex<double> z)\n    // - std::isfinite(double x)\n    // - std::isinf(double x)\n    // - std::isnan(double x)\n    // - std::isnormal(double x)\n    // - std::signbit(double x)\n    // - std::fpclassify(double x)\n    // - std::abs(double x)\n    // - std::sqrt(double x)\n    // - std::exp(double x)\n    // - std::log(double x)\n    // - std::log10(double x)\n    // - std::cos(double x)\n    // - std::sin(double x)\n    // - std::tan(double x)\n    // - std::acos(double x)\n    // - std::asin(double x)\n    // - std::atan(double x)\n    // - std::cosh(double x)\n    // - std::sinh(double x)\n    // - std::tanh(double x)\n    // - std::acosh(double x)\n    // - std::asinh(double x)\n    // - std::atanh(double x)\n    // - std::pow(double x, int n)\n    // - std::pow(double x, double y)\n    // - std::conj(double x)\n    // - std::norm(double x)\n    // - std::arg(double x)\n    // - std::polar(double r, double theta)\n    // - std::isfinite(double x)\n    // - std::isinf(double x)\n    // - std::isnan(double x)\n    // - std::isnormal(double x)\n    // - std::signbit(double x)\n    // - std::fpclassify(double x)\n    // - std::abs(int x)\n    // - std::sqrt(int x)\n    // - std::exp(int x)\n    // - std::log(int x)\n    // - std::log10(int x)\n    // - std::cos(int x)\n    // - std::sin(int x)\n    // - std::tan(int x)\n    // - std::acos(int x)\n    // - std::asin",
            "int n = x.size();\n    int m = 1;\n    int k = 0;\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> z(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> w_inv(n);\n    std::vector<std::complex<double>> w_pow(n);\n    std::vector<std::complex<double>> w_pow_inv(n);\n    std::vector<std::complex<double>> w_pow_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m_inv_m(n);\n    std::vector<std::complex<double>> w_pow_m_inv_m_inv_m_inv_m_inv_m",
            "int n = x.size();\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k+1];\n    }\n\n    std::vector<double> r_even(n/2);\n    std::vector<double> r_odd(n/2);\n    std::vector<double> i_even(n/2);\n    std::vector<double> i_odd(n/2);\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            fft(x_even, r_even, i_even);\n        }\n        #pragma omp section\n        {\n            fft(x_odd, r_odd, i_odd);\n        }\n    }\n\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*k/n) * x_odd[k];\n        r[k] = r_even[k] + t.real();\n        r[k+n/2] = r_even[k] - t.real();\n        i[k] = i_even[k] + t.imag();\n        i[k+n/2] = -i_even[k] + t.imag();\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    int m = log2(n);\n    int N = 1 << m;\n    std::vector<std::complex<double>> x_pad(N);\n    std::vector<std::complex<double>> y_pad(N);\n    std::vector<std::complex<double>> x_temp(N);\n    std::vector<std::complex<double>> y_temp(N);\n    std::vector<std::complex<double>> x_temp2(N);\n    std::vector<std::complex<double>> y_temp2(N);\n    std::vector<std::complex<double>> x_temp3(N);\n    std::vector<std::complex<double>> y_temp3(N);\n    std::vector<std::complex<double>> x_temp4(N);\n    std::vector<std::complex<double>> y_temp4(N);\n    std::vector<std::complex<double>> x_temp5(N);\n    std::vector<std::complex<double>> y_temp5(N);\n    std::vector<std::complex<double>> x_temp6(N);\n    std::vector<std::complex<double>> y_temp6(N);\n    std::vector<std::complex<double>> x_temp7(N);\n    std::vector<std::complex<double>> y_temp7(N);\n    std::vector<std::complex<double>> x_temp8(N);\n    std::vector<std::complex<double>> y_temp8(N);\n    std::vector<std::complex<double>> x_temp9(N);\n    std::vector<std::complex<double>> y_temp9(N);\n    std::vector<std::complex<double>> x_temp10(N);\n    std::vector<std::complex<double>> y_temp10(N);\n    std::vector<std::complex<double>> x_temp11(N);\n    std::vector<std::complex<double>> y_temp11(N);\n    std::vector<std::complex<double>> x_temp12(N);\n    std::vector<std::complex<double>> y_temp12(N);\n    std::vector<std::complex<double>> x_temp13(N);\n    std::vector<std::complex<double>> y_temp13(N);\n    std::vector<std::complex<double>> x_temp14(N);\n    std::vector<std::complex<double>> y_temp14(N);\n    std::vector<std::complex<double>> x_temp15(N);\n    std::vector<std::complex<double>> y_temp15(N);\n    std::vector<std::complex<double>> x_temp16(N);\n    std::vector<std::complex<double>> y_temp16(N);\n    std::vector<std::complex<double>> x_temp17(N);\n    std::vector<std::complex<double>> y_temp17(N);\n    std::vector<std::complex<double>> x_temp18(N);\n    std::vector<std::complex<double>> y_temp18(N);\n    std::vector<std::complex<double>> x_temp19(N);\n    std::vector<std::complex<double>> y_temp19(N);\n    std::vector<std::complex<double>> x_temp20(N);\n    std::vector<std::complex<double>> y_temp20(N);\n    std::vector<std::complex<double>> x_temp21(N);\n    std::vector<std::complex<double>> y_temp21(N);\n    std::vector<std::complex<double>> x_temp22(N);\n    std::vector<std::complex<double>> y_temp22(N);\n    std::vector<std::complex<double>> x_temp23(N);\n    std::vector<std::complex<double>> y_temp23(N);\n    std::vector<std::complex<double>> x_temp24(N);\n    std::vector<std::complex<double>> y_temp24(N);\n    std::vector<std::complex<",
            "int n = x.size();\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n    std::vector<double> r_even(n/2);\n    std::vector<double> i_even(n/2);\n    std::vector<double> r_odd(n/2);\n    std::vector<double> i_odd(n/2);\n    if (n > 2) {\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n    } else {\n        r_even[0] = x_even[0].real();\n        i_even[0] = x_even[0].imag();\n        r_odd[0] = x_odd[0].real();\n        i_odd[0] = x_odd[0].imag();\n    }\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*k/n) * x_odd[k];\n        r[k] = r_even[k] + t.real();\n        i[k] = i_even[k] + t.imag();\n        r[k+n/2] = r_even[k] - t.real();\n        i[k+n/2] = i_even[k] - t.imag();\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> x_even(n/2), x_odd(n/2);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n\n    std::vector<double> r_even, r_odd, i_even, i_odd;\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n/2; i++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * x_odd[i];\n        r[i] = r_even[i] + t.real();\n        i[i] = i_even[i] + t.imag();\n        r[i+n/2] = r_even[i] - t.real();\n        i[i+n/2] = -i_even[i] + t.imag();\n    }\n}",
            "int n = x.size();\n    int n2 = n / 2;\n    std::vector<std::complex<double>> even(n2);\n    std::vector<std::complex<double>> odd(n2);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i+1];\n    }\n\n    fft(even, r, i);\n    fft(odd, r, i);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n2; i++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * i / n) * odd[i];\n        r[i] = r[i] + t.real();\n        r[i+n2] = r[i+n2] + t.real();\n        i[i] = i[i] + t.imag();\n        i[i+n2] = i[i+n2] + t.imag();\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        if (i < m) {\n            std::swap(x[i], x[m]);\n        }\n        int k = n / 2;\n        while (k >= 1 && m >= k) {\n            m -= k;\n            k /= 2;\n        }\n        m += k;\n    }\n    for (int l = 1; l < n; l *= 2) {\n        double u = -2 * M_PI / l;\n        std::complex<double> w(cos(u), sin(u));\n        for (int j = 0; j < l / 2; j++) {\n            std::complex<double> t;\n            for (int i = j; i < n; i += l) {\n                int k = i + l / 2;\n                t = x[k] * w;\n                x[k] = x[i] - t;\n                x[i] = x[i] + t;\n            }\n            w *= w;\n        }\n    }\n    r.resize(n);\n    i.resize(n);\n    for (int i = 0; i < n; i++) {\n        r[i] = x[i].real();\n        i[i] = x[i].imag();\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> even(n/2);\n    std::vector<std::complex<double>> odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i+1];\n    }\n    fft(even, r, i);\n    fft(odd, r, i);\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2.0*M_PI*k/n)*odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k+n/2] = r[k] - t.real();\n        i[k+n/2] = i[k] - t.imag();\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> x_copy(x);\n    std::vector<std::complex<double>> y(n);\n    std::vector<std::complex<double>> w(n);\n    std::vector<std::complex<double>> w_copy(n);\n    std::vector<std::complex<double>> w_inv(n);\n    std::vector<std::complex<double>> w_inv_copy(n);\n\n    // Compute the w values\n    for (int i = 0; i < n; i++) {\n        double angle = 2 * M_PI * i / n;\n        w[i] = std::complex<double>(cos(angle), sin(angle));\n        w_copy[i] = w[i];\n        w_inv[i] = std::complex<double>(cos(angle), -sin(angle));\n        w_inv_copy[i] = w_inv[i];\n    }\n\n    // Compute the fft\n    for (int m = 1; m < n; m *= 2) {\n        int m2 = m / 2;\n        for (int i = 0; i < m; i++) {\n            for (int j = 0; j < n / m; j++) {\n                int k = j * m + i;\n                int k2 = k + m2;\n                std::complex<double> x_k = x_copy[k];\n                std::complex<double> x_k2 = x_copy[k2];\n                std::complex<double> w_k = w[k];\n                std::complex<double> w_k2 = w[k2];\n                y[k] = x_k + w_k * x_k2;\n                y[k2] = x_k - w_k2 * x_k2;\n            }\n        }\n        x_copy = y;\n        w = w_copy;\n        w_copy = w_inv_copy;\n        w_inv = w_inv_copy;\n        w_inv_copy = w;\n    }\n\n    // Store the results\n    for (int i = 0; i < n; i++) {\n        r[i] = x_copy[i].real();\n        i[i] = x_copy[i].imag();\n    }\n}",
            "// TODO: implement this function\n    int n = x.size();\n    int m = 1;\n    while (m < n) {\n        int k = 0;\n        for (int j = 0; j < m; j++) {\n            for (int i = j; i < n; i += m + m) {\n                int l = i + m;\n                std::complex<double> t = x[l];\n                x[l] = x[i] - t;\n                x[i] += t;\n            }\n            k += m;\n        }\n        m += m;\n    }\n    for (int i = 0; i < n; i++) {\n        r[i] = x[i].real();\n        i[i] = x[i].imag();\n    }\n}",
            "std::vector<std::complex<double>> x_copy(x);\n    std::vector<std::complex<double>> y(x.size());\n    std::vector<std::complex<double>> z(x.size());\n    std::vector<std::complex<double>> w(x.size());\n\n    // Compute the FFT of x\n    fft_recursive(x_copy, y, z, w, 0, x.size());\n\n    // Copy the real part of y into r and the imaginary part into i\n    r.resize(x.size());\n    i.resize(x.size());\n    for (size_t k = 0; k < x.size(); ++k) {\n        r[k] = y[k].real();\n        i[k] = y[k].imag();\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        r.push_back(x[0].real());\n        i.push_back(x[0].imag());\n        return;\n    }\n    std::vector<std::complex<double>> even;\n    std::vector<std::complex<double>> odd;\n    for (int i = 0; i < n; i++) {\n        if (i % 2 == 0) {\n            even.push_back(x[i]);\n        } else {\n            odd.push_back(x[i]);\n        }\n    }\n    fft(even, r, i);\n    fft(odd, r, i);\n    for (int k = 0; k < n / 2; k++) {\n        auto t = std::exp(-2 * M_PI * 1.0i * k / n) * odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k + n / 2] = r[k] - t.real();\n        i[k + n / 2] = i[k] - t.imag();\n    }\n}",
            "int n = x.size();\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n    fft(x_even, r, i);\n    fft(x_odd, r, i);\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*k/n) * x_odd[k];\n        r[k] = r[k] + t.real();\n        r[k+n/2] = r[k] - t.real();\n        i[k] = i[k] + t.imag();\n        i[k+n/2] = i[k] - t.imag();\n    }\n}",
            "// TODO: implement this function\n}",
            "std::vector<std::complex<double>> X(x.size());\n    std::vector<std::complex<double>> Y(x.size());\n    std::vector<std::complex<double>> Z(x.size());\n    std::vector<std::complex<double>> W(x.size());\n    std::vector<std::complex<double>> T(x.size());\n    std::vector<std::complex<double>> U(x.size());\n    std::vector<std::complex<double>> V(x.size());\n    std::vector<std::complex<double>> W1(x.size());\n    std::vector<std::complex<double>> W2(x.size());\n    std::vector<std::complex<double>> W3(x.size());\n    std::vector<std::complex<double>> W4(x.size());\n    std::vector<std::complex<double>> W5(x.size());\n    std::vector<std::complex<double>> W6(x.size());\n    std::vector<std::complex<double>> W7(x.size());\n    std::vector<std::complex<double>> W8(x.size());\n    std::vector<std::complex<double>> W9(x.size());\n    std::vector<std::complex<double>> W10(x.size());\n    std::vector<std::complex<double>> W11(x.size());\n    std::vector<std::complex<double>> W12(x.size());\n    std::vector<std::complex<double>> W13(x.size());\n    std::vector<std::complex<double>> W14(x.size());\n    std::vector<std::complex<double>> W15(x.size());\n    std::vector<std::complex<double>> W16(x.size());\n    std::vector<std::complex<double>> W17(x.size());\n    std::vector<std::complex<double>> W18(x.size());\n    std::vector<std::complex<double>> W19(x.size());\n    std::vector<std::complex<double>> W20(x.size());\n    std::vector<std::complex<double>> W21(x.size());\n    std::vector<std::complex<double>> W22(x.size());\n    std::vector<std::complex<double>> W23(x.size());\n    std::vector<std::complex<double>> W24(x.size());\n    std::vector<std::complex<double>> W25(x.size());\n    std::vector<std::complex<double>> W26(x.size());\n    std::vector<std::complex<double>> W27(x.size());\n    std::vector<std::complex<double>> W28(x.size());\n    std::vector<std::complex<double>> W29(x.size());\n    std::vector<std::complex<double>> W30(x.size());\n    std::vector<std::complex<double>> W31(x.size());\n    std::vector<std::complex<double>> W32(x.size());\n    std::vector<std::complex<double>> W33(x.size());\n    std::vector<std::complex<double>> W34(x.size());\n    std::vector<std::complex<double>> W35(x.size());\n    std::vector<std::complex<double>> W36(x.size());\n    std::vector<std::complex<double>> W37(x.size());\n    std::vector<std::complex<double>> W38(x.size());\n    std::vector<std::complex<double>> W39(x.size());\n    std::vector<std::complex<double>> W40(x.size());\n    std::vector<std::complex<double>> W41(x.size());\n    std::vector<std::complex<double>> W42(x.size());\n    std::vector<std::complex<double>> W43(x.size());\n    std::vector<std::complex<double>> W44(x.size());\n    std::vector<std::complex<double>> W45(x.size());\n    std::vector<std",
            "int n = x.size();\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> even(n/2);\n    std::vector<std::complex<double>> odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        even[k] = x[2*k];\n        odd[k] = x[2*k+1];\n    }\n    fft(even, r, i);\n    fft(odd, r, i);\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*k/n) * odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k+n/2] = r[k] - t.real();\n        i[k+n/2] = i[k] - t.imag();\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    std::vector<std::complex<double>> even(n/2);\n    std::vector<std::complex<double>> odd(n/2);\n\n    for (int i = 0; i < n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i+1];\n    }\n\n    fft(even, r, i);\n    fft(odd, r, i);\n\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / n) * odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k + n/2] = r[k] - t.real();\n        i[k + n/2] = i[k] - t.imag();\n    }\n}",
            "// TODO: Implement this function\n    // 1. Create a vector of size 2^n, where n is the size of x.\n    // 2. Copy x into the first n elements of the vector.\n    // 3. Fill the remaining elements with 0.\n    // 4. Apply the FFT algorithm.\n    // 5. Copy the real part of the results into r and the imaginary part into i.\n    // 6. Return.\n\n    // 1. Create a vector of size 2^n, where n is the size of x.\n    int n = x.size();\n    int m = 1;\n    while (m < n) {\n        m *= 2;\n    }\n    std::vector<std::complex<double>> y(m);\n\n    // 2. Copy x into the first n elements of the vector.\n    for (int i = 0; i < n; i++) {\n        y[i] = x[i];\n    }\n\n    // 3. Fill the remaining elements with 0.\n    for (int i = n; i < m; i++) {\n        y[i] = 0;\n    }\n\n    // 4. Apply the FFT algorithm.\n    fft_recursive(y, 0, m, 1);\n\n    // 5. Copy the real part of the results into r and the imaginary part into i.\n    for (int i = 0; i < m; i++) {\n        r[i] = y[i].real();\n        i[i] = y[i].imag();\n    }\n}",
            "int n = x.size();\n    if (n == 1) {\n        r.push_back(x[0].real());\n        i.push_back(x[0].imag());\n        return;\n    }\n    std::vector<std::complex<double>> even;\n    std::vector<std::complex<double>> odd;\n    for (int i = 0; i < n / 2; i++) {\n        even.push_back(x[2 * i]);\n        odd.push_back(x[2 * i + 1]);\n    }\n    fft(even, r, i);\n    fft(odd, r, i);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> t = std::polar(1.0, -2 * PI * k / n) * odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k + n / 2] = r[k] - t.real();\n        i[k + n / 2] = i[k] - t.imag();\n    }\n}",
            "// TODO: Implement this function\n  int N = x.size();\n  std::vector<std::complex<double>> X(N);\n  std::vector<std::complex<double>> Y(N);\n  std::vector<std::complex<double>> Z(N);\n  std::vector<std::complex<double>> W(N);\n  std::vector<std::complex<double>> T(N);\n  std::vector<std::complex<double>> U(N);\n  std::vector<std::complex<double>> V(N);\n  std::vector<std::complex<double>> W1(N);\n  std::vector<std::complex<double>> W2(N);\n  std::vector<std::complex<double>> W3(N);\n  std::vector<std::complex<double>> W4(N);\n  std::vector<std::complex<double>> W5(N);\n  std::vector<std::complex<double>> W6(N);\n  std::vector<std::complex<double>> W7(N);\n  std::vector<std::complex<double>> W8(N);\n  std::vector<std::complex<double>> W9(N);\n  std::vector<std::complex<double>> W10(N);\n  std::vector<std::complex<double>> W11(N);\n  std::vector<std::complex<double>> W12(N);\n  std::vector<std::complex<double>> W13(N);\n  std::vector<std::complex<double>> W14(N);\n  std::vector<std::complex<double>> W15(N);\n  std::vector<std::complex<double>> W16(N);\n  std::vector<std::complex<double>> W17(N);\n  std::vector<std::complex<double>> W18(N);\n  std::vector<std::complex<double>> W19(N);\n  std::vector<std::complex<double>> W20(N);\n  std::vector<std::complex<double>> W21(N);\n  std::vector<std::complex<double>> W22(N);\n  std::vector<std::complex<double>> W23(N);\n  std::vector<std::complex<double>> W24(N);\n  std::vector<std::complex<double>> W25(N);\n  std::vector<std::complex<double>> W26(N);\n  std::vector<std::complex<double>> W27(N);\n  std::vector<std::complex<double>> W28(N);\n  std::vector<std::complex<double>> W29(N);\n  std::vector<std::complex<double>> W30(N);\n  std::vector<std::complex<double>> W31(N);\n  std::vector<std::complex<double>> W32(N);\n  std::vector<std::complex<double>> W33(N);\n  std::vector<std::complex<double>> W34(N);\n  std::vector<std::complex<double>> W35(N);\n  std::vector<std::complex<double>> W36(N);\n  std::vector<std::complex<double>> W37(N);\n  std::vector<std::complex<double>> W38(N);\n  std::vector<std::complex<double>> W39(N);\n  std::vector<std::complex<double>> W40(N);\n  std::vector<std::complex<double>> W41(N);\n  std::vector<std::complex<double>> W42(N);\n  std::vector<std::complex<double>> W43(N);\n  std::vector<std::complex<double>> W44(N);\n  std::vector<std::complex<double>> W45(N);\n  std::vector<std::complex<double>> W46(N);\n  std::vector<std::complex<double>> W47(N);\n  std::vector<std::complex<double>> W48(N);\n  std::vector<std::complex<double>> W49(N);\n  std::vector<std::complex<double>> W50(N);\n  std",
            "int n = x.size();\n  if (n == 1) {\n    r[0] = x[0].real();\n    i[0] = x[0].imag();\n    return;\n  }\n\n  std::vector<std::complex<double>> even(n/2);\n  std::vector<std::complex<double>> odd(n/2);\n  for (int i = 0; i < n/2; i++) {\n    even[i] = x[2*i];\n    odd[i] = x[2*i+1];\n  }\n\n  std::vector<double> re(n/2);\n  std::vector<double> im(n/2);\n  fft(even, re, im);\n  fft(odd, re, im);\n\n  for (int i = 0; i < n/2; i++) {\n    std::complex<double> t = std::polar(1.0, -2*M_PI*i/n) * odd[i];\n    r[i] = re[i] + t.real();\n    r[i+n/2] = re[i] - t.real();\n    i[i] = im[i] + t.imag();\n    i[i+n/2] = -im[i] + t.imag();\n  }\n}",
            "int n = x.size();\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> even(n/2);\n    std::vector<std::complex<double>> odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i+1];\n    }\n    fft(even, r, i);\n    fft(odd, r, i);\n    for (int k = 0; k < n/2; k++) {\n        std::complex<double> t = std::polar(1.0, -2*M_PI*k/n) * odd[k];\n        r[k] = r[k] + t.real();\n        i[k] = i[k] + t.imag();\n        r[k+n/2] = r[k] - t.real();\n        i[k+n/2] = i[k] - t.imag();\n    }\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        if (i < m) {\n            std::swap(x[i], x[m]);\n        }\n        int k = n / 2;\n        while (k >= 1 && m >= k) {\n            m -= k;\n            k /= 2;\n        }\n        m += k;\n    }\n    for (int l = 1; l < n; l *= 2) {\n        double u = -2 * M_PI / l;\n        std::complex<double> w(cos(u), sin(u));\n        for (int j = 0; j < l / 2; j++) {\n            std::complex<double> t = w * x[j + l / 2];\n            std::complex<double> xj = x[j];\n            x[j] = xj + t;\n            x[j + l / 2] = xj - t;\n        }\n    }\n    r.resize(n);\n    i.resize(n);\n    for (int k = 0; k < n; k++) {\n        r[k] = x[k].real();\n        i[k] = x[k].imag();\n    }\n}",
            "std::vector<std::complex<double>> y(x.size());\n    fft(x, y);\n    r.resize(y.size());\n    i.resize(y.size());\n    for (size_t k = 0; k < y.size(); ++k) {\n        r[k] = y[k].real();\n        i[k] = y[k].imag();\n    }\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "hip",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const hipDoubleComplex *x, hipDoubleComplex *r, hipDoubleComplex *i, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    size_t stride = hipBlockDim_x * hipGridDim_x;\n    size_t half = N / 2;\n\n    for (size_t i = tid; i < N; i += stride) {\n        hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n        for (size_t j = 0; j < N; j++) {\n            hipDoubleComplex z = make_hipDoubleComplex(0, -2 * M_PI * i * j / N);\n            hipDoubleComplex w = hipCexp(z);\n            sum = hipCadd(sum, hipCmul(x[j], w));\n        }\n        r[i] = hipCreal(sum);\n        i[i] = hipCimag(sum);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t stride = hipBlockDim_x * hipGridDim_x;\n\n    // Do the bit reversal\n    size_t j = reverseBits(tid, N);\n\n    // Load the input\n    hipDoubleComplex c = x[j];\n\n    // Compute the FFT\n    hipDoubleComplex even = make_hipDoubleComplex(hipCos(PI * j / N), 0);\n    hipDoubleComplex odd = make_hipDoubleComplex(hipSin(-PI * j / N), 0);\n\n    for (size_t n = 2; n <= N; n <<= 1) {\n        size_t half_n = n >> 1;\n        hipDoubleComplex t = even * c[half_n] + odd * c[half_n + j];\n        c[half_n + j] = even * c[half_n + j] - odd * c[half_n];\n        c[half_n] = t;\n        even = even * even - odd * odd;\n        odd = even * odd + odd * even;\n    }\n\n    // Store the output\n    r[tid] = hipCreal(c);\n    i[tid] = hipCimag(c);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t n = 0; n < N; n++) {\n        hipDoubleComplex z = x[n];\n        hipDoubleComplex w = make_hipDoubleComplex(cos(-2 * M_PI * tid * n / N), sin(-2 * M_PI * tid * n / N));\n        sum = hipCadd(sum, hipCmul(z, w));\n    }\n    r[tid] = hipCmul(sum, make_hipDoubleComplex(1.0 / N, 0.0));\n    i[tid] = hipCmul(sum, make_hipDoubleComplex(-1.0 / N, 0.0));\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // The bit reversal operation\n    size_t j = bit_reversal(tid, log2(N));\n\n    // The butterfly operation\n    hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n    for (size_t k = 0; k < N; k += 2 * j) {\n        hipDoubleComplex y = x[k + j];\n        hipDoubleComplex z = x[k + j + 1];\n        sum = make_hipDoubleComplex(hipCos(PI * k / N), hipSin(PI * k / N)) * make_hipDoubleComplex(z.x, -z.y);\n        r[tid] += y + sum;\n        i[tid] += y - sum;\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // Compute the FFT of x\n    hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t n = 0; n < N; n++) {\n        hipDoubleComplex z = make_hipDoubleComplex(0.0, 0.0);\n        if (n < tid) {\n            z = x[n];\n        } else if (n > tid) {\n            z = conj(x[N - n]);\n        }\n        hipDoubleComplex w = hipExp(make_hipDoubleComplex(0.0, -2.0 * M_PI * n * tid / N));\n        sum = hipCadd(sum, hipCmul(z, w));\n    }\n\n    // Store the results\n    r[tid] = hipCreal(sum);\n    i[tid] = hipCimag(sum);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // Compute the FFT of x\n    hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n    for (size_t n = 0; n < N; n++) {\n        hipDoubleComplex z = x[n];\n        hipDoubleComplex w = make_hipDoubleComplex(cos(2 * M_PI * tid * n / N),\n                                                   -sin(2 * M_PI * tid * n / N));\n        sum = hipCadd(sum, hipCmul(z, w));\n    }\n\n    // Store the results\n    r[tid] = hipCreal(sum);\n    i[tid] = hipCimag(sum);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n    hipDoubleComplex z = x[tid];\n    r[tid] = hipCcos(z);\n    i[tid] = hipCsin(z);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  double theta = -2 * M_PI * idx / N;\n  hipDoubleComplex w = make_hipDoubleComplex(cos(theta), sin(theta));\n  hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n  for (size_t k = 0; k < N; k++) {\n    hipDoubleComplex z = x[k];\n    hipDoubleComplex term = hipCmul(w, z);\n    sum = hipCadd(sum, term);\n    w = hipCmul(w, w);\n  }\n  r[idx] = hipCreal(sum);\n  i[idx] = hipCimag(sum);\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    size_t step = 1;\n    size_t halfN = N / 2;\n    size_t i1, i2;\n    hipDoubleComplex z, w, t;\n\n    // Perform the bit reversal\n    i1 = bit_reversal(tid, N);\n    i2 = bit_reversal(tid + halfN, N);\n\n    // Read the complex data\n    z = x[i1];\n    w = x[i2];\n\n    // Perform the butterfly operation\n    t = hipCmul(w, hipMakeDouble2(cos(2 * M_PI * tid / N), -sin(2 * M_PI * tid / N)));\n    r[tid] = hipCadd(z, t);\n    i[tid] = hipCsub(z, t);\n\n    // Perform the reduction\n    while (step < N) {\n        // Wait for the step to complete\n        __syncthreads();\n\n        // Perform the butterfly operation\n        i1 = tid * 2 * step;\n        i2 = i1 + step;\n        z = r[i1];\n        w = r[i2];\n        t = hipCmul(w, hipMakeDouble2(cos(M_PI * tid / N), -sin(M_PI * tid / N)));\n        r[i1] = hipCadd(z, t);\n        r[i2] = hipCsub(z, t);\n\n        // Increment the step\n        step *= 2;\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // Use a lookup table for the bit reversal\n    size_t j = bit_reversal_lookup[tid];\n\n    // Use a lookup table for the twiddle factors\n    hipDoubleComplex t = x[j];\n\n    // Do the butterfly\n    r[tid] = hipCadd(r[tid], hipCmul(t, twiddle_factors[tid]));\n    i[tid] = hipCsub(i[tid], hipCmul(t, twiddle_factors[N/2 + tid]));\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    hipDoubleComplex c = x[i];\n    r[i] = hipCmul(c, hipConj(c));\n    i[i] = hipCmul(c, hipDoubleComplex(-1.0, 1.0));\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n        for (size_t n = 0; n < N; n++) {\n            hipDoubleComplex z = x[n];\n            hipDoubleComplex w = make_hipDoubleComplex(cos(2 * M_PI * n * tid / N),\n                                                       -sin(2 * M_PI * n * tid / N));\n            sum = hipCadd(sum, hipCmul(z, w));\n        }\n        r[tid] = hipCreal(sum);\n        i[tid] = hipCimag(sum);\n    }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n\n    // Create a bit reversed index\n    size_t bit_reversed_index = 0;\n    for (size_t j = 0; j < log2(N); j++) {\n        bit_reversed_index |= (tid & (1 << j)) << (log2(N) - 1 - j);\n    }\n\n    // Load input from global memory\n    hipDoubleComplex input = x[tid];\n\n    // Calculate output\n    hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n    for (size_t n = 0; n < N; n++) {\n        hipDoubleComplex e = make_hipDoubleComplex(cos(2 * M_PI * n * bit_reversed_index / N),\n                                                   -sin(2 * M_PI * n * bit_reversed_index / N));\n        sum = hipCadd(sum, hipCmul(e, x[n]));\n    }\n\n    // Store output in global memory\n    r[tid] = hipCreal(sum);\n    i[tid] = hipCimag(sum);\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n    double theta = -2 * M_PI * tid / N;\n    hipDoubleComplex w = make_hipDoubleComplex(cos(theta), sin(theta));\n    hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t k = 0; k < N; k++) {\n        hipDoubleComplex z = x[k];\n        hipDoubleComplex term = hipCmul(w, z);\n        sum = hipCadd(sum, term);\n        w = hipCmul(w, w);\n    }\n    r[tid] = hipCmul(sum, make_hipDoubleComplex(1.0 / N, 0.0));\n    i[tid] = hipCmul(sum, make_hipDoubleComplex(-1.0 / N, 0.0));\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // Bit reversal\n    size_t j = reverseBits(tid, log2(N));\n\n    // Twiddle factors\n    double theta = -2.0 * M_PI * j / N;\n    hipDoubleComplex w = make_hipDoubleComplex(cos(theta), sin(theta));\n\n    // FFT\n    hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t k = 0; k < N; k++) {\n        hipDoubleComplex z = x[j];\n        hipDoubleComplex wk = hipCmul(w, z);\n        sum = hipCadd(sum, wk);\n        w = hipCmul(w, w);\n        j += (N >> 1);\n        if (j >= N) j -= N;\n    }\n    r[tid] = hipCreal(sum);\n    i[tid] = hipCimag(sum);\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // The bit reversal step\n    size_t j = bitReverse(tid, log2(N));\n\n    // The butterfly step\n    hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n    for (size_t k = 0; k < N; k += 1 << (log2(N) + 1)) {\n        hipDoubleComplex z = x[j + k];\n        hipDoubleComplex w = x[j + k + (1 << log2(N))];\n        sum = hipCadd(sum, hipCmul(z, hipConj(w)));\n    }\n    r[tid] = hipCadd(r[tid], hipCreal(sum));\n    i[tid] = hipCadd(i[tid], hipCimag(sum));\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid >= N) return;\n\n  // TODO: implement FFT\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t stride = hipBlockDim_x * hipGridDim_x;\n    hipDoubleComplex c = make_hipDoubleComplex(0.0, 0.0);\n    hipDoubleComplex d = make_hipDoubleComplex(0.0, 0.0);\n    for (size_t n = tid; n < N; n += stride) {\n        c = x[n];\n        d = make_hipDoubleComplex(cos(2 * M_PI * n / N), -sin(2 * M_PI * n / N));\n        r[n] = hipCmul(c, d);\n        i[n] = hipCmul(c, hipConj(d));\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    // Bit reversal\n    size_t j = 0;\n    for (size_t k = N / 2; k > 0; k /= 2) {\n        j ^= k & tid;\n    }\n\n    // FFT\n    size_t k;\n    hipDoubleComplex z, w, t;\n    for (k = 2; k <= N; k *= 2) {\n        size_t m = k / 2;\n        w = hipExp(hipComplex(0, -2 * M_PI * (hipDoubleComplex)tid / (hipDoubleComplex)k));\n        for (size_t j = 0; j < m; j++) {\n            size_t i1 = j * k + tid;\n            size_t i2 = i1 + m;\n            z = x[i2];\n            t = w * z;\n            r[i2] = r[i1] - t;\n            i[i2] = i[i1] - hipConj(t);\n            r[i1] += t;\n            i[i1] += hipConj(t);\n        }\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t bid = hipBlockIdx_x;\n    size_t idx = tid + bid * hipBlockDim_x;\n\n    // Each thread computes one FFT\n    if (idx < N) {\n        hipDoubleComplex z = x[idx];\n        hipDoubleComplex sum = make_hipDoubleComplex(0, 0);\n        for (size_t k = 0; k < N; k++) {\n            hipDoubleComplex w = make_hipDoubleComplex(cos(-2 * M_PI * k * idx / N),\n                                                       sin(-2 * M_PI * k * idx / N));\n            sum = hipCadd(sum, hipCmul(z, w));\n        }\n        r[idx] = hipCreal(sum);\n        i[idx] = hipCimag(sum);\n    }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement me\n}",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use std::complex<double> to represent complex numbers\n    // Hint: Use std::vector<std::complex<double>> to represent arrays of complex numbers\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n}",
            "// TODO: your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send each rank a subset of the input vector\n  // Hint: Use MPI_Reduce to combine the results from each rank\n  // Hint: Use MPI_Gather to collect the results from each rank\n\n  // TODO: Implement FFT in parallel\n  // Hint: Use MPI_Scatter to send",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement FFT\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume that the length of x is divisible by the number of ranks.\n    // You may also assume that the length of x is divisible by 2.\n\n    // TODO: Implement the FFT algorithm here.\n    // You may assume that the length of x is a power of 2.\n    // You may also assume",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *r, cuDoubleComplex *i, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n    cuDoubleComplex z = x[tid];\n    r[tid] = cuCadd(z, cuConj(z));\n    i[tid] = make_cuDoubleComplex(0.0, 0.0);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex x_k = x[tid];\n    cuDoubleComplex r_k = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex i_k = make_cuDoubleComplex(0.0, 0.0);\n\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * PI * k * tid / N), sin(2 * PI * k * tid / N));\n        cuDoubleComplex x_k_w = cuCmul(x_k, w);\n        r_k = cuCadd(r_k, cuCmul(x_k_w, make_cuDoubleComplex(cos(PI * k * tid / N), 0.0)));\n        i_k = cuCadd(i_k, cuCmul(x_k_w, make_cuDoubleComplex(sin(PI * k * tid / N), 0.0)));\n    }\n\n    r[tid] = r_k;\n    i[tid] = i_k;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex z = x[tid];\n        r[tid] = cuCadd(z, cuConj(z));\n        i[tid] = make_cuDoubleComplex(0.0, 0.0);\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t n = N / 2;\n    size_t m = n / 2;\n    size_t k = tid;\n\n    cuDoubleComplex xk, xkm, xkm2, xkp, xkp2, xkp3, xkp4;\n    cuDoubleComplex wk, wkm, wkm2, wkp, wkp2, wkp3, wkp4;\n\n    while (k < n) {\n        xk = x[k];\n        xkm = x[k + m];\n        xkm2 = x[k + m + m];\n        xkp = x[k + n];\n        xkp2 = x[k + n + n];\n        xkp3 = x[k + n + n + n];\n        xkp4 = x[k + n + n + n + n];\n\n        wk = make_cuDoubleComplex(cos(2 * PI * k / N), -sin(2 * PI * k / N));\n        wkm = make_cuDoubleComplex(cos(2 * PI * (k - m) / N), -sin(2 * PI * (k - m) / N));\n        wkm2 = make_cuDoubleComplex(cos(2 * PI * (k - m - m) / N), -sin(2 * PI * (k - m - m) / N));\n        wkp = make_cuDoubleComplex(cos(2 * PI * (k + n) / N), -sin(2 * PI * (k + n) / N));\n        wkp2 = make_cuDoubleComplex(cos(2 * PI * (k + n + n) / N), -sin(2 * PI * (k + n + n) / N));\n        wkp3 = make_cuDoubleComplex(cos(2 * PI * (k + n + n + n) / N), -sin(2 * PI * (k + n + n + n) / N));\n        wkp4 = make_cuDoubleComplex(cos(2 * PI * (k + n + n + n + n) / N), -sin(2 * PI * (k + n + n + n + n) / N));\n\n        r[k] = xk + xkp + xkm + xkm2 + xkp2 + xkp3 + xkp4;\n        i[k] = (xk - xkp) + (xkm - xkm2) + (xkp2 - xkp3) + (xkp3 - xkp4) + (xkp4 - xkp2) + (xkm2 - xkm) + (xkp - xk);\n\n        r[k + m] = xk + xkp - xkm + xkm2 - xkp2 + xkp3 - xkp4;\n        i[k + m] = (xk - xkp) - (xkm - xkm2) - (xkp2 - xkp3) + (xkp3 - xkp4) - (xkp4 - xkp2) + (xkm2 - xkm) - (xkp - xk);\n\n        r[k + m + m] = xk - xkp + xkm - xkm2 + xkp2 - xkp3 + xkp4 - xkp2;\n        i[k + m + m] = (xk - xkp) - (xkm - xkm2) - (xkp2 - xkp3) - (xkp3 - xkp4) + (xkp4 - xkp2) - (xkm2 - xkm) + (xkp - xk);\n\n        r[k + n] = xk - xkp - xkm + xkm2 + xkp2 - xkp3 - xkp4 + xkp2;\n        i[k + n] = (xk - xkp) + (xkm - xkm2) - (xkp2 - xkp3) - (xkp3 - xkp4) - (xkp4 - xkp2) + (",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex x_ = x[tid];\n    cuDoubleComplex r_ = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex i_ = make_cuDoubleComplex(0, 0);\n\n    for (size_t n = 0; n < N; n++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * n * tid / N), -sin(2 * M_PI * n * tid / N));\n        cuDoubleComplex y = x_ * w;\n        r_ = cuCadd(r_, cuCmul(y, make_cuDoubleComplex(cos(M_PI * n / N), 0)));\n        i_ = cuCadd(i_, cuCmul(y, make_cuDoubleComplex(sin(M_PI * n / N), 0)));\n    }\n\n    r[tid] = r_;\n    i[tid] = i_;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (size_t n = tid; n < N; n += stride) {\n        cuDoubleComplex z = x[n];\n        r[n] = cuCadd(cuCmul(z, cuCexp(make_cuDoubleComplex(0, -2 * M_PI * n / N))),\n                      cuCmul(r[n], cuCexp(make_cuDoubleComplex(0, 2 * M_PI * n / N))));\n        i[n] = cuCsub(cuCmul(z, cuCexp(make_cuDoubleComplex(0, 2 * M_PI * n / N))),\n                      cuCmul(i[n], cuCexp(make_cuDoubleComplex(0, -2 * M_PI * n / N))));\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex x_k = x[tid];\n    cuDoubleComplex r_k = make_cuDoubleComplex(0.0, 0.0);\n    cuDoubleComplex i_k = make_cuDoubleComplex(0.0, 0.0);\n\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * PI * k * tid / N), -sin(2 * PI * k * tid / N));\n        cuDoubleComplex x_k_w = cuCmul(x_k, w);\n        r_k = cuCadd(r_k, cuCmul(x_k_w, make_cuDoubleComplex(cos(PI * k * tid / N), 0.0)));\n        i_k = cuCadd(i_k, cuCmul(x_k_w, make_cuDoubleComplex(sin(PI * k * tid / N), 0.0)));\n    }\n\n    r[tid] = r_k;\n    i[tid] = i_k;\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n    size_t gid = bid * blockDim.x + tid;\n\n    // Check if we are in bounds\n    if (gid >= N) return;\n\n    // Compute the FFT\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex z = make_cuDoubleComplex(0.0, 0.0);\n        if (k & gid) {\n            z = make_cuDoubleComplex(0.0, -2.0 * M_PI * gid * k / N);\n        } else {\n            z = make_cuDoubleComplex(0.0, 0.0);\n        }\n        cuDoubleComplex w = make_cuDoubleComplex(cos(z.y), sin(z.y));\n        cuDoubleComplex y = x[k] * w;\n        sum = cuCadd(sum, y);\n    }\n\n    // Store the result\n    r[gid] = cuCreal(sum);\n    i[gid] = cuCimag(sum);\n}",
            "size_t tid = threadIdx.x;\n    size_t idx = tid;\n    cuDoubleComplex x_k, x_k1, x_k2, x_k3, x_k4, x_k5, x_k6, x_k7;\n    cuDoubleComplex w_k, w_k1, w_k2, w_k3, w_k4, w_k5, w_k6, w_k7;\n    cuDoubleComplex t1, t2, t3, t4, t5, t6, t7, t8;\n    cuDoubleComplex u1, u2, u3, u4, u5, u6, u7, u8;\n    cuDoubleComplex v1, v2, v3, v4, v5, v6, v7, v8;\n    cuDoubleComplex wd, we, wn, wne, wnw, wse, wsw, wnwse, wnwne, wswne, wswse;\n    cuDoubleComplex w_k_conj, w_k1_conj, w_k2_conj, w_k3_conj, w_k4_conj, w_k5_conj, w_k6_conj, w_k7_conj;\n    cuDoubleComplex x_k_conj, x_k1_conj, x_k2_conj, x_k3_conj, x_k4_conj, x_k5_conj, x_k6_conj, x_k7_conj;\n    cuDoubleComplex x_k_conj_mul_w_k, x_k1_conj_mul_w_k1, x_k2_conj_mul_w_k2, x_k3_conj_mul_w_k3, x_k4_conj_mul_w_k4, x_k5_conj_mul_w_k5, x_k6_conj_mul_w_k6, x_k7_conj_mul_w_k7;\n    cuDoubleComplex x_k_mul_w_k, x_k1_mul_w_k1, x_k2_mul_w_k2, x_k3_mul_w_k3, x_k4_mul_w_k4, x_k5_mul_w_k5, x_k6_mul_w_k6, x_k7_mul_w_k7;\n    cuDoubleComplex x_k_mul_w_k_conj, x_k1_mul_w_k1_conj, x_k2_mul_w_k2_conj, x_k3_mul_w_k3_conj, x_k4_mul_w_k4_conj, x_k5_mul_w_k5_conj, x_k6_mul_w_k6_conj, x_k7_mul_w_k7_conj;\n    cuDoubleComplex x_k_mul_w_k_conj_add_x_k1_mul_w_k1, x_k2_mul_w_k2_conj_add_x_k3_mul_w_k3, x_k4_mul_w_k4_conj_add_x_k5_mul_w_k5, x_k6_mul_w_k6_conj_add_x_k7_mul_w_k7;\n    cuDoubleComplex x_k_mul_w_k_conj_add_x_k1_mul_w_k1_add_x_k2_mul_w_k2_conj_add_x_k3_mul_w_k3, x_k4_mul_w_k4_conj_add_x_k5_mul_w_k5_add_x_k6_mul_w_k6_conj_add_x_k7_mul_w_k7;\n    cuDoubleComplex x_k_mul_w_k_conj_add_x_k1_mul_w_k1_add_x_k2_mul_w_k2_",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex x_ = x[tid];\n    cuDoubleComplex r_ = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex i_ = make_cuDoubleComplex(0, 0);\n\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * k * tid / N), sin(2 * M_PI * k * tid / N));\n        cuDoubleComplex y = x_ * w;\n        r_ += cuCreal(y);\n        i_ += cuCimag(y);\n    }\n\n    r[tid] = make_cuDoubleComplex(r_.x / N, r_.y / N);\n    i[tid] = make_cuDoubleComplex(i_.x / N, i_.y / N);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    cuDoubleComplex x_i = x[idx];\n    cuDoubleComplex r_i = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex i_i = make_cuDoubleComplex(0, 0);\n\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(-2 * M_PI * idx * k / N), sin(-2 * M_PI * idx * k / N));\n        cuDoubleComplex x_k = x[k];\n        r_i = cuCadd(r_i, cuCmul(x_k, cuConj(w)));\n        i_i = cuCadd(i_i, cuCmul(x_k, w));\n    }\n\n    r[idx] = r_i;\n    i[idx] = i_i;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    cuDoubleComplex c = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex d = make_cuDoubleComplex(0, 0);\n\n    for (size_t k = 0; k < N; k += stride) {\n        cuDoubleComplex z = x[k + tid];\n        cuDoubleComplex w = cexpf(-make_cuDoubleComplex(0, 2 * M_PI * k / N) * z);\n        c += w;\n        d += w * z;\n    }\n\n    r[tid] = c;\n    i[tid] = d;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    cuDoubleComplex c = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex d = make_cuDoubleComplex(0, 0);\n\n    for (size_t k = 0; k < N; k += stride) {\n        cuDoubleComplex z = x[k + tid];\n        c = cuCadd(c, cuCmul(z, make_cuDoubleComplex(cos(2 * M_PI * k * tid / N), -sin(2 * M_PI * k * tid / N))));\n        d = cuCadd(d, cuCmul(z, make_cuDoubleComplex(sin(2 * M_PI * k * tid / N), cos(2 * M_PI * k * tid / N))));\n    }\n\n    r[tid] = cuCreal(c);\n    i[tid] = cuCimag(d);\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t nthreads = blockDim.x;\n  size_t nblocks = gridDim.x;\n\n  // Compute the number of elements per thread\n  size_t n = N / nthreads;\n\n  // Compute the offset for this thread\n  size_t offset = bid * nthreads * n + tid * n;\n\n  // Allocate memory for the bit reversed indices\n  size_t *indices = (size_t *)malloc(nthreads * sizeof(size_t));\n\n  // Compute the bit reversed indices\n  for (size_t i = 0; i < nthreads; i++) {\n    indices[i] = bit_reverse(i, log2(nthreads));\n  }\n\n  // Allocate memory for the twiddle factors\n  cuDoubleComplex *twiddles = (cuDoubleComplex *)malloc(nthreads * sizeof(cuDoubleComplex));\n\n  // Compute the twiddle factors\n  for (size_t i = 0; i < nthreads; i++) {\n    twiddles[i] = make_cuDoubleComplex(cos(2 * M_PI * i / nthreads), -sin(2 * M_PI * i / nthreads));\n  }\n\n  // Allocate memory for the temporary results\n  cuDoubleComplex *temp = (cuDoubleComplex *)malloc(nthreads * sizeof(cuDoubleComplex));\n\n  // Compute the FFT\n  for (size_t k = 0; k < log2(nthreads); k++) {\n    // Compute the twiddle factor for this iteration\n    cuDoubleComplex w = make_cuDoubleComplex(1, 0);\n    for (size_t j = 0; j < k; j++) {\n      w = cuCmul(w, twiddles[nthreads / (1 << (j + 1))]);\n    }\n\n    // Compute the FFT for this iteration\n    for (size_t j = 0; j < nthreads; j++) {\n      size_t even = 2 * j;\n      size_t odd = 2 * j + 1;\n      size_t rev_even = indices[j];\n      size_t rev_odd = indices[j] + (1 << k);\n      if (rev_odd < nthreads) {\n        temp[j] = cuCadd(cuCmul(x[offset + rev_even], w), cuCmul(x[offset + rev_odd], cuConj(w)));\n        temp[j + nthreads] = cuCsub(cuCmul(x[offset + rev_even], w), cuCmul(x[offset + rev_odd], cuConj(w)));\n      } else {\n        temp[j] = cuCmul(x[offset + rev_even], w);\n        temp[j + nthreads] = make_cuDoubleComplex(0, 0);\n      }\n    }\n\n    // Copy the results back to the input\n    for (size_t j = 0; j < nthreads; j++) {\n      x[offset + j] = temp[j];\n    }\n\n    // Update the twiddle factor\n    w = cuCmul(w, twiddles[nthreads / (1 << (k + 1))]);\n  }\n\n  // Copy the results to the output\n  for (size_t i = 0; i < n; i++) {\n    r[offset + i] = cuCreal(x[offset + i]);\n    i[offset + i] = cuCimag(x[offset + i]);\n  }\n\n  // Free the memory\n  free(indices);\n  free(twiddles);\n  free(temp);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex x_k = x[tid];\n    cuDoubleComplex r_k = make_cuDoubleComplex(0, 0);\n    cuDoubleComplex i_k = make_cuDoubleComplex(0, 0);\n\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex w = make_cuDoubleComplex(cos(2 * PI * tid * k / N), -sin(2 * PI * tid * k / N));\n        cuDoubleComplex x_k_w = cuCmul(x_k, w);\n        r_k = cuCadd(r_k, cuCmul(x_k_w, make_cuDoubleComplex(cos(PI * k / N), 0)));\n        i_k = cuCadd(i_k, cuCmul(x_k_w, make_cuDoubleComplex(sin(PI * k / N), 0)));\n    }\n\n    r[tid] = r_k;\n    i[tid] = i_k;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex x_ = x[tid];\n        r[tid] = make_cuDoubleComplex(cuCreal(x_), 0.0);\n        i[tid] = make_cuDoubleComplex(0.0, cuCimag(x_));\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t idx = tid;\n\n    // The first half of the array is the real part of the complex number\n    cuDoubleComplex x_r = x[idx];\n\n    // The second half of the array is the imaginary part of the complex number\n    cuDoubleComplex x_i = x[idx + N / 2];\n\n    // Compute the real part of the complex number\n    r[idx] = x_r + x_i;\n\n    // Compute the imaginary part of the complex number\n    i[idx] = x_r - x_i;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex z = x[tid];\n    cuDoubleComplex w = make_cuDoubleComplex(0, 0);\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex z2 = make_cuDoubleComplex(z.x * cos(2 * M_PI * k * tid / N) - z.y * sin(2 * M_PI * k * tid / N),\n                                                  z.x * sin(2 * M_PI * k * tid / N) + z.y * cos(2 * M_PI * k * tid / N));\n        w = cuCadd(w, z2);\n    }\n    r[tid] = make_cuDoubleComplex(w.x / N, 0);\n    i[tid] = make_cuDoubleComplex(w.y / N, 0);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // Compute the FFT of x\n    cuDoubleComplex sum = make_cuDoubleComplex(0, 0);\n    for (size_t k = 0; k < N; k++) {\n        cuDoubleComplex z = make_cuDoubleComplex(0, -2 * M_PI * k * tid / N);\n        cuDoubleComplex w = cuCexp(z);\n        cuDoubleComplex xk = x[k];\n        sum = cuCadd(sum, cuCmul(xk, w));\n    }\n\n    // Store the results in r and i\n    r[tid] = cuCreal(sum);\n    i[tid] = cuCimag(sum);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    cuDoubleComplex z = x[i];\n    r[i] = make_cuDoubleComplex(cuCreal(z), 0.0);\n    i[i] = make_cuDoubleComplex(0.0, 0.0);\n  }\n}"
        ]
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_split_fft",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement me\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n\n    // Send the data to rank 0\n    MPI_Send(&r[0], r.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Send(&i[0], i.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\n    // Receive the data from rank 0\n    if (rank == 0) {\n        std::vector<double> r_recv(size);\n        std::vector<double> i_recv(size);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&r_recv[0], r.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&i_recv[0], i.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < r.size(); j++) {\n                r[j] += r_recv[j];\n                i[j] += i_recv[j];\n            }\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_proc = n / size;\n  int n_rem = n % size;\n\n  std::vector<std::complex<double>> x_proc(n_per_proc);\n  std::vector<std::complex<double>> x_proc_out(n_per_proc);\n  std::vector<double> r_proc(n_per_proc);\n  std::vector<double> i_proc(n_per_proc);\n\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      if (i < n_rem) {\n        MPI_Send(&x[i * (n_per_proc + 1)], n_per_proc + 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      } else {\n        MPI_Send(&x[i * n_per_proc + n_rem], n_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n    }\n  }\n\n  if (rank < n_rem) {\n    MPI_Recv(&x_proc[0], n_per_proc + 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  } else {\n    MPI_Recv(&x_proc[0], n_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_proc; i++) {\n    x_proc_out[i] = x_proc[i];\n  }\n\n  for (int i = 1; i < n_per_proc; i++) {\n    x_proc_out[i] *= std::exp(-2 * M_PI * i * rank / n);\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_proc; i++) {\n    r_proc[i] = x_proc_out[i].real();\n    i_proc[i] = x_proc_out[i].imag();\n  }\n\n  MPI_Gather(&r_proc[0], n_per_proc, MPI_DOUBLE, &r[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Gather(&i_proc[0], n_per_proc, MPI_DOUBLE, &i[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n\n  // compute local fft\n  std::vector<std::complex<double>> x_local(n_per_rank);\n  std::vector<std::complex<double>> y_local(n_per_rank);\n  std::vector<std::complex<double>> z_local(n_per_rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n  }\n\n  MPI_Scatter(x.data(), n_per_rank, MPI_DOUBLE, x_local.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_rank; i++) {\n    y_local[i] = x_local[i] * std::exp(-2.0 * M_PI * i * I / n);\n  }\n\n  MPI_Gather(y_local.data(), n_per_rank, MPI_DOUBLE, z_local.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      r[i] = std::real(z_local[i]);\n      i[i] = std::imag(z_local[i]);\n    }\n  }\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    std::vector<std::complex<double>> x_local(n_per_rank + n_extra);\n    std::vector<std::complex<double>> x_local_fft(n_per_rank + n_extra);\n\n    if (rank == 0) {\n        std::copy(x.begin(), x.end(), x_local.begin());\n    }\n\n    MPI_Scatter(x_local.data(), n_per_rank + n_extra, MPI_DOUBLE, x_local_fft.data(), n_per_rank + n_extra, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_local_fft_out(n_per_rank + n_extra);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank + n_extra; i++) {\n        x_local_fft_out[i] = std::complex<double>(0.0, 0.0);\n        for (int j = 0; j < n_per_rank + n_extra; j++) {\n            x_local_fft_out[i] += x_local_fft[j] * std::exp(-2.0 * M_PI * 1.0i * j * i / n);\n        }\n    }\n\n    MPI_Gather(x_local_fft_out.data(), n_per_rank + n_extra, MPI_DOUBLE, x_local.data(), n_per_rank + n_extra, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        r = std::vector<double>(n);\n        i = std::vector<double>(n);\n        for (int i = 0; i < n; i++) {\n            r[i] = x_local[i].real();\n            i[i] = x_local[i].imag();\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\n    \"axpy\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      z(i) = alpha*x(i) + y(i);\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that will be executed in parallel\n  Kokkos::parallel_for(\n    \"axpy\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, x.extent(0), 0, y.extent(0)),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      z(i, j) = alpha*x(i, j) + y(i, j);\n    }\n  );\n\n  // Force the lambda function to finish before returning\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha*x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function to compute the result\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    z(i) = alpha*x(i) + y(i);\n  });\n  // Force the parallel_for to finish before returning\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes a single integer argument that is the index\n  // of the element that should be computed.\n  Kokkos::parallel_for(\n    \"axpy\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      z(i) = alpha * x(i) + y(i);\n    }\n  );\n\n  // Force the parallel region to finish.\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a parallel_for lambda function that computes the result\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha*x(i) + y(i);\n  });\n\n  // Force the parallel_for to complete before returning\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that will be executed in parallel.\n  // The lambda function takes as input the index of the element to be computed.\n  // The lambda function computes the value of the element at that index.\n  Kokkos::parallel_for(\n    \"axpy\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      z(i) = alpha*x(i) + y(i);\n    }\n  );\n\n  // Force the lambda function to finish before returning.\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a Kokkos parallel_for lambda function to compute the result\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha*x(i)+y(i);\n  });\n\n  // Force the Kokkos parallel_for to finish\n  Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int n = x.size();\n    int i;\n    #pragma omp parallel for private(i)\n    for (i=0; i<n; i++) {\n        z[i] = alpha*x[i] + y[i];\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        z[i] = alpha*x[i] + y[i];\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "// TODO: implement this function\n    // You can use the following variables:\n    //   - int n = x.size()\n    //   - double alpha\n    //   - std::vector<double> const& x\n    //   - std::vector<double> const& y\n    //   - std::vector<double> &z\n\n    // You can use the following OpenMP directives:\n    //   - #pragma omp parallel for\n    //   - #pragma omp parallel for schedule(static)\n    //   - #pragma omp parallel for schedule(dynamic)\n    //   - #pragma omp parallel for schedule(guided)\n    //   - #pragma omp parallel for schedule(runtime)\n    //   - #pragma omp parallel for schedule(auto)\n    //   - #pragma omp parallel for schedule(static, chunk_size)\n    //   - #pragma omp parallel for schedule(dynamic, chunk_size)\n    //   - #pragma omp parallel for schedule(guided, chunk_size)\n    //   - #pragma omp parallel for schedule(runtime, chunk_size)\n    //   - #pragma omp parallel for schedule(auto, chunk_size)\n    //   - #pragma omp parallel for collapse(2)\n    //   - #pragma omp parallel for collapse(2) schedule(static)\n    //   - #pragma omp parallel for collapse(2) schedule(dynamic)\n    //   - #pragma omp parallel for collapse(2) schedule(guided)\n    //   - #pragma omp parallel for collapse(2) schedule(runtime)\n    //   - #pragma omp parallel for collapse(2) schedule(auto)\n    //   - #pragma omp parallel for collapse(2) schedule(static, chunk_size)\n    //   - #pragma omp parallel for collapse(2) schedule(dynamic, chunk_size)\n    //   - #pragma omp parallel for collapse(2) schedule(guided, chunk_size)\n    //   - #pragma omp parallel for collapse(2) schedule(runtime, chunk_size)\n    //   - #pragma omp parallel for collapse(2) schedule(auto, chunk_size)\n    //   - #pragma omp parallel for ordered\n    //   - #pragma omp parallel for ordered schedule(static)\n    //   - #pragma omp parallel for ordered schedule(dynamic)\n    //   - #pragma omp parallel for ordered schedule(guided)\n    //   - #pragma omp parallel for ordered schedule(runtime)\n    //   - #pragma omp parallel for ordered schedule(auto)\n    //   - #pragma omp parallel for ordered schedule(static, chunk_size)\n    //   - #pragma omp parallel for ordered schedule(dynamic, chunk_size)\n    //   - #pragma omp parallel for ordered schedule(guided, chunk_size)\n    //   - #pragma omp parallel for ordered schedule(runtime, chunk_size)\n    //   - #pragma omp parallel for ordered schedule(auto, chunk_size)\n    //   - #pragma omp parallel for ordered collapse(2)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(static)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(dynamic)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(guided)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(runtime)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(auto)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(static, chunk_size)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(dynamic, chunk_size)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(guided, chunk_size)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(runtime, chunk_size)\n    //   - #pragma omp parallel for ordered collapse(2) schedule(auto, chunk_size)\n    //   - #pragma omp parallel for private(i)\n    //   - #pragma omp parallel for firstprivate(i)\n    //   - #pragma omp parallel for lastprivate(i)\n    //   - #pragma omp parallel for reduction(+:sum)\n    //   - #pragma omp parallel for reduction(+:sum) private(i)\n    //",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "// TODO: Implement this function\n\n  // The number of threads to use\n  int num_threads = 4;\n\n  // Set the number of threads to use\n  omp_set_num_threads(num_threads);\n\n  // The number of elements in the vectors\n  int n = x.size();\n\n  // The number of elements per thread\n  int chunk_size = n / num_threads;\n\n  // The number of elements in the last chunk\n  int remainder = n % num_threads;\n\n  // The number of iterations to perform\n  int num_iterations = num_threads;\n\n  // The index of the first element of the current chunk\n  int first_index;\n\n  // The index of the last element of the current chunk\n  int last_index;\n\n  // The number of elements in the current chunk\n  int chunk_size_local;\n\n  // The number of iterations to perform\n  int num_iterations_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local_local_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local_local_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local_local_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local_local_local_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local_local_local_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local_local_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local_local_local_local_local;\n\n  // The index of the first element of the current chunk\n  int first_index_local_local_local_local_local_local_local;\n\n  // The index of the last element of the current chunk\n  int last_index_local_local_local_local_local_local_local;\n\n  // The number of elements in the current chunk\n  int chunk_size_local_local_local_local_local_local_local_local;\n\n  // The number of iterations to perform\n  int num_iterations_local_local_local_local_",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha*x[i] + y[i];\n    }\n}",
            "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "// TODO: implement this function\n\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "serial",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "if (x.size()!= y.size()) {\n        throw std::invalid_argument(\"x and y must have the same size\");\n    }\n    if (x.size()!= z.size()) {\n        throw std::invalid_argument(\"x and z must have the same size\");\n    }\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "if (x.size()!= y.size()) {\n        throw std::invalid_argument(\"Vectors must be of equal size\");\n    }\n    if (x.size()!= z.size()) {\n        throw std::invalid_argument(\"Vectors must be of equal size\");\n    }\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "if (x.size()!= y.size()) {\n        throw std::invalid_argument(\"Vectors must have the same size\");\n    }\n    if (x.size()!= z.size()) {\n        throw std::invalid_argument(\"Vectors must have the same size\");\n    }\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n  assert(x.size() == z.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
            "assert(x.size() == y.size());\n  assert(x.size() == z.size());\n  for (int i = 0; i < x.size(); i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "if (x.size()!= y.size()) {\n    throw std::runtime_error(\"axpy: x and y must have the same size\");\n  }\n  if (x.size()!= z.size()) {\n    throw std::runtime_error(\"axpy: x and z must have the same size\");\n  }\n  for (size_t i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n  assert(x.size() == z.size());\n  for (int i = 0; i < x.size(); i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "if (x.size()!= y.size()) {\n    throw std::invalid_argument(\"Vectors x and y must have the same size.\");\n  }\n  if (x.size()!= z.size()) {\n    throw std::invalid_argument(\"Vectors x and z must have the same size.\");\n  }\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n  assert(x.size() == z.size());\n  for (int i = 0; i < x.size(); i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "assert(x.size() == y.size());\n  assert(x.size() == z.size());\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "assert(x.size() == y.size());\n    assert(x.size() == z.size());\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha*x[i] + y[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "hip",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  if (rank == 0) {\n    z.resize(n);\n  }\n\n  if (rank < remainder) {\n    start += rank;\n    end += rank + 1;\n  } else {\n    start += remainder;\n    end += remainder;\n  }\n\n  for (int i = start; i < end; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, &z[0], n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = x.size();\n    int chunk_size = n / size;\n    int remainder = n % size;\n    int start = rank * chunk_size;\n    int end = start + chunk_size;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n    MPI_Reduce(MPI_IN_PLACE, z.data(), n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_proc = n/size;\n  int n_rem = n%size;\n  int n_start = rank*n_per_proc;\n  int n_end = n_start + n_per_proc;\n  if (rank == size-1) {\n    n_end += n_rem;\n  }\n  std::vector<double> x_proc(n_per_proc);\n  std::vector<double> y_proc(n_per_proc);\n  std::vector<double> z_proc(n_per_proc);\n  for (int i = 0; i < n_per_proc; i++) {\n    x_proc[i] = x[n_start + i];\n    y_proc[i] = y[n_start + i];\n  }\n  for (int i = 0; i < n_per_proc; i++) {\n    z_proc[i] = alpha*x_proc[i] + y_proc[i];\n  }\n  std::vector<double> z_all(n);\n  MPI_Gather(&z_proc[0], n_per_proc, MPI_DOUBLE, &z_all[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < n_rem; i++) {\n      z_all[n_per_proc*size + i] = z_proc[n_per_proc + i];\n    }\n    z = z_all;\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    if (rank == 0) {\n        z = x;\n        for (int i = 0; i < y.size(); i++) {\n            z[i] += alpha * y[i];\n        }\n    }\n    else {\n        for (int i = 0; i < y.size(); i++) {\n            z[i] += alpha * y[i];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // Hint: Use MPI_Send and MPI_Recv\n  // Hint: Use MPI_Scatter and MPI_Gather\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  for (int i = start; i < end; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement\n\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n  // Hint: use MPI_Send, MPI_Recv, and MPI_Bcast\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    int n = x.size();\n    int chunk_size = n / size;\n    int remainder = n % size;\n\n    int start = rank * chunk_size;\n    int end = start + chunk_size;\n\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    MPI_Reduce(&z[0], &z[0], n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "cuda",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    z[i] = alpha * x[i] + y[i];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_remainder = n % size;\n\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n    if (rank == size - 1) {\n        end += n_remainder;\n    }\n\n    std::vector<double> local_x(x.begin() + start, x.begin() + end);\n    std::vector<double> local_y(y.begin() + start, y.begin() + end);\n    std::vector<double> local_z(n_per_rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank; i++) {\n        local_z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    if (rank == 0) {\n        z.resize(n);\n    }\n\n    MPI_Gather(local_z.data(), n_per_rank, MPI_DOUBLE, z.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_rem = n % size;\n\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n\n    if (rank == 0) {\n        end += n_rem;\n    }\n\n    std::vector<double> local_x(x.begin() + start, x.begin() + end);\n    std::vector<double> local_y(y.begin() + start, y.begin() + end);\n\n    std::vector<double> local_z(n_per_rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank; i++) {\n        local_z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    if (rank == 0) {\n        z.resize(n);\n    }\n\n    MPI_Gather(local_z.data(), n_per_rank, MPI_DOUBLE, z.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        z.resize(n);\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk_size = n / size;\n    int remainder = n % size;\n\n    std::vector<double> local_x(chunk_size + (rank < remainder? 1 : 0));\n    std::vector<double> local_y(chunk_size + (rank < remainder? 1 : 0));\n    std::vector<double> local_z(chunk_size + (rank < remainder? 1 : 0));\n\n    MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(y.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, local_y.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    MPI_Gather(local_z.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, z.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<double> local_x(chunk + (rank < remainder? 1 : 0));\n    std::vector<double> local_y(chunk + (rank < remainder? 1 : 0));\n    std::vector<double> local_z(chunk + (rank < remainder? 1 : 0));\n\n    MPI_Scatter(x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(y.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, local_y.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    MPI_Gather(local_z.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, z.data(), chunk + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end += remainder;\n  }\n  std::vector<double> local_x(x.begin() + start, x.begin() + end);\n  std::vector<double> local_y(y.begin() + start, y.begin() + end);\n  std::vector<double> local_z(local_x.size());\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_z[i] = alpha * local_x[i] + local_y[i];\n  }\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n  MPI_Gather(local_z.data(), local_z.size(), MPI_DOUBLE, z.data(), local_z.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n    int n_local = n_per_rank + (rank < n_extra? 1 : 0);\n    int start = n_per_rank * rank + (rank < n_extra? rank : n_extra);\n    int end = start + n_local;\n    std::vector<double> x_local(n_local);\n    std::vector<double> y_local(n_local);\n    std::vector<double> z_local(n_local);\n    for (int i = 0; i < n_local; ++i) {\n        x_local[i] = x[start + i];\n        y_local[i] = y[start + i];\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; ++i) {\n        z_local[i] = alpha * x_local[i] + y_local[i];\n    }\n    if (rank == 0) {\n        z.resize(n);\n    }\n    MPI_Gather(z_local.data(), n_local, MPI_DOUBLE, z.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk_size = n / size;\n    int remainder = n % size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&y[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n\n        for (int i = 0; i < remainder; i++) {\n            MPI_Send(&x[i * chunk_size + chunk_size * size], 1, MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD);\n            MPI_Send(&y[i * chunk_size + chunk_size * size], 1, MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    std::vector<double> local_x(chunk_size + remainder);\n    std::vector<double> local_y(chunk_size + remainder);\n\n    if (rank == 0) {\n        local_x = std::vector<double>(x.begin(), x.begin() + chunk_size);\n        local_y = std::vector<double>(y.begin(), y.begin() + chunk_size);\n    } else {\n        MPI_Recv(&local_x[0], chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&local_y[0], chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&local_x[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&local_y[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        for (int i = 0; i < remainder; i++) {\n            MPI_Recv(&local_x[i * chunk_size + chunk_size * size], 1, MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&local_y[i * chunk_size + chunk_size * size], 1, MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&z[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n\n        for (int i = 0; i < remainder; i++) {\n            MPI_Send(&z[i * chunk_size + chunk_size * size], 1, MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(&z[0], chunk_size, MPI_DOUBLE",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    std::vector<double> local_x(n_per_rank + (rank < n_extra? 1 : 0));\n    std::vector<double> local_y(n_per_rank + (rank < n_extra? 1 : 0));\n    std::vector<double> local_z(n_per_rank + (rank < n_extra? 1 : 0));\n\n    MPI_Scatter(x.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, local_x.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(y.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, local_y.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_rank + (rank < n_extra? 1 : 0); i++) {\n        local_z[i] = alpha * local_x[i] + local_y[i];\n    }\n\n    MPI_Gather(local_z.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, z.data(), n_per_rank + (rank < n_extra? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk_size = x.size() / size;\n  int chunk_start = rank * chunk_size;\n  int chunk_end = (rank + 1) * chunk_size;\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  #pragma omp parallel for\n  for (int i = chunk_start; i < chunk_end; i++) {\n    z[i] = alpha * x[i] + y[i];\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, z.data(), z.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = x.size();\n    int chunk = n/size;\n    int remainder = n%size;\n    int start = rank*chunk;\n    int end = start + chunk;\n    if (rank == 0) {\n        end += remainder;\n    }\n    std::vector<double> local_x(x.begin()+start, x.begin()+end);\n    std::vector<double> local_y(y.begin()+start, y.begin()+end);\n    std::vector<double> local_z(end-start);\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        local_z[i] = alpha*local_x[i] + local_y[i];\n    }\n    if (rank == 0) {\n        z.resize(n);\n    }\n    MPI_Gather(local_z.data(), local_z.size(), MPI_DOUBLE, z.data(), local_z.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_proc = n/size;\n  int n_rem = n%size;\n  int start = rank*n_per_proc;\n  int end = start + n_per_proc;\n  if (rank == size-1) {\n    end += n_rem;\n  }\n  std::vector<double> local_x(x.begin()+start, x.begin()+end);\n  std::vector<double> local_y(y.begin()+start, y.begin()+end);\n  std::vector<double> local_z(n_per_proc);\n  if (rank == 0) {\n    z.resize(n);\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_proc; i++) {\n    local_z[i] = alpha*local_x[i] + local_y[i];\n  }\n  MPI_Gather(local_z.data(), n_per_proc, MPI_DOUBLE, z.data(), n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    std::vector<double> x_proc(n_per_proc + (rank < n_rem? 1 : 0));\n    std::vector<double> y_proc(n_per_proc + (rank < n_rem? 1 : 0));\n    std::vector<double> z_proc(n_per_proc + (rank < n_rem? 1 : 0));\n\n    if (rank < n_rem) {\n        for (int i = 0; i < n_per_proc + 1; i++) {\n            x_proc[i] = x[i + rank * (n_per_proc + 1)];\n            y_proc[i] = y[i + rank * (n_per_proc + 1)];\n        }\n    } else {\n        for (int i = 0; i < n_per_proc; i++) {\n            x_proc[i] = x[i + rank * n_per_proc];\n            y_proc[i] = y[i + rank * n_per_proc];\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_per_proc + (rank < n_rem? 1 : 0); i++) {\n        z_proc[i] = alpha * x_proc[i] + y_proc[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            z[i] = 0;\n        }\n    }\n\n    MPI_Gather(z_proc.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, z.data(), n_per_proc + (rank < n_rem? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_rem; i++) {\n            z[i * (n_per_proc + 1) + n_per_proc] = z_proc[n_per_proc];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n/size;\n  int start = rank*chunk;\n  int end = (rank+1)*chunk;\n  if (rank == size-1) end = n;\n\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    z[i] = alpha*x[i] + y[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&z[i*chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&z[start], chunk, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk_size = n / size;\n  int remainder = n % size;\n\n  if (rank == 0) {\n    z.resize(n);\n  }\n\n  std::vector<double> local_x(chunk_size + (rank < remainder? 1 : 0));\n  std::vector<double> local_y(chunk_size + (rank < remainder? 1 : 0));\n  std::vector<double> local_z(chunk_size + (rank < remainder? 1 : 0));\n\n  MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, local_x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(y.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, local_y.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_z[i] = alpha * local_x[i] + local_y[i];\n  }\n\n  MPI_Gather(local_z.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, z.data(), chunk_size + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix. x and b have N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(Kokkos::View<const double**> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Create a Kokkos::RangePolicy to iterate over the rows of A\n  Kokkos::RangePolicy<Kokkos::Rank<2>> policy(0, N, 0, N);\n\n  // Create a Kokkos::View to store the sum of each row of A\n  Kokkos::View<double*> rowSum(\"rowSum\", N);\n\n  // Compute the sum of each row of A in parallel\n  Kokkos::parallel_for(\n    \"computeRowSum\",\n    policy,\n    KOKKOS_LAMBDA(const int i, const int j) {\n      rowSum(i) += A(i, j);\n    }\n  );\n\n  // Compute the solution in parallel\n  Kokkos::parallel_for(\n    \"computeSolution\",\n    policy,\n    KOKKOS_LAMBDA(const int i, const int j) {\n      x(i) = (b(i) - A(i, j) * x(j)) / rowSum(i);\n    }\n  );\n\n  // Wait for the solution to be computed\n  Kokkos::fence();\n}",
            "// TODO: implement this function\n}",
            "// TODO: Fill in the code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    // Hint: You can use the following code to compute the inverse of a matrix\n    //       (but it is not the most efficient way to do it)\n    //       Eigen::MatrixXd Ainv = A.inverse();\n    //       Eigen::VectorXd x = Ainv * b;\n    //       x.transpose();\n}",
            "// TODO: implement this function\n    // Hint: use the following formula to compute x_i\n    // x_i = (b_i - A_i,j * x_j) / A_i,i\n    // where A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product\n    // and * is the multiplication\n    // and / is the division\n    // and () is the grouping\n    // and i,j are the row and column indices\n    // and A_i,j is the element at row i and column j of A\n    // and A_i,i is the element at row i and column i of A\n    // and b_i is the element at row i of b\n    // and x_j is the element at row j of x\n    // and x_i is the element at row i of x\n    // and i,j are the row and column indices\n    // and, is the dot product",
            "// TODO: implement this function\n    // You can use the following variables:\n    // - A: the matrix A\n    // - b: the vector b\n    // - x: the vector x\n    // - N: the size of the matrix A\n    // - omp_get_thread_num(): returns the thread number\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_limit(): returns the maximum number of threads\n    // - omp_get_num_teams(): returns the number of teams\n    // - omp_get_team_num(): returns the team number\n    // - omp_get_num_threads(): returns the number of threads in the team\n    // - omp_get_thread_limit(): returns the maximum number of threads in the team\n    // - omp_in_parallel(): returns 1 if in parallel region, 0 otherwise\n    // - omp_in_final(): returns 1 if in final region, 0 otherwise\n    // - omp_get_cancellation(): returns 1 if cancel is enabled, 0 otherwise\n    // - omp_get_dynamic(): returns 1 if dynamic is enabled, 0 otherwise\n    // - omp_get_nested(): returns 1 if nested is enabled, 0 otherwise\n    // - omp_get_max_active_levels(): returns the maximum number of active levels\n    // - omp_get_level(): returns the current level\n    // - omp_get_ancestor_thread_num(level): returns the thread number of the ancestor thread at level\n    // - omp_get_team_size(level): returns the size of the team at level\n    // - omp_get_active_level(): returns the current active level\n    // - omp_get_max_task_priority(): returns the maximum task priority\n    // - omp_get_initial_device(): returns the initial device number\n    // - omp_get_default_device(): returns the default device number\n    // - omp_get_num_devices(): returns the number of devices\n    // - omp_get_num_teams(): returns the number of teams\n    // - omp_get_team_num(): returns the team number\n    // - omp_get_cancellation(): returns 1 if cancel is enabled, 0 otherwise\n    // - omp_get_dynamic(): returns 1 if dynamic is enabled, 0 otherwise\n    // - omp_get_nested(): returns 1 if nested is enabled, 0 otherwise\n    // - omp_get_max_active_levels(): returns the maximum number of active levels\n    // - omp_get_level(): returns the current level\n    // - omp_get_ancestor_thread_num(level): returns the thread number of the ancestor thread at level\n    // - omp_get_team_size(level): returns the size of the team at level\n    // - omp_get_active_level(): returns the current active level\n    // - omp_get_max_task_priority(): returns the maximum task priority\n    // - omp_get_initial_device(): returns the initial device number\n    // - omp_get_default_device(): returns the default device number\n    // - omp_get_num_devices(): returns the number of devices\n    // - omp_get_num_teams(): returns the number of teams\n    // - omp_get_team_num(): returns the team number\n    // - omp_get_cancellation(): returns 1 if cancel is enabled, 0 otherwise\n    // - omp_get_dynamic(): returns 1 if dynamic is enabled, 0 otherwise\n    // - omp_get_nested(): returns 1 if nested is enabled, 0 otherwise\n    // - omp_get_max_active_levels(): returns the maximum number of active levels\n    // - omp_get_level(): returns the current level\n    // - omp_get_ancestor_thread_num(level): returns the thread number of the ancestor thread at level\n    // - omp_get_team_size(level): returns the size of the team at level\n    // - omp_get_active_level(): returns the current active",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n    // x is initialized to zero\n    for (size_t i = 0; i < N; i++) {\n        x[i] = 0;\n    }\n\n    // TODO: use OpenMP to parallelize the following loop\n    for (size_t i = 0; i < N; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i*N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}",
            "// TODO: implement this function\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallelize the for loop\n    // Hint: use OpenMP to parallel",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n    // Hint: use OpenMP to parallelize the computation\n\n}",
            "// TODO: implement this function\n    // You may use the following functions:\n    // - omp_get_num_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_num_threads(int)\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_init_lock(omp_lock_t *)\n    // - omp_destroy_lock(omp_lock_t *)\n    // - omp_set_lock(omp_lock_t *)\n    // - omp_unset_lock(omp_lock_t *)\n    // - omp_test_lock(omp_lock_t *)\n    // - omp_init_nest_lock(omp_nest_lock_t *)\n    // - omp_destroy_nest_lock(omp_nest_lock_t *)\n    // - omp_set_nest_lock(omp_nest_lock_t *)\n    // - omp_unset_nest_lock(omp_nest_lock_t *)\n    // - omp_test_nest_lock(omp_nest_lock_t *)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_num_threads(int)\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_init_lock(omp_lock_t *)\n    // - omp_destroy_lock(omp_lock_t *)\n    // - omp_set_lock(omp_lock_t *)\n    // - omp_unset_lock(omp_lock_t *)\n    // - omp_test_lock(omp_lock_t *)\n    // - omp_init_nest_lock(omp_nest_lock_t *)\n    // - omp_destroy_nest_lock(omp_nest_lock_t *)\n    // - omp_set_nest_lock(omp_nest_lock_t *)\n    // - omp_unset_nest_lock(omp_nest_lock_t *)\n    // - omp_test_nest_lock(omp_nest_lock_t *)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_num_threads(int)\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_init_lock(omp_lock_t *)\n    // - omp_destroy_lock(omp_lock_t *)\n    // - omp_set_lock(omp_lock_t *)\n    // - omp_unset_lock(omp_lock_t *)\n    // - omp_test_lock(omp_lock_t *)\n    // - omp_init_nest_lock(omp_nest_lock_t *)\n    // - omp_destroy_nest_lock(omp_nest_lock_t *)\n    // - omp_set_nest_lock(omp_nest_lock_t *)\n    // - omp_unset_nest_lock(omp_nest_lock_t *)\n    // - omp_test_nest_lock(omp_nest_lock_t *)",
            "// TODO: Implement this function\n    // You can use the following variables:\n    // - N: the size of the matrix\n    // - A: the matrix in row-major\n    // - b: the right-hand side\n    // - x: the solution\n    // - i, j: loop variables\n    // - a: a temporary variable\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors\n    // - omp_get_max_threads(): returns the maximum number of threads\n    // - omp_get_thread_num(): returns the thread id\n    // - omp_get_num_threads(): returns the number of threads\n    // - omp_get_num_procs(): returns the number of processors",
            "// TODO: implement this function\n  // Hint: use OpenMP to parallelize the computation\n  // Hint: use the following formula to solve the system:\n  // x[i] = (b[i] - sum(A[i,j]*x[j]) for j=0..i-1) / A[i,i]\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for(size_t i = 0; i < N; i++) {\n        double sum = 0;\n        for(size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop over i\n    // Hint: Use OpenMP to parallelize the loop over j\n\n    // Initialize x to zero\n    for (size_t i = 0; i < N; i++) {\n        x[i] = 0;\n    }\n\n    // Solve the linear system\n    for (size_t i = 0; i < N; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use OpenMP to parallelize the loop\n    // Hint: Use",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "serial",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    // You can use the following functions:\n    // - std::vector::at()\n    // - std::vector::size()\n    // - std::vector::begin()\n    // - std::vector::end()\n    // - std::vector::push_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::data()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector",
            "// TODO: implement this function\n    // You can use the following functions:\n    // - std::vector::at()\n    // - std::vector::size()\n    // - std::vector::begin()\n    // - std::vector::end()\n    // - std::vector::push_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::empty()\n    // - std::vector::operator[]()\n    // - std::vector::operator=()\n    // - std::vector::operator==()\n    // - std::vector::operator!=()\n    // - std::vector::operator<()\n    // - std::vector::operator>()\n    // - std::vector::operator<=()\n    // - std::vector::operator>=()\n    // - std::vector::rbegin()\n    // - std::vector::rend()\n    // - std::vector::cbegin()\n    // - std::vector::cend()\n    // - std::vector::crbegin()\n    // - std::vector::crend()\n    // - std::vector::get_allocator()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::shrink_to_fit()\n    // - std::vector::emplace()\n    // - std::vector::emplace_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::push_back()\n    // - std::vector::pop_back()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::shrink_to_fit()\n    // - std::vector::emplace()\n    // - std::vector::emplace_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::push_back()\n    // - std::vector::pop_back()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::shrink_to_fit()\n    // - std::vector::emplace()\n    // - std::vector::emplace_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::push_back()\n    // - std::vector::pop_back()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::shrink_to_fit()\n    // - std::vector::emplace()\n    // - std::vector::emplace_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::push_back()\n    // - std::vector::pop_back()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector::resize()\n    // - std::vector::reserve()\n    // - std::vector::shrink_to_fit()\n    // - std::vector::emplace()\n    // - std::vector::emplace_back()\n    // - std::vector::insert()\n    // - std::vector::erase()\n    // - std::vector::clear()\n    // - std::vector::push_back()\n    // - std::vector::pop_back()\n    // - std::vector::swap()\n    // - std::vector::assign()\n    // - std::vector",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.org/wiki/Triangular_matrix\n  // http://en.wikipedia.org/wiki/Back_substitution\n\n  // TODO: Implement this function\n  // Hint: Use the Gaussian elimination algorithm\n  // http://en.wikipedia.org/wiki/Gaussian_elimination\n  // http://en.wikipedia.",
            "// TODO: implement this function\n  std::vector<double> L(N*N);\n  std::vector<double> U(N*N);\n  std::vector<double> y(N);\n  std::vector<double> z(N);\n  std::vector<double> x_temp(N);\n  std::vector<double> b_temp(N);\n  std::vector<double> x_temp2(N);\n  std::vector<double> b_temp2(N);\n  std::vector<double> x_temp3(N);\n  std::vector<double> b_temp3(N);\n  std::vector<double> x_temp4(N);\n  std::vector<double> b_temp4(N);\n  std::vector<double> x_temp5(N);\n  std::vector<double> b_temp5(N);\n  std::vector<double> x_temp6(N);\n  std::vector<double> b_temp6(N);\n  std::vector<double> x_temp7(N);\n  std::vector<double> b_temp7(N);\n  std::vector<double> x_temp8(N);\n  std::vector<double> b_temp8(N);\n  std::vector<double> x_temp9(N);\n  std::vector<double> b_temp9(N);\n  std::vector<double> x_temp10(N);\n  std::vector<double> b_temp10(N);\n  std::vector<double> x_temp11(N);\n  std::vector<double> b_temp11(N);\n  std::vector<double> x_temp12(N);\n  std::vector<double> b_temp12(N);\n  std::vector<double> x_temp13(N);\n  std::vector<double> b_temp13(N);\n  std::vector<double> x_temp14(N);\n  std::vector<double> b_temp14(N);\n  std::vector<double> x_temp15(N);\n  std::vector<double> b_temp15(N);\n  std::vector<double> x_temp16(N);\n  std::vector<double> b_temp16(N);\n  std::vector<double> x_temp17(N);\n  std::vector<double> b_temp17(N);\n  std::vector<double> x_temp18(N);\n  std::vector<double> b_temp18(N);\n  std::vector<double> x_temp19(N);\n  std::vector<double> b_temp19(N);\n  std::vector<double> x_temp20(N);\n  std::vector<double> b_temp20(N);\n  std::vector<double> x_temp21(N);\n  std::vector<double> b_temp21(N);\n  std::vector<double> x_temp22(N);\n  std::vector<double> b_temp22(N);\n  std::vector<double> x_temp23(N);\n  std::vector<double> b_temp23(N);\n  std::vector<double> x_temp24(N);\n  std::vector<double> b_temp24(N);\n  std::vector<double> x_temp25(N);\n  std::vector<double> b_temp25(N);\n  std::vector<double> x_temp26(N);\n  std::vector<double> b_temp26(N);\n  std::vector<double> x_temp27(N);\n  std::vector<double> b_temp27(N);\n  std::vector<double> x_temp28(N);\n  std::vector<double> b_temp28(N);\n  std::vector<double> x_temp29(N);\n  std::vector<double> b_temp29(N);\n  std::vector<double> x_temp30(N);\n  std::vector<double> b_temp30(N);\n  std::vector<double> x_temp31(N);\n  std::vector<double>",
            "std::vector<double> L(N*N);\n    std::vector<double> U(N*N);\n    std::vector<double> y(N);\n    std::vector<double> z(N);\n    std::vector<double> x_temp(N);\n\n    // LU decomposition\n    LUdecomposition(A, L, U, N);\n\n    // Forward substitution\n    forwardSubstitution(L, b, y, N);\n\n    // Backward substitution\n    backwardSubstitution(U, y, z, N);\n\n    // Solve Ax=b\n    for (size_t i = 0; i < N; i++) {\n        x_temp[i] = z[i] / A[i*N + i];\n    }\n\n    // Copy the result to x\n    x = x_temp;\n}",
            "// TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       and the back substitution algorithm\n    //       to solve the linear system\n    //       A x = b\n    //       where A is an NxN matrix in row-major\n    //       x and b have N elements\n    //       Example:\n    //       \n    //       input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n    //       output: x=[3, 1, 2]\n}",
            "std::vector<double> L(N*N);\n  std::vector<double> U(N*N);\n  std::vector<double> y(N);\n  std::vector<double> z(N);\n  std::vector<double> x_temp(N);\n\n  // LU decomposition\n  LUdecomposition(A, L, U, N);\n\n  // forward substitution\n  forwardSubstitution(L, b, y, N);\n\n  // backward substitution\n  backwardSubstitution(U, y, z, N);\n\n  // solve for x\n  for (size_t i = 0; i < N; i++) {\n    x_temp[i] = z[i] / U[i*N + i];\n  }\n\n  x = x_temp;\n}",
            "// TODO: Implement this function\n  // Hint: Use the LU decomposition\n  // Hint: Use the back-substitution\n  // Hint: Use the forward-substitution\n  // Hint: Use the Gaussian elimination\n  // Hint: Use the Gauss-Jordan elimination\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use the LU decomposition\n  // Hint: Use",
            "// TODO: implement this function\n  // Hint: use the LU decomposition\n  // Hint: use the forward and backward substitution\n  // Hint: use the function LUDecomposition\n  // Hint: use the function forwardSubstitution\n  // Hint: use the function backwardSubstitution\n  // Hint: use the function swapRows\n  // Hint: use the function swapColumns\n  // Hint: use the function printMatrix\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint: use the function printVector\n  // Hint",
            "// TODO: implement this function\n    // Hint: use the LU decomposition\n    // Hint: use the forward substitution\n    // Hint: use the backward substitution\n}",
            "// TODO: Implement this function\n    // Hint: Use Gaussian elimination\n    // Hint: Use the fact that A is symmetric\n    // Hint: Use the fact that A is positive definite\n    // Hint: Use the fact that A is diagonalizable\n    // Hint: Use the fact that A is diagonalizable and symmetric\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors and real eigenvectors are orthogonal\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors and real eigenvectors are orthogonal and orthogonal eigenvectors are orthonormal\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors and real eigenvectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors and real eigenvectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal\n    // Hint: Use the fact that A is diagonalizable and symmetric and positive definite and invertible and square and NxN and real and real entries and real eigenvalues and real eigenvectors and real eigenvectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenvectors are unit vectors and unit vectors are orthogonal and orthogonal eigenvectors are orthonormal and orthonormal eigenve",
            "std::vector<double> L(N*N);\n    std::vector<double> U(N*N);\n    std::vector<double> y(N);\n    std::vector<double> z(N);\n\n    // LU decomposition\n    LUdecomposition(A, L, U, N);\n\n    // forward substitution\n    forwardSubstitution(L, b, y, N);\n\n    // backward substitution\n    backwardSubstitution(U, y, z, N);\n\n    // copy z to x\n    x = z;\n}",
            "std::vector<double> L(N*N);\n  std::vector<double> U(N*N);\n  std::vector<double> y(N);\n  std::vector<double> z(N);\n  std::vector<double> x_temp(N);\n\n  // LU decomposition\n  LUdecomposition(A, L, U, N);\n\n  // forward substitution\n  forwardSubstitution(L, b, y, N);\n\n  // backward substitution\n  backwardSubstitution(U, y, z, N);\n\n  // solve for x\n  for (size_t i = 0; i < N; i++) {\n    x_temp[i] = z[i] / U[i*N + i];\n  }\n\n  // copy x_temp to x\n  for (size_t i = 0; i < N; i++) {\n    x[i] = x_temp[i];\n  }\n}",
            "// TODO: implement this function\n    std::vector<double> L(N*N, 0);\n    std::vector<double> U(N*N, 0);\n    std::vector<double> y(N, 0);\n    std::vector<double> x_(N, 0);\n    std::vector<double> x__(N, 0);\n    std::vector<double> x_temp(N, 0);\n    std::vector<double> x_temp2(N, 0);\n    std::vector<double> x_temp3(N, 0);\n    std::vector<double> x_temp4(N, 0);\n    std::vector<double> x_temp5(N, 0);\n    std::vector<double> x_temp6(N, 0);\n    std::vector<double> x_temp7(N, 0);\n    std::vector<double> x_temp8(N, 0);\n    std::vector<double> x_temp9(N, 0);\n    std::vector<double> x_temp10(N, 0);\n    std::vector<double> x_temp11(N, 0);\n    std::vector<double> x_temp12(N, 0);\n    std::vector<double> x_temp13(N, 0);\n    std::vector<double> x_temp14(N, 0);\n    std::vector<double> x_temp15(N, 0);\n    std::vector<double> x_temp16(N, 0);\n    std::vector<double> x_temp17(N, 0);\n    std::vector<double> x_temp18(N, 0);\n    std::vector<double> x_temp19(N, 0);\n    std::vector<double> x_temp20(N, 0);\n    std::vector<double> x_temp21(N, 0);\n    std::vector<double> x_temp22(N, 0);\n    std::vector<double> x_temp23(N, 0);\n    std::vector<double> x_temp24(N, 0);\n    std::vector<double> x_temp25(N, 0);\n    std::vector<double> x_temp26(N, 0);\n    std::vector<double> x_temp27(N, 0);\n    std::vector<double> x_temp28(N, 0);\n    std::vector<double> x_temp29(N, 0);\n    std::vector<double> x_temp30(N, 0);\n    std::vector<double> x_temp31(N, 0);\n    std::vector<double> x_temp32(N, 0);\n    std::vector<double> x_temp33(N, 0);\n    std::vector<double> x_temp34(N, 0);\n    std::vector<double> x_temp35(N, 0);\n    std::vector<double> x_temp36(N, 0);\n    std::vector<double> x_temp37(N, 0);\n    std::vector<double> x_temp38(N, 0);\n    std::vector<double> x_temp39(N, 0);\n    std::vector<double> x_temp40(N, 0);\n    std::vector<double> x_temp41(N, 0);\n    std::vector<double> x_temp42(N, 0);\n    std::vector<double> x_temp43(N, 0);\n    std::vector<double> x_temp44(N, 0);\n    std::vector<double> x_temp45(N, 0);\n    std::vector<double> x_temp46(N, 0);\n    std::vector<double> x_temp47(N, 0);\n    std::vector<double> x_temp48(N, 0);\n    std::vector<double> x_temp49(N, 0);\n    std::vector<double> x_temp50(N,",
            "// TODO: implement this function\n  // Hint: use the LU decomposition\n  // Hint: use the forward substitution\n  // Hint: use the backward substitution\n  // Hint: use the function \"luDecomposition\"\n  // Hint: use the function \"forwardSubstitution\"\n  // Hint: use the function \"backwardSubstitution\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printMatrix\"\n  // Hint: use the function \"printMatrix\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"\n  // Hint: use the function \"printVector\"",
            "// TODO: Implement this function\n  // Hint: You can use the LU decomposition of A to solve the linear system.\n  //       You can use the function LUDecomposition to get the L and U matrices\n  //       and the permutation vector.\n  //       You can use the function forwardSubstitution to solve the linear system\n  //       with the L matrix.\n  //       You can use the function backSubstitution to solve the linear system\n  //       with the U matrix.\n  //       You can use the function permute to permute the solution vector.\n  //       You can use the function printVector to print the solution vector.\n  //       You can use the function printMatrix to print the L and U matrices.\n  //       You can use the function printVector to print the permutation vector.\n  //       You can use the function printVector to print the b vector.\n  //       You can use the function printVector to print the x vector.\n\n  std::vector<double> L, U;\n  std::vector<int> p;\n  LUDecomposition(A, L, U, p, N);\n  std::vector<double> y(N);\n  forwardSubstitution(L, b, y, N);\n  backSubstitution(U, y, x, N);\n  permute(p, x, N);\n  printVector(x, N);\n  printMatrix(L, N);\n  printMatrix(U, N);\n  printVector(p, N);\n  printVector(b, N);\n}",
            "// TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org/wiki/LU_decomposition\n    //       http://en.wikipedia.org/wiki/Triangular_matrix\n    //       http://en.wikipedia.org/wiki/Back_substitution\n    //       http://en.wikipedia.org/wiki/Laplace_expansion\n\n    // TODO: implement this function\n    // Hint: you can use the LU decomposition of A\n    //       to solve the linear system\n    //       http://en.wikipedia.org",
            "// TODO: implement this function\n    // Hint: use the LU decomposition\n    // Hint: use the forward and backward substitution\n    // Hint: use the function LUDecomposition\n    // Hint: use the function forwardSubstitution\n    // Hint: use the function backwardSubstitution\n    // Hint: use the function swapRows\n    // Hint: use the function swapColumns\n    // Hint: use the function multiplyMatrixVector\n    // Hint: use the function multiplyMatrixMatrix\n    // Hint: use the function printMatrix\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    // Hint: use the function printVector\n    //",
            "// TODO: Implement this function\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax=b for x.\n    // You can use the function solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N)\n    // to solve the linear system Ax"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "hip",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "// Get the row and column of the thread\n    size_t row = blockIdx.x;\n    size_t col = blockIdx.y;\n\n    // Get the thread's index in the matrix\n    size_t index = row * N + col;\n\n    // Get the thread's index in the vector\n    size_t i = col;\n\n    // Get the value of the element in the matrix\n    double A_ij = A[index];\n\n    // Get the value of the element in the vector\n    double b_i = b[i];\n\n    // Compute the value of the element in the vector\n    double x_i = b_i / A_ij;\n\n    // Store the value of the element in the vector\n    x[i] = x_i;\n}",
            "// Get the global thread index\n  int globalThreadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n  // Get the global row index\n  int globalRowIdx = globalThreadIdx / N;\n  // Get the global column index\n  int globalColIdx = globalThreadIdx % N;\n  // Get the global row index of the current thread\n  int globalRowIdx_t = globalThreadIdx / N;\n  // Get the global column index of the current thread\n  int globalColIdx_t = globalThreadIdx % N;\n\n  // Initialize the shared memory\n  __shared__ double s_A[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ double s_b[BLOCK_SIZE];\n  __shared__ double s_x[BLOCK_SIZE];\n\n  // Load the matrix A into shared memory\n  s_A[globalRowIdx_t][globalColIdx_t] = A[globalThreadIdx];\n  // Load the vector b into shared memory\n  s_b[globalRowIdx_t] = b[globalRowIdx_t];\n  // Load the vector x into shared memory\n  s_x[globalRowIdx_t] = x[globalRowIdx_t];\n\n  // Synchronize the threads in the block\n  __syncthreads();\n\n  // Compute the solution\n  for (int i = 0; i < N; i++) {\n    // Compute the sum\n    double sum = 0;\n    for (int j = 0; j < N; j++) {\n      sum += s_A[globalRowIdx_t][j] * s_x[j];\n    }\n    // Update the solution\n    s_x[globalRowIdx_t] = (s_b[globalRowIdx_t] - sum) / s_A[globalRowIdx_t][globalColIdx_t];\n    // Synchronize the threads in the block\n    __syncthreads();\n  }\n\n  // Store the solution\n  x[globalThreadIdx] = s_x[globalRowIdx_t];\n}",
            "// Get the thread index\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check if the thread is in bounds\n    if (i < N) {\n        // Initialize the sum\n        double sum = 0;\n\n        // Loop over the columns of A\n        for (size_t j = 0; j < N; j++) {\n            // Get the element of A\n            double a_ij = A[i * N + j];\n\n            // Add the product of the element and the corresponding element of x\n            sum += a_ij * x[j];\n        }\n\n        // Store the result in x\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// Get the thread index\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        // Solve the linear system\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// Get the thread index\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        // Solve the linear system\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// Get the index of the thread in the grid\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // Compute the sum of the i-th row of A multiplied by x\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    // Compute the value of x[i]\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "// Get the row and column of the thread\n  size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check if the thread is in the matrix\n  if (row >= N || col >= N) {\n    return;\n  }\n\n  // Get the value of the thread\n  double value = A[row * N + col];\n\n  // Check if the thread is on the diagonal\n  if (row == col) {\n    // If it is on the diagonal, compute the inverse of the value\n    value = 1.0 / value;\n  } else {\n    // If it is not on the diagonal, compute the value as 0\n    value = 0.0;\n  }\n\n  // Write the value to the output\n  x[row * N + col] = value;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> local_b(b.begin() + rank * N / size, b.begin() + (rank + 1) * N / size);\n    std::vector<double> local_A(A.begin() + rank * N * N / size, A.begin() + (rank + 1) * N * N / size);\n    std::vector<double> local_x(N / size);\n\n    for (size_t i = 0; i < N / size; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N / size; j++) {\n            sum += local_A[i * N / size + j] * local_x[j];\n        }\n        local_x[i] = (local_b[i] - sum) / local_A[i * N / size + i];\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < N / size; i++) {\n            x[i] = local_x[i];\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "cuda",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  int i = blockIdx.x;\n  int j = threadIdx.x;\n  double sum = 0;\n  for (int k = 0; k < N; k++) {\n    sum += A[i * N + k] * x[k];\n  }\n  x[i] = (b[i] - sum) / A[i * N + i];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (int j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// TODO: implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        for (int k = 0; k < N; k++) {\n            sum += A[i * N + k] * x[k];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// TODO: Your code here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    x[i] = (b[i] - sum) / A[i * N + i];\n  }\n}",
            "// TODO: Implement this function\n    //\n    // Hint:\n    // 1. Use the threadIdx.x and blockIdx.x to compute the row and column of the current thread.\n    // 2. Use the threadIdx.x and blockIdx.x to compute the row and column of the current thread.\n    // 3. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 4. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 5. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 6. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 7. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 8. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 9. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 10. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 11. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 12. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 13. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 14. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 15. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 16. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 17. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 18. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 19. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 20. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 21. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 22. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 23. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 24. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 25. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 26. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 27. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 28. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 29. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 30. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 31. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 32. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 33. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 34. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 35. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 36. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 37. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 38. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 39. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 40. Use the atomicAdd() function to compute the sum of the elements of a column.\n    // 41. Use the atomicAdd() function to compute the sum of the elements of a row.\n    // 42. Use the atomicAdd() function to compute the sum",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n    // You may assume that the size of A is NxN\n    // You may assume that the size of b is N\n    // You may assume that the size of x is N\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume that the size of x is a multiple of the number of MPI ranks\n    // You may assume that the size of A is the same as the size of b and x\n    // You may assume that A is a square matrix\n    // You may assume that A is a row-major matrix\n    // You may assume that the size of A is a multiple of the number of MPI ranks\n    // You may assume that the size of b is a multiple of the number of MPI ranks\n    // You may assume",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<double> local_A(N*N);\n    std::vector<double> local_b(N);\n    std::vector<double> local_x(N);\n    if (rank == 0) {\n        for (size_t i = 0; i < N; i++) {\n            for (size_t j = 0; j < N; j++) {\n                local_A[i*N+j] = A[i*N+j];\n            }\n            local_b[i] = b[i];\n        }\n    }\n    MPI_Bcast(local_A.data(), N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(local_b.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < N; i++) {\n        local_x[i] = 0;\n    }\n    for (size_t i = 0; i < N; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += local_A[i*N+j]*local_x[j];\n        }\n        local_x[i] = (local_b[i] - sum)/local_A[i*N+i];\n    }\n    MPI_Gather(local_x.data(), N, MPI_DOUBLE, x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n    // You can use the following variables:\n    // - N: the size of the matrix\n    // - A: the matrix\n    // - b: the right-hand side\n    // - x: the result\n    // - rank: the rank of the current process\n    // - size: the number of processes\n    // - omp_get_num_threads(): the number of threads per process\n    // - omp_get_thread_num(): the rank of the current thread\n\n    // You can use the following functions:\n    // - MPI_Comm_rank(MPI_COMM_WORLD, &rank)\n    // - MPI_Comm_size(MPI_COMM_WORLD, &size)\n    // - MPI_Send(x, N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD)\n    // - MPI_Recv(x, N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE)\n    // - omp_get_num_threads()\n    // - omp_get_thread_num()\n\n    // You can use the following operators:\n    // - []: access an element of a vector\n    // - +, -, *, /, +=, -=, *=, /=\n    // - ==,!=, <, >, <=, >=\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::vector<double> v(N, 0.0)\n    // - std::",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        x.resize(N);\n    }\n    std::vector<double> x_local(N);\n    std::vector<double> b_local(N);\n    std::vector<double> A_local(N * N);\n    if (rank == 0) {\n        A_local = A;\n        b_local = b;\n    }\n    MPI_Bcast(&A_local[0], N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&b_local[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (i == j) {\n                x_local[i] = b_local[i] / A_local[i * N + j];\n            } else {\n                x_local[i] -= A_local[i * N + j] * x_local[j];\n            }\n        }\n    }\n    MPI_Gather(&x_local[0], N, MPI_DOUBLE, &x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M), KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      C(i, j) = 0;\n      for (int k = 0; k < K; k++) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n}",
            "// TODO: Implement this function\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N), KOKKOS_LAMBDA(const int i, const int j) {\n    C(i, j) = 0;\n    for (int k = 0; k < K; k++) {\n      C(i, j) += A(i, k) * B(k, j);\n    }\n  });\n}",
            "// TODO: Implement this function\n\n}",
            "Kokkos::parallel_for(\"gemm\", Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N), KOKKOS_LAMBDA(const int i, const int j) {\n    double sum = 0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A(i, k) * B(k, j);\n    }\n    C(i, j) = sum;\n  });\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // You may need to use the following Kokkos functions:\n  // Kokkos::parallel_for\n  // Kokkos::RangePolicy\n  // Kokkos::TeamPolicy\n  // Kokkos::TeamThreadRange\n  // Kokkos::parallel_for\n  // Kokkos::parallel_reduce\n  // Kokkos::parallel_scan\n  // Kokkos::parallel_scan\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_for\n  // Kokkos::parallel_",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n  // Hint: You can use the Kokkos::parallel_for function to parallelize the outer loop\n  // Hint: You can use the Kokkos::parallel_reduce function to parallelize the inner loop\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a matrix\n  // Hint: You can use the Kokkos::subview function to get a subvector of a vector\n  // Hint: You can use the Kokkos::subview function to get a submatrix of a",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_for to parallelize the loop over the rows of C\n  // Hint: Use Kokkos::subview to access the submatrix of A and B\n  // Hint: Use Kokkos::subview to access the submatrix of C\n  // Hint: Use Kokkos::subview to access the subvector of a matrix row\n  // Hint: Use Kokkos::subview to access the subvector of a matrix column\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of a vector\n  // Hint: Use Kokkos::subview to access the subvector of"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num_procs() to get the number of processors\n    // Hint: use omp_get_thread_num() to get the thread id\n    // Hint: use omp_get_num_threads() to get the number of threads\n    // Hint: use omp_get_num",
            "// TODO: Implement this function\n    // You can use the following variables:\n    // - M, K, N: the dimensions of the matrices\n    // - A, B, C: the matrices\n    // - i, j, k: loop variables\n    // - C[i*N + j]: the element of C at row i, column j\n    // - A[i*K + k]: the element of A at row i, column k\n    // - B[k*N + j]: the element of B at row k, column j\n\n    // You can use the following OpenMP directives:\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_limit(): get the thread limit\n    // - omp_get_dynamic(): get the dynamic thread adjustment\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_set_dynamic(0/1): set the dynamic thread adjustment\n    // - omp_set_nested(0/1): set nested parallelism\n    // - omp_in_parallel(): check if in parallel\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_set_dynamic(0/1): set the dynamic thread adjustment\n    // - omp_set_nested(0/1): set nested parallelism\n    // - omp_in_parallel(): check if in parallel\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_set_dynamic(0/1): set the dynamic thread adjustment\n    // - omp_set_nested(0/1): set nested parallelism\n    // - omp_in_parallel(): check if in parallel\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_set_dynamic(0/1): set the dynamic thread adjustment\n    // - omp_set_nested(0/1): set nested parallelism\n    // - omp_in_parallel(): check if in parallel\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_set_dynamic(0/1): set the dynamic thread adjustment\n    // - omp_set_nested(0/1): set nested parallelism\n    // - omp_in_parallel(): check if in parallel\n    // - omp_set_num_threads(n): set the number of threads\n    // - omp_get_num_threads(): get the number of threads\n    // - omp_get_max_threads(): get the maximum number of threads\n    // - omp_get_thread_num(): get the thread number\n    // - omp_get_num_procs(): get the number of processors\n    // - omp_set_dynamic(0/",
            "// TODO: Implement this function\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n    // Hint: You can use the following formulas to compute the indices of the matrices A, B, and C\n    // C[i,j] = A[i,k] * B[k,j]\n    // A[i,k] = A[i*K + k]\n    // B[k,j] = B[k*N + j]\n    // C[i,j] = C[i*N + j]\n\n    // TODO: You can use the following variables to get the size of the matrices\n    // size_t M = A.size() / K;\n    // size_t N = B.size() / N;\n    // size_t K = A.size() / (M * K);\n\n    // TODO: You can use the following variables to get the number of threads and the thread id\n    // int num_threads = omp_get_num_threads();\n    // int thread_id = omp_get_thread_num();\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = M / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = M;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = N / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = N;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = K / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = K;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = M / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = M;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = N / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = N;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = K / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = K;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = M / num_threads;\n    // size_t start_iteration = thread_id * num_iterations;\n    // size_t end_iteration = start_iteration + num_iterations;\n    // if (thread_id == num_threads - 1) {\n    //     end_iteration = M;\n    // }\n\n    // TODO: You can use the following variables to get the number of iterations\n    // size_t num_iterations = N / num_threads",
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "// TODO: Implement this function\n    // You may assume that the matrices are stored in row-major order\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxK and KxN\n\n    // You may use the following variables:\n    // size_t M, size_t N, size_t K\n    // std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C\n\n    // You may use the following functions:\n    // std::min(size_t a, size_t b)\n    // std::max(size_t a, size_t b)\n    // std::pow(double a, double b)\n    // std::sqrt(double a)\n    // std::floor(double a)\n    // std::ceil(double a)\n    // std::round(double a)\n    // std::fmod(double a, double b)\n    // std::abs(double a)\n    // std::exp(double a)\n    // std::log(double a)\n    // std::log10(double a)\n    // std::sin(double a)\n    // std::cos(double a)\n    // std::tan(double a)\n    // std::asin(double a)\n    // std::acos(double a)\n    // std::atan(double a)\n    // std::atan2(double a, double b)\n    // std::sinh(double a)\n    // std::cosh(double a)\n    // std::tanh(double a)\n    // std::asinh(double a)\n    // std::acosh(double a)\n    // std::atanh(double a)\n    // std::pow(double a, double b)\n    // std::cbrt(double a)\n    // std::hypot(double a, double b)\n    // std::erf(double a)\n    // std::erfc(double a)\n    // std::tgamma(double a)\n    // std::lgamma(double a)\n    // std::ceil(double a)\n    // std::trunc(double a)\n    // std::fma(double a, double b, double c)\n    // std::fmax(double a, double b)\n    // std::fmin(double a, double b)\n    // std::fdim(double a, double b)\n    // std::copysign(double a, double b)\n    // std::abs(double a)\n    // std::fabs(double a)\n    // std::remainder(double a, double b)\n    // std::remquo(double a, double b, int* quo)\n    // std::nextafter(double a, double b)\n    // std::nan(const char* s)\n    // std::isfinite(double a)\n    // std::isinf(double a)\n    // std::isnan(double a)\n    // std::isnormal(double a)\n    // std::signbit(double a)\n    // std::fpclassify(double a)\n    // std::isfinite(double a)\n    // std::isinf(double a)\n    // std::isnan(double a)\n    // std::isnormal(double a)\n    // std::signbit(double a)\n    // std::fpclassify(double a)\n    // std::isgreater(double a, double b)\n    // std::isgreaterequal(double a, double b)\n    // std::isless(double a, double b)\n    // std::islessequal(double a, double b)\n    // std::islessgreater(double a, double b)\n    // std::isunordered(double a, double b)\n    // std::isnan(double a)\n    // std::isinf(double a)\n    // std::isnormal(double a)\n    // std::signbit(double a)\n    // std::fpclassify(double a)\n    // std::isgreater(double a, double b)\n    // std::isgreaterequal(double a, double b)\n    // std::isless(double a, double b)\n    // std::islessequal(double a, double",
            "// TODO: Fill in your code here\n    // Use OpenMP to parallelize the computation\n\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for(size_t i = 0; i < M; i++){\n        for(size_t j = 0; j < N; j++){\n            C[i*N+j] = 0;\n            for(size_t k = 0; k < K; k++){\n                C[i*N+j] += A[i*K+k] * B[k*N+j];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Fill in your code here\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(A[i][k] * B[k][j])\n    // C[i][j] = sum(",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n    // You may assume that the matrices are stored in row-major format\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size KxN\n    // You may assume that the matrices are all of size MxK\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that the matrices are all of size MxN\n    // You may assume that",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0;\n      for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0;\n      for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
            "for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; k++) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; k++) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n  for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0;\n      for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
            "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      C[i * N + j] = 0;\n      for (size_t k = 0; k < K; k++) {\n        C[i * N + j] += A[i * K + k] * B[k * N + j];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; k++) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0;\n      for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      C[i * N + j] = 0;\n      for (size_t k = 0; k < K; k++) {\n        C[i * N + j] += A[i * K + k] * B[k * N + j];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      C[i * N + j] = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C[i * N + j] += A[i * K + k] * B[k * N + j];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "hip",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < K; ++k) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (row < M && col < N) {\n    double sum = 0.0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A[row * K + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n  }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n  double sum = 0;\n  for (size_t k = 0; k < K; ++k) {\n    sum += A[row * K + k] * B[k * N + col];\n  }\n  C[row * N + col] = sum;\n}",
            "// The thread index\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  // The thread computes a single element in the result matrix C\n  if (i < M && j < N) {\n    // Compute the sum of the products of the elements in the same column of A and the same row of B\n    double sum = 0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i >= M || j >= N) {\n    return;\n  }\n\n  double sum = 0;\n  for (size_t k = 0; k < K; ++k) {\n    sum += A[i * K + k] * B[k * N + j];\n  }\n  C[i * N + j] = sum;\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M && j < N) {\n    double sum = 0.0;\n    for (size_t k = 0; k < K; ++k) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M && col < N) {\n    double sum = 0;\n    for (size_t i = 0; i < K; i++) {\n      sum += A[row * K + i] * B[i * N + col];\n    }\n    C[row * N + col] = sum;\n  }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "// Get the row and column of the thread\n    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check if the thread is outside the matrix\n    if (row >= M || col >= N) {\n        return;\n    }\n\n    // Compute the sum of the product of the elements of the row of A and the column of B\n    double sum = 0;\n    for (size_t i = 0; i < K; i++) {\n        sum += A[row * K + i] * B[i * N + col];\n    }\n\n    // Store the result in the matrix C\n    C[row * N + col] = sum;\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < K; ++k) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n  double sum = 0.0;\n  for (size_t k = 0; k < K; k++) {\n    sum += A[row * K + k] * B[k * N + col];\n  }\n  C[row * N + col] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= M || j >= N) return;\n  double sum = 0;\n  for (size_t k = 0; k < K; ++k) {\n    sum += A[i * K + k] * B[k * N + j];\n  }\n  C[i * N + j] = sum;\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i >= M || j >= N) {\n        return;\n    }\n\n    double sum = 0.0;\n    for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n    }\n\n    C[i * N + j] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < M && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    if (i < M && j < N) {\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "// Get the row and column of the thread\n  size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check if the thread is within the bounds of the matrix\n  if (row < M && col < N) {\n    // Initialize the accumulator\n    double sum = 0.0;\n\n    // Loop over the K elements of the row\n    for (size_t k = 0; k < K; k++) {\n      // Get the element of the matrices A and B at the row and column\n      double a = A[row * K + k];\n      double b = B[k * N + col];\n\n      // Accumulate the product of the elements\n      sum += a * b;\n    }\n\n    // Store the result in the matrix C\n    C[row * N + col] = sum;\n  }\n}",
            "// The row and column of the C matrix that this thread computes.\n  size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Compute the dot product of the row of A and the column of B.\n  double sum = 0;\n  for (size_t k = 0; k < K; k++) {\n    sum += A[row * K + k] * B[k * N + col];\n  }\n\n  // Store the result in the C matrix.\n  C[row * N + col] = sum;\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "cuda",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    int i = blockIdx.x;\n    int j = blockIdx.y;\n    int k = threadIdx.x;\n    double sum = 0;\n    for (int l = 0; l < K; l++) {\n        sum += A[i * K + l] * B[l * N + j];\n    }\n    C[i * N + j] = sum;\n}",
            "// TODO: Implement the kernel\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "// TODO: Implement this function.\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (size_t i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "// TODO: Implement the kernel\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n}",
            "// TODO: Implement the kernel\n    // Use the grid and block dimensions to compute the row and column of the thread\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < M && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement the kernel\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (int k = 0; k < K; k++) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}",
            "// TODO: Implement the matrix multiplication.\n    int i = blockIdx.x;\n    int j = blockIdx.y;\n    int k = threadIdx.x;\n    double sum = 0;\n    for (int kk = 0; kk < K; kk++) {\n        sum += A[i * K + kk] * B[kk * N + j];\n    }\n    C[i * N + j] = sum;\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    if (row < M && col < N) {\n        for (size_t i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    if (row < M && col < N) {\n        for (size_t k = 0; k < K; k++) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= M || j >= N) return;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < M && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < K; k++) {\n      sum += A[i * K + k] * B[k * N + j];\n    }\n    C[i * N + j] = sum;\n  }\n}",
            "size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < M && j < N) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n    // You can use the following functions to compute the result:\n    // - std::vector<double>::at()\n    // - std::vector<double>::size()\n    // - std::vector<double>::begin()\n    // - std::vector<double>::end()\n    // - std::vector<double>::iterator\n    // - std::vector<double>::const_iterator\n    // - std::vector<double>::iterator::operator*()\n    // - std::vector<double>::iterator::operator++()\n    // - std::vector<double>::iterator::operator==()\n    // - std::vector<double>::iterator::operator!=()\n    // - std::vector<double>::const_iterator::operator*()\n    // - std::vector<double>::const_iterator::operator++()\n    // - std::vector<double>::const_iterator::operator==()\n    // - std::vector<double>::const_iterator::operator!=()\n    // - std::distance()\n    // - std::min()\n    // - std::max()\n    // - std::fill()\n    // - std::copy()\n    // - std::transform()\n    // - std::inner_product()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    // - std::accumulate()\n    //",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement this function\n\n    // MPI_Barrier(MPI_COMM_WORLD);\n    // std::cout << \"rank \" << rank << \" done\" << std::endl;\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_for\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint: Use Kokkos::subview\n  // Hint:",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M),\n    KOKKOS_LAMBDA(const int i) {\n      y(i) = 0;\n      for (size_t j = 0; j < N; j++) {\n        y(i) += A(i, j) * x(j);\n      }\n    }\n  );\n}",
            "Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      y(i) += A(i, j) * x(j);\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      y(i) += A(i, j) * x(j);\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n  // You may need to use the Kokkos::parallel_for() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function\n  // You may need to use the Kokkos::subview() function",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M),\n                       KOKKOS_LAMBDA(const int i) {\n                         y(i) = 0;\n                         for (int j = 0; j < N; j++) {\n                           y(i) += A(i, j) * x(j);\n                         }\n                       });\n  Kokkos::fence();\n}",
            "// Create a parallel_for loop that will execute in parallel\n  Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      y(i) += A(i, j) * x(j);\n    }\n  );\n\n  // Force the parallel_for loop to complete before returning\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M),\n                       KOKKOS_LAMBDA(const int i) {\n                         double sum = 0;\n                         for (int j = 0; j < N; j++) {\n                           sum += A(i, j) * x(j);\n                         }\n                         y(i) = sum;\n                       });\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_for to iterate over the rows of A\n  // Hint: Use Kokkos::subview to access the rows of A\n  // Hint: Use Kokkos::subview to access the elements of x\n  // Hint: Use Kokkos::subview to access the elements of y\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::subview to access the elements of a column of A\n  // Hint: Use Kokkos::subview to access the elements of a column of x\n  // Hint: Use Kokkos::subview to access the elements of a column of y\n  // Hint: Use Kokkos::parallel_for to iterate over the columns of A\n  // Hint: Use Kokkos::subview to access the columns of A\n  // Hint: Use Kokkos::subview to access the elements of a column of A\n  // Hint: Use Kokkos::subview to access the elements of a column of x\n  // Hint: Use Kokkos::subview to access the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::subview to access the elements of a column of A\n  // Hint: Use Kokkos::subview to access the elements of a column of x\n  // Hint: Use Kokkos::subview to access the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of x\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kok",
            "// Create a Kokkos parallel_for loop to compute the dot product of each row of A with x\n  Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      y(i) += A(i, j) * x(j);\n    }\n  );\n\n  // Synchronize the memory space of y\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"gemv\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, M, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      y(i) += A(i, j) * x(j);\n    }\n  );\n}",
            "// TODO: Fill in this function\n  // Hint: Use Kokkos::parallel_for to iterate over the rows of A\n  // Hint: Use Kokkos::subview to access the rows of A\n  // Hint: Use Kokkos::subview to access the elements of x\n  // Hint: Use Kokkos::subview to access the elements of y\n  // Hint: Use Kokkos::subview to access the elements of a row of A\n  // Hint: Use Kokkos::subview to access the elements of a row of y\n  // Hint: Use Kokkos::subview to access the elements of a column of A\n  // Hint: Use Kokkos::subview to access the elements of a column of x\n  // Hint: Use Kokkos::subview to access the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of y\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of A\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a row of x\n  // Hint: Use Kokkos::parallel_reduce to sum the elements of a column of x"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "// TODO: Implement this function\n  // You can use the following variables:\n  //   - M: number of rows in A\n  //   - N: number of columns in A\n  //   - A: matrix A stored in row-major\n  //   - x: vector x\n  //   - y: vector y\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < M; i++) {\n    y[i] = 0;\n    for (size_t j = 0; j < N; j++) {\n      y[i] += A[i * N + j] * x[j];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "// TODO: Fill in your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "// TODO: implement this function\n\n  // You can use the following code to test your implementation\n  // (but you should remove it before submitting your assignment)\n  //\n  //  std::vector<double> A_test = {{1, -1, 2}, {0, -3, 1}};\n  //  std::vector<double> x_test = {2, 1, 0};\n  //  std::vector<double> y_test(M);\n  //  gemv(A_test, x_test, y_test, M, N);\n  //  std::cout << \"y_test = \" << y_test[0] << \", \" << y_test[1] << std::endl;\n  //\n  //  if (y_test[0]!= 1 || y_test[1]!= -3) {\n  //    std::cout << \"Test failed!\" << std::endl;\n  //    exit(1);\n  //  }\n}",
            "// TODO: implement this function\n    // You can use the following code as a starting point\n    //\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < M; i++) {\n    //     double sum = 0;\n    //     for (size_t j = 0; j < N; j++) {\n    //         sum += A[i * N + j] * x[j];\n    //     }\n    //     y[i] = sum;\n    // }\n}",
            "// TODO: Implement this function\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the omp_get_num_procs() function to determine the number of processors\n    // Hint: You can use the omp_get_max_threads() function to determine the maximum number of threads\n    // Hint: You can use the omp_get_num_threads() function to determine the number of threads\n    // Hint: You can use the omp_get_thread_num() function to determine the thread ID\n    // Hint: You can use the",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n    // You can use the following code as a starting point.\n    // You may want to use the following functions:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following variables:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following functions:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following variables:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following functions:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following variables:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_dynamic()\n    // - omp_get_nested()\n    // - omp_in_parallel()\n    // - omp_set_dynamic(int)\n    // - omp_set_nested(int)\n    // - omp_set_num_threads(int)\n    // - omp_get_wtime()\n    // - omp_get_wtick()\n\n    // You can also use the following functions:\n    // - omp_get_",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n    // For each row of A\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        // Initialize y[i] to zero\n        y[i] = 0;\n        // For each column of A\n        for (size_t j = 0; j < N; j++) {\n            // Add the product of A[i][j] and x[j] to y[i]\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (size_t i = 0; i < M; ++i) {\n    y[i] = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y[i] += A[i * N + j] * x[j];\n    }\n  }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n    y[i] = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y[i] += A[i * N + j] * x[j];\n    }\n  }\n}",
            "for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; i++) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
            "for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "hip",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0;\n    for (size_t col = 0; col < N; col++) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (int j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0.0;\n    for (size_t col = 0; col < N; col++) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0;\n    for (size_t col = 0; col < N; col++) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
            "// TODO: Implement this function\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (size_t col = 0; col < N; col++) {\n            sum += A[row * N + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0;\n    for (size_t col = 0; col < N; col++) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rows_per_proc = M / size;\n    int start_row = rank * rows_per_proc;\n    int end_row = (rank + 1) * rows_per_proc;\n    if (rank == size - 1) {\n        end_row = M;\n    }\n    std::vector<double> local_y(rows_per_proc);\n    for (size_t i = 0; i < rows_per_proc; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[start_row + i + j * M] * x[j];\n        }\n        local_y[i] = sum;\n    }\n    if (rank == 0) {\n        for (size_t i = 1; i < size; i++) {\n            MPI_Recv(&local_y[0], rows_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&local_y[0], rows_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        for (size_t i = 0; i < rows_per_proc; i++) {\n            y[start_row + i] = local_y[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; i++) {\n            double sum = 0;\n            for (size_t j = 0; j < N; j++) {\n                sum += A[i * N + j] * x[j];\n            }\n            y[i] = sum;\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; i++) {\n            y[i] = 0;\n            for (size_t j = 0; j < N; j++) {\n                y[i] += A[i * N + j] * x[j];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        for (size_t i = 0; i < M; i++) {\n            y[i] = 0;\n            for (size_t j = 0; j < N; j++) {\n                y[i] += A[i * N + j] * x[j];\n            }\n        }\n    }\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: implement this function\n\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "cuda",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < M) {\n    double sum = 0;\n    for (size_t j = 0; j < N; j++) {\n      sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (size_t col = 0; col < N; col++) {\n            sum += A[row * N + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (size_t col = 0; col < N; col++) {\n            sum += A[row * N + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (size_t col = 0; col < N; col++) {\n            sum += A[row * N + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (int i = 0; i < N; i++) {\n            sum += A[row * N + i] * x[i];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        double sum = 0;\n        for (size_t col = 0; col < N; col++) {\n            sum += A[row * N + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
            "// TODO: implement this\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Fill in your code here\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Bcast(&M, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(A.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::vector<double> y_local(M);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y_local[i] = sum;\n    }\n\n    MPI_Reduce(y_local.data(), y.data(), M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunk = M / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = M;\n    }\n    std::vector<double> local_y(chunk);\n    for (int i = start; i < end; i++) {\n        double sum = 0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - start] = sum;\n    }\n    MPI_Reduce(local_y.data(), y.data(), chunk, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Bcast(&M, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<double> local_A(M * N);\n    std::vector<double> local_x(N);\n    std::vector<double> local_y(M);\n\n    if (rank == 0) {\n        local_A = A;\n        local_x = x;\n    }\n\n    MPI_Scatter(A.data(), M * N, MPI_DOUBLE, local_A.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(x.data(), N, MPI_DOUBLE, local_x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < M; i++) {\n        double sum = 0;\n        for (size_t j = 0; j < N; j++) {\n            sum += local_A[i * N + j] * local_x[j];\n        }\n        local_y[i] = sum;\n    }\n\n    MPI_Gather(local_y.data(), M, MPI_DOUBLE, y.data(), M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y are stored in the same order on all ranks\n\n    // TODO: Your code here\n    // You may assume that A and x have already been allocated and have the correct size\n    // You may assume that y has already been allocated and has the correct size\n    // You may assume that M and N have been set correctly\n    // You may not assume that A, x, and y",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    // TODO: Your code here\n\n}",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "// TODO: implement\n\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(Kokkos::View<double**> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i > j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n        for (size_t k = j + 1; k < N; k++) {\n          A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n        }\n      }\n    }\n  }\n}",
            "// TODO: Fill in the code to implement the LU factorization\n\n  // For each row\n  for (size_t i = 0; i < N; i++) {\n    // For each column\n    for (size_t j = 0; j < N; j++) {\n      // TODO: Compute the value of the LU factorization\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] /= A[j * N + j];\n            }\n            if (i < j) {\n                A[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] /= A[j * N + j];\n            }\n            if (i < j) {\n                A[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] /= A[j * N + j];\n                for (size_t k = j + 1; k < N; k++) {\n                    A[i * N + k] -= A[i * N + j] * A[j * N + k];\n                }\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < i; ++j) {\n      A[i * N + j] /= A[j * N + j];\n      for (size_t k = j + 1; k < N; ++k) {\n        A[i * N + k] -= A[i * N + j] * A[j * N + k];\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] = A[i * N + j] / A[j * N + j];\n            } else if (i < j) {\n                A[i * N + j] = A[i * N + j] - A[i * N + j] * A[j * N + j];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i == j) {\n        A[i * N + j] = 1;\n      } else if (i < j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n      } else {\n        A[i * N + j] = A[i * N + j] - A[i * N + k] * A[k * N + j];\n      }\n    }\n  }\n}",
            "// TODO: Fill in your code here\n\n}",
            "// TODO: implement this function\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            A[i * N + j] /= A[j * N + j];\n            for (size_t k = j + 1; k < N; ++k) {\n                A[i * N + k] -= A[i * N + j] * A[j * N + k];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = i; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < i; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = A[i * N + j] - sum;\n        }\n        for (size_t j = i + 1; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < i; k++) {\n                sum += A[j * N + k] * A[k * N + i];\n            }\n            A[j * N + i] = (A[j * N + i] - sum) / A[i * N + i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] = A[i * N + j] / A[j * N + j];\n            }\n            else if (i < j) {\n                A[i * N + j] = A[i * N + j] - A[i * N + j] * A[j * N + j];\n            }\n        }\n    }\n}",
            "// TODO: Implement me!\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i == j) {\n        A[i * N + j] = 1;\n      } else {\n        A[i * N + j] = 0;\n      }\n    }\n  }\n\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      double sum = 0;\n      for (size_t j = 0; j < k; j++) {\n        sum += A[i * N + j] * A[k * N + j];\n      }\n      A[i * N + k] = (A[i * N + k] - sum) / A[k * N + k];\n    }\n  }\n}",
            "for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[i * N + j] = A[i * N + j] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == j) {\n                continue;\n            }\n            A[i * N + j] /= A[j * N + j];\n            for (size_t k = 0; k < N; k++) {\n                if (k == j) {\n                    continue;\n                }\n                A[i * N + k] -= A[i * N + j] * A[j * N + k];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == j) {\n                A[i * N + j] = 1;\n            } else if (i > j) {\n                double sum = 0;\n                for (size_t k = 0; k < j; k++) {\n                    sum += A[i * N + k] * A[k * N + j];\n                }\n                A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < i; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = (A[i * N + j] - sum) / A[i * N + i];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[i * N + j] = A[i * N + j] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[k * N + j] = A[k * N + j] - A[k * N + i] * A[i * N + j];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i > j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n        for (size_t k = j + 1; k < N; k++) {\n          A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n        }\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] = A[i * N + j] / A[j * N + j];\n                for (size_t k = j + 1; k < N; k++) {\n                    A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n                }\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                A[i * N + j] /= A[j * N + j];\n            }\n            if (i < j) {\n                A[i * N + j] -= A[i * N + j] * A[j * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            A[i * N + j] /= A[i * N + i];\n            for (size_t k = i + 1; k < N; k++) {\n                A[j * N + k] -= A[j * N + i] * A[i * N + k];\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i > j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n        for (size_t k = j + 1; k < N; k++) {\n          A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n        }\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == j) {\n                continue;\n            }\n            double sum = 0;\n            for (size_t k = 0; k < i; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = (A[i * N + j] - sum) / A[i * N + i];\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == j) {\n                continue;\n            }\n            double sum = 0;\n            for (size_t k = 0; k < i; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = (A[i * N + j] - sum) / A[i * N + i];\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (i > j) {\n        double sum = 0;\n        for (size_t k = 0; k < j; ++k) {\n          sum += A[i * N + k] * A[k * N + j];\n        }\n        A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n      } else if (i < j) {\n        double sum = 0;\n        for (size_t k = 0; k < i; ++k) {\n          sum += A[i * N + k] * A[k * N + j];\n        }\n        A[i * N + j] = A[i * N + j] - sum;\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[i * N + j] /= A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] -= A[i * N + k] * A[i * N + j];\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                continue;\n            }\n            double sum = 0;\n            for (size_t k = 0; k < j; ++k) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                A[i * N + j] = 1;\n            } else if (i > j) {\n                A[i * N + j] = A[i * N + j] / A[j * N + j];\n            } else {\n                A[i * N + j] = A[i * N + j] - A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (i > j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n        for (size_t k = j + 1; k < N; ++k) {\n          A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n        }\n      }\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "hip",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Get the row and column of the thread\n  size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Do not process out of bounds elements\n  if (row >= N || col >= N) return;\n\n  // Compute the LU factorization\n  if (row < col) {\n    A[row * N + col] = A[row * N + col] / A[col * N + col];\n  } else if (row == col) {\n    A[row * N + col] = 1.0;\n  } else {\n    A[row * N + col] = 0.0;\n  }\n}",
            "int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (row < N && col < N) {\n    if (row < col) {\n      A[row * N + col] = A[row * N + col] / A[col * N + col];\n    } else if (row > col) {\n      A[row * N + col] = A[row * N + col] - A[row * N + col] * A[col * N + col];\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] /= A[j * N + j];\n            for (size_t k = j + 1; k < N; k++) {\n                A[i * N + k] -= A[i * N + j] * A[j * N + k];\n            }\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    double sum = 0;\n    for (size_t k = 0; k < i; k++) {\n      sum += A[i * N + k] * A[k * N + j];\n    }\n    A[i * N + j] = A[i * N + j] - sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (i > j) {\n    A[i * N + j] = A[j * N + i] / A[j * N + j];\n  } else if (i == j) {\n    A[i * N + j] = 1.0;\n  } else {\n    A[i * N + j] = 0.0;\n  }\n}",
            "// Get the row and column of the thread\n    size_t row = blockIdx.x;\n    size_t col = blockIdx.y;\n\n    // Get the size of the matrix\n    size_t size = N * N;\n\n    // Get the index of the thread\n    size_t index = row * N + col;\n\n    // If the thread is not in the main diagonal\n    if (row!= col) {\n        // Get the value of the element in the main diagonal\n        double mainDiagonal = A[row * N + row];\n\n        // Compute the value of the element in the current position\n        double value = A[index] / mainDiagonal;\n\n        // Store the value in the current position\n        A[index] = value;\n\n        // Iterate over the rows below the main diagonal\n        for (size_t i = row + 1; i < N; i++) {\n            // Get the index of the element in the row below the main diagonal\n            size_t indexBelow = i * N + col;\n\n            // Compute the value of the element in the row below the main diagonal\n            double valueBelow = A[indexBelow] - value * A[i * N + row];\n\n            // Store the value in the row below the main diagonal\n            A[indexBelow] = valueBelow;\n        }\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t col = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (row < N && col < N) {\n    double sum = 0.0;\n    for (size_t i = 0; i < row; i++) {\n      sum += A[row * N + i] * A[i * N + col];\n    }\n    A[row * N + col] = A[row * N + col] - sum;\n  }\n}",
            "// Get the row and column of the thread\n    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // If the thread is outside the matrix, return\n    if (row >= N || col >= N) return;\n\n    // If the thread is on the diagonal, set the diagonal to 1\n    if (row == col) {\n        A[row * N + col] = 1;\n        return;\n    }\n\n    // Get the value of the element\n    double value = A[row * N + col];\n\n    // If the thread is below the diagonal, set the value to 0\n    if (row > col) {\n        A[row * N + col] = 0;\n        return;\n    }\n\n    // If the thread is above the diagonal, compute the value\n    double sum = 0;\n    for (size_t i = 0; i < col; i++) {\n        sum += A[row * N + i] * A[i * N + col];\n    }\n    A[row * N + col] = value - sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[j * N + i] / A[j * N + j];\n    } else if (i == j) {\n      A[i * N + j] = 1.0;\n    } else {\n      A[i * N + j] = 0.0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[j * N + i] / A[j * N + j];\n    } else if (i == j) {\n      A[i * N + j] = 1.0;\n    } else {\n      A[i * N + j] = 0.0;\n    }\n  }\n}",
            "int row = blockIdx.x * blockDim.x + threadIdx.x;\n  int col = blockIdx.y * blockDim.y + threadIdx.y;\n  if (row < N && col < N) {\n    if (row > col) {\n      double sum = 0.0;\n      for (int i = 0; i < col; i++) {\n        sum += A[row * N + i] * A[col * N + i];\n      }\n      A[row * N + col] = (A[row * N + col] - sum) / A[col * N + col];\n    } else if (row == col) {\n      double sum = 0.0;\n      for (int i = 0; i < row; i++) {\n        sum += A[row * N + i] * A[col * N + i];\n      }\n      A[row * N + col] = A[row * N + col] - sum;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < j; k++) {\n        sum += A[i * N + k] * A[k * N + j];\n      }\n      A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n    } else if (i == j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < i; k++) {\n        sum += A[i * N + k] * A[k * N + j];\n      }\n      A[i * N + j] = A[i * N + j] - sum;\n    } else {\n      A[i * N + j] = 0.0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[j * N + i] / A[j * N + j];\n    } else if (i == j) {\n      A[i * N + j] = 1.0;\n    } else {\n      A[i * N + j] = 0.0;\n    }\n  }\n}",
            "int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < N) {\n    double sum = 0;\n    for (int i = 0; i < row; i++) {\n      sum += A[row * N + i] * A[i * N + col];\n    }\n    A[row * N + col] = A[row * N + col] - sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  if (i == j) {\n    A[i * N + j] = 1.0;\n    return;\n  }\n  double sum = 0.0;\n  for (size_t k = 0; k < i; k++) {\n    sum += A[i * N + k] * A[k * N + j];\n  }\n  A[i * N + j] = A[i * N + j] - sum;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  if (i > j) {\n    double sum = 0.0;\n    for (size_t k = 0; k < j; k++) {\n      sum += A[i * N + k] * A[j * N + k];\n    }\n    A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n  } else if (i == j) {\n    double sum = 0.0;\n    for (size_t k = 0; k < i; k++) {\n      sum += A[i * N + k] * A[j * N + k];\n    }\n    A[i * N + j] = A[i * N + j] - sum;\n  }\n}",
            "size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < N && col < N) {\n        double sum = 0;\n        for (size_t i = 0; i < row; i++) {\n            sum += A[row * N + i] * A[i * N + col];\n        }\n        A[row * N + col] = A[row * N + col] - sum;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i >= N || j >= N) {\n    return;\n  }\n\n  if (i == j) {\n    A[i * N + j] = 1;\n  } else if (i > j) {\n    A[i * N + j] = A[i * N + j] / A[j * N + j];\n  } else {\n    double sum = 0;\n    for (size_t k = 0; k < j; k++) {\n      sum += A[i * N + k] * A[k * N + j];\n    }\n    A[i * N + j] = A[i * N + j] - sum;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i >= N || j >= N)\n    return;\n\n  if (i > j) {\n    double sum = 0;\n    for (size_t k = 0; k < j; k++)\n      sum += A[i * N + k] * A[k * N + j];\n    A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n  } else if (i == j) {\n    double sum = 0;\n    for (size_t k = 0; k < j; k++)\n      sum += A[i * N + k] * A[k * N + j];\n    A[i * N + j] = A[i * N + j] - sum;\n  } else {\n    A[i * N + j] = 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  if (i > j) {\n    A[i * N + j] = A[j * N + i] / A[j * N + j];\n  } else if (i == j) {\n    A[i * N + j] = 1.0;\n  } else {\n    A[i * N + j] = 0.0;\n  }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N;\n    int n_block = n / size;\n    int n_block_remain = n % size;\n    int n_block_rank = n_block + (rank < n_block_remain? 1 : 0);\n    int n_block_rank_remain = n_block + (rank >= n_block_remain? 1 : 0);\n    int n_block_rank_start = n_block * rank + std::min(rank, n_block_remain);\n    int n_block_rank_end = n_block_rank_start + n_block_rank;\n\n    std::vector<double> A_rank(n_block_rank * n_block_rank);\n    std::vector<double> L_rank(n_block_rank * n_block_rank);\n    std::vector<double> U_rank(n_block_rank * n_block_rank);\n\n    for (int i = 0; i < n_block_rank; i++) {\n        for (int j = 0; j < n_block_rank; j++) {\n            A_rank[i * n_block_rank + j] = A[n_block_rank_start + i * n_block_rank_end + j];\n        }\n    }\n\n    for (int i = 0; i < n_block_rank; i++) {\n        for (int j = 0; j < n_block_rank; j++) {\n            if (i > j) {\n                L_rank[i * n_block_rank + j] = A_rank[i * n_block_rank + j] / A_rank[j * n_block_rank + j];\n            } else if (i == j) {\n                L_rank[i * n_block_rank + j] = 1;\n            } else {\n                L_rank[i * n_block_rank + j] = 0;\n            }\n        }\n    }\n\n    for (int i = 0; i < n_block_rank; i++) {\n        for (int j = 0; j < n_block_rank; j++) {\n            if (i < j) {\n                U_rank[i * n_block_rank + j] = A_rank[i * n_block_rank + j] / A_rank[j * n_block_rank + j];\n            } else if (i == j) {\n                U_rank[i * n_block_rank + j] = 1;\n            } else {\n                U_rank[i * n_block_rank + j] = 0;\n            }\n        }\n    }\n\n    for (int i = 0; i < n_block_rank; i++) {\n        for (int j = 0; j < n_block_rank; j++) {\n            A_rank[i * n_block_rank + j] = L_rank[i * n_block_rank + j] * U_rank[i * n_block_rank + j];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_block_rank; i++) {\n            for (int j = 0; j < n_block_rank; j++) {\n                A[n_block_rank_start + i * n_block_rank_end + j] = A_rank[i * n_block_rank + j];\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n}",
            "// TODO: your code here\n}",
            "// TODO: your code here\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "cuda",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (i < j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i > j) {\n            A[i * N + j] = A[i * N + j] - A[i * N + j] * A[j * N + j];\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1.0;\n        }\n    }\n}",
            "// TODO: implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[i * N + j] / A[j * N + j];\n      for (int k = j + 1; k < N; k++) {\n        A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1;\n        } else {\n            A[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    if (i == j) {\n        A[i * N + j] = 1.0;\n    } else if (i > j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n    } else if (i < j) {\n        A[i * N + j] = 0.0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1;\n        } else {\n            A[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[i * N + j] / A[j * N + j];\n      for (int k = j + 1; k < N; k++) {\n        A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i < j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i > j) {\n            A[i * N + j] = A[i * N + j] - A[i * N + j] * A[j * N + j];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i == j) {\n            A[i * N + j] = 1;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        if (i > j) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = (A[i * N + j] - sum) / A[j * N + j];\n        } else if (i == j) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] = A[i * N + j] - sum;\n        }\n    }\n}",
            "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t col = blockIdx.y * blockDim.y + threadIdx.y;\n  if (row < N && col < N) {\n    if (row > col) {\n      A[row * N + col] = A[row * N + col] / A[col * N + col];\n      for (size_t i = col + 1; i < N; i++) {\n        A[row * N + i] = A[row * N + i] - A[row * N + col] * A[col * N + i];\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    if (i > j) {\n      A[i * N + j] = A[i * N + j] / A[j * N + j];\n      for (int k = j + 1; k < N; k++) {\n        A[i * N + k] = A[i * N + k] - A[i * N + j] * A[j * N + k];\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        if (i > j) {\n            A[i * N + j] = A[i * N + j] / A[j * N + j];\n        } else if (i < j) {\n            A[i * N + j] = 0;\n        }\n    }\n}",
            "int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < N) {\n    double sum = 0;\n    for (int i = 0; i < row; i++) {\n      sum += A[row * N + i] * A[i * N + col];\n    }\n    A[row * N + col] = A[row * N + col] - sum;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    if (i == j) {\n        A[i * N + j] = 1;\n    } else if (i < j) {\n        A[i * N + j] = A[i * N + j] / A[j * N + j];\n    } else {\n        A[i * N + j] = A[i * N + j] - A[i * N + k] * A[k * N + j];\n    }\n}"
        ]
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n    // You can use the following functions:\n    // - omp_get_num_threads()\n    // - omp_get_thread_num()\n    // - MPI_Comm_rank()\n    // - MPI_Comm_size()\n    // - MPI_Send()\n    // - MPI_Recv()\n    // - MPI_Bcast()\n    // - MPI_Reduce()\n    // - MPI_Allreduce()\n    // - MPI_Scatter()\n    // - MPI_Gather()\n    // - MPI_Scatterv()\n    // - MPI_Gatherv()\n    // - MPI_Allgather()\n    // - MPI_Allgatherv()\n    // - MPI_Alltoall()\n    // - MPI_Alltoallv()\n    // - MPI_Alltoallw()\n    // - MPI_Barrier()\n    // - MPI_Wait()\n    // - MPI_Waitall()\n    // - MPI_Waitany()\n    // - MPI_Waitall()\n    // - MPI_Waitany()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()\n    // - MPI_Wait()",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: your code here\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N;\n    int n_local = n / size;\n    int n_local_rest = n % size;\n\n    int start = rank * n_local;\n    int end = start + n_local;\n\n    if (rank == 0) {\n        end += n_local_rest;\n    }\n\n    if (rank == size - 1) {\n        end = n;\n    }\n\n    std::vector<double> A_local(n_local * n_local);\n    std::vector<double> L_local(n_local * n_local);\n    std::vector<double> U_local(n_local * n_local);\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n_local; j++) {\n            A_local[i * n_local + j] = A[start + i * n + j];\n        }\n    }\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n_local; j++) {\n            L_local[i * n_local + j] = 0;\n            U_local[i * n_local + j] = 0;\n        }\n    }\n\n    for (int i = 0; i < n_local; i++) {\n        L_local[i * n_local + i] = 1;\n    }\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n_local; j++) {\n            if (i > j) {\n                double sum = 0;\n                for (int k = 0; k < j; k++) {\n                    sum += L_local[i * n_local + k] * U_local[k * n_local + j];\n                }\n                U_local[i * n_local + j] = (A_local[i * n_local + j] - sum) / L_local[j * n_local + j];\n            }\n            if (i <= j) {\n                double sum = 0;\n                for (int k = 0; k < i; k++) {\n                    sum += L_local[k * n_local + i] * U_local[k * n_local + j];\n                }\n                L_local[i * n_local + j] = (A_local[i * n_local + j] - sum) / U_local[i * n_local + i];\n            }\n        }\n    }\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n_local; j++) {\n            A[start + i * n + j] = L_local[i * n_local + j];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            for (int j = 0; j < n_local; j++) {\n                A[i * n + j] = U_local[i * n_local + j];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = N;\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n    if (rank == size - 1) {\n        end += n_rem;\n    }\n    int n_local = end - start;\n    std::vector<double> A_local(n_local * n_local);\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n_local; j++) {\n            A_local[i * n_local + j] = A[start + i * n + j];\n        }\n    }\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for (int j = 0; j < n_local; j++) {\n    //         printf(\"%f \", A_local[i * n_local + j]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n    // printf(\"\\n\");\n    // printf(\"rank %d: %d %d %d %d\\n\", rank, start, end, n_local, n_per_proc);\n    // for (int i = 0; i < n_local; i++) {\n    //     for",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use Kokkos to sort in parallel. Assume Kokkos is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(Kokkos::View<int*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "int n = x.size();\n  int n1 = n/2;\n  int n2 = n - n1;\n  Kokkos::View<int*> x1(\"x1\", n1);\n  Kokkos::View<int*> x2(\"x2\", n2);\n  Kokkos::parallel_for(n1, KOKKOS_LAMBDA(int i) {\n    x1(i) = x(i);\n  });\n  Kokkos::parallel_for(n2, KOKKOS_LAMBDA(int i) {\n    x2(i) = x(i + n1);\n  });\n  Kokkos::parallel_sort(x1);\n  Kokkos::parallel_sort(x2);\n  Kokkos::parallel_for(n1, KOKKOS_LAMBDA(int i) {\n    x(i) = x1(n1 - i - 1);\n  });\n  Kokkos::parallel_for(n2, KOKKOS_LAMBDA(int i) {\n    x(i + n1) = x2(i);\n  });\n}",
            "// TODO: Your code here\n\n}",
            "int n = x.size();\n  if (n <= 1) return;\n  int n2 = n / 2;\n  Kokkos::View<int*> x2(\"x2\", n2);\n  Kokkos::parallel_for(n2, KOKKOS_LAMBDA(int i) {\n    x2(i) = x(i);\n  });\n  Kokkos::parallel_sort(x2);\n  Kokkos::parallel_for(n2, KOKKOS_LAMBDA(int i) {\n    x(i) = x2(n2 - i - 1);\n  });\n  if (n % 2 == 1) {\n    Kokkos::parallel_for(1, KOKKOS_LAMBDA(int i) {\n      x(n2) = x(n2 + 1);\n    });\n  }\n}",
            "// TODO: implement this function\n}",
            "int n = x.size();\n  int n_half = n / 2;\n  int n_odd = n % 2;\n  int n_half_odd = n_half % 2;\n  int n_half_even = n_half - n_half_odd;\n  int n_first_half = n_half + n_odd;\n\n  Kokkos::View<int*> x_first_half(\"x_first_half\", n_first_half);\n  Kokkos::View<int*> x_second_half(\"x_second_half\", n - n_first_half);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n_first_half),\n    KOKKOS_LAMBDA(const int i) {\n      x_first_half(i) = x(i);\n    });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n - n_first_half),\n    KOKKOS_LAMBDA(const int i) {\n      x_second_half(i) = x(i + n_first_half);\n    });\n\n  Kokkos::parallel_sort(x_first_half);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n_first_half),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = x_first_half(i);\n    });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n - n_first_half),\n    KOKKOS_LAMBDA(const int i) {\n      x(i + n_first_half) = x_second_half(i);\n    });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use OpenMP to sort in parallel.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int n_2 = n/2;\n  int n_1 = n - n_2;\n  int n_2_1 = n_2 + 1;\n  int n_2_2 = n_2 + 2;\n  int n_2_3 = n_2 + 3;\n  int n_2_4 = n_2 + 4;\n  int n_2_5 = n_2 + 5;\n  int n_2_6 = n_2 + 6;\n  int n_2_7 = n_2 + 7;\n  int n_2_8 = n_2 + 8;\n  int n_2_9 = n_2 + 9;\n  int n_2_10 = n_2 + 10;\n  int n_2_11 = n_2 + 11;\n  int n_2_12 = n_2 + 12;\n  int n_2_13 = n_2 + 13;\n  int n_2_14 = n_2 + 14;\n  int n_2_15 = n_2 + 15;\n  int n_2_16 = n_2 + 16;\n  int n_2_17 = n_2 + 17;\n  int n_2_18 = n_2 + 18;\n  int n_2_19 = n_2 + 19;\n  int n_2_20 = n_2 + 20;\n  int n_2_21 = n_2 + 21;\n  int n_2_22 = n_2 + 22;\n  int n_2_23 = n_2 + 23;\n  int n_2_24 = n_2 + 24;\n  int n_2_25 = n_2 + 25;\n  int n_2_26 = n_2 + 26;\n  int n_2_27 = n_2 + 27;\n  int n_2_28 = n_2 + 28;\n  int n_2_29 = n_2 + 29;\n  int n_2_30 = n_2 + 30;\n  int n_2_31 = n_2 + 31;\n  int n_2_32 = n_2 + 32;\n  int n_2_33 = n_2 + 33;\n  int n_2_34 = n_2 + 34;\n  int n_2_35 = n_2 + 35;\n  int n_2_36 = n_2 + 36;\n  int n_2_37 = n_2 + 37;\n  int n_2_38 = n_2 + 38;\n  int n_2_39 = n_2 + 39;\n  int n_2_40 = n_2 + 40;\n  int n_2_41 = n_2 + 41;\n  int n_2_42 = n_2 + 42;\n  int n_2_43 = n_2 + 43;\n  int n_2_44 = n_2 + 44;\n  int n_2_45 = n_2 + 45;\n  int n_2_46 = n_2 + 46;\n  int n_2_47 = n_2 + 47;\n  int n_2_48 = n_2 + 48;\n  int n_2_49 = n_2 + 49;\n  int n_2_50 = n_2 + 50;\n  int n_2_51 = n_2 + 51;\n  int n_2_52 = n_2 + 52;\n  int n_2_53 = n_2 + 53;\n  int n_2_54 = n_2 + 54;\n  int n_2_55 = n_2 + 55;\n  int n_2_56 = n",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  int n2 = n/2;\n  int n1 = n - n2;\n  std::vector<int> x1(n1);\n  std::vector<int> x2(n2);\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      #pragma omp task\n      {\n        for (int i = 0; i < n1; i++) {\n          x1[i] = x[i];\n        }\n      }\n      #pragma omp task\n      {\n        for (int i = n1; i < n; i++) {\n          x2[i-n1] = x[i];\n        }\n      }\n      #pragma omp taskwait\n      std::sort(x1.begin(), x1.end(), std::greater<int>());\n      for (int i = 0; i < n1; i++) {\n        x[i] = x1[i];\n      }\n      for (int i = 0; i < n2; i++) {\n        x[i+n1] = x2[i];\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n    int n = x.size();\n    int n2 = n/2;\n    int n1 = n - n2;\n    std::vector<int> y(n2);\n    #pragma omp parallel for\n    for(int i = 0; i < n2; i++){\n        y[i] = x[i];\n    }\n    std::sort(y.begin(), y.end());\n    std::reverse(y.begin(), y.end());\n    #pragma omp parallel for\n    for(int i = 0; i < n2; i++){\n        x[i] = y[i];\n    }\n    if(n % 2 == 1){\n        #pragma omp parallel for\n        for(int i = n2; i < n; i++){\n            x[i] = x[i+1];\n        }\n    }\n}",
            "int n = x.size();\n  if (n < 2) {\n    return;\n  }\n  int n2 = n / 2;\n  int n1 = n - n2;\n  std::vector<int> x1(n1);\n  std::vector<int> x2(n2);\n  for (int i = 0; i < n1; i++) {\n    x1[i] = x[i];\n  }\n  for (int i = 0; i < n2; i++) {\n    x2[i] = x[i + n1];\n  }\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      sortDescending(x1);\n      sortDescending(x2);\n    }\n  }\n  for (int i = 0; i < n1; i++) {\n    x[i] = x1[i];\n  }\n  for (int i = 0; i < n2; i++) {\n    x[i + n1] = x2[i];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int n2 = n/2;\n  int n1 = n - n2;\n  int n1_2 = n1/2;\n  int n2_2 = n2/2;\n  int n1_2_2 = n1_2/2;\n  int n2_2_2 = n2_2/2;\n  int n1_2_2_2 = n1_2_2/2;\n  int n2_2_2_2 = n2_2_2/2;\n  int n1_2_2_2_2 = n1_2_2_2/2;\n  int n2_2_2_2_2 = n2_2_2_2/2;\n  int n1_2_2_2_2_2 = n1_2_2_2_2/2;\n  int n2_2_2_2_2_2 = n2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2 = n1_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2 = n2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2_2_2_2_2 = n1_2_2_2_2_2_2_2_2_2_2_2_2/2;\n  int n2_2_2_2_2_2_2_2_2_2_2_2_2_2 = n2_2_2_2_2_2_2_2_2_2_2_2_2/2;\n  int n1_2_2_2_2_2_2_2_2_2",
            "// TODO: Your code here\n    int n = x.size();\n    int n1 = n/2;\n    int n2 = n - n1;\n    std::vector<int> y(n1);\n    std::vector<int> z(n2);\n    #pragma omp parallel for\n    for(int i = 0; i < n1; i++)\n    {\n        y[i] = x[i];\n    }\n    #pragma omp parallel for\n    for(int i = 0; i < n2; i++)\n    {\n        z[i] = x[i+n1];\n    }\n    std::sort(y.begin(), y.end());\n    std::reverse(y.begin(), y.end());\n    #pragma omp parallel for\n    for(int i = 0; i < n1; i++)\n    {\n        x[i] = y[i];\n    }\n    #pragma omp parallel for\n    for(int i = 0; i < n2; i++)\n    {\n        x[i+n1] = z[i];\n    }\n}",
            "int n = x.size();\n    int n_2 = n / 2;\n    int n_2_1 = n_2 + 1;\n    int n_2_2 = n_2 + 2;\n    int n_2_3 = n_2 + 3;\n    int n_2_4 = n_2 + 4;\n    int n_2_5 = n_2 + 5;\n    int n_2_6 = n_2 + 6;\n    int n_2_7 = n_2 + 7;\n    int n_2_8 = n_2 + 8;\n    int n_2_9 = n_2 + 9;\n    int n_2_10 = n_2 + 10;\n    int n_2_11 = n_2 + 11;\n    int n_2_12 = n_2 + 12;\n    int n_2_13 = n_2 + 13;\n    int n_2_14 = n_2 + 14;\n    int n_2_15 = n_2 + 15;\n    int n_2_16 = n_2 + 16;\n    int n_2_17 = n_2 + 17;\n    int n_2_18 = n_2 + 18;\n    int n_2_19 = n_2 + 19;\n    int n_2_20 = n_2 + 20;\n    int n_2_21 = n_2 + 21;\n    int n_2_22 = n_2 + 22;\n    int n_2_23 = n_2 + 23;\n    int n_2_24 = n_2 + 24;\n    int n_2_25 = n_2 + 25;\n    int n_2_26 = n_2 + 26;\n    int n_2_27 = n_2 + 27;\n    int n_2_28 = n_2 + 28;\n    int n_2_29 = n_2 + 29;\n    int n_2_30 = n_2 + 30;\n    int n_2_31 = n_2 + 31;\n    int n_2_32 = n_2 + 32;\n    int n_2_33 = n_2 + 33;\n    int n_2_34 = n_2 + 34;\n    int n_2_35 = n_2 + 35;\n    int n_2_36 = n_2 + 36;\n    int n_2_37 = n_2 + 37;\n    int n_2_38 = n_2 + 38;\n    int n_2_39 = n_2 + 39;\n    int n_2_40 = n_2 + 40;\n    int n_2_41 = n_2 + 41;\n    int n_2_42 = n_2 + 42;\n    int n_2_43 = n_2 + 43;\n    int n_2_44 = n_2 + 44;\n    int n_2_45 = n_2 + 45;\n    int n_2_46 = n_2 + 46;\n    int n_2_47 = n_2 + 47;\n    int n_2_48 = n_2 + 48;\n    int n_2_49 = n_2 + 49;\n    int n_2_50 = n_2 + 50;\n    int n_2_51 = n_2 + 51;\n    int n_2_52 = n_2 + 52;\n    int n_2_53 = n_2 + 53;\n    int n_2_54 = n_2 + 54;\n    int n_2_55 = n_2 + 55;\n    int n_2_56 = n_2 + 56;\n    int n_2_57 = n_2",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int nthreads = omp_get_num_threads();\n    int tid = omp_get_thread_num();\n    int chunk = n/nthreads;\n    int start = tid*chunk;\n    int end = (tid+1)*chunk;\n    if(tid == nthreads-1) end = n;\n    std::vector<int> temp(n);\n    for(int i = start; i < end; i++) temp[i] = x[i];\n    std::sort(temp.begin(), temp.end(), std::greater<int>());\n    for(int i = start; i < end; i++) x[i] = temp[i];\n}",
            "// TODO: implement this function\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int n_2 = n/2;\n  int n_1 = n - n_2;\n  int *x_1 = new int[n_1];\n  int *x_2 = new int[n_2];\n  int i;\n  #pragma omp parallel for\n  for(i = 0; i < n_1; i++){\n    x_1[i] = x[i];\n  }\n  #pragma omp parallel for\n  for(i = 0; i < n_2; i++){\n    x_2[i] = x[i+n_1];\n  }\n  std::sort(x_1, x_1+n_1);\n  std::reverse(x_1, x_1+n_1);\n  std::sort(x_2, x_2+n_2);\n  std::reverse(x_2, x_2+n_2);\n  #pragma omp parallel for\n  for(i = 0; i < n_1; i++){\n    x[i] = x_1[i];\n  }\n  #pragma omp parallel for\n  for(i = 0; i < n_2; i++){\n    x[i+n_1] = x_2[i];\n  }\n  delete [] x_1;\n  delete [] x_2;\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "serial",
        "prompt": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n  int mid = x.size() / 2;\n  std::vector<int> firstHalf;\n  for (int i = 0; i < mid; i++) {\n    firstHalf.push_back(x[i]);\n  }\n  std::sort(firstHalf.begin(), firstHalf.end(), std::greater<int>());\n  for (int i = 0; i < mid; i++) {\n    x[i] = firstHalf[i];\n  }\n}",
            "int middle = x.size() / 2;\n    std::vector<int> firstHalf(x.begin(), x.begin() + middle);\n    std::sort(firstHalf.begin(), firstHalf.end(), std::greater<int>());\n    std::copy(firstHalf.begin(), firstHalf.end(), x.begin());\n}",
            "// TODO: Your code here\n    int mid = x.size() / 2;\n    int left = 0;\n    int right = mid;\n    std::vector<int> temp;\n    while (left < mid && right < x.size()) {\n        if (x[left] > x[right]) {\n            temp.push_back(x[left]);\n            left++;\n        } else {\n            temp.push_back(x[right]);\n            right++;\n        }\n    }\n    while (left < mid) {\n        temp.push_back(x[left]);\n        left++;\n    }\n    while (right < x.size()) {\n        temp.push_back(x[right]);\n        right++;\n    }\n    x = temp;\n}",
            "// TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int i = 0;\n    int j = mid;\n    int k = 0;\n    std::vector<int> temp(mid);\n    while(i < mid && j < n){\n        if(x[i] > x[j]){\n            temp[k] = x[i];\n            i++;\n        }\n        else{\n            temp[k] = x[j];\n            j++;\n        }\n        k++;\n    }\n    while(i < mid){\n        temp[k] = x[i];\n        i++;\n        k++;\n    }\n    while(j < n){\n        temp[k] = x[j];\n        j++;\n        k++;\n    }\n    for(int i = 0; i < mid; i++){\n        x[i] = temp[i];\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int mid = n/2;\n  std::vector<int> firstHalf;\n  for(int i = 0; i < mid; i++){\n    firstHalf.push_back(x[i]);\n  }\n  std::sort(firstHalf.begin(), firstHalf.end(), std::greater<int>());\n  for(int i = 0; i < mid; i++){\n    x[i] = firstHalf[i];\n  }\n}",
            "// TODO: Your code here\n    int mid = x.size() / 2;\n    int i = 0, j = mid;\n    std::vector<int> temp;\n    while (i < mid && j < x.size()) {\n        if (x[i] > x[j]) {\n            temp.push_back(x[i]);\n            i++;\n        } else {\n            temp.push_back(x[j]);\n            j++;\n        }\n    }\n    while (i < mid) {\n        temp.push_back(x[i]);\n        i++;\n    }\n    while (j < x.size()) {\n        temp.push_back(x[j]);\n        j++;\n    }\n    x = temp;\n}",
            "// TODO: Implement this function\n  int mid = x.size() / 2;\n  std::vector<int> firstHalf;\n  for (int i = 0; i < mid; i++) {\n    firstHalf.push_back(x[i]);\n  }\n  std::sort(firstHalf.begin(), firstHalf.end(), std::greater<int>());\n  for (int i = 0; i < mid; i++) {\n    x[i] = firstHalf[i];\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n    int n = x.size();\n    if (n == 0)\n        return;\n    if (n == 1)\n        return;\n    if (n == 2) {\n        if (x[0] < x[1]) {\n            int temp = x[0];\n            x[0] = x[1];\n            x[1] = temp;\n        }\n        return;\n    }\n    int mid = n / 2;\n    std::vector<int> firstHalf(x.begin(), x.begin() + mid);\n    std::vector<int> secondHalf(x.begin() + mid, x.end());\n    sortFirstHalfDescending(firstHalf);\n    sortFirstHalfDescending(secondHalf);\n    std::vector<int> result;\n    int i = 0, j = 0;\n    while (i < firstHalf.size() && j < secondHalf.size()) {\n        if (firstHalf[i] > secondHalf[j]) {\n            result.push_back(firstHalf[i]);\n            i++;\n        } else {\n            result.push_back(secondHalf[j]);\n            j++;\n        }\n    }\n    while (i < firstHalf.size()) {\n        result.push_back(firstHalf[i]);\n        i++;\n    }\n    while (j < secondHalf.size()) {\n        result.push_back(secondHalf[j]);\n        j++;\n    }\n    x = result;\n}",
            "// TODO: implement this function\n}",
            "int n = x.size();\n  if (n == 0) {\n    return;\n  }\n  int mid = n / 2;\n  std::vector<int> firstHalf(x.begin(), x.begin() + mid);\n  std::sort(firstHalf.begin(), firstHalf.end(), std::greater<int>());\n  for (int i = 0; i < mid; ++i) {\n    x[i] = firstHalf[i];\n  }\n}",
            "// TODO: Implement this function\n    int n = x.size();\n    int mid = n/2;\n    std::vector<int> temp;\n    for(int i=0; i<mid; i++){\n        temp.push_back(x[i]);\n    }\n    std::sort(temp.begin(), temp.end(), std::greater<int>());\n    for(int i=0; i<mid; i++){\n        x[i] = temp[i];\n    }\n}",
            "int n = x.size();\n    if (n == 0) {\n        return;\n    }\n    if (n == 1) {\n        return;\n    }\n    if (n == 2) {\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        return;\n    }\n    if (n == 3) {\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        if (x[1] < x[2]) {\n            std::swap(x[1], x[2]);\n        }\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        return;\n    }\n\n    // n >= 4\n    int mid = n / 2;\n    if (n % 2 == 0) {\n        // n is even\n        if (x[mid - 1] < x[mid]) {\n            std::swap(x[mid - 1], x[mid]);\n        }\n        if (x[mid] < x[mid + 1]) {\n            std::swap(x[mid], x[mid + 1]);\n        }\n        if (x[mid - 1] < x[mid]) {\n            std::swap(x[mid - 1], x[mid]);\n        }\n    } else {\n        // n is odd\n        if (x[mid] < x[mid + 1]) {\n            std::swap(x[mid], x[mid + 1]);\n        }\n    }\n\n    // Sort the first half of the vector x in descending order.\n    // Leave the second half in-place.\n    sortFirstHalfDescending(std::vector<int>(x.begin(), x.begin() + mid));\n}",
            "// TODO: implement this function\n}",
            "int mid = x.size() / 2;\n  std::nth_element(x.begin(), x.begin() + mid, x.end(), std::greater<int>());\n}",
            "int n = x.size();\n    if (n == 0) {\n        return;\n    }\n    if (n == 1) {\n        return;\n    }\n    if (n == 2) {\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        return;\n    }\n    if (n == 3) {\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        if (x[1] < x[2]) {\n            std::swap(x[1], x[2]);\n        }\n        if (x[0] < x[1]) {\n            std::swap(x[0], x[1]);\n        }\n        return;\n    }\n\n    // n >= 4\n    int mid = n / 2;\n    std::vector<int> firstHalf(x.begin(), x.begin() + mid);\n    std::vector<int> secondHalf(x.begin() + mid, x.end());\n    sortFirstHalfDescending(firstHalf);\n    sortFirstHalfDescending(secondHalf);\n\n    // Merge the two sorted halves into x.\n    int i = 0;\n    int j = 0;\n    int k = 0;\n    while (i < firstHalf.size() && j < secondHalf.size()) {\n        if (firstHalf[i] >= secondHalf[j]) {\n            x[k] = firstHalf[i];\n            i++;\n        } else {\n            x[k] = secondHalf[j];\n            j++;\n        }\n        k++;\n    }\n    while (i < firstHalf.size()) {\n        x[k] = firstHalf[i];\n        i++;\n        k++;\n    }\n    while (j < secondHalf.size()) {\n        x[k] = secondHalf[j];\n        j++;\n        k++;\n    }\n}",
            "// TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int i = 0;\n    int j = mid;\n    int k = 0;\n    std::vector<int> temp(mid);\n    while(i < mid && j < n){\n        if(x[i] > x[j]){\n            temp[k] = x[i];\n            i++;\n        }\n        else{\n            temp[k] = x[j];\n            j++;\n        }\n        k++;\n    }\n    while(i < mid){\n        temp[k] = x[i];\n        i++;\n        k++;\n    }\n    while(j < n){\n        temp[k] = x[j];\n        j++;\n        k++;\n    }\n    for(int i = 0; i < mid; i++){\n        x[i] = temp[i];\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n  int n = x.size();\n  if (n == 0) return;\n  int mid = n / 2;\n  std::vector<int> y(x.begin(), x.begin() + mid);\n  std::sort(y.begin(), y.end(), std::greater<int>());\n  for (int i = 0; i < mid; i++) {\n    x[i] = y[i];\n  }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "hip",
        "prompt": "/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use AMD HIP to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N/2) return;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N-1 && x[j] > x[j+1]) {\n      int temp = x[j];\n      x[j] = x[j+1];\n      x[j+1] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = 2 * i + 1;\n        if (j < N - 1 && x[j] > x[j + 1]) {\n            swap(x, j, j + 1);\n        }\n        if (j > 0 && x[j] > x[j - 1]) {\n            swap(x, j, j - 1);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = N - i - 1;\n    if (i < j) {\n      int tmp = x[i];\n      x[i] = x[j];\n      x[j] = tmp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = 2*i + 1;\n        if (j < N-1 && x[j] > x[j+1]) {\n            int temp = x[j];\n            x[j] = x[j+1];\n            x[j+1] = temp;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N / 2) {\n        int left = 2 * tid;\n        int right = 2 * tid + 1;\n        if (right < N) {\n            if (x[left] < x[right]) {\n                int temp = x[left];\n                x[left] = x[right];\n                x[right] = temp;\n            }\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N/2) return;\n    int j = N - i - 1;\n    if (x[i] < x[j]) {\n        int tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int j = N - i - 1;\n    if (i < j) {\n      int tmp = x[i];\n      x[i] = x[j];\n      x[j] = tmp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = 2*i + 1;\n        if (j < N-1 && x[j] > x[j+1]) {\n            int temp = x[j];\n            x[j] = x[j+1];\n            x[j+1] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = N - i - 1;\n    if (i < j) {\n      int tmp = x[i];\n      x[i] = x[j];\n      x[j] = tmp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N) {\n      if (x[i] < x[j]) {\n        int tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N) {\n      if (x[i] < x[j]) {\n        int temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N/2) return;\n  int j = N - 1 - i;\n  if (j >= N/2) return;\n  if (x[i] < x[j]) {\n    int tmp = x[i];\n    x[i] = x[j];\n    x[j] = tmp;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int j = N - i - 1;\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N-1 && x[j] > x[j+1]) {\n      int tmp = x[j];\n      x[j] = x[j+1];\n      x[j+1] = tmp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = i + N/2;\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N / 2) {\n    int i = tid;\n    int j = N - tid - 1;\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int j = 2 * i + 1;\n    if (j < N - 1 && x[j] > x[j + 1]) {\n      int temp = x[j];\n      x[j] = x[j + 1];\n      x[j + 1] = temp;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int temp = x[i];\n    int j = i;\n    while (j > 0 && x[j - 1] < temp) {\n      x[j] = x[j - 1];\n      j--;\n    }\n    x[j] = temp;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int j = N - i - 1;\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N-1 && x[j] > x[j+1]) {\n      int tmp = x[j];\n      x[j] = x[j+1];\n      x[j+1] = tmp;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI to sort in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "int size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO: Your code here\n    // Use MPI_Send and MPI_Recv to sort the first half of x in descending order.\n    // Store the result in x on rank 0.\n\n    // TODO: Your code here\n    // Use MPI_Send and MPI_Recv to sort the second half of x in ascending order.\n    // Store the result in x on rank 0.\n\n    // TODO: Your code here\n    // Use MPI_Send and MPI_Recv to sort the middle element in x in ascending order.\n    // Store the result in x on rank 0.\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n  int rank = 0;\n  int num_ranks = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (size == 0) {\n    return;\n  }\n\n  // TODO: Your code here\n  int *x_ptr = &x[0];\n  int *x_first_half = new int[size/2];\n  int *x_second_half = new int[size/2];\n  int *x_first_half_sorted = new int[size/2];\n\n  if (rank == 0) {\n    for (int i = 0; i < size/2; i++) {\n      x_first_half[i] = x_ptr[i];\n    }\n  }\n\n  MPI_Bcast(x_first_half, size/2, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = size/2; i < size; i++) {\n      x_second_half[i - size/2] = x_ptr[i];\n    }\n  }\n\n  MPI_Bcast(x_second_half, size/2, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < size/2; i++) {\n      x_first_half_sorted[i] = x_first_half[i];\n    }\n  }\n\n  MPI_Bcast(x_first_half_sorted, size/2, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < size/2; i++) {\n      x_ptr[i] = x_first_half_sorted[i];\n    }\n  }\n\n  MPI_Bcast(x_ptr, size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  delete[] x_first_half;\n  delete[] x_second_half;\n  delete[] x_first_half_sorted;\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n  int rank = 0;\n  int num_procs = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_ranks = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int num_per_rank = size / num_ranks;\n  int remainder = size % num_ranks;\n  int start = rank * num_per_rank;\n  int end = start + num_per_rank;\n  if (rank == num_ranks - 1) {\n    end += remainder;\n  }\n  std::vector<int> my_x(x.begin() + start, x.begin() + end);\n  std::sort(my_x.begin(), my_x.end());\n  if (rank == 0) {\n    std::copy(my_x.begin(), my_x.end(), x.begin());\n  }\n  MPI_Gather(my_x.data(), my_x.size(), MPI_INT, x.data(), my_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int half_size = size / 2;\n    if (size % 2 == 1) {\n        half_size++;\n    }\n    int my_half_size = half_size / num_ranks;\n    if (rank == 0) {\n        std::vector<int> x_first_half(half_size);\n        for (int i = 0; i < half_size; i++) {\n            x_first_half[i] = x[i];\n        }\n        std::vector<int> x_second_half(size - half_size);\n        for (int i = 0; i < size - half_size; i++) {\n            x_second_half[i] = x[i + half_size];\n        }\n        std::vector<int> x_first_half_sorted(half_size);\n        std::vector<int> x_second_half_sorted(size - half_size);\n        MPI_Scatter(x_first_half.data(), my_half_size, MPI_INT, x_first_half_sorted.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Scatter(x_second_half.data(), my_half_size, MPI_INT, x_second_half_sorted.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(x_first_half_sorted.begin(), x_first_half_sorted.end(), std::greater<int>());\n        std::sort(x_second_half_sorted.begin(), x_second_half_sorted.end(), std::greater<int>());\n        MPI_Gather(x_first_half_sorted.data(), my_half_size, MPI_INT, x.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Gather(x_second_half_sorted.data(), my_half_size, MPI_INT, x.data() + half_size, my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> x_first_half_sorted(my_half_size);\n        std::vector<int> x_second_half_sorted(my_half_size);\n        MPI_Scatter(x.data(), my_half_size, MPI_INT, x_first_half_sorted.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Scatter(x.data() + half_size, my_half_size, MPI_INT, x_second_half_sorted.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(x_first_half_sorted.begin(), x_first_half_sorted.end(), std::greater<int>());\n        std::sort(x_second_half_sorted.begin(), x_second_half_sorted.end(), std::greater<int>());\n        MPI_Gather(x_first_half_sorted.data(), my_half_size, MPI_INT, x.data(), my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Gather(x_second_half_sorted.data(), my_half_size, MPI_INT, x.data() + half_size, my_half_size, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n  int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  int n = size / 2;\n  int n_per_proc = n / nproc;\n  int n_extra = n % nproc;\n  int start = rank * n_per_proc + std::min(rank, n_extra);\n  int end = start + n_per_proc + (rank < n_extra);\n  std::vector<int> x_local(end - start);\n  std::copy(x.begin() + start, x.begin() + end, x_local.begin());\n  std::sort(x_local.begin(), x_local.end(), std::greater<int>());\n  MPI_Gather(x_local.data(), x_local.size(), MPI_INT, x.data(), x_local.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "int size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    if (size == 0) {\n        return;\n    }\n    if (size == 1) {\n        return;\n    }\n    if (size == 2) {\n        if (x[0] < x[1]) {\n            int temp = x[0];\n            x[0] = x[1];\n            x[1] = temp;\n        }\n        return;\n    }\n    if (size % 2 == 0) {\n        int half = size / 2;\n        std::vector<int> x1(x.begin(), x.begin() + half);\n        std::vector<int> x2(x.begin() + half, x.end());\n        sortFirstHalfDescending(x1);\n        sortFirstHalfDescending(x2);\n        if (rank == 0) {\n            int i = 0;\n            int j = 0;\n            while (i < half && j < half) {\n                if (x1[i] > x2[j]) {\n                    x[i + j] = x1[i];\n                    i++;\n                } else {\n                    x[i + j] = x2[j];\n                    j++;\n                }\n            }\n            while (i < half) {\n                x[i + j] = x1[i];\n                i++;\n            }\n            while (j < half) {\n                x[i + j] = x2[j];\n                j++;\n            }\n        }\n    } else {\n        int half = size / 2;\n        std::vector<int> x1(x.begin(), x.begin() + half);\n        std::vector<int> x2(x.begin() + half + 1, x.end());\n        sortFirstHalfDescending(x1);\n        sortFirstHalfDescending(x2);\n        if (rank == 0) {\n            int i = 0;\n            int j = 0;\n            while (i < half && j < half) {\n                if (x1[i] > x2[j]) {\n                    x[i + j] = x1[i];\n                    i++;\n                } else {\n                    x[i + j] = x2[j];\n                    j++;\n                }\n            }\n            while (i < half) {\n                x[i + j] = x1[i];\n                i++;\n            }\n            while (j < half) {\n                x[i + j] = x2[j];\n                j++;\n            }\n            x[half] = x[half + 1];\n        }\n    }\n}",
            "int size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use CUDA to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = i + N/2;\n        if (x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N) {\n      if (x[i] < x[j]) {\n        int temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = N - i - 1;\n        int temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N / 2) {\n    int j = N - i - 1;\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = 2*i + 1;\n        if (j < N && x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N-1 && x[j] > x[j+1]) {\n      int tmp = x[j];\n      x[j] = x[j+1];\n      x[j+1] = tmp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = N - i - 1;\n        if (x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = i + (N / 2);\n  if (i < N / 2) {\n    if (x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N/2) {\n    int j = 2*i + 1;\n    if (j < N && x[i] < x[j]) {\n      int temp = x[i];\n      x[i] = x[j];\n      x[j] = temp;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = 2 * i + 1;\n        if (j < N) {\n            if (x[i] < x[j]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = i + N/2;\n        if (x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N/2) {\n        int j = N - 1 - i;\n        if (x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = N - i - 1;\n        if (x[i] < x[j]) {\n            int temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N / 2) {\n        int j = 2 * i + 1;\n        if (j < N - 1 && x[j] > x[j + 1]) {\n            swap(x, j, j + 1);\n        }\n        if (i > 0 && x[i] < x[i - 1]) {\n            swap(x, i, i - 1);\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "38_sort_sort_first_half_in_descending_order",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI and OpenMP to sort in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n  int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // TODO: Your code here\n  int n = size / 2;\n  int m = size - n;\n  int *x_rank = new int[n];\n  int *x_rank_sorted = new int[n];\n  int *x_rank_sorted_recv = new int[n];\n  int *x_rank_sorted_recv_temp = new int[n];\n  int *x_rank_sorted_recv_temp2 = new int[n];\n  int *x_rank_sorted_recv_temp3 = new int[n];\n  int *x_rank_sorted_recv_temp4 = new int[n];\n  int *x_rank_sorted_recv_temp5 = new int[n];\n  int *x_rank_sorted_recv_temp6 = new int[n];\n  int *x_rank_sorted_recv_temp7 = new int[n];\n  int *x_rank_sorted_recv_temp8 = new int[n];\n  int *x_rank_sorted_recv_temp9 = new int[n];\n  int *x_rank_sorted_recv_temp10 = new int[n];\n  int *x_rank_sorted_recv_temp11 = new int[n];\n  int *x_rank_sorted_recv_temp12 = new int[n];\n  int *x_rank_sorted_recv_temp13 = new int[n];\n  int *x_rank_sorted_recv_temp14 = new int[n];\n  int *x_rank_sorted_recv_temp15 = new int[n];\n  int *x_rank_sorted_recv_temp16 = new int[n];\n  int *x_rank_sorted_recv_temp17 = new int[n];\n  int *x_rank_sorted_recv_temp18 = new int[n];\n  int *x_rank_sorted_recv_temp19 = new int[n];\n  int *x_rank_sorted_recv_temp20 = new int[n];\n  int *x_rank_sorted_recv_temp21 = new int[n];\n  int *x_rank_sorted_recv_temp22 = new int[n];\n  int *x_rank_sorted_recv_temp23 = new int[n];\n  int *x_rank_sorted_recv_temp24 = new int[n];\n  int *x_rank_sorted_recv_temp25 = new int[n];\n  int *x_rank_sorted_recv_temp26 = new int[n];\n  int *x_rank_sorted_recv_temp27 = new int[n];\n  int *x_rank_sorted_recv_temp28 = new int[n];\n  int *x_rank_sorted_recv_temp29 = new int[n];\n  int *x_rank_sorted_recv_temp30 = new int[n];\n  int *x_rank_sorted_recv_temp31 = new int[n];\n  int *x_rank_sorted_recv_temp32 = new int[n];\n  int *x_rank_sorted_recv_temp33 = new int[n];\n  int *x_rank_sorted_recv_temp34 = new int[n];\n  int *x_rank_sorted_recv_temp35 = new int[n];\n  int *x_rank_sorted_recv_temp36 = new int[n];\n  int *x_rank_sorted_recv_temp37 = new int[n];\n  int *x_rank_sorted_recv_temp38 = new int[n];\n  int *x_rank_sorted_recv_temp39 = new int[n];\n  int *x_rank_sorted_recv_temp40 = new int[n];\n  int *x_rank_sorted_recv_temp41",
            "int size = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int num_threads = omp_get_max_threads();\n  int num_per_thread = size / num_threads;\n  int num_extra = size % num_threads;\n  int start = rank * num_per_thread + std::min(rank, num_extra);\n  int end = (rank + 1) * num_per_thread + std::min(rank + 1, num_extra);\n  std::vector<int> local_x(end - start);\n  std::copy(x.begin() + start, x.begin() + end, local_x.begin());\n  std::sort(local_x.begin(), local_x.end(), std::greater<int>());\n  if (rank == 0) {\n    std::copy(local_x.begin(), local_x.end(), x.begin());\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int size = x.size();\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    int n = size / 2;\n    int n_per_proc = n / nproc;\n    int n_rem = n % nproc;\n    int n_local = n_per_proc + (rank < n_rem? 1 : 0);\n\n    std::vector<int> x_local(n_local);\n    std::vector<int> x_local_sorted(n_local);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            x_local[i] = x[i];\n        }\n    }\n\n    MPI_Scatter(x.data(), n_local, MPI_INT, x_local.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; i++) {\n        x_local_sorted[i] = x_local[i];\n    }\n\n    std::sort(x_local_sorted.begin(), x_local_sorted.end(), std::greater<int>());\n\n    MPI_Gather(x_local_sorted.data(), n_local, MPI_INT, x.data(), n_local, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            x[i] = x_local[i];\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size = x.size();\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = size / num_threads;\n    int num_extra = size % num_threads;\n    int start = rank * num_per_thread + std::min(rank, num_extra);\n    int end = (rank + 1) * num_per_thread + std::min(rank + 1, num_extra);\n    std::vector<int> local_x(x.begin() + start, x.begin() + end);\n    std::sort(local_x.begin(), local_x.end(), std::greater<int>());\n    if (rank == 0) {\n        x.resize(size / 2);\n        std::copy(local_x.begin(), local_x.end(), x.begin());\n    }\n    MPI_Gather(local_x.data(), local_x.size(), MPI_INT, x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        std::sort(x.begin(), x.end(), std::greater<int>());\n    }\n}",
            "int size = x.size();\n    int rank = 0;\n    int num_procs = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int num_threads = 0;\n    int num_elements = 0;\n    int num_elements_per_thread = 0;\n    int num_elements_per_proc = 0;\n    int num_elements_per_proc_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread = 0;\n    int num_elements_per_proc_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_per_thread_per_rank_",
            "int size = x.size();\n    int rank = 0;\n    int num_ranks = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int num_threads = 0;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n    int num_threads_per_rank = num_threads / num_ranks;\n    int num_elements_per_rank = size / num_ranks;\n    int num_elements_per_thread = num_elements_per_rank / num_threads_per_rank;\n    int num_elements_per_thread_last_rank = num_elements_per_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_last_thread = num_elements_per_thread_last_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_thread = num_elements_per_thread + num_elements_per_thread_last_thread;\n    int num_elements_per_thread_first_rank = num_elements_per_thread_first_thread + (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread = num_elements_per_thread_first_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank = num_elements_per_thread_first_rank_last_thread - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread = num_elements_per_thread_first_rank_last_thread_last_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank = num_elements_per_thread_first_rank_last_thread_last_rank_last_thread - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread = num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank = num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank_last_thread = num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank = num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_thread_last_rank_last_thread - (num_threads_per_rank - 1) * num_elements_per_thread;\n    int num_elements_per_thread_first_rank_last_thread_last_rank_last_thread_last_rank_last_",
            "// TODO: Implement this function\n}",
            "int size = x.size();\n  int rank = 0;\n  int num_ranks = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int num_threads = omp_get_max_threads();\n  int num_threads_per_rank = num_threads / num_ranks;\n  int num_elements_per_thread = size / num_threads_per_rank;\n  int num_elements_per_rank = size / num_ranks;\n  int num_elements_per_thread_per_rank = num_elements_per_thread * num_threads_per_rank;\n  int num_elements_per_rank_per_thread = num_elements_per_rank / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread = num_elements_per_rank_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread / num_threads_per_rank;\n  int num_elements_per_rank_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread_per_thread = num_elements_per",
            "int size = x.size();\n    int rank;\n    int num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO: your code here\n    int start = 0;\n    int end = size / 2;\n    int mid = size / 2;\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = (size / 2) / num_threads;\n    int num_extra = (size / 2) % num_threads;\n    int num_per_thread_extra = num_per_thread + 1;\n    int num_per_thread_normal = num_per_thread;\n    int start_extra = 0;\n    int start_normal = 0;\n    int end_extra = 0;\n    int end_normal = 0;\n    int num_extra_ranks = num_ranks - num_threads;\n    int num_normal_ranks = num_ranks - num_extra_ranks;\n    int num_extra_per_rank = num_extra / num_extra_ranks;\n    int num_extra_per_rank_extra = num_extra_per_rank + 1;\n    int num_extra_per_rank_normal = num_extra_per_rank;\n    int num_normal_per_rank = num_per_thread / num_normal_ranks;\n    int num_normal_per_rank_extra = num_normal_per_rank + 1;\n    int num_normal_per_rank_normal = num_normal_per_rank;\n    int start_extra_rank = 0;\n    int start_normal_rank = 0;\n    int end_extra_rank = 0;\n    int end_normal_rank = 0;\n    int start_extra_thread = 0;\n    int start_normal_thread = 0;\n    int end_extra_thread = 0;\n    int end_normal_thread = 0;\n    int extra_rank = rank / num_threads;\n    int normal_rank = rank % num_threads;\n    int extra_thread = rank % num_threads;\n    int normal_thread = rank / num_threads;\n    int start_extra_rank_extra = 0;\n    int start_extra_rank_normal = 0;\n    int start_normal_rank_extra = 0;\n    int start_normal_rank_normal = 0;\n    int end_extra_rank_extra = 0;\n    int end_extra_rank_normal = 0;\n    int end_normal_rank_extra = 0;\n    int end_normal_rank_normal = 0;\n    int start_extra_thread_extra = 0;\n    int start_extra_thread_normal = 0;\n    int start_normal_thread_extra = 0;\n    int start_normal_thread_normal = 0;\n    int end_extra_thread_extra = 0;\n    int end_extra_thread_normal = 0;\n    int end_normal_thread_extra = 0;\n    int end_normal_thread_normal = 0;\n    int start_extra_rank_extra_thread_extra = 0;\n    int start_extra_rank_extra_thread_normal = 0;\n    int start_extra_rank_normal_thread_extra = 0;\n    int start_extra_rank_normal_thread_normal = 0;\n    int end_extra_rank_extra_thread_extra = 0;\n    int end_extra_rank_extra_thread_normal = 0;\n    int end_extra_rank_normal_thread_extra = 0;\n    int end_extra_rank_normal_thread_normal = 0;\n    int start_normal_rank_extra_thread_extra = 0;\n    int start_normal_rank_extra_thread_normal = 0;\n    int start_normal_rank_normal_thread_extra = 0;\n    int start_normal_rank_normal_thread_normal = 0;\n    int end_normal_rank_extra_thread_extra = 0;\n    int end_normal",
            "int size = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // TODO: Your code here\n  int half = size / 2;\n  int remainder = size % 2;\n  int start = rank * half;\n  int end = start + half;\n  if (rank == 0) {\n    end += remainder;\n  }\n  std::vector<int> local_x(end - start);\n  for (int i = start; i < end; i++) {\n    local_x[i - start] = x[i];\n  }\n\n  int local_size = local_x.size();\n  int local_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &local_rank);\n  int local_num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &local_num_ranks);\n\n  // TODO: Your code here\n  int local_half = local_size / 2;\n  int local_remainder = local_size % 2;\n  int local_start = local_rank * local_half;\n  int local_end = local_start + local_half;\n  if (local_rank == 0) {\n    local_end += local_remainder;\n  }\n  std::vector<int> local_local_x(local_end - local_start);\n  for (int i = local_start; i < local_end; i++) {\n    local_local_x[i - local_start] = local_x[i];\n  }\n\n  int local_local_size = local_local_x.size();\n  int local_local_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &local_local_rank);\n  int local_local_num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &local_local_num_ranks);\n\n  // TODO: Your code here\n  int local_local_half = local_local_size / 2;\n  int local_local_remainder = local_local_size % 2;\n  int local_local_start = local_local_rank * local_local_half;\n  int local_local_end = local_local_start + local_local_half;\n  if (local_local_rank == 0) {\n    local_local_end += local_local_remainder;\n  }\n  std::vector<int> local_local_local_x(local_local_end - local_local_start);\n  for (int i = local_local_start; i < local_local_end; i++) {\n    local_local_local_x[i - local_local_start] = local_local_x[i];\n  }\n\n  int local_local_local_size = local_local_local_x.size();\n  int local_local_local_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &local_local_local_rank);\n  int local_local_local_num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &local_local_local_num_ranks);\n\n  // TODO: Your code here\n  int local_local_local_half = local_local_local_size / 2;\n  int local_local_local_remainder = local_local_local_size % 2;\n  int local_local_local_start = local_local_local_rank * local_local_local_half;\n  int local_local_local_end = local_local_local_start + local_local_local_half;\n  if (local_local_local_rank == 0) {\n    local_local_local_end += local_local_local_remainder;\n  }\n  std::vector<int> local_local_local_local_x(local_local_local_end - local_local_local_start);\n  for (int i = local_local_local_start; i <",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_left = n % size;\n\n  // Send the first half of the vector to the next rank\n  std::vector<int> x_next(n_per_rank + (rank < n_left));\n  MPI_Send(&x[0], n_per_rank + (rank < n_left), MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);\n\n  // Receive the first half of the vector from the previous rank\n  std::vector<int> x_prev(n_per_rank + (rank > 0));\n  MPI_Recv(&x_prev[0], n_per_rank + (rank > 0), MPI_INT, (rank + size - 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  // Sort the first half of the vector\n  std::sort(x_prev.begin(), x_prev.end(), std::greater<int>());\n\n  // Merge the sorted first half with the second half\n  std::vector<int> x_merged(n);\n  std::merge(x_prev.begin(), x_prev.end(), x.begin(), x.end(), x_merged.begin());\n\n  // Send the merged vector to the previous rank\n  MPI_Send(&x_merged[0], n, MPI_INT, (rank + size - 1) % size, 0, MPI_COMM_WORLD);\n\n  // Receive the merged vector from the next rank\n  MPI_Recv(&x[0], n, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  // Sort the first half of the vector\n  std::sort(x.begin(), x.begin() + n_per_rank + (rank < n_left), std::greater<int>());\n\n  // Merge the sorted first half with the second half\n  std::vector<int> x_merged2(n);\n  std::merge(x.begin(), x.begin() + n_per_rank + (rank < n_left), x_next.begin(), x_next.end(), x_merged2.begin());\n\n  // Send the merged vector to the next rank\n  MPI_Send(&x_merged2[0], n, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);\n\n  // Receive the merged vector from the previous rank\n  MPI_Recv(&x[0], n, MPI_INT, (rank + size - 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  if (rank == 0) {\n    // Sort the first half of the vector\n    std::sort(x.begin(), x.begin() + n_per_rank + (rank < n_left), std::greater<int>());\n\n    // Merge the sorted first half with the second half\n    std::vector<int> x_merged3(n);\n    std::merge(x.begin(), x.begin() + n_per_rank + (rank < n_left), x_prev.begin(), x_prev.end(), x_merged3.begin());\n\n    // Send the merged vector to the previous rank\n    MPI_Send(&x_merged3[0], n, MPI_INT, (rank + size - 1) % size, 0, MPI_COMM_WORLD);\n\n    // Receive the merged vector from the next rank\n    MPI_Recv(&x[0], n, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_per_rank_extra = n % size;\n  int n_per_rank_rank0 = n_per_rank + n_per_rank_extra;\n  int n_per_rank_other_ranks = n_per_rank;\n  int n_per_rank_rank0_extra = n_per_rank_extra;\n  int n_per_rank_other_ranks_extra = 0;\n  if (rank == 0) {\n    std::vector<int> x_rank0(n_per_rank_rank0);\n    std::copy(x.begin(), x.begin() + n_per_rank_rank0, x_rank0.begin());\n    std::vector<int> x_other_ranks(n_per_rank_other_ranks * (size - 1));\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(x_other_ranks.data() + (i - 1) * n_per_rank_other_ranks,\n               n_per_rank_other_ranks, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    std::vector<int> x_rank0_sorted(n_per_rank_rank0);\n    std::copy(x_rank0.begin(), x_rank0.begin() + n_per_rank_rank0_extra, x_rank0_sorted.begin());\n    std::sort(x_rank0_sorted.begin() + n_per_rank_rank0_extra, x_rank0_sorted.end());\n    std::copy(x_rank0_sorted.begin(), x_rank0_sorted.end(), x_rank0.begin());\n    std::vector<int> x_other_ranks_sorted(n_per_rank_other_ranks * (size - 1));\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(x_other_ranks_sorted.data() + (i - 1) * n_per_rank_other_ranks,\n               n_per_rank_other_ranks, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    std::vector<int> x_sorted(n);\n    std::copy(x_rank0.begin(), x_rank0.end(), x_sorted.begin());\n    std::copy(x_other_ranks_sorted.begin(), x_other_ranks_sorted.end(), x_sorted.begin() + n_per_rank_rank0);\n    std::copy(x_other_ranks.begin(), x_other_ranks.end(), x_sorted.begin() + n_per_rank_rank0);\n    std::copy(x_sorted.begin(), x_sorted.end(), x.begin());\n  } else {\n    std::vector<int> x_rank0(n_per_rank_rank0);\n    std::copy(x.begin(), x.begin() + n_per_rank_rank0, x_rank0.begin());\n    std::vector<int> x_other_ranks(n_per_rank_other_ranks * (size - 1));\n    std::copy(x.begin() + n_per_rank_rank0, x.end(), x_other_ranks.begin());\n    std::vector<int> x_rank0_sorted(n_per_rank_rank0);\n    std::copy(x_rank0.begin(), x_rank0.begin() + n_per_rank_rank0_extra, x_rank0_sorted.begin());\n    std::sort(x_rank0_sorted.begin() + n_per_rank_rank0_extra, x_rank0_sorted.end());\n    std::copy(x_rank0_sorted.begin(), x_rank0_sorted.",
            "int size = x.size();\n    int rank = 0;\n    int num_procs = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    int num_threads = omp_get_max_threads();\n    int num_per_thread = size / num_threads;\n    int num_left = size % num_threads;\n    int start = rank * num_per_thread;\n    int end = start + num_per_thread;\n    if (rank == 0) {\n        end += num_left;\n    }\n    std::vector<int> local_x(end - start);\n    for (int i = start; i < end; i++) {\n        local_x[i - start] = x[i];\n    }\n    std::sort(local_x.begin(), local_x.end(), std::greater<int>());\n    if (rank == 0) {\n        for (int i = 0; i < num_per_thread; i++) {\n            x[i] = local_x[i];\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = num_per_thread; i < size; i++) {\n            x[i] = local_x[i - num_per_thread];\n        }\n    }\n}",
            "int size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int num_threads = omp_get_max_threads();\n    int num_threads_per_rank = num_threads / num_ranks;\n    int num_threads_per_rank_remainder = num_threads % num_ranks;\n    int num_threads_per_rank_rank_0 = num_threads_per_rank + num_threads_per_rank_remainder;\n    int num_threads_per_rank_other_ranks = num_threads_per_rank;\n    int num_threads_per_rank_this_rank = rank == 0? num_threads_per_rank_rank_0 : num_threads_per_rank_other_ranks;\n    int num_elements_per_thread = size / num_threads_per_rank_this_rank;\n    int num_elements_per_thread_remainder = size % num_threads_per_rank_this_rank;\n    int num_elements_per_thread_rank_0 = num_elements_per_thread + num_elements_per_thread_remainder;\n    int num_elements_per_thread_other_ranks = num_elements_per_thread;\n    int num_elements_per_thread_this_rank = rank == 0? num_elements_per_thread_rank_0 : num_elements_per_thread_other_ranks;\n    int num_elements_per_thread_this_rank_remainder = num_elements_per_thread_this_rank % 2;\n    int num_elements_per_thread_this_rank_even = num_elements_per_thread_this_rank - num_elements_per_thread_this_rank_remainder;\n    int num_elements_per_thread_this_rank_odd = num_elements_per_thread_this_rank_even + 1;\n    int num_elements_per_thread_this_rank_even_per_thread = num_elements_per_thread_this_rank_even / num_threads_per_rank_this_rank;\n    int num_elements_per_thread_this_rank_even_per_thread_remainder = num_elements_per_thread_this_rank_even % num_threads_per_rank_this_rank;\n    int num_elements_per_thread_this_rank_even_per_thread_rank_0 = num_elements_per_thread_this_rank_even_per_thread + num_elements_per_thread_this_rank_even_per_thread_remainder;\n    int num_elements_per_thread_this_rank_even_per_thread_other_ranks = num_elements_per_thread_this_rank_even_per_thread;\n    int num_elements_per_thread_this_rank_even_per_thread_this_rank = rank == 0? num_elements_per_thread_this_rank_even_per_thread_rank_0 : num_elements_per_thread_this_rank_even_per_thread_other_ranks;\n    int num_elements_per_thread_this_rank_odd_per_thread = num_elements_per_thread_this_rank_odd / num_threads_per_rank_this_rank;\n    int num_elements_per_thread_this_rank_odd_per_thread_remainder = num_elements_per_thread_this_rank_odd % num_threads_per_rank_this_rank;\n    int num_elements_per_thread_this_rank_odd_per_thread_rank_0 = num_elements_per_thread_this_rank_odd_per_thread + num_elements_per_thread_this_rank_odd_per_thread_remainder;\n    int num_elements_per_thread_this_rank_odd_per_thread_other_ranks = num_elements_per_thread_this_rank_odd_per_thread;\n    int num_elements_per_thread_this_rank_odd_per_thread_this_rank = rank == 0? num_elements",
            "int size = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // TODO: Your code here\n  if (rank == 0) {\n    std::vector<int> x_copy(x);\n    x.clear();\n    for (int i = 0; i < size / 2; i++) {\n      int max_index = 0;\n      int max_value = x_copy[0];\n      for (int j = 0; j < size / 2; j++) {\n        if (x_copy[j] > max_value) {\n          max_value = x_copy[j];\n          max_index = j;\n        }\n      }\n      x.push_back(max_value);\n      x_copy.erase(x_copy.begin() + max_index);\n    }\n    if (size % 2 == 1) {\n      x.push_back(x_copy[0]);\n    }\n  }\n\n  MPI_Bcast(&x[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   #pragma omp parallel for\n   for(int i = 0; i < results.size(); i++)\n   {\n       for(int j = i + 1; j < results.size(); j++)\n       {\n           if(results[i].startTime > results[j].startTime)\n           {\n               Result temp = results[i];\n               results[i] = results[j];\n               results[j] = temp;\n           }\n       }\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   // Hint: Use the OpenMP sort function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n   // Hint: Use the omp_get_thread_num() function to determine the thread ID\n   // Hint: Use the omp_get_num_threads() function to determine the number of threads\n   // Hint: Use the omp_get_num_procs() function to determine the number of processors\n   // Hint: Use the omp_get_max_threads() function to determine the maximum number of threads\n   // Hint: Use the omp_get_dynamic() function to determine if dynamic threads are enabled\n   // Hint: Use the omp_get_nested() function to determine if nested parallelism is enabled\n   // Hint: Use the omp_in_parallel() function to determine if the code is running in parallel\n   // Hint: Use the omp_set_num_threads() function to set the number of threads\n   // Hint: Use the omp_set_dynamic() function to enable/disable dynamic threads\n   // Hint: Use the omp_set_nested() function to enable/disable nested parallelism\n   // Hint: Use the omp_set_schedule() function to set the schedule type\n   // Hint: Use the omp_get_schedule() function to get the schedule type\n   // Hint: Use the omp_get_thread_limit() function to get the thread limit\n   // Hint: Use the omp_set_max_active_levels() function to set the maximum active levels\n   // Hint: Use the omp_get_max_active_levels() function to get the maximum active levels\n   // Hint: Use the omp_get_cancellation() function to get the cancellation status\n   // Hint: Use the omp_set_cancel() function to enable/disable cancellation\n   // Hint: Use the omp_test_cancel() function to test if cancellation has been requested\n   // Hint: Use the omp_set_lock() function to acquire a lock\n   // Hint: Use the omp_unset_lock() function to release a lock\n   // Hint: Use the omp_test_lock() function to test if a lock is available\n   // Hint: Use the omp_init_lock() function to initialize a lock\n   // Hint: Use the omp_destroy_lock() function to destroy a lock\n   // Hint: Use the omp_set_nest_lock() function to acquire a nested lock\n   // Hint: Use the omp_unset_nest_lock() function to release a nested lock\n   // Hint: Use the omp_test_nest_lock() function to test if a nested lock is available\n   // Hint: Use the omp_init_nest_lock() function to initialize a nested lock\n   // Hint: Use the omp_destroy_nest_lock() function to destroy a nested lock\n   // Hint: Use the omp_get_wtime() function to get the wall clock time\n   // Hint: Use the omp_get_wtick() function to get the time tick\n   // Hint: Use the omp_get_num_teams() function to get the number of teams\n   // Hint: Use the omp_get_team_num() function to get the team number\n   // Hint: Use the omp_is_initial_device() function to determine if the device is the initial device\n   // Hint: Use the omp_get_initial_device() function to get the initial device\n   // Hint: Use the omp_get_default_device() function to get the default device\n   // Hint: Use the omp_set_default_device() function to set the default device\n   // Hint: Use the omp_get_num_devices() function to get the number of devices\n   // Hint: Use the omp_get_num_procs() function to get the number of processors\n   // Hint: Use the omp_get_dynamic() function to determine if dynamic threads are enabled\n   // Hint: Use the omp_get_nested() function to determine if nested parallelism is enabled\n   // Hint: Use the omp_in_parallel() function to determine if the code is running in parallel\n   // Hint: Use the omp_set_num_threads() function to set the number of threads\n   // Hint: Use the omp_set_dynamic() function to enable/disable dynamic threads\n   //",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](Result &a, Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "// TODO: Implement this function\n   std::sort(results.begin(), results.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "// TODO: implement this function\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "// TODO: implement this function\n}",
            "std::sort(results.begin(), results.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "std::sort(results.begin(), results.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "hip",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      // TODO: Implement this function\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      int startTime = results[idx].startTime;\n      int duration = results[idx].duration;\n      float value = results[idx].value;\n      for (int i = idx + 1; i < N; i++) {\n         if (results[i].startTime < startTime) {\n            startTime = results[i].startTime;\n            duration = results[i].duration;\n            value = results[i].value;\n         }\n      }\n      results[idx].startTime = startTime;\n      results[idx].duration = duration;\n      results[idx].value = value;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int j = i;\n      while (j > 0 && results[j-1].startTime > results[j].startTime) {\n         Result temp = results[j];\n         results[j] = results[j-1];\n         results[j-1] = temp;\n         j--;\n      }\n   }\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = i + 1; j < N; j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid < N) {\n      for (int i = 1; i < N; i++) {\n         if (results[i].startTime < results[i - 1].startTime) {\n            Result temp = results[i];\n            results[i] = results[i - 1];\n            results[i - 1] = temp;\n         }\n      }\n   }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   // Find the minimum start time in the range [i, N)\n   int minStartTime = results[i].startTime;\n   for (int j = i + 1; j < N; ++j) {\n      if (results[j].startTime < minStartTime) {\n         minStartTime = results[j].startTime;\n      }\n   }\n\n   // Swap the element at index i with the element at index minStartTime\n   Result temp = results[i];\n   results[i] = results[minStartTime];\n   results[minStartTime] = temp;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int j = i;\n      while (j > 0 && results[j-1].startTime > results[j].startTime) {\n         Result temp = results[j];\n         results[j] = results[j-1];\n         results[j-1] = temp;\n         j--;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      for (int i = 0; i < N; i++) {\n         if (results[i].startTime > results[idx].startTime) {\n            Result tmp = results[i];\n            results[i] = results[idx];\n            results[idx] = tmp;\n         }\n      }\n   }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   int j = i;\n   while (j > 0 && results[j-1].startTime > results[j].startTime) {\n      Result tmp = results[j];\n      results[j] = results[j-1];\n      results[j-1] = tmp;\n      j--;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   for (int i = 0; i < N; i++) {\n      for (int j = i + 1; j < N; j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      for (int i = 0; i < N - 1; i++) {\n         if (results[i].startTime > results[i + 1].startTime) {\n            Result temp = results[i];\n            results[i] = results[i + 1];\n            results[i + 1] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   for (int j = i + 1; j < N; j++) {\n      if (results[i].startTime > results[j].startTime) {\n         Result temp = results[i];\n         results[i] = results[j];\n         results[j] = temp;\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int j = i;\n      while (j > 0 && results[j].startTime < results[j-1].startTime) {\n         Result temp = results[j];\n         results[j] = results[j-1];\n         results[j-1] = temp;\n         j--;\n      }\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   for (int i = 0; i < N; i++) {\n      int j = i;\n      while (j > 0 && results[j].startTime < results[j-1].startTime) {\n         Result tmp = results[j];\n         results[j] = results[j-1];\n         results[j-1] = tmp;\n         j--;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "cuda",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   for (int j = 0; j < N - 1; j++) {\n      if (results[j].startTime > results[j + 1].startTime) {\n         Result temp = results[j];\n         results[j] = results[j + 1];\n         results[j + 1] = temp;\n      }\n   }\n}",
            "// TODO: Implement this\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   int j = i;\n   while (j > 0 && results[j-1].startTime > results[j].startTime) {\n      Result tmp = results[j];\n      results[j] = results[j-1];\n      results[j-1] = tmp;\n      j--;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      // TODO: sort by start time\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "int idx = threadIdx.x;\n   if (idx < N) {\n      // TODO: implement this\n   }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   // TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = 0; j < N - 1; j++) {\n         if (results[j].startTime > results[j + 1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j + 1];\n            results[j + 1] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      for (int j = i + 1; j < N; j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}",
            "// TODO: Implement this function\n}",
            "int i = threadIdx.x;\n   int j = i + 1;\n   while (j < N) {\n      if (results[i].startTime > results[j].startTime) {\n         Result tmp = results[i];\n         results[i] = results[j];\n         results[j] = tmp;\n      }\n      j++;\n   }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n   int left = 2 * idx + 1;\n   int right = 2 * idx + 2;\n   if (left < N && results[left].startTime < results[idx].startTime) {\n      Result temp = results[idx];\n      results[idx] = results[left];\n      results[left] = temp;\n   }\n   if (right < N && results[right].startTime < results[idx].startTime) {\n      Result temp = results[idx];\n      results[idx] = results[right];\n      results[right] = temp;\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      for (int i = 0; i < N - 1; i++) {\n         if (results[i].startTime > results[i + 1].startTime) {\n            Result temp = results[i];\n            results[i] = results[i + 1];\n            results[i + 1] = temp;\n         }\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      // TODO: sort results[i] by start time in ascending order\n   }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the outputs in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "int numRanks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Implement this function\n}",
            "// TODO: Your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      std::vector<Result> results_0(results.begin(), results.end());\n      std::vector<Result> results_1(results.begin(), results.end());\n      std::vector<Result> results_2(results.begin(), results.end());\n      std::vector<Result> results_3(results.begin(), results.end());\n\n      std::vector<Result> results_0_sorted(results.begin(), results.end());\n      std::vector<Result> results_1_sorted(results.begin(), results.end());\n      std::vector<Result> results_2_sorted(results.begin(), results.end());\n      std::vector<Result> results_3_sorted(results.begin(), results.end());\n\n      std::vector<Result> results_sorted(results.begin(), results.end());\n\n      // sort results_0\n      int size_0 = results_0.size();\n      int size_1 = results_1.size();\n      int size_2 = results_2.size();\n      int size_3 = results_3.size();\n\n      int size_0_sorted = results_0_sorted.size();\n      int size_1_sorted = results_1_sorted.size();\n      int size_2_sorted = results_2_sorted.size();\n      int size_3_sorted = results_3_sorted.size();\n\n      int size_sorted = results_sorted.size();\n\n      int size_0_sorted_1 = size_0_sorted / 2;\n      int size_1_sorted_1 = size_1_sorted / 2;\n      int size_2_sorted_1 = size_2_sorted / 2;\n      int size_3_sorted_1 = size_3_sorted / 2;\n\n      int size_sorted_1 = size_sorted / 2;\n\n      int size_0_sorted_2 = size_0_sorted / 4;\n      int size_1_sorted_2 = size_1_sorted / 4;\n      int size_2_sorted_2 = size_2_sorted / 4;\n      int size_3_sorted_2 = size_3_sorted / 4;\n\n      int size_sorted_2 = size_sorted / 4;\n\n      int size_0_sorted_3 = size_0_sorted / 8;\n      int size_1_sorted_3 = size_1_sorted / 8;\n      int size_2_sorted_3 = size_2_sorted / 8;\n      int size_3_sorted_3 = size_3_sorted / 8;\n\n      int size_sorted_3 = size_sorted / 8;\n\n      int size_0_sorted_4 = size_0_sorted / 16;\n      int size_1_sorted_4 = size_1_sorted / 16;\n      int size_2_sorted_4 = size_2_sorted / 16;\n      int size_3_sorted_4 = size_3_sorted / 16;\n\n      int size_sorted_4 = size_sorted / 16;\n\n      int size_0_sorted_5 = size_0_sorted / 32;\n      int size_1_sorted_5 = size_1_sorted / 32;\n      int size_2_sorted_5 = size_2_sorted / 32;\n      int size_3_sorted_5 = size_3_sorted / 32;\n\n      int size_sorted_5 = size_sorted / 32;\n\n      int size_0_sorted_6 = size_0_sorted / 64;\n      int size_1_sorted_6 = size_1_sorted / 64;\n      int size_2_sorted_6 = size_2_sorted / 64;\n      int size_3_sorted_6 = size_3_sorted / 64;\n\n      int size_sorted_6 = size_sorted / 64;\n\n      int size_0_sorted_7 = size_0_sorted / 1",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<Result> localResults;\n   std::vector<Result> globalResults;\n   std::vector<Result> temp;\n\n   if (rank == 0) {\n      localResults = results;\n   }\n\n   MPI_Bcast(&localResults.size(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank!= 0) {\n      localResults.resize(localResults.size());\n   }\n\n   MPI_Bcast(&localResults[0], localResults.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   int chunkSize = localResults.size() / size;\n   int remainder = localResults.size() % size;\n   int start = rank * chunkSize;\n   int end = start + chunkSize;\n\n   if (rank == size - 1) {\n      end += remainder;\n   }\n\n   for (int i = start; i < end; i++) {\n      temp.push_back(localResults[i]);\n   }\n\n   std::sort(temp.begin(), temp.end(), [](Result a, Result b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(&temp.size(), 1, MPI_INT, &globalResults.size(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      globalResults.resize(std::accumulate(globalResults.begin(), globalResults.end(), 0));\n   }\n\n   MPI_Gatherv(&temp[0], temp.size(), MPI_INT, &globalResults[0], &globalResults.size(), &globalResults.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      results = globalResults;\n   }\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(Kokkos::View<int*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a view of the indices\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    idx(i) = i;\n  });\n\n  // Sort the indices based on the values in x\n  Kokkos::parallel_sort(idx, [&](int i, int j) {\n    return x(i) < x(j);\n  });\n\n  // Reorder the values in x based on the sorted indices\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    int j = idx(i);\n    int tmp = x(i);\n    x(i) = x(j);\n    x(j) = tmp;\n  });\n}",
            "// Create a view of the indices\n  Kokkos::View<int*> indices(\"indices\", x.size());\n\n  // Initialize the indices to the identity permutation\n  Kokkos::parallel_for(\n    \"Initialize indices\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      indices(i) = i;\n    }\n  );\n\n  // Sort the indices by the values in x\n  Kokkos::parallel_sort(\n    \"Sort indices\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      return x(i) < x(j);\n    },\n    indices\n  );\n\n  // Sort the values in x by the sorted indices\n  Kokkos::parallel_for(\n    \"Sort values\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      const int j = indices(i);\n      if (j!= i) {\n        const int tmp = x(i);\n        x(i) = x(j);\n        x(j) = tmp;\n      }\n    }\n  );\n}",
            "// Create a copy of x with the zero valued elements removed\n  Kokkos::View<int*> x_no_zero(Kokkos::view_alloc(Kokkos::WithoutInitializing, \"x_no_zero\"), x.extent(0));\n  int x_no_zero_size = 0;\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i)!= 0) {\n      x_no_zero(x_no_zero_size) = x(i);\n      x_no_zero_size++;\n    }\n  });\n\n  // Sort the copy of x\n  Kokkos::sort(x_no_zero);\n\n  // Copy the sorted copy back to x\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i)!= 0) {\n      x(i) = x_no_zero(i);\n    }\n  });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename ExecutionSpace::device_type;\n  using MemorySpace = typename ExecutionSpace::memory_space;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n\n  const int N = x.extent(0);\n  Kokkos::parallel_for(\n      \"sortIgnoreZero\",\n      Kokkos::TeamPolicy<ExecutionSpace>(N, Kokkos::AUTO),\n      KOKKOS_LAMBDA(const MemberType &member) {\n        const int i = member.league_rank();\n        if (x(i) == 0) {\n          return;\n        }\n        int j = i;\n        while (j > 0 && x(j - 1) > x(j)) {\n          Kokkos::swap(x(j - 1), x(j));\n          j--;\n        }\n      });\n}",
            "// Create a view of the indices of the array\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    idx(i) = i;\n  });\n\n  // Sort the indices by the values in the array\n  Kokkos::sort(idx, x);\n\n  // Create a view of the values of the array\n  Kokkos::View<int*> y(\"y\", x.size());\n\n  // Copy the values of the array into the new view\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    y(i) = x(i);\n  });\n\n  // Sort the values of the array\n  Kokkos::sort(y);\n\n  // Copy the sorted values back into the array\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = y(i);\n  });\n\n  // Copy the sorted indices back into the array\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = idx(i);\n  });\n}",
            "// Create a new array to hold the indices of the elements in x.\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices based on the values in x.\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i, int& update, bool final) {\n      if (x(i) > 0) {\n        update = x(i);\n      }\n    },\n    idx\n  );\n\n  // Sort the values in x based on the indices in idx.\n  Kokkos::parallel_for(\n    \"sort_x\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      int j = idx(i);\n      if (j!= i) {\n        int tmp = x(i);\n        x(i) = x(j);\n        x(j) = tmp;\n      }\n    }\n  );\n}",
            "// Create a Kokkos view of the indices of the array x.\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices by the values of x.\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      return x(i) < x(j);\n    },\n    idx\n  );\n\n  // Sort the values of x by the indices.\n  Kokkos::parallel_for(\n    \"sort_x\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      const int j = idx(i);\n      if (i!= j) {\n        const int tmp = x(i);\n        x(i) = x(j);\n        x(j) = tmp;\n      }\n    }\n  );\n}",
            "// Create a view of the indices of the array\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices by the values of the array\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i, int &update, bool final) {\n      if (x(i) == 0) {\n        update = 1;\n      }\n    },\n    KOKKOS_LAMBDA(int i, int j, int &update, bool final) {\n      if (x(i) < x(j)) {\n        update = 1;\n      }\n    },\n    idx\n  );\n\n  // Apply the permutation to the array\n  Kokkos::parallel_for(\n    \"apply_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      int tmp = x(i);\n      x(i) = x(idx(i));\n      x(idx(i)) = tmp;\n    }\n  );\n}",
            "// Create a view to hold the indices of the elements to sort\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation\n  Kokkos::parallel_for(\n    \"Initialize indices\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices by the values in x\n  Kokkos::parallel_sort(\n    \"Sort indices\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i, int& update, bool final) {\n      if (x(i) == 0) {\n        // If the value is zero, swap it with the last non-zero element\n        // and decrement the size of the array to sort\n        const int last = x.size() - 1;\n        if (i < last) {\n          Kokkos::swap(x(i), x(last));\n          Kokkos::swap(idx(i), idx(last));\n        }\n        update = -1;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i, int& update, bool final) {\n      // Compare the values in x\n      if (x(i) < x(i + update)) {\n        Kokkos::swap(x(i), x(i + update));\n        Kokkos::swap(idx(i), idx(i + update));\n      }\n    }\n  );\n\n  // Copy the sorted values back into x\n  Kokkos::parallel_for(\n    \"Copy sorted values\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = x(idx(i));\n    }\n  );\n}",
            "// Create a view for the indices of the elements to be sorted.\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices by the values of x.\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      return x(i) < x(j);\n    },\n    idx\n  );\n\n  // Apply the permutation to x.\n  Kokkos::parallel_for(\n    \"apply_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      const int j = idx(i);\n      if (j!= i) {\n        const int tmp = x(i);\n        x(i) = x(j);\n        x(j) = tmp;\n      }\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using PolicyType = Kokkos::RangePolicy<ExecutionSpace>;\n  using MemberType = Kokkos::MemberType<ExecutionSpace>;\n\n  // Create a parallel_for lambda to sort the array\n  Kokkos::parallel_for(\n    \"sortIgnoreZero\",\n    PolicyType(0, x.size()),\n    KOKKOS_LAMBDA(const MemberType &member) {\n      // Get the index of the current element\n      const int i = member.league_rank();\n\n      // Get the value of the current element\n      const int x_i = x(i);\n\n      // If the value is zero, do nothing\n      if (x_i == 0) {\n        return;\n      }\n\n      // Otherwise, find the index of the smallest element greater than x_i\n      int j = i;\n      while (j > 0 && x(j - 1) > x_i) {\n        x(j) = x(j - 1);\n        j--;\n      }\n\n      // Put x_i in its correct place\n      x(j) = x_i;\n    });\n\n  // Synchronize the view\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// Create a view to hold the indices of the elements to be sorted.\n  Kokkos::View<int*> idx(\"idx\", x.size());\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      idx[i] = i;\n    }\n  );\n\n  // Sort the indices based on the values in x.\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i, int& update, bool final) {\n      if (x[i] == 0) {\n        // If the value is zero, put it at the end of the list.\n        update = x.size() - 1;\n      } else {\n        // Otherwise, put it in the correct position.\n        update = i;\n      }\n    },\n    idx\n  );\n\n  // Reorder the elements in x based on the sorted indices.\n  Kokkos::parallel_for(\n    \"reorder_x\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      int j = idx[i];\n      if (j!= i) {\n        // Swap the value at position i with the value at position j.\n        int tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n      }\n    }\n  );\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n  int n = x.size();\n  int n_zero = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i] == 0) {\n      n_zero++;\n    }\n  }\n  int n_nonzero = n - n_zero;\n  std::vector<int> x_nonzero(n_nonzero);\n  int j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x_nonzero[j] = x[i];\n      j++;\n    }\n  }\n  std::sort(x_nonzero.begin(), x_nonzero.end());\n  j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x[i] = x_nonzero[j];\n      j++;\n    }\n  }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  int n_nonzero = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      n_nonzero++;\n    }\n  }\n  std::vector<int> x_nonzero(n_nonzero);\n  int j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x_nonzero[j] = x[i];\n      j++;\n    }\n  }\n  std::sort(x_nonzero.begin(), x_nonzero.end());\n  j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x[i] = x_nonzero[j];\n      j++;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  int n_zero = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i] == 0) {\n      n_zero++;\n    }\n  }\n  std::vector<int> x_no_zero(n - n_zero);\n  int j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x_no_zero[j] = x[i];\n      j++;\n    }\n  }\n\n  // sort x_no_zero\n  int n_threads = omp_get_max_threads();\n  int n_per_thread = x_no_zero.size() / n_threads;\n  int n_remain = x_no_zero.size() % n_threads;\n  std::vector<std::vector<int>> x_no_zero_split(n_threads);\n  for (int i = 0; i < n_threads; i++) {\n    if (i < n_remain) {\n      x_no_zero_split[i].resize(n_per_thread + 1);\n    } else {\n      x_no_zero_split[i].resize(n_per_thread);\n    }\n  }\n  for (int i = 0; i < n_threads; i++) {\n    for (int j = 0; j < x_no_zero_split[i].size(); j++) {\n      x_no_zero_split[i][j] = x_no_zero[i * n_per_thread + j];\n    }\n  }\n  std::vector<int> x_no_zero_sorted(x_no_zero.size());\n  #pragma omp parallel for\n  for (int i = 0; i < n_threads; i++) {\n    std::sort(x_no_zero_split[i].begin(), x_no_zero_split[i].end());\n  }\n  int k = 0;\n  for (int i = 0; i < n_threads; i++) {\n    for (int j = 0; j < x_no_zero_split[i].size(); j++) {\n      x_no_zero_sorted[k] = x_no_zero_split[i][j];\n      k++;\n    }\n  }\n\n  // merge x_no_zero_sorted and x_no_zero\n  std::vector<int> x_sorted(n);\n  int i = 0;\n  int j = 0;\n  for (int k = 0; k < n; k++) {\n    if (i < x_no_zero_sorted.size() && j < x_no_zero.size()) {\n      if (x_no_zero_sorted[i] < x_no_zero[j]) {\n        x_sorted[k] = x_no_zero_sorted[i];\n        i++;\n      } else {\n        x_sorted[k] = x_no_zero[j];\n        j++;\n      }\n    } else if (i < x_no_zero_sorted.size()) {\n      x_sorted[k] = x_no_zero_sorted[i];\n      i++;\n    } else if (j < x_no_zero.size()) {\n      x_sorted[k] = x_no_zero[j];\n      j++;\n    }\n  }\n\n  x = x_sorted;\n}",
            "// TODO: Your code here\n\tint n = x.size();\n\tint count = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tstd::vector<int> y(count);\n\tint j = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty[j] = x[i];\n\t\t\tj++;\n\t\t}\n\t}\n\tint k = 0;\n\tfor (int i = 0; i < n; i++) {\n\t\tif (x[i] == 0) {\n\t\t\tx[i] = y[k];\n\t\t\tk++;\n\t\t}\n\t}\n}",
            "// TODO: Your code here\n  int n = x.size();\n  std::vector<int> y(n);\n  int i = 0;\n  int j = 0;\n  int k = 0;\n  int l = 0;\n  int m = 0;\n  int nn = 0;\n  int count = 0;\n  int count1 = 0;\n  int count2 = 0;\n  int count3 = 0;\n  int count4 = 0;\n  int count5 = 0;\n  int count6 = 0;\n  int count7 = 0;\n  int count8 = 0;\n  int count9 = 0;\n  int count10 = 0;\n  int count11 = 0;\n  int count12 = 0;\n  int count13 = 0;\n  int count14 = 0;\n  int count15 = 0;\n  int count16 = 0;\n  int count17 = 0;\n  int count18 = 0;\n  int count19 = 0;\n  int count20 = 0;\n  int count21 = 0;\n  int count22 = 0;\n  int count23 = 0;\n  int count24 = 0;\n  int count25 = 0;\n  int count26 = 0;\n  int count27 = 0;\n  int count28 = 0;\n  int count29 = 0;\n  int count30 = 0;\n  int count31 = 0;\n  int count32 = 0;\n  int count33 = 0;\n  int count34 = 0;\n  int count35 = 0;\n  int count36 = 0;\n  int count37 = 0;\n  int count38 = 0;\n  int count39 = 0;\n  int count40 = 0;\n  int count41 = 0;\n  int count42 = 0;\n  int count43 = 0;\n  int count44 = 0;\n  int count45 = 0;\n  int count46 = 0;\n  int count47 = 0;\n  int count48 = 0;\n  int count49 = 0;\n  int count50 = 0;\n  int count51 = 0;\n  int count52 = 0;\n  int count53 = 0;\n  int count54 = 0;\n  int count55 = 0;\n  int count56 = 0;\n  int count57 = 0;\n  int count58 = 0;\n  int count59 = 0;\n  int count60 = 0;\n  int count61 = 0;\n  int count62 = 0;\n  int count63 = 0;\n  int count64 = 0;\n  int count65 = 0;\n  int count66 = 0;\n  int count67 = 0;\n  int count68 = 0;\n  int count69 = 0;\n  int count70 = 0;\n  int count71 = 0;\n  int count72 = 0;\n  int count73 = 0;\n  int count74 = 0;\n  int count75 = 0;\n  int count76 = 0;\n  int count77 = 0;\n  int count78 = 0;\n  int count79 = 0;\n  int count80 = 0;\n  int count81 = 0;\n  int count82 = 0;\n  int count83 = 0;\n  int count84 = 0;\n  int count85 = 0;\n  int count86 = 0;\n  int count87 = 0;\n  int count88 = 0;\n  int count89 = 0;\n  int count90 = 0;\n  int count91 = 0;\n  int count92 = 0;\n  int count93 = 0;\n  int count94 = 0;\n  int count95",
            "// TODO: implement this function\n}",
            "// TODO: Fill this in\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement this function\n\n  // Sort the vector x in ascending order ignoring elements with value 0.\n  // Leave zero valued elements in-place.\n  // Use OpenMP to sort x in parallel.\n  // Example:\n  //\n  // input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n  // output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n\n  // TODO: Implement",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use OpenMP to parallelize the loop\n  // Hint: Use the OpenMP reduction clause to sum the number of zero valued elements\n  int num_zero = 0;\n  #pragma omp parallel for reduction(+:num_zero)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == 0) {\n      num_zero++;\n    }\n  }\n  // TODO: Implement this function\n  // Hint: Use OpenMP to parallelize the loop\n  // Hint: Use the OpenMP reduction clause to sum the number of zero valued elements\n  std::vector<int> y(x.size() - num_zero);\n  int j = 0;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y[j] = x[i];\n      j++;\n    }\n  }\n  std::sort(y.begin(), y.end());\n  j = 0;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y[j];\n      j++;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\tint n = x.size();\n\tint *x_ptr = &x[0];\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tif (x_ptr[i] > x_ptr[j]) {\n\t\t\t\tint temp = x_ptr[i];\n\t\t\t\tx_ptr[i] = x_ptr[j];\n\t\t\t\tx_ptr[j] = temp;\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: Implement this function\n}",
            "// TODO: Your code here\n  int n = x.size();\n  int *x_ptr = &x[0];\n  int *x_ptr_sorted = new int[n];\n  int *x_ptr_sorted_ptr = &x_ptr_sorted[0];\n  int *x_ptr_sorted_ptr_end = &x_ptr_sorted[n];\n  int *x_ptr_sorted_ptr_end_minus_one = &x_ptr_sorted[n-1];\n  int *x_ptr_sorted_ptr_end_minus_two = &x_ptr_sorted[n-2];\n  int *x_ptr_sorted_ptr_end_minus_three = &x_ptr_sorted[n-3];\n  int *x_ptr_sorted_ptr_end_minus_four = &x_ptr_sorted[n-4];\n  int *x_ptr_sorted_ptr_end_minus_five = &x_ptr_sorted[n-5];\n  int *x_ptr_sorted_ptr_end_minus_six = &x_ptr_sorted[n-6];\n  int *x_ptr_sorted_ptr_end_minus_seven = &x_ptr_sorted[n-7];\n  int *x_ptr_sorted_ptr_end_minus_eight = &x_ptr_sorted[n-8];\n  int *x_ptr_sorted_ptr_end_minus_nine = &x_ptr_sorted[n-9];\n  int *x_ptr_sorted_ptr_end_minus_ten = &x_ptr_sorted[n-10];\n  int *x_ptr_sorted_ptr_end_minus_eleven = &x_ptr_sorted[n-11];\n  int *x_ptr_sorted_ptr_end_minus_twelve = &x_ptr_sorted[n-12];\n  int *x_ptr_sorted_ptr_end_minus_thirteen = &x_ptr_sorted[n-13];\n  int *x_ptr_sorted_ptr_end_minus_fourteen = &x_ptr_sorted[n-14];\n  int *x_ptr_sorted_ptr_end_minus_fifteen = &x_ptr_sorted[n-15];\n  int *x_ptr_sorted_ptr_end_minus_sixteen = &x_ptr_sorted[n-16];\n  int *x_ptr_sorted_ptr_end_minus_seventeen = &x_ptr_sorted[n-17];\n  int *x_ptr_sorted_ptr_end_minus_eighteen = &x_ptr_sorted[n-18];\n  int *x_ptr_sorted_ptr_end_minus_nineteen = &x_ptr_sorted[n-19];\n  int *x_ptr_sorted_ptr_end_minus_twenty = &x_ptr_sorted[n-20];\n  int *x_ptr_sorted_ptr_end_minus_twenty_one = &x_ptr_sorted[n-21];\n  int *x_ptr_sorted_ptr_end_minus_twenty_two = &x_ptr_sorted[n-22];\n  int *x_ptr_sorted_ptr_end_minus_twenty_three = &x_ptr_sorted[n-23];\n  int *x_ptr_sorted_ptr_end_minus_twenty_four = &x_ptr_sorted[n-24];\n  int *x_ptr_sorted_ptr_end_minus_twenty_five = &x_ptr_sorted[n-25];\n  int *x_ptr_sorted_ptr_end_minus_twenty_six = &x_ptr_sorted[n-26];\n  int *x_ptr_sorted_ptr_end_minus_twenty_seven = &x_ptr_sorted[n-27];\n  int *x_ptr_sorted_ptr_end_minus_twenty_eight = &x_ptr_sorted[n-28];\n  int *x_ptr_sorted_ptr_end_minus_twenty_nine = &x_ptr_sorted[n-29];\n  int *x_ptr_sorted_ptr_end_minus_thirty = &x_ptr_sorted",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n  int n = x.size();\n  int n_zero = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i] == 0) {\n      n_zero++;\n    }\n  }\n  int n_nonzero = n - n_zero;\n  std::vector<int> x_nonzero(n_nonzero);\n  int j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x_nonzero[j] = x[i];\n      j++;\n    }\n  }\n  std::sort(x_nonzero.begin(), x_nonzero.end());\n  j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x[i] = x_nonzero[j];\n      j++;\n    }\n  }\n}",
            "// TODO: Implement this function\n  int n = x.size();\n  int n_zero = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i] == 0) {\n      n_zero++;\n    }\n  }\n  int n_nonzero = n - n_zero;\n  std::vector<int> x_nonzero(n_nonzero);\n  std::vector<int> x_zero(n_zero);\n  int j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x_nonzero[j] = x[i];\n      j++;\n    } else {\n      x_zero[j] = x[i];\n      j++;\n    }\n  }\n  std::sort(x_nonzero.begin(), x_nonzero.end());\n  j = 0;\n  for (int i = 0; i < n; i++) {\n    if (x[i]!= 0) {\n      x[i] = x_nonzero[j];\n      j++;\n    } else {\n      x[i] = x_zero[j];\n      j++;\n    }\n  }\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<int> y;\n  for (auto i : x) {\n    if (i!= 0) {\n      y.push_back(i);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y[i];\n    }\n  }\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tx[i] = y[i];\n\t\t}\n\t}\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tx[i] = y[0];\n\t\t\ty.erase(y.begin());\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n  std::vector<int> temp;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      temp.push_back(x[i]);\n    }\n  }\n  std::sort(temp.begin(), temp.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = temp[i];\n    }\n  }\n}",
            "std::vector<int> y;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y.push_back(x[i]);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] == 0) {\n      y.push_back(0);\n    }\n  }\n  x = y;\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tx[i] = y[i];\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tint n = x.size();\n\tfor (int i = 0; i < n; i++) {\n\t\tint min = i;\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tif (x[j]!= 0 && x[j] < x[min]) {\n\t\t\t\tmin = j;\n\t\t\t}\n\t\t}\n\t\tif (min!= i) {\n\t\t\tint temp = x[i];\n\t\t\tx[i] = x[min];\n\t\t\tx[min] = temp;\n\t\t}\n\t}\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] == 0) {\n\t\t\ty.push_back(0);\n\t\t}\n\t}\n\tx = y;\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] == 0) {\n\t\t\ty.push_back(0);\n\t\t}\n\t}\n\tx = y;\n}",
            "std::vector<int> y;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y.push_back(x[i]);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y[i];\n    }\n  }\n}",
            "int n = x.size();\n\tfor (int i = 0; i < n; i++) {\n\t\tint j = i;\n\t\twhile (j > 0 && x[j - 1] > x[j]) {\n\t\t\tstd::swap(x[j - 1], x[j]);\n\t\t\tj--;\n\t\t}\n\t}\n}",
            "std::vector<int> y;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y.push_back(x[i]);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y.back();\n      y.pop_back();\n    }\n  }\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] == 0) {\n\t\t\ty.push_back(0);\n\t\t}\n\t}\n\tx = y;\n}",
            "// TODO: implement\n}",
            "std::vector<int> y;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y.push_back(x[i]);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y[i];\n    }\n  }\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] == 0) {\n\t\t\ty.push_back(0);\n\t\t}\n\t}\n\tx = y;\n}",
            "std::vector<int> y;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      y.push_back(x[i]);\n    }\n  }\n  std::sort(y.begin(), y.end());\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      x[i] = y[i];\n    }\n  }\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tx[i] = y[i];\n\t\t}\n\t}\n}",
            "std::vector<int> y;\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\ty.push_back(x[i]);\n\t\t}\n\t}\n\tstd::sort(y.begin(), y.end());\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i]!= 0) {\n\t\t\tx[i] = y[0];\n\t\t\ty.erase(y.begin());\n\t\t}\n\t}\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "hip",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int i = tid;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int temp = x[i];\n        x[i] = x[i - 1];\n        x[i - 1] = temp;\n        i--;\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int j = tid;\n      while (j > 0 && x[j - 1] > x[j]) {\n        int temp = x[j];\n        x[j] = x[j - 1];\n        x[j - 1] = temp;\n        j--;\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int i = tid;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int temp = x[i];\n        x[i] = x[i - 1];\n        x[i - 1] = temp;\n        i--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tif (x[i]!= 0) {\n\t\t\tint j = i - 1;\n\t\t\twhile (j >= 0 && x[j] > x[i]) {\n\t\t\t\tx[j + 1] = x[j];\n\t\t\t\tj--;\n\t\t\t}\n\t\t\tx[j + 1] = x[i];\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int i = tid;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int temp = x[i - 1];\n        x[i - 1] = x[i];\n        x[i] = temp;\n        i--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int i = tid;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int temp = x[i];\n        x[i] = x[i - 1];\n        x[i - 1] = temp;\n        i--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid]!= 0) {\n      int i = tid;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int tmp = x[i];\n        x[i] = x[i - 1];\n        x[i - 1] = tmp;\n        i--;\n      }\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (x[idx]!= 0) {\n      int i = idx;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int tmp = x[i];\n        x[i] = x[i - 1];\n        x[i - 1] = tmp;\n        i--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i >= N) return;\n\tif (x[i] == 0) return;\n\tint j = i;\n\twhile (j > 0 && x[j-1] > x[j]) {\n\t\tint tmp = x[j];\n\t\tx[j] = x[j-1];\n\t\tx[j-1] = tmp;\n\t\tj--;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size() / size;\n  int local_rank = rank * local_size;\n\n  std::vector<int> local_x(local_size);\n  std::copy(x.begin() + local_rank, x.begin() + local_rank + local_size, local_x.begin());\n\n  std::sort(local_x.begin(), local_x.end());\n\n  if (rank == 0) {\n    std::copy(local_x.begin(), local_x.end(), x.begin() + local_rank);\n  }\n\n  MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::sort(x.begin(), x.end());\n  }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  //\n  // Hint: You can use the std::sort function from the C++ standard library.\n  //\n  // Hint: You can use the std::stable_sort function from the C++ standard library.\n  //\n  // Hint: You can use the std::partition function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::sort_heap function from the C++ standard library.\n  //\n  // Hint: You can use the std::partial_sort function from the C++ standard library.\n  //\n  // Hint: You can use the std::partial_sort_copy function from the C++ standard library.\n  //\n  // Hint: You can use the std::partial_sort_copy_if function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.\n  //\n  // Hint: You can use the std::nth_element function from the C++ standard library.",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  //...\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = x.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<int> x_local(n_per_proc);\n\tstd::vector<int> x_local_sorted(n_per_proc);\n\tstd::vector<int> x_local_sorted_no_zero(n_per_proc);\n\tstd::vector<int> x_local_sorted_no_zero_recv(n_per_proc);\n\tstd::vector<int> x_local_sorted_no_zero_recv_all(n);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n; i++) {\n\t\t\tx_local[i] = x[i];\n\t\t}\n\t}\n\n\tMPI_Scatter(x_local.data(), n_per_proc, MPI_INT, x_local_sorted.data(), n_per_proc, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < n_per_proc; i++) {\n\t\tif (x_local_sorted[i]!= 0) {\n\t\t\tx_local_sorted_no_zero.push_back(x_local_sorted[i]);\n\t\t}\n\t}\n\n\tMPI_Gather(x_local_sorted_no_zero.data(), n_per_proc, MPI_INT, x_local_sorted_no_zero_recv.data(), n_per_proc, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\tfor (int j = 0; j < n_per_proc; j++) {\n\t\t\t\tx_local_sorted_no_zero_recv_all[i * n_per_proc + j] = x_local_sorted_no_zero_recv[j];\n\t\t\t}\n\t\t}\n\t\tstd::sort(x_local_sorted_no_zero_recv_all.begin(), x_local_sorted_no_zero_recv_all.end());\n\t\tfor (int i = 0; i < n; i++) {\n\t\t\tx[i] = x_local_sorted_no_zero_recv_all[i];\n\t\t}\n\t}\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // TODO: implement\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  std::vector<int> recv(x.size());\n  int count = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i]!= 0) {\n      count++;\n    }\n  }\n  int recv_count = 0;\n  MPI_Reduce(&count, &recv_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  std::vector<int> recv_x(recv_count);\n  MPI_Gatherv(&x[0], count, MPI_INT, &recv_x[0], &count, &count, MPI_INT, 0,\n              MPI_COMM_WORLD);\n  if (rank == 0) {\n    std::vector<int> sorted_x(recv_x.size());\n    std::copy(recv_x.begin(), recv_x.end(), sorted_x.begin());\n    std::sort(sorted_x.begin(), sorted_x.end());\n    std::copy(sorted_x.begin(), sorted_x.end(), recv_x.begin());\n    std::vector<int> send_x(x.size());\n    int send_count = 0;\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i]!= 0) {\n        send_x[send_count] = recv_x[send_count];\n        send_count++;\n      } else {\n        send_x[send_count] = x[i];\n        send_count++;\n      }\n    }\n    MPI_Scatterv(&send_x[0], &count, &count, MPI_INT, &x[0], count, MPI_INT, 0,\n                 MPI_COMM_WORLD);\n  } else {\n    MPI_Scatterv(&recv_x[0], &count, &count, MPI_INT, &x[0], count, MPI_INT, 0,\n                 MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  std::vector<int> x_sorted(n);\n  std::vector<int> x_sorted_local(n);\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code\n\n  // TODO: Fill in the code",
            "// TODO: Your code here\n}",
            "}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "// TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  // TODO: Implement this function\n\n  //",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N && x[i]!= 0) {\n    int j = i - 1;\n    int temp = x[i];\n    while (j >= 0 && x[j] > temp) {\n      x[j + 1] = x[j];\n      j = j - 1;\n    }\n    x[j + 1] = temp;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tif (x[i]!= 0) {\n\t\t\tint j = i - 1;\n\t\t\twhile (j >= 0 && x[j] > x[i]) {\n\t\t\t\tx[j + 1] = x[j];\n\t\t\t\tj--;\n\t\t\t}\n\t\t\tx[j + 1] = x[i];\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N && x[i]!= 0) {\n\t\tint j = i - 1;\n\t\tint temp = x[i];\n\t\twhile (j >= 0 && x[j] > temp) {\n\t\t\tx[j + 1] = x[j];\n\t\t\tj = j - 1;\n\t\t}\n\t\tx[j + 1] = temp;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i;\n      while (j > 0 && x[j - 1] > x[j]) {\n        int temp = x[j];\n        x[j] = x[j - 1];\n        x[j - 1] = temp;\n        j--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (x[idx]!= 0) {\n      int min = x[idx];\n      int minIdx = idx;\n      for (int i = idx + 1; i < N; i++) {\n        if (x[i]!= 0 && x[i] < min) {\n          min = x[i];\n          minIdx = i;\n        }\n      }\n      x[minIdx] = x[idx];\n      x[idx] = min;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i]!= 0) {\n            int j = i - 1;\n            while (j >= 0 && x[j] > x[i]) {\n                x[j + 1] = x[j];\n                j--;\n            }\n            x[j + 1] = x[i];\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i - 1;\n      while (j >= 0 && x[j] > x[i]) {\n        x[j + 1] = x[j];\n        j--;\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tif (x[idx]!= 0) {\n\t\t\tint temp = x[idx];\n\t\t\tint i = idx - 1;\n\t\t\twhile (i >= 0 && x[i] > temp) {\n\t\t\t\tx[i + 1] = x[i];\n\t\t\t\ti--;\n\t\t\t}\n\t\t\tx[i + 1] = temp;\n\t\t}\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    if (x[idx] == 0) return;\n    int min = x[idx];\n    int minIdx = idx;\n    for (int i = idx + 1; i < N; i++) {\n        if (x[i] == 0) continue;\n        if (x[i] < min) {\n            min = x[i];\n            minIdx = i;\n        }\n    }\n    x[minIdx] = x[idx];\n    x[idx] = min;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j;\n      for (j = i - 1; j >= 0 && x[j] > x[i]; j--) {\n        x[j + 1] = x[j];\n      }\n      x[j + 1] = x[i];\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i]!= 0) {\n      int j = i;\n      while (j > 0 && x[j - 1] > x[j]) {\n        int tmp = x[j];\n        x[j] = x[j - 1];\n        x[j - 1] = tmp;\n        j--;\n      }\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (x[idx]!= 0) {\n      int i = idx;\n      while (i > 0 && x[i - 1] > x[i]) {\n        int tmp = x[i - 1];\n        x[i - 1] = x[i];\n        x[i] = tmp;\n        i--;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tif (x[i]!= 0) {\n\t\t\tint j = i - 1;\n\t\t\twhile (j >= 0 && x[j] > x[i]) {\n\t\t\t\tx[j + 1] = x[j];\n\t\t\t\tj--;\n\t\t\t}\n\t\t\tx[j + 1] = x[i];\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_sort_non-zero_elements",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI and OpenMP to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  // You may use OpenMP to parallelize the sorting\n  // You may use MPI to distribute the sorting\n  // You may use both MPI and OpenMP to parallelize the sorting\n\n  // Example:\n  // #pragma omp parallel for\n  // for (int i = 0; i < x.size(); i++) {\n  //   x[i] = x[i] + 1;\n  // }\n}",
            "// TODO: Your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  std::vector<int> x_local;\n  std::vector<int> x_local_sorted;\n  std::vector<int> x_global;\n  std::vector<int> x_global_sorted;\n\n  if (rank == 0) {\n    x_global = x;\n  }\n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    x_local.resize(n);\n  }\n\n  MPI_Scatter(x_global.data(), n, MPI_INT, x_local.data(), n, MPI_INT, 0,\n              MPI_COMM_WORLD);\n\n  x_local_sorted = x_local;\n  std::sort(x_local_sorted.begin(), x_local_sorted.end());\n\n  MPI_Gather(x_local_sorted.data(), n, MPI_INT, x_global_sorted.data(), n,\n             MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = x_global_sorted;\n  }\n}",
            "// TODO: Implement this function\n\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank\n  // Hint: use std::sort\n\n  // TODO: sort x in parallel\n  // Hint: use MPI_Send, MPI_Recv, MPI_Scatter, MPI_Gather, MPI_Allgather\n  // Hint: use OpenMP to sort the subvector of x on each rank",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_elements = x.size();\n  int num_elements_per_rank = num_elements / size;\n  int num_elements_left = num_elements % size;\n\n  int num_elements_per_rank_local = num_elements_per_rank;\n  if (rank < num_elements_left) {\n    num_elements_per_rank_local++;\n  }\n\n  std::vector<int> x_local(num_elements_per_rank_local);\n  MPI_Scatter(x.data(), num_elements_per_rank, MPI_INT, x_local.data(),\n              num_elements_per_rank_local, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Sort x_local in-place\n  #pragma omp parallel for\n  for (int i = 0; i < num_elements_per_rank_local; i++) {\n    for (int j = i + 1; j < num_elements_per_rank_local; j++) {\n      if (x_local[i] > x_local[j]) {\n        int temp = x_local[i];\n        x_local[i] = x_local[j];\n        x_local[j] = temp;\n      }\n    }\n  }\n\n  // Gather the sorted local vectors into x\n  MPI_Gather(x_local.data(), num_elements_per_rank_local, MPI_INT, x.data(),\n             num_elements_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Sort x on rank 0\n  if (rank == 0) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; i++) {\n      for (int j = i + 1; j < num_elements; j++) {\n        if (x[i] > x[j]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_proc = n / size;\n\tint n_rem = n % size;\n\n\tstd::vector<int> x_local(n_per_proc + n_rem);\n\tMPI_Scatter(x.data(), n_per_proc + n_rem, MPI_INT, x_local.data(), n_per_proc + n_rem, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t// sort x_local\n\tstd::sort(x_local.begin(), x_local.end());\n\n\t// gather x_local\n\tstd::vector<int> x_global(n);\n\tMPI_Gather(x_local.data(), n_per_proc + n_rem, MPI_INT, x_global.data(), n_per_proc + n_rem, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t// copy x_global to x\n\tif (rank == 0) {\n\t\tx = x_global;\n\t}\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  std::vector<int> x_local(n_per_rank);\n  std::vector<int> x_local_sorted(n_per_rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i + rank * n_per_rank];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i + rank * n_per_rank];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i] = x_local[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i + rank * n_per_rank] = x_local[i];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i + rank * n_per_rank];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i] = x_local_sorted[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i + rank * n_per_rank] = x_local_sorted[i];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i + rank * n_per_rank];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local_sorted[i] = x[i + rank * n_per_rank];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i] = x_local_sorted[i];\n    }\n  } else {\n    for (int i = 0; i < n_per_rank; i++) {\n      x[i + rank * n_per_rank] = x_local_sorted[i];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < n_per_rank; i++) {\n      x_local[i] = x[i];\n    }\n  } else {\n    for (int i = 0;",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  // You may need to use MPI_Send, MPI_Recv, MPI_Sendrecv, MPI_Bcast, MPI_Reduce, MPI_Allreduce, MPI_Scatter, MPI_Gather, MPI_Allgather, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Barrier, MPI_Wait, MPI_Waitall, MPI_Waitany, MPI_Waitany, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Waitall, MPI_Wait",
            "int size = x.size();\n  int rank = 0;\n  int num_ranks = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // TODO: Your code here\n  int *x_copy = new int[size];\n  for (int i = 0; i < size; i++) {\n    x_copy[i] = x[i];\n  }\n\n  int *x_sorted = new int[size];\n  int *x_sorted_copy = new int[size];\n  int *x_sorted_copy_temp = new int[size];\n  int *x_sorted_copy_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp = new int[size];\n\n  int *x_sorted_copy_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[size];\n  int *x_sorted_copy_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint local_size = x.size() / size;\n\tint local_offset = rank * local_size;\n\n\tstd::vector<int> local_x(local_size);\n\tfor (int i = 0; i < local_size; i++) {\n\t\tlocal_x[i] = x[local_offset + i];\n\t}\n\n\tstd::vector<int> local_sorted_x(local_size);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < local_size; i++) {\n\t\tlocal_sorted_x[i] = local_x[i];\n\t}\n\n\tstd::sort(local_sorted_x.begin(), local_sorted_x.end());\n\n\tfor (int i = 0; i < local_size; i++) {\n\t\tx[local_offset + i] = local_sorted_x[i];\n\t}\n\n\tif (rank == 0) {\n\t\tstd::vector<int> sorted_x(x.size());\n\t\tfor (int i = 0; i < x.size(); i++) {\n\t\t\tsorted_x[i] = x[i];\n\t\t}\n\n\t\tstd::sort(sorted_x.begin(), sorted_x.end());\n\n\t\tfor (int i = 0; i < x.size(); i++) {\n\t\t\tx[i] = sorted_x[i];\n\t\t}\n\t}\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = x.size();\n\tint n_per_rank = n / size;\n\tint n_extra = n % size;\n\n\tstd::vector<int> x_local(n_per_rank + (rank < n_extra));\n\tstd::vector<int> x_local_sorted(n_per_rank + (rank < n_extra));\n\n\tif (rank < n_extra) {\n\t\tfor (int i = 0; i < n_per_rank + 1; i++) {\n\t\t\tx_local[i] = x[rank * n_per_rank + i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n_per_rank; i++) {\n\t\t\tx_local[i] = x[rank * n_per_rank + i];\n\t\t}\n\t}\n\n\t// Sort x_local in parallel\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n_per_rank + (rank < n_extra); i++) {\n\t\tx_local_sorted[i] = x_local[i];\n\t}\n\n\tstd::sort(x_local_sorted.begin(), x_local_sorted.end());\n\n\t// Gather the sorted vectors from all ranks\n\tstd::vector<int> x_sorted(n);\n\tMPI_Gather(x_local_sorted.data(), n_per_rank + (rank < n_extra), MPI_INT, x_sorted.data(), n_per_rank + (rank < n_extra), MPI_INT, 0, MPI_COMM_WORLD);\n\n\t// Copy the sorted vector back to x\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n; i++) {\n\t\t\tx[i] = x_sorted[i];\n\t\t}\n\t}\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n\n  std::vector<int> x_local(n_per_rank + n_remainder);\n  std::vector<int> x_local_sorted(n_per_rank + n_remainder);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x_local[i] = x[i];\n    }\n  }\n\n  MPI_Scatter(x_local.data(), n_per_rank + n_remainder, MPI_INT,\n              x_local_sorted.data(), n_per_rank + n_remainder, MPI_INT, 0,\n              MPI_COMM_WORLD);\n\n  // Sort x_local_sorted in parallel\n  #pragma omp parallel for\n  for (int i = 0; i < n_per_rank + n_remainder; i++) {\n    if (x_local_sorted[i]!= 0) {\n      std::sort(x_local_sorted.begin() + i, x_local_sorted.end());\n    }\n  }\n\n  MPI_Gather(x_local_sorted.data(), n_per_rank + n_remainder, MPI_INT,\n             x.data(), n_per_rank + n_remainder, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = x_local[i];\n    }\n  }\n}",
            "int size = x.size();\n\tint rank = 0;\n\tint num_ranks = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n\tint num_threads = omp_get_max_threads();\n\tint chunk_size = size / num_threads;\n\tint remainder = size % num_threads;\n\n\tstd::vector<int> local_x(chunk_size + remainder);\n\tstd::vector<int> local_y(chunk_size + remainder);\n\n\t// Copy x to local_x\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++) {\n\t\tlocal_x[i] = x[i];\n\t}\n\n\t// Sort local_x\n\t#pragma omp parallel for\n\tfor (int i = 0; i < chunk_size + remainder; i++) {\n\t\tif (local_x[i]!= 0) {\n\t\t\tlocal_y[i] = local_x[i];\n\t\t}\n\t}\n\n\t// Sort local_y\n\t#pragma omp parallel for\n\tfor (int i = 0; i < chunk_size + remainder; i++) {\n\t\tif (local_y[i]!= 0) {\n\t\t\tlocal_x[i] = local_y[i];\n\t\t}\n\t}\n\n\t// Copy local_x to x\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++) {\n\t\tx[i] = local_x[i];\n\t}\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  // Hint: Use the `std::sort` function to sort the vector `x`\n  // Hint: Use the `std::lower_bound` function to find the index of a value in a sorted vector\n  // Hint: Use the `omp_get_num_threads` and `omp_get_thread_num` functions to get the number of threads and the thread number\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // Hint: Use the `omp_get_num_threads` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num` function to get the thread number\n  // H",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use the `std::vector` constructor that takes a size and a value\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num_procs()` to get the number of processors\n    // Hint: Use `omp_get_num_threads()` to get the number of threads\n    // Hint: Use `omp_get_thread_num()` to get the thread number\n    // Hint: Use `omp_get_num",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n  // Hint: You can use std::sort to sort the vector\n  // Hint: You can use std::lower_bound to find the index of an element in a sorted vector\n  // Hint: You can use std::distance to compute the distance between two iterators\n\n  // Sort the vector\n  std::vector<float> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n\n  // Find the index of each element in the sorted vector\n  ranks.resize(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    auto it = std::lower_bound(x_sorted.begin(), x_sorted.end(), x[i]);\n    ranks[i] = std::distance(x_sorted.begin(), it);\n  }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num()` function to get the thread id\n  // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n  // Hint: Use the `omp_get_num_procs()` function to get the number of processors\n  // Hint: Use the `omp_get_thread_num",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  // Hint: Use the `std::sort` function to sort the vector `x`\n  // Hint: Use the `std::lower_bound` function to find the index of an element in a sorted vector\n  // Hint: Use the `omp_get_num_threads` and `omp_get_thread_num` functions to find the number of threads and the thread id\n  // Hint: Use the `omp_get_num_procs` function to find the number of processors\n  // Hint: Use the `omp_get_max_threads` function to find the maximum number of threads\n  // Hint: Use the `omp_get_dynamic` function to find the dynamic thread adjustment\n  // Hint: Use the `omp_get_nested` function to find the nested parallelism\n  // Hint: Use the `omp_get_cancellation` function to find the cancellation\n  // Hint: Use the `omp_get_thread_limit` function to find the thread limit\n  // Hint: Use the `omp_in_parallel` function to find if the code is running in parallel\n  // Hint: Use the `omp_get_num_procs` function to find the number of processors\n  // Hint: Use the `omp_get_num_threads` function to find the number of threads\n  // Hint: Use the `omp_get_thread_num` function to find the thread id\n  // Hint: Use the `omp_get_max_threads` function to find the maximum number of threads\n  // Hint: Use the `omp_get_dynamic` function to find the dynamic thread adjustment\n  // Hint: Use the `omp_get_nested` function to find the nested parallelism\n  // Hint: Use the `omp_get_cancellation` function to find the cancellation\n  // Hint: Use the `omp_get_thread_limit` function to find the thread limit\n  // Hint: Use the `omp_in_parallel` function to find if the code is running in parallel\n  // Hint: Use the `omp_get_num_procs` function to find the number of processors\n  // Hint: Use the `omp_get_num_threads` function to find the number of threads\n  // Hint: Use the `omp_get_thread_num` function to find the thread id\n  // Hint: Use the `omp_get_max_threads` function to find the maximum number of threads\n  // Hint: Use the `omp_get_dynamic` function to find the dynamic thread adjustment\n  // Hint: Use the `omp_get_nested` function to find the nested parallelism\n  // Hint: Use the `omp_get_cancellation` function to find the cancellation\n  // Hint: Use the `omp_get_thread_limit` function to find the thread limit\n  // Hint: Use the `omp_in_parallel` function to find if the code is running in parallel\n  // Hint: Use the `omp_get_num_procs` function to find the number of processors\n  // Hint: Use the `omp_get_num_threads` function to find the number of threads\n  // Hint: Use the `omp_get_thread_num` function to find the thread id\n  // Hint: Use the `omp_get_max_threads` function to find the maximum number of threads\n  // Hint: Use the `omp_get_dynamic` function to find the dynamic thread adjustment\n  // Hint: Use the `omp_get_nested` function to find the nested parallelism\n  // Hint: Use the `omp_get_cancellation` function to find the cancellation\n  // Hint: Use the `omp_get_thread_limit` function to find the thread limit\n  // Hint: Use the `omp_in_parallel` function to find if the code is running in parallel\n  // Hint: Use the `omp_get_num_procs` function to find the number of processors\n  // Hint: Use the `omp_get_num_threads` function to find the number of threads\n  // Hint: Use the `omp_get_thread_num` function to find the thread id\n  // Hint: Use the `omp_get_max_threads` function to find the maximum number of threads\n  // Hint: Use the `omp_get_dynamic` function to find the dynamic thread adjustment",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "serial",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    for (size_t i = 0; i < x.size(); i++) {\n        for (size_t j = 0; j < sorted_x.size(); j++) {\n            if (x[i] == sorted_x[j]) {\n                ranks[i] = j;\n            }\n        }\n    }\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); i++) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO: Implement this function\n    std::vector<float> x_sorted = x;\n    std::sort(x_sorted.begin(), x_sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); i++) {\n        ranks[i] = std::distance(x_sorted.begin(), std::find(x_sorted.begin(), x_sorted.end(), x[i]));\n    }\n}",
            "// TODO: Implement this function\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); i++) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO: Implement this function\n  std::vector<float> sorted_x = x;\n  std::sort(sorted_x.begin(), sorted_x.end());\n  ranks.resize(x.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n  }\n}",
            "std::vector<float> sorted_x = x;\n  std::sort(sorted_x.begin(), sorted_x.end());\n  ranks.resize(x.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n  }\n}",
            "std::vector<float> sorted_x = x;\n  std::sort(sorted_x.begin(), sorted_x.end());\n  ranks.resize(x.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n  }\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO: Implement this function\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "std::vector<float> sorted_x = x;\n  std::sort(sorted_x.begin(), sorted_x.end());\n  ranks.resize(x.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n  }\n}",
            "std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}",
            "// TODO: implement this function\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n    }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "hip",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "// TODO: Implement this function\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int i = 0;\n    for (i = 0; i < N; i++) {\n      if (x[tid] < x[i]) {\n        ranks[tid] = i;\n        break;\n      }\n    }\n    if (i == N) {\n      ranks[tid] = i;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "// TODO: Implement this function\n  //\n  // Hint: Use the atomicMin function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the maximum value in x.\n  //       Use the atomicAdd function to increment the value at the index.\n  //       Use the atomicCAS function to find the index of the minimum value in x.",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    ranks[tid] = tid;\n  }\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        ranks[tid] = tid;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    ranks[tid] = tid;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  // You may assume that the length of x is divisible by size\n  // You may assume that x is sorted\n  // You may assume that ranks is empty\n\n  // TODO: Your code here\n  // You may assume that the length of x is divisible by size\n  // You may assume that x is sorted\n  // You may assume that ranks is empty\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    ranks.resize(x.size());\n  }\n  // TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    std::vector<float> local_x;\n    std::vector<size_t> local_ranks;\n    if (rank == 0) {\n        local_x = x;\n    }\n    else {\n        local_x = std::vector<float>(x.size() / size);\n    }\n    MPI_Scatter(x.data(), local_x.size(), MPI_FLOAT, local_x.data(), local_x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    std::sort(local_x.begin(), local_x.end());\n    local_ranks.resize(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::find(local_x.begin(), local_x.end(), local_x[i]));\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  std::vector<float> x_local;\n  std::vector<size_t> ranks_local;\n  if (rank == 0) {\n    x_local = x;\n    ranks_local.resize(x.size());\n  }\n  else {\n    x_local.resize(x.size() / size);\n    ranks_local.resize(x.size() / size);\n  }\n  MPI_Scatter(x.data(), x.size() / size, MPI_FLOAT, x_local.data(), x.size() / size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n  for (size_t i = 0; i < x_local.size(); i++) {\n    ranks_local[i] = i;\n  }\n  std::sort(x_local.begin(), x_local.end());\n  for (size_t i = 0; i < x_local.size(); i++) {\n    for (size_t j = 0; j < x_local.size(); j++) {\n      if (x_local[i] == x[j]) {\n        ranks_local[i] = j;\n        break;\n      }\n    }\n  }\n  MPI_Gather(ranks_local.data(), ranks_local.size(), MPI_UNSIGNED_LONG, ranks.data(), ranks_local.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n\n  // For debugging\n  if (rank == 0) {\n    for (size_t i = 0; i < ranks.size(); ++i) {\n      std::cout << \"Rank \" << ranks[i] << \" for value \" << x[i] << std::endl;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "cuda",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use atomicAdd() to update the ranks\n  // Hint: Use __syncthreads() to synchronize threads\n  // Hint: Use __shfl_down() to share values between threads\n  // Hint: Use __shfl_up() to share values between threads\n  // Hint: Use __shfl() to share values between threads\n  // Hint: Use __shfl_sync() to share values between threads\n  // Hint: Use __shfl_up_sync() to share values between threads\n  // Hint: Use __shfl_down_sync() to share values between threads\n  // Hint: Use __shfl_xor() to share values between threads\n  // Hint: Use __shfl_xor_sync() to share values between threads\n  // Hint: Use __ballot() to share values between threads\n  // Hint: Use __ballot_sync() to share values between threads\n  // Hint: Use __any() to share values between threads\n  // Hint: Use __any_sync() to share values between threads\n  // Hint: Use __all() to share values between threads\n  // Hint: Use __all_sync() to share values between threads\n  // Hint: Use __popc() to share values between threads\n  // Hint: Use __popcll() to share values between threads\n  // Hint: Use __clz() to share values between threads\n  // Hint: Use __clzll() to share values between threads\n  // Hint: Use __ffs() to share values between threads\n  // Hint: Use __ffsll() to share values between threads\n  // Hint: Use __brev() to share values between threads\n  // Hint: Use __brevll() to share values between threads\n  // Hint: Use __byte_perm() to share values between threads\n  // Hint: Use __byte_permll() to share values between threads\n  // Hint: Use __syncwarp() to synchronize threads\n  // Hint: Use __syncwarp_or() to synchronize threads\n  // Hint: Use __syncwarp_and() to synchronize threads\n  // Hint: Use __syncwarp_xor() to synchronize threads\n  // Hint: Use __syncwarp_add() to synchronize threads\n  // Hint: Use __syncwarp_max() to synchronize threads\n  // Hint: Use __syncwarp_min() to synchronize threads\n  // Hint: Use __syncwarp_or_sync() to synchronize threads\n  // Hint: Use __syncwarp_and_sync() to synchronize threads\n  // Hint: Use __syncwarp_xor_sync() to synchronize threads\n  // Hint: Use __syncwarp_add_sync() to synchronize threads\n  // Hint: Use __syncwarp_max_sync() to synchronize threads\n  // Hint: Use __syncwarp_min_sync() to synchronize threads\n  // Hint: Use __syncwarp_count() to synchronize threads\n  // Hint: Use __syncwarp_count_sync() to synchronize threads\n  // Hint: Use __syncwarp_active() to synchronize threads\n  // Hint: Use __syncwarp_active_sync() to synchronize threads\n  // Hint: Use __syncwarp_active_mask() to synchronize threads\n  // Hint: Use __syncwarp_active_mask_sync() to synchronize threads\n  // Hint: Use __syncwarp_ballot() to synchronize threads\n  // Hint: Use __syncwarp_ballot_sync() to synchronize threads\n  // Hint: Use __syncwarp_match_any() to synchronize threads\n  // Hint: Use __syncwarp_match_any_sync() to synchronize threads\n  // Hint: Use __syncwarp_match_all() to synchronize threads\n  // Hint: Use __syncwarp_match_all_sync() to synchronize threads\n  // Hint: Use __syncwarp_nand() to synchronize threads\n  // Hint: Use __syncwarp_nand_sync() to synchronize threads\n  // Hint: Use __syncwarp_nor() to synchronize threads\n  // Hint: Use __syncwarp_nor_sync() to synchronize threads",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    // TODO: Implement this\n  }\n}",
            "// TODO: Implement this function\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        ranks[idx] = idx;\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int block_size = blockDim.x;\n    int i = bid * block_size + tid;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    ranks[i] = i;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        ranks[i] = i;\n    }\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "39_sort_sorted_ranks",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * chunk + std::min(rank, remainder);\n  int end = (rank + 1) * chunk + std::min(rank + 1, remainder);\n  std::vector<float> x_rank(x.begin() + start, x.begin() + end);\n  std::vector<size_t> ranks_rank(x_rank.size());\n  std::vector<float> x_sorted(x_rank);\n  std::sort(x_sorted.begin(), x_sorted.end());\n  for (int i = 0; i < x_rank.size(); i++) {\n    for (int j = 0; j < x_sorted.size(); j++) {\n      if (x_rank[i] == x_sorted[j]) {\n        ranks_rank[i] = j;\n        break;\n      }\n    }\n  }\n  if (rank == 0) {\n    ranks.resize(x.size());\n  }\n  MPI_Gather(ranks_rank.data(), x_rank.size(), MPI_UNSIGNED_LONG, ranks.data(), x_rank.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  // You can use OpenMP to parallelize the inner loop\n  // You can use MPI to parallelize the outer loop\n  // You can use MPI to send and receive data\n  // You can use MPI to synchronize processes\n  // You can use MPI to broadcast data\n  // You can use MPI to reduce data\n  // You can use MPI to scatter data\n  // You can use MPI to gather data\n  // You can use MPI to all-to-all data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use MPI to all-to-allw data\n  // You can use MPI to all-to-allv data\n  // You can use",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_per_proc = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<float> local_x;\n    if (rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            local_x.push_back(x[i]);\n        }\n    }\n    for (int i = remainder; i < x.size(); i += size) {\n        local_x.push_back(x[i]);\n    }\n\n    std::vector<float> local_sorted_x;\n    if (rank == 0) {\n        local_sorted_x = local_x;\n        std::sort(local_sorted_x.begin(), local_sorted_x.end());\n    }\n\n    MPI_Bcast(local_sorted_x.data(), local_sorted_x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    std::vector<size_t> local_ranks;\n    if (rank == 0) {\n        local_ranks.resize(local_x.size());\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); i++) {\n        int rank = std::lower_bound(local_sorted_x.begin(), local_sorted_x.end(), local_x[i]) - local_sorted_x.begin();\n        local_ranks[i] = rank;\n    }\n\n    MPI_Gather(local_ranks.data(), num_per_proc + (rank < remainder? 1 : 0), MPI_UNSIGNED_LONG,\n               ranks.data(), num_per_proc + (rank < remainder? 1 : 0), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    std::vector<float> x_local;\n    std::vector<size_t> ranks_local;\n    int n = x.size();\n    int n_local = n / size;\n    int n_local_last = n - (n_local * (size - 1));\n    if (rank == 0) {\n        x_local.resize(n_local_last);\n        ranks_local.resize(n_local_last);\n        for (int i = 0; i < n_local_last; i++) {\n            x_local[i] = x[i];\n        }\n    } else {\n        x_local.resize(n_local);\n        ranks_local.resize(n_local);\n        for (int i = 0; i < n_local; i++) {\n            x_local[i] = x[i + (n_local * (rank - 1))];\n        }\n    }\n\n    std::vector<float> x_sorted(x_local);\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    #pragma omp parallel for\n    for (int i = 0; i < x_local.size(); i++) {\n        for (int j = 0; j < x_sorted.size(); j++) {\n            if (x_local[i] == x_sorted[j]) {\n                ranks_local[i] = j;\n                break;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        ranks.resize(n);\n        for (int i = 0; i < n_local_last; i++) {\n            ranks[i] = ranks_local[i];\n        }\n        for (int i = 0; i < size - 1; i++) {\n            MPI_Recv(&ranks[n_local_last + (n_local * i)], n_local, MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&ranks_local[0], n_local, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n\n}",
            "// TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x of complex numbers by their magnitude in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// Create a parallel_for lambda function to sort the array x.\n  Kokkos::parallel_for(\n    \"sortComplexByMagnitude\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      // Sort the array x by its magnitude in ascending order.\n      // Use the following code to sort the array x by its magnitude in\n      // ascending order.\n      //\n      // 1. Find the index of the element with the smallest magnitude.\n      //    Store the index in the variable minIndex.\n      // 2. Swap the element at index i with the element at index minIndex.\n      // 3. Repeat steps 1 and 2 until the array is sorted.\n      //\n      // Hint: Use the following code to find the index of the element with\n      // the smallest magnitude.\n      //\n      // int minIndex = i;\n      // for (int j = i + 1; j < x.extent(0); j++) {\n      //   if (std::abs(x(j)) < std::abs(x(minIndex))) {\n      //     minIndex = j;\n      //   }\n      // }\n      //\n      // Hint: Use the following code to swap two elements in the array x.\n      //\n      // Kokkos::complex<double> temp = x(i);\n      // x(i) = x(minIndex);\n      // x(minIndex) = temp;\n    }\n  );\n}",
            "// Create a parallel_for lambda function that will sort the array x.\n  // The lambda function will be executed in parallel.\n  Kokkos::parallel_for(\n    \"sortComplexByMagnitude\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n\n      // Sort the array x by its magnitude in ascending order.\n      // The following code is based on the selection sort algorithm.\n      // See https://en.wikipedia.org/wiki/Selection_sort\n\n      // Find the smallest element in the array x.\n      int smallest = i;\n      for (int j = i + 1; j < x.extent(0); j++) {\n        if (std::abs(x(j)) < std::abs(x(smallest))) {\n          smallest = j;\n        }\n      }\n\n      // Swap the smallest element with the element at index i.\n      Kokkos::complex<double> temp = x(i);\n      x(i) = x(smallest);\n      x(smallest) = temp;\n    }\n  );\n\n  // Force the lambda function to finish before returning.\n  Kokkos::fence();\n}",
            "// Sort the array x by its magnitude in ascending order.\n  // Use Kokkos to sort in parallel.\n  // Assume Kokkos has already been initialized.\n\n  // Create a parallel view of the array x.\n  Kokkos::View<Kokkos::complex<double>*> x_par(\"x_par\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_par(i) = x(i);\n  });\n  Kokkos::fence();\n\n  // Create a parallel view of the array x's indices.\n  Kokkos::View<int*> indices(\"indices\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n  Kokkos::fence();\n\n  // Sort the array x_par by its magnitude in ascending order.\n  // Use Kokkos to sort in parallel.\n  // Assume Kokkos has already been initialized.\n  Kokkos::parallel_sort(x_par.size(), Kokkos::RangePolicy<Kokkos::Cuda>(0, x_par.size()),\n    KOKKOS_LAMBDA(const int i) {\n      return std::abs(x_par(i));\n    },\n    KOKKOS_LAMBDA(const int i, const int j) {\n      std::swap(x_par(i), x_par(j));\n      std::swap(indices(i), indices(j));\n    }\n  );\n  Kokkos::fence();\n\n  // Copy the sorted array x_par back to x.\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x_par(i);\n  });\n  Kokkos::fence();\n}",
            "using Kokkos::complex;\n  using Kokkos::View;\n  using Kokkos::DefaultHostExecutionSpace;\n  using Kokkos::DefaultHostMemorySpace;\n  using Kokkos::DefaultExecutionSpace;\n  using Kokkos::DefaultMemorySpace;\n  using Kokkos::HostSpace;\n  using Kokkos::MemoryTraits;\n  using Kokkos::All;\n  using Kokkos::RangePolicy;\n  using Kokkos::Schedule;\n  using Kokkos::parallel_for;\n  using Kokkos::Experimental::require;\n\n  // Create a copy of the input array.\n  View<complex<double>*, HostSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_host(\"x_host\", x.extent(0));\n  Kokkos::deep_copy(x_host, x);\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultHostExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_device(\"x_device\", x.extent(0));\n  Kokkos::deep_copy(x_device, x);\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted(\"x_sorted\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device(\"x_sorted_device\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_host(\"x_sorted_host\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_host_device(\"x_sorted_host_device\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host(\"x_sorted_device_host\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device(\"x_sorted_device_host_device\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device_host(\"x_sorted_device_host_device_host\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device_host_device(\"x_sorted_device_host_device_host_device\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device_host_device_host(\"x_sorted_device_host_device_host_device_host\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device_host_device_host_device(\"x_sorted_device_host_device_host_device_host_device\", x.extent(0));\n\n  // Create a copy of the input array.\n  View<complex<double>*, DefaultExecutionSpace, MemoryTraits<MemoryTraits::Unmanaged>>\n    x_sorted_device_host_device_host_device_host_device_host(\"x_sorted_device_host_device_host_device_host_device_host\", x.",
            "// Create a parallel view of the indices of the array x.\n  Kokkos::View<int*> indices(\"indices\", x.extent(0));\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"initialize_indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      indices(i) = i;\n    }\n  );\n\n  // Sort the indices by the magnitude of the corresponding elements of x.\n  Kokkos::parallel_sort(\n    \"sort_indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int j) {\n      return std::abs(x(indices(i))) < std::abs(x(indices(j)));\n    },\n    indices\n  );\n\n  // Reorder the elements of x according to the sorted indices.\n  Kokkos::parallel_for(\n    \"reorder_elements\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      Kokkos::complex<double> tmp = x(i);\n      x(i) = x(indices(i));\n      x(indices(i)) = tmp;\n    }\n  );\n}",
            "// Create a parallel view of the array of complex numbers\n  Kokkos::View<Kokkos::complex<double>*> x_par(\"x_par\", x.extent(0));\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x_par(i) = x(i);\n  });\n  Kokkos::fence();\n\n  // Sort the array in parallel\n  Kokkos::sort(x_par);\n\n  // Copy the sorted array back to the original array\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = x_par(i);\n  });\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename ExecutionSpace::device_type;\n  using MemoryTraits = Kokkos::MemoryTraits<Kokkos::Unmanaged>;\n  using ComplexType = Kokkos::complex<double>;\n  using ScalarType = double;\n  using ViewType = Kokkos::View<ComplexType*, MemoryTraits>;\n  using ViewScalarType = Kokkos::View<ScalarType*, MemoryTraits>;\n\n  // Create a view of the real and imaginary parts of the complex numbers.\n  ViewType x_real(\"x_real\", x.extent(0));\n  ViewType x_imag(\"x_imag\", x.extent(0));\n  Kokkos::parallel_for(\n    \"get_real_imag\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x_real(i) = x(i).real();\n      x_imag(i) = x(i).imag();\n    }\n  );\n  Kokkos::fence();\n\n  // Sort the real and imaginary parts separately.\n  Kokkos::sort(x_real);\n  Kokkos::sort(x_imag);\n  Kokkos::fence();\n\n  // Create a view of the indices of the sorted real and imaginary parts.\n  ViewScalarType x_real_indices(\"x_real_indices\", x.extent(0));\n  ViewScalarType x_imag_indices(\"x_imag_indices\", x.extent(0));\n  Kokkos::parallel_for(\n    \"get_indices\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x_real_indices(i) = i;\n      x_imag_indices(i) = i;\n    }\n  );\n  Kokkos::fence();\n\n  // Sort the indices of the real and imaginary parts separately.\n  Kokkos::sort(x_real_indices, [&](const int i, const int j) {\n    return x_real(i) < x_real(j);\n  });\n  Kokkos::sort(x_imag_indices, [&](const int i, const int j) {\n    return x_imag(i) < x_imag(j);\n  });\n  Kokkos::fence();\n\n  // Create a view of the sorted complex numbers.\n  ViewType x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::parallel_for(\n    \"get_sorted\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x_sorted(i) = ComplexType(x_real(x_real_indices(i)), x_imag(x_imag_indices(i)));\n    }\n  );\n  Kokkos::fence();\n\n  // Copy the sorted complex numbers back to the original view.\n  Kokkos::parallel_for(\n    \"copy_sorted\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      x(i) = x_sorted(i);\n    }\n  );\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda that will sort the array x.\n  // The lambda will be executed in parallel.\n  Kokkos::parallel_for(\n    \"sortComplexByMagnitude\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      // Sort the array x by the magnitude of its elements.\n      // Use the following algorithm:\n      // 1. Find the smallest element in the array.\n      // 2. Swap the smallest element with the first element.\n      // 3. Find the smallest element in the array that is greater than the\n      //    first element.\n      // 4. Swap the smallest element with the second element.\n      // 5. Find the smallest element in the array that is greater than the\n      //    first two elements.\n      // 6. Swap the smallest element with the third element.\n      // 7. Continue until the array is sorted.\n      //\n      // This algorithm is called selection sort.\n      //\n      // The following code is a simplified version of the algorithm.\n      // It assumes that the array x is already sorted.\n      // It also assumes that the array x is sorted in ascending order.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not thread-safe.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not a correct implementation of selection sort.\n      // It is only intended to illustrate the general idea of selection sort.\n      //\n      // The following code is not efficient.\n      // It is only intended to illustrate the general idea of selection sort.",
            "// Create a view of the real part of the complex numbers\n  Kokkos::View<double*> x_real(\"x_real\", x.extent(0));\n  // Copy the real part of the complex numbers to the real view\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x_real(i) = x(i).real();\n  });\n  // Sort the real part of the complex numbers\n  Kokkos::sort(x_real);\n  // Copy the sorted real part back to the complex numbers\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x(i).real(x_real(i));\n  });\n  // Create a view of the imaginary part of the complex numbers\n  Kokkos::View<double*> x_imag(\"x_imag\", x.extent(0));\n  // Copy the imaginary part of the complex numbers to the imaginary view\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x_imag(i) = x(i).imag();\n  });\n  // Sort the imaginary part of the complex numbers\n  Kokkos::sort(x_imag);\n  // Copy the sorted imaginary part back to the complex numbers\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    x(i).imag(x_imag(i));\n  });\n}",
            "// Create a parallel view of the indices of the array x.\n  Kokkos::View<int*> idx(\"idx\", x.extent(0));\n\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"Initialize indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      idx(i) = i;\n    }\n  );\n\n  // Sort the indices by the magnitude of the corresponding elements of x.\n  Kokkos::parallel_sort(\n    \"Sort indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int j) {\n      return std::abs(x(idx(i))) < std::abs(x(idx(j)));\n    },\n    idx\n  );\n\n  // Reorder the elements of x according to the sorted indices.\n  Kokkos::parallel_for(\n    \"Reorder elements\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      Kokkos::complex<double> tmp = x(idx(i));\n      x(idx(i)) = x(i);\n      x(i) = tmp;\n    }\n  );\n}",
            "// Create a view of the real part of the complex numbers.\n  Kokkos::View<double*> x_real(\"x_real\", x.size());\n  // Copy the real part of the complex numbers to the real view.\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_real(i) = x(i).real();\n  });\n  // Sort the real part of the complex numbers.\n  Kokkos::sort(x_real);\n  // Copy the sorted real part back to the complex numbers.\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i).real(x_real(i));\n  });\n}",
            "using Kokkos::complex;\n  using Kokkos::View;\n  using Kokkos::ALL;\n  using Kokkos::DefaultHostExecutionSpace;\n  using Kokkos::DefaultHostExecutionSpace::memory_space;\n  using Kokkos::DefaultHostExecutionSpace::scratch_memory_space;\n  using Kokkos::DefaultHostExecutionSpace::array_layout;\n  using Kokkos::DefaultHostExecutionSpace::device_type;\n  using Kokkos::DefaultHostExecutionSpace::is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::fence;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_initialize;\n  using Kokkos::DefaultHostExecutionSpace::impl_finalize;\n  using Kokkos::DefaultHostExecutionSpace::impl_print_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_print_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_finalize_async;\n  using Kokkos::DefaultHostExecutionSpace::impl_wait;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_ready;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initializing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initializing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initializing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initializing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initializing;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_initialized;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_printing_configuration_summary;\n  using Kokkos::DefaultHostExecutionSpace::impl_is_finalizing;\n  using Kokkos::Default",
            "// Create a parallel_for lambda function to sort the array x.\n  Kokkos::parallel_for(\n    \"sortComplexByMagnitude\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n\n      // Find the index of the smallest element in the array x.\n      int minIndex = i;\n      for (int j = i + 1; j < x.extent(0); j++) {\n        if (abs(x(j)) < abs(x(minIndex))) {\n          minIndex = j;\n        }\n      }\n\n      // Swap the element at index i with the smallest element.\n      Kokkos::complex<double> temp = x(i);\n      x(i) = x(minIndex);\n      x(minIndex) = temp;\n    }\n  );\n\n  // Force the parallel_for to complete before returning.\n  Kokkos::fence();\n}",
            "// Create a view of the real and imaginary parts of x\n  Kokkos::View<double*> x_real(\"x_real\", x.size());\n  Kokkos::View<double*> x_imag(\"x_imag\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    x_real(i) = x(i).real();\n    x_imag(i) = x(i).imag();\n  });\n  Kokkos::fence();\n\n  // Sort the real and imaginary parts of x in parallel\n  Kokkos::sort(x_real);\n  Kokkos::sort(x_imag);\n  Kokkos::fence();\n\n  // Recombine the real and imaginary parts of x\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    x(i) = Kokkos::complex<double>(x_real(i), x_imag(i));\n  });\n  Kokkos::fence();\n}",
            "// Create a parallel_for lambda function that sorts the array x.\n  // The lambda function is executed in parallel.\n  Kokkos::parallel_for(\n    \"sortComplexByMagnitude\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      // Sort the array x by the magnitude of its elements.\n      // Use the following algorithm:\n      // 1. Find the smallest element in the array x.\n      // 2. Swap the smallest element with the first element in the array x.\n      // 3. Repeat steps 1 and 2 until the array x is sorted.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.\n      // The algorithm is not efficient, but it is simple.",
            "// Create a parallel view of the indices of the array x.\n  Kokkos::View<int*> idx(\"idx\", x.extent(0));\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"init_idx\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      idx(i) = i;\n    }\n  );\n  // Sort the indices by the magnitude of the corresponding elements of x.\n  Kokkos::parallel_sort(\n    \"sort_idx\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int j) {\n      return std::abs(x(idx(i))) < std::abs(x(idx(j)));\n    },\n    idx\n  );\n  // Sort the elements of x by their magnitude.\n  Kokkos::parallel_for(\n    \"sort_x\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      Kokkos::complex<double> tmp = x(idx(i));\n      x(idx(i)) = x(i);\n      x(i) = tmp;\n    }\n  );\n}",
            "// Create a view of the real part of the complex numbers.\n  Kokkos::View<double*> x_real(\"x_real\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_real(i) = x(i).real();\n  });\n\n  // Sort the real part.\n  Kokkos::sort(x_real);\n\n  // Create a view of the indices of the sorted real part.\n  Kokkos::View<int*> x_real_indices(\"x_real_indices\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_real_indices(i) = i;\n  });\n\n  // Sort the indices of the real part.\n  Kokkos::sort(x_real_indices, [&](const int i, const int j) {\n    return x_real(i) < x_real(j);\n  });\n\n  // Create a view of the complex numbers sorted by their real part.\n  Kokkos::View<Kokkos::complex<double>*> x_sorted(\"x_sorted\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_sorted(i) = x(x_real_indices(i));\n  });\n\n  // Create a view of the imaginary part of the complex numbers.\n  Kokkos::View<double*> x_imag(\"x_imag\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_imag(i) = x(i).imag();\n  });\n\n  // Sort the imaginary part.\n  Kokkos::sort(x_imag);\n\n  // Create a view of the indices of the sorted imaginary part.\n  Kokkos::View<int*> x_imag_indices(\"x_imag_indices\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_imag_indices(i) = i;\n  });\n\n  // Sort the indices of the imaginary part.\n  Kokkos::sort(x_imag_indices, [&](const int i, const int j) {\n    return x_imag(i) < x_imag(j);\n  });\n\n  // Create a view of the complex numbers sorted by their imaginary part.\n  Kokkos::View<Kokkos::complex<double>*> x_sorted_by_imag(\"x_sorted_by_imag\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_sorted_by_imag(i) = x(x_imag_indices(i));\n  });\n\n  // Create a view of the indices of the sorted complex numbers.\n  Kokkos::View<int*> x_indices(\"x_indices\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_indices(i) = i;\n  });\n\n  // Sort the indices of the complex numbers.\n  Kokkos::sort(x_indices, [&](const int i, const int j) {\n    return x_sorted(i).real() < x_sorted(j).real();\n  });\n\n  // Create a view of the complex numbers sorted by their real part.\n  Kokkos::View<Kokkos::complex<double>*> x_sorted_by_real(\"x_sorted_by_real\", x.size());\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x_sorted_by_real(i) = x(x_indices(i));\n  });\n\n  // Create a view of the complex numbers sorted by",
            "using Kokkos::complex;\n  using Kokkos::parallel_for;\n  using Kokkos::RangePolicy;\n  using Kokkos::DefaultHostExecutionSpace;\n  using Kokkos::DefaultHostExecutionSpace::memory_space;\n  using Kokkos::DefaultHostExecutionSpace::scratch_memory_space;\n\n  // Create a temporary array to hold the indices of the elements in x.\n  Kokkos::View<int*> indices(\"indices\", x.extent(0));\n\n  // Create a temporary array to hold the magnitudes of the elements in x.\n  Kokkos::View<double*> magnitudes(\"magnitudes\", x.extent(0));\n\n  // Fill the temporary arrays with the indices and magnitudes of the elements\n  // in x.\n  parallel_for(RangePolicy<DefaultHostExecutionSpace>(0, x.extent(0)),\n               KOKKOS_LAMBDA(const int i) {\n                 indices(i) = i;\n                 magnitudes(i) = abs(x(i));\n               });\n\n  // Sort the temporary arrays by the magnitudes of the elements in x.\n  Kokkos::sort_by_key(magnitudes, indices);\n\n  // Fill x with the elements in x in the order specified by the indices.\n  parallel_for(RangePolicy<DefaultHostExecutionSpace>(0, x.extent(0)),\n               KOKKOS_LAMBDA(const int i) {\n                 x(i) = x(indices(i));\n               });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename ExecutionSpace::device_type;\n  using MemorySpace = typename ExecutionSpace::memory_space;\n  using ComplexType = Kokkos::complex<double>;\n  using RealType = double;\n  using IndexType = int;\n\n  // Create a parallel view of the indices of the array x.\n  Kokkos::View<IndexType*> idx(\"idx\", x.extent(0));\n\n  // Create a parallel view of the magnitudes of the array x.\n  Kokkos::View<RealType*> mag(\"mag\", x.extent(0));\n\n  // Create a parallel view of the sorted indices of the array x.\n  Kokkos::View<IndexType*> sortedIdx(\"sortedIdx\", x.extent(0));\n\n  // Create a parallel view of the sorted magnitudes of the array x.\n  Kokkos::View<RealType*> sortedMag(\"sortedMag\", x.extent(0));\n\n  // Create a parallel view of the sorted array x.\n  Kokkos::View<ComplexType*> sortedX(\"sortedX\", x.extent(0));\n\n  // Create a parallel view of the temporary array x.\n  Kokkos::View<ComplexType*> tmpX(\"tmpX\", x.extent(0));\n\n  // Initialize the indices of the array x.\n  Kokkos::parallel_for(\n    \"Initialize indices\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      idx(i) = i;\n    }\n  );\n\n  // Initialize the magnitudes of the array x.\n  Kokkos::parallel_for(\n    \"Initialize magnitudes\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      mag(i) = std::abs(x(i));\n    }\n  );\n\n  // Sort the magnitudes of the array x.\n  Kokkos::parallel_for(\n    \"Sort magnitudes\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      sortedMag(i) = mag(i);\n      sortedIdx(i) = idx(i);\n    }\n  );\n  Kokkos::sort(sortedMag, sortedIdx);\n\n  // Sort the array x.\n  Kokkos::parallel_for(\n    \"Sort array\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      sortedX(i) = x(sortedIdx(i));\n    }\n  );\n\n  // Copy the sorted array x to the temporary array tmpX.\n  Kokkos::parallel_for(\n    \"Copy array\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      tmpX(i) = sortedX(i);\n    }\n  );\n\n  // Copy the temporary array tmpX to the array x.\n  Kokkos::parallel_for(\n    \"Copy array\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const IndexType i) {\n      x(i) = tmpX(i);\n    }\n  );\n}",
            "// Create a parallel view of the indices.\n  Kokkos::View<int*> indices(\"indices\", x.extent(0));\n  // Initialize the indices to the identity permutation.\n  Kokkos::parallel_for(\n    \"initialize_indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      indices(i) = i;\n    }\n  );\n  // Sort the indices by the magnitude of the corresponding elements of x.\n  Kokkos::parallel_sort(\n    \"sort_indices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i, int j) {\n      return std::abs(x(indices(i))) < std::abs(x(indices(j)));\n    },\n    indices\n  );\n  // Sort the elements of x by the corresponding elements of indices.\n  Kokkos::parallel_for(\n    \"sort_x\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      Kokkos::complex<double> tmp = x(i);\n      x(i) = x(indices(i));\n      x(indices(i)) = tmp;\n    }\n  );\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n  int n = x.size();\n  std::vector<double> mag(n);\n  for (int i = 0; i < n; i++) {\n    mag[i] = std::abs(x[i]);\n  }\n  std::vector<int> idx(n);\n  for (int i = 0; i < n; i++) {\n    idx[i] = i;\n  }\n  std::sort(idx.begin(), idx.end(), [&mag](int i, int j) { return mag[i] < mag[j]; });\n  std::vector<std::complex<double>> x_sorted(n);\n  for (int i = 0; i < n; i++) {\n    x_sorted[i] = x[idx[i]];\n  }\n  x = x_sorted;\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n    std::vector<int> idx(n);\n    std::iota(idx.begin(), idx.end(), 0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        for (int j = i + 1; j < n; j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n                std::swap(idx[i], idx[j]);\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int n = x.size();\n  std::vector<double> magnitudes(n);\n  for (int i = 0; i < n; ++i) {\n    magnitudes[i] = std::abs(x[i]);\n  }\n  std::vector<int> indices(n);\n  for (int i = 0; i < n; ++i) {\n    indices[i] = i;\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    for (int j = i + 1; j < n; ++j) {\n      if (magnitudes[i] > magnitudes[j]) {\n        std::swap(magnitudes[i], magnitudes[j]);\n        std::swap(indices[i], indices[j]);\n      }\n    }\n  }\n  std::vector<std::complex<double>> sorted(n);\n  for (int i = 0; i < n; ++i) {\n    sorted[i] = x[indices[i]];\n  }\n  x = sorted;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "int n = x.size();\n    std::vector<double> magnitudes(n);\n    for (int i = 0; i < n; i++) {\n        magnitudes[i] = std::abs(x[i]);\n    }\n    std::vector<int> order(n);\n    for (int i = 0; i < n; i++) {\n        order[i] = i;\n    }\n    std::sort(order.begin(), order.end(), [&magnitudes](int i, int j) {\n        return magnitudes[i] < magnitudes[j];\n    });\n    std::vector<std::complex<double>> sorted(n);\n    for (int i = 0; i < n; i++) {\n        sorted[i] = x[order[i]];\n    }\n    x = sorted;\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "int n = x.size();\n  std::vector<double> magnitudes(n);\n  for (int i = 0; i < n; i++) {\n    magnitudes[i] = std::abs(x[i]);\n  }\n  std::vector<int> indices(n);\n  for (int i = 0; i < n; i++) {\n    indices[i] = i;\n  }\n  // sort the magnitudes and the corresponding indices\n  // using std::sort\n  std::sort(indices.begin(), indices.end(), [&magnitudes](int i, int j) {\n    return magnitudes[i] < magnitudes[j];\n  });\n  // sort the vector x using the sorted indices\n  std::vector<std::complex<double>> sortedX(n);\n  for (int i = 0; i < n; i++) {\n    sortedX[i] = x[indices[i]];\n  }\n  x = sortedX;\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "std::vector<std::complex<double>> x_copy(x);\n  std::vector<std::complex<double>> x_sorted;\n  std::vector<double> magnitudes;\n  for (auto &x_i : x_copy) {\n    magnitudes.push_back(abs(x_i));\n  }\n  std::vector<int> indices(magnitudes.size());\n  std::iota(indices.begin(), indices.end(), 0);\n  std::sort(indices.begin(), indices.end(), [&magnitudes](int i, int j) {\n    return magnitudes[i] < magnitudes[j];\n  });\n  for (auto &index : indices) {\n    x_sorted.push_back(x_copy[index]);\n  }\n  x = x_sorted;\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::vector<std::complex<double>> x_sorted;\n    std::vector<double> x_magnitudes;\n    for (auto &x_i : x_copy) {\n        x_magnitudes.push_back(abs(x_i));\n    }\n    std::vector<int> x_magnitudes_indices = sortIndices(x_magnitudes);\n    for (auto &x_magnitudes_indices_i : x_magnitudes_indices) {\n        x_sorted.push_back(x_copy[x_magnitudes_indices_i]);\n    }\n    x = x_sorted;\n}",
            "std::vector<std::complex<double>> x_sorted;\n  std::vector<double> magnitudes;\n  for (auto &x_i : x) {\n    magnitudes.push_back(std::abs(x_i));\n  }\n  std::vector<size_t> indices = sortIndices(magnitudes);\n  for (auto &i : indices) {\n    x_sorted.push_back(x[i]);\n  }\n  x = x_sorted;\n}",
            "std::vector<std::complex<double>> xSorted;\n    std::vector<std::complex<double>> xUnsorted;\n    std::vector<double> xMagnitudes;\n    std::vector<double> xMagnitudesSorted;\n    std::vector<int> xIndices;\n    std::vector<int> xIndicesSorted;\n    std::vector<int> xIndicesUnsorted;\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        xMagnitudes.push_back(abs(x[i]));\n        xIndices.push_back(i);\n    }\n    xMagnitudesSorted = xMagnitudes;\n    std::sort(xMagnitudesSorted.begin(), xMagnitudesSorted.end());\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            if (xMagnitudes[j] == xMagnitudesSorted[i]) {\n                xIndicesSorted.push_back(xIndices[j]);\n            }\n        }\n    }\n    for (int i = 0; i < n; i++) {\n        xUnsorted.push_back(x[xIndicesSorted[i]]);\n    }\n    x = xUnsorted;\n}",
            "std::vector<std::complex<double>> x_sorted;\n    std::vector<std::complex<double>> x_temp;\n    std::vector<double> x_magnitude;\n    std::vector<double> x_magnitude_sorted;\n    std::vector<int> x_index;\n    std::vector<int> x_index_sorted;\n    int i, j;\n    \n    // Copy the input vector to a temporary vector\n    for (i = 0; i < x.size(); i++) {\n        x_temp.push_back(x[i]);\n    }\n    \n    // Compute the magnitude of each complex number\n    for (i = 0; i < x.size(); i++) {\n        x_magnitude.push_back(std::abs(x[i]));\n    }\n    \n    // Sort the magnitude vector\n    x_magnitude_sorted = sortVector(x_magnitude);\n    \n    // Sort the index vector\n    x_index = sortVector(x_magnitude);\n    \n    // Sort the input vector\n    for (i = 0; i < x.size(); i++) {\n        x_sorted.push_back(x_temp[x_index[i]]);\n    }\n    \n    // Copy the sorted vector to the input vector\n    for (i = 0; i < x.size(); i++) {\n        x[i] = x_sorted[i];\n    }\n}",
            "std::vector<std::complex<double>> x_sorted;\n    std::vector<double> magnitudes;\n    for (int i = 0; i < x.size(); i++) {\n        magnitudes.push_back(std::abs(x[i]));\n    }\n    std::vector<int> indices = sortVector(magnitudes);\n    for (int i = 0; i < indices.size(); i++) {\n        x_sorted.push_back(x[indices[i]]);\n    }\n    x = x_sorted;\n}",
            "std::vector<std::complex<double>> xSorted(x.size());\n  std::vector<double> magnitudes(x.size());\n  for (int i = 0; i < x.size(); i++) {\n    magnitudes[i] = std::abs(x[i]);\n  }\n  std::vector<int> indices(x.size());\n  for (int i = 0; i < x.size(); i++) {\n    indices[i] = i;\n  }\n  std::sort(indices.begin(), indices.end(), [&](int i, int j) {\n    return magnitudes[i] < magnitudes[j];\n  });\n  for (int i = 0; i < x.size(); i++) {\n    xSorted[i] = x[indices[i]];\n  }\n  x = xSorted;\n}",
            "std::vector<std::complex<double>> xSorted;\n    std::vector<double> magnitudes;\n    for (int i = 0; i < x.size(); i++) {\n        magnitudes.push_back(std::abs(x[i]));\n    }\n    std::vector<int> indices = sortVector(magnitudes);\n    for (int i = 0; i < indices.size(); i++) {\n        xSorted.push_back(x[indices[i]]);\n    }\n    x = xSorted;\n}",
            "std::vector<std::complex<double>> x_copy(x);\n  std::sort(x_copy.begin(), x_copy.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n    return std::abs(a) < std::abs(b);\n  });\n  x = x_copy;\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::sort(x_copy.begin(), x_copy.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n    x = x_copy;\n}",
            "std::vector<std::complex<double>> temp;\n    for (int i = 0; i < x.size(); i++) {\n        temp.push_back(x[i]);\n    }\n    std::sort(temp.begin(), temp.end(), [](std::complex<double> a, std::complex<double> b) {\n        return abs(a) < abs(b);\n    });\n    x.clear();\n    for (int i = 0; i < temp.size(); i++) {\n        x.push_back(temp[i]);\n    }\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::sort(x.begin(), x.end(), [&x_copy](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(x_copy[a.real()]) < std::abs(x_copy[b.real()]);\n    });\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::vector<std::complex<double>> x_sorted;\n    std::vector<std::complex<double>> x_sorted_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<std::complex<double>> x_sorted_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy;\n    std::vector<",
            "std::vector<std::complex<double>> xSorted;\n    std::vector<double> magnitudes;\n    for (int i = 0; i < x.size(); i++) {\n        magnitudes.push_back(std::abs(x[i]));\n    }\n    std::vector<int> indices = sortVector(magnitudes);\n    for (int i = 0; i < indices.size(); i++) {\n        xSorted.push_back(x[indices[i]]);\n    }\n    x = xSorted;\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::vector<std::complex<double>> x_sorted;\n    while (x_copy.size() > 0) {\n        double min_mag = std::numeric_limits<double>::max();\n        int min_index = -1;\n        for (int i = 0; i < x_copy.size(); i++) {\n            if (std::abs(x_copy[i]) < min_mag) {\n                min_mag = std::abs(x_copy[i]);\n                min_index = i;\n            }\n        }\n        x_sorted.push_back(x_copy[min_index]);\n        x_copy.erase(x_copy.begin() + min_index);\n    }\n    x = x_sorted;\n}",
            "std::vector<std::complex<double>> x_sorted;\n    std::vector<double> x_magnitudes;\n    for (auto &x_i : x) {\n        x_magnitudes.push_back(std::abs(x_i));\n    }\n    std::vector<int> x_magnitudes_indices(x_magnitudes.size());\n    std::iota(x_magnitudes_indices.begin(), x_magnitudes_indices.end(), 0);\n    std::sort(x_magnitudes_indices.begin(), x_magnitudes_indices.end(),\n              [&x_magnitudes](int i, int j) { return x_magnitudes[i] < x_magnitudes[j]; });\n    for (auto &x_i : x_magnitudes_indices) {\n        x_sorted.push_back(x[x_i]);\n    }\n    x = x_sorted;\n}",
            "std::vector<std::complex<double>> x_sorted;\n    std::vector<double> magnitudes;\n    for (auto &x_i : x) {\n        magnitudes.push_back(std::abs(x_i));\n    }\n    std::vector<int> indices = sortIndices(magnitudes);\n    for (auto &index : indices) {\n        x_sorted.push_back(x[index]);\n    }\n    x = x_sorted;\n}",
            "std::vector<std::complex<double>> x_copy = x;\n  std::vector<std::complex<double>> x_sorted;\n  std::vector<double> magnitudes;\n  for (int i = 0; i < x.size(); i++) {\n    magnitudes.push_back(std::abs(x[i]));\n  }\n  std::vector<int> indices = sortVector(magnitudes);\n  for (int i = 0; i < indices.size(); i++) {\n    x_sorted.push_back(x_copy[indices[i]]);\n  }\n  x = x_sorted;\n}",
            "std::vector<std::complex<double>> xSorted;\n    std::vector<double> magnitudes;\n    for (int i = 0; i < x.size(); i++) {\n        magnitudes.push_back(std::abs(x[i]));\n    }\n    std::vector<int> indices = sortVector(magnitudes);\n    for (int i = 0; i < indices.size(); i++) {\n        xSorted.push_back(x[indices[i]]);\n    }\n    x = xSorted;\n}",
            "std::vector<std::complex<double>> x_copy = x;\n    std::sort(x_copy.begin(), x_copy.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n    x = x_copy;\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "hip",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(hipDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int nthreads = hipBlockDim_x;\n  int nblocks = hipGridDim_x;\n  int i = bid * nthreads + tid;\n  if (i < N) {\n    double xr = hipCrealf(x[i]);\n    double xi = hipCimagf(x[i]);\n    double xm = sqrt(xr*xr + xi*xi);\n    int j = i;\n    while (j > 0 && xm < sqrt(hipCrealf(x[j-1])*hipCrealf(x[j-1]) + hipCimagf(x[j-1])*hipCimagf(x[j-1]))) {\n      x[j] = x[j-1];\n      j--;\n    }\n    x[j] = hipDoubleComplex(xr, xi);\n  }\n}",
            "// Get the index of the thread in the block\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    // Get the magnitude of the complex number\n    double mag = hipCabsf(x[tid]);\n    // Sort the vector x by magnitude\n    x[tid] = hipCmplx(mag, tid);\n  }\n}",
            "// Get the index of the current thread\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= N) return;\n\n  // Get the magnitude of the current element\n  double mag = hipCabsf(x[index]);\n\n  // Sort the elements by their magnitude\n  __syncthreads();\n  if (index > 0 && mag < hipCabsf(x[index - 1])) {\n    // Swap the current element with the previous element\n    hipDoubleComplex temp = x[index];\n    x[index] = x[index - 1];\n    x[index - 1] = temp;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double x_real = x[tid].x;\n        double x_imag = x[tid].y;\n        double x_mag = sqrt(x_real * x_real + x_imag * x_imag);\n        int i = tid;\n        while (i > 0 && x[i - 1].y > x_mag) {\n            x[i] = x[i - 1];\n            i--;\n        }\n        x[i] = make_hipDoubleComplex(x_real, x_mag);\n    }\n}",
            "// Sort the vector x of complex numbers by their magnitude in ascending order.\n  // Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n  // Example:\n  //\n  // input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n  // output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n\n  // TODO: Implement this function\n  //\n  // Hint: Use the following functions:\n  //\n  // hipMallocManaged(&x, N*sizeof(hipDoubleComplex));\n  // hipFree(x);\n  // hipMemcpy(x, x_host, N*sizeof(hipDoubleComplex), hipMemcpyHostToDevice);\n  // hipMemcpy(x_host, x, N*sizeof(hipDoubleComplex), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted, N*sizeof(hipDoubleComplex));\n  // hipFree(x_sorted);\n  // hipMemcpy(x_sorted, x_sorted_host, N*sizeof(hipDoubleComplex), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_host, x_sorted, N*sizeof(hipDoubleComplex), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted_indices, N*sizeof(int));\n  // hipFree(x_sorted_indices);\n  // hipMemcpy(x_sorted_indices, x_sorted_indices_host, N*sizeof(int), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_indices_host, x_sorted_indices, N*sizeof(int), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted_indices_sorted, N*sizeof(int));\n  // hipFree(x_sorted_indices_sorted);\n  // hipMemcpy(x_sorted_indices_sorted, x_sorted_indices_sorted_host, N*sizeof(int), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_indices_sorted_host, x_sorted_indices_sorted, N*sizeof(int), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted_indices_sorted_indices, N*sizeof(int));\n  // hipFree(x_sorted_indices_sorted_indices);\n  // hipMemcpy(x_sorted_indices_sorted_indices, x_sorted_indices_sorted_indices_host, N*sizeof(int), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_indices_sorted_indices_host, x_sorted_indices_sorted_indices, N*sizeof(int), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted_indices_sorted_indices_sorted, N*sizeof(int));\n  // hipFree(x_sorted_indices_sorted_indices_sorted);\n  // hipMemcpy(x_sorted_indices_sorted_indices_sorted, x_sorted_indices_sorted_indices_sorted_host, N*sizeof(int), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_indices_sorted_indices_sorted_host, x_sorted_indices_sorted_indices_sorted, N*sizeof(int), hipMemcpyDeviceToHost);\n  //\n  // hipMallocManaged(&x_sorted_indices_sorted_indices_sorted_indices, N*sizeof(int));\n  // hipFree(x_sorted_indices_sorted_indices_sorted_indices);\n  // hipMemcpy(x_sorted_indices_sorted_indices_sorted_indices, x_sorted_indices_sorted_indices_sorted_indices_host, N*sizeof(int), hipMemcpyHostToDevice);\n  // hipMemcpy(x_sorted_indices_sorted",
            "// Get the index of the thread\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // Get the magnitude of the complex number\n    double mag = sqrt(x[tid].x * x[tid].x + x[tid].y * x[tid].y);\n\n    // Store the index of the complex number in the shared memory\n    __shared__ int index[BLOCK_SIZE];\n    index[threadIdx.x] = tid;\n\n    // Sort the index array using AMD HIP\n    __syncthreads();\n    bitonicSort(index, mag, N);\n    __syncthreads();\n\n    // Swap the complex numbers in the global memory\n    hipDoubleComplex temp = x[tid];\n    x[tid] = x[index[threadIdx.x]];\n    x[index[threadIdx.x]] = temp;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double mag = hipCabsf(x[tid]);\n    x[tid] = make_hipDoubleComplex(mag, tid);\n  }\n}",
            "// Get the global thread index\n    size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i >= N) return;\n\n    // Get the magnitude of the complex number\n    double mag = sqrt(x[i].x * x[i].x + x[i].y * x[i].y);\n\n    // Get the global thread index of the thread with the smallest magnitude\n    size_t min_idx = i;\n    for (size_t j = i + 1; j < N; j++) {\n        double mag_j = sqrt(x[j].x * x[j].x + x[j].y * x[j].y);\n        if (mag_j < mag) {\n            min_idx = j;\n            mag = mag_j;\n        }\n    }\n\n    // Swap the complex numbers if the current thread has the smallest magnitude\n    if (i == min_idx) {\n        hipDoubleComplex tmp = x[i];\n        x[i] = x[min_idx];\n        x[min_idx] = tmp;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double x_real = hipCrealf(x[tid]);\n    double x_imag = hipCimagf(x[tid]);\n    double x_mag = sqrt(x_real*x_real + x_imag*x_imag);\n    x[tid] = make_hipDoubleComplex(x_mag, tid);\n  }\n}",
            "// The thread index\n  int tid = hipThreadIdx_x;\n  // The block index\n  int bid = hipBlockIdx_x;\n  // The number of threads in the block\n  int nt = hipBlockDim_x;\n  // The number of blocks\n  int nb = hipGridDim_x;\n\n  // The number of elements in the input vector\n  int N_local = N / nb;\n  // The first element to sort\n  int first = bid * N_local;\n  // The last element to sort\n  int last = first + N_local;\n  // The number of elements to sort\n  int N_local_local = last - first;\n\n  // The input vector\n  __shared__ double x_shared[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ double y_shared[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ double x_shared_complex[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ double y_shared_complex[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ double x_shared_imag[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ double y_shared_imag[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ double x_shared_magnitude[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ double y_shared_magnitude[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_complex[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_complex[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_imag[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_imag[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude_complex[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude_complex[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude_imag[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude_imag[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude_imag_complex[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude_imag_complex[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude_imag_complex_imag[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude_imag_complex_imag[MAX_THREADS_PER_BLOCK];\n\n  // The input vector\n  __shared__ int x_shared_index_magnitude_imag_complex_imag_complex[MAX_THREADS_PER_BLOCK];\n  // The output vector\n  __shared__ int y_shared_index_magnitude_im",
            "// Get the global thread index\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n\n  // Get the value of x[tid]\n  hipDoubleComplex x_tid = x[tid];\n\n  // Get the magnitude of x[tid]\n  double x_tid_mag = hipCabsf(x_tid);\n\n  // Get the global index of the thread that has the smallest magnitude\n  int min_mag_tid = 0;\n  for (int i = 0; i < N; i++) {\n    if (x[i].x < x[min_mag_tid].x) {\n      min_mag_tid = i;\n    }\n  }\n\n  // Swap x[tid] with x[min_mag_tid]\n  x[tid] = x[min_mag_tid];\n  x[min_mag_tid] = x_tid;\n}",
            "int tid = hipThreadIdx_x;\n    if (tid < N) {\n        double mag = sqrt(x[tid].x * x[tid].x + x[tid].y * x[tid].y);\n        int pos = 0;\n        for (int i = 0; i < N; i++) {\n            if (mag < sqrt(x[i].x * x[i].x + x[i].y * x[i].y)) {\n                pos++;\n            }\n        }\n        for (int i = N - 1; i > pos; i--) {\n            x[i] = x[i - 1];\n        }\n        x[pos] = x[tid];\n    }\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + tid;\n  int stride = blockDim.x * gridDim.x;\n  for (size_t i = gid; i < N; i += stride) {\n    double mag = sqrt(x[i].x * x[i].x + x[i].y * x[i].y);\n    x[i].x = mag;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double x_real = x[tid].x;\n    double x_imag = x[tid].y;\n    double x_mag = sqrt(x_real*x_real + x_imag*x_imag);\n    int i = 0;\n    while (i < tid) {\n      double x_mag_prev = sqrt(x[i].x*x[i].x + x[i].y*x[i].y);\n      if (x_mag_prev > x_mag) {\n        x[tid].x = x[i].x;\n        x[tid].y = x[i].y;\n        x[i].x = x_real;\n        x[i].y = x_imag;\n        x_real = x[tid].x;\n        x_imag = x[tid].y;\n        x_mag = x_mag_prev;\n      }\n      i++;\n    }\n  }\n}",
            "// Get the index of the current thread\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n\n  // Get the magnitude of the current element\n  double mag = hipCabsf(x[tid]);\n\n  // Get the index of the current element in the sorted array\n  int idx = 0;\n  for (int i = 0; i < tid; i++) {\n    if (hipCabsf(x[i]) < mag) idx++;\n  }\n\n  // Move the current element to its sorted position\n  for (int i = tid; i > idx; i--) {\n    x[i] = x[i-1];\n  }\n  x[idx] = x[tid];\n}",
            "// Get the global thread index\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // Do nothing if the thread index is greater than the number of elements in x\n  if (tid >= N) return;\n\n  // Get the magnitude of the complex number at index tid\n  double mag = hipCabsf(x[tid]);\n\n  // Use a binary search to find the location where the magnitude should be inserted\n  int low = 0;\n  int high = N - 1;\n  int mid = (low + high) / 2;\n  while (low <= high) {\n    if (mag < hipCabsf(x[mid])) {\n      high = mid - 1;\n    } else {\n      low = mid + 1;\n    }\n    mid = (low + high) / 2;\n  }\n\n  // Shift the elements in x from mid to tid to make room for the magnitude\n  for (int i = tid; i > mid; i--) {\n    x[i] = x[i - 1];\n  }\n\n  // Insert the magnitude at index tid\n  x[tid] = hipCmplx(mag, 0.0);\n}",
            "// Sort the vector x of complex numbers by their magnitude in ascending order.\n  // Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n  // Example:\n  //\n  // input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n  // output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n\n  // The thread ID\n  int tid = hipThreadIdx_x;\n\n  // The number of threads in the block\n  int nthreads = hipBlockDim_x;\n\n  // The number of elements in the input array\n  int N = N;\n\n  // Do not handle elements outside of the input array\n  if (tid >= N) return;\n\n  // The array of keys\n  __shared__ double s_keys[MAX_BLOCK_SIZE];\n\n  // The array of values\n  __shared__ int s_values[MAX_BLOCK_SIZE];\n\n  // The key for the current thread\n  double key = hipCabsf(x[tid]);\n\n  // The value for the current thread\n  int value = tid;\n\n  // Store the keys and values in shared memory\n  s_keys[tid] = key;\n  s_values[tid] = value;\n\n  // Synchronize the threads in the block\n  __syncthreads();\n\n  // The location of the first element of the sorted sequence\n  int start = 0;\n\n  // The location of the last element of the sorted sequence\n  int end = N - 1;\n\n  // The location of the middle element of the sorted sequence\n  int mid = (start + end) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted = (firstUnsorted + lastUnsorted) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted2 = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted2 = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted2 = (firstUnsorted2 + lastUnsorted2) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted3 = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted3 = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted3 = (firstUnsorted3 + lastUnsorted3) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted4 = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted4 = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted4 = (firstUnsorted4 + lastUnsorted4) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted5 = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted5 = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted5 = (firstUnsorted5 + lastUnsorted5) / 2;\n\n  // The location of the first element of the unsorted sequence\n  int firstUnsorted6 = 0;\n\n  // The location of the last element of the unsorted sequence\n  int lastUnsorted6 = N - 1;\n\n  // The location of the middle element of the unsorted sequence\n  int midUnsorted6 = (firstUnsorted6 + lastUnsorted6) / 2;\n\n  // The location of the first",
            "// Get the index of the current thread\n  int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n\n  // Get the magnitude of the current element\n  double mag = hipCabsf(x[i]);\n\n  // Get the index of the current element in the sorted vector\n  int j = i;\n  while (j > 0 && mag > hipCabsf(x[j-1])) {\n    x[j] = x[j-1];\n    j--;\n  }\n  x[j] = x[i];\n}",
            "// Get the global thread index\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // Get the magnitude of the complex number\n    double mag = sqrt(x[tid].x * x[tid].x + x[tid].y * x[tid].y);\n\n    // Sort the complex numbers by magnitude\n    __syncthreads();\n    for (int i = 1; i < N; i *= 2) {\n        int other = tid ^ i;\n        if (other < N && other > tid) {\n            double otherMag = sqrt(x[other].x * x[other].x + x[other].y * x[other].y);\n            if (otherMag > mag) {\n                hipDoubleComplex temp = x[tid];\n                x[tid] = x[other];\n                x[other] = temp;\n                mag = otherMag;\n            }\n        }\n        __syncthreads();\n    }\n}",
            "// Get the global thread index\n  size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n\n  // Get the magnitude of the complex number\n  double mag = hipCabsf(x[i]);\n\n  // Sort the complex numbers by their magnitude\n  __syncthreads();\n  AMD_HIP_SORT(mag, x[i]);\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int n_per_rank = n / size;\n  int n_remainder = n % size;\n  int n_local = n_per_rank + (rank < n_remainder);\n  std::vector<std::complex<double>> x_local(n_local);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE_COMPLEX, x_local.data(), n_local,\n              MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n  std::sort(x_local.begin(), x_local.end(),\n            [](const std::complex<double> &a, const std::complex<double> &b) {\n              return std::abs(a) < std::abs(b);\n            });\n  MPI_Gather(x_local.data(), n_local, MPI_DOUBLE_COMPLEX, x.data(), n_local,\n             MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: sort x by magnitude in ascending order\n\n  // TODO: gather the sorted vectors on rank 0\n\n  // TODO: sort the gathered vector on rank 0\n\n  // TODO: broadcast the sorted vector to all ranks\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(cuDoubleComplex *x, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex temp = x[tid];\n        double temp_mag = cuCabs(temp);\n        int i = tid;\n        while (i > 0 && cuCabs(x[i - 1]) > temp_mag) {\n            x[i] = x[i - 1];\n            i--;\n        }\n        x[i] = temp;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// Get the index of the current thread\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    // Only do work if the index is within the bounds of the array\n    if (i < N) {\n        // Get the magnitude of the current complex number\n        double mag = cuCabs(x[i]);\n        // Find the index of the first element in the array that has a magnitude greater than the current element\n        size_t j = 0;\n        while (j < N && cuCabs(x[j]) <= mag) {\n            j++;\n        }\n        // Shift all elements between the current element and the first element with a greater magnitude to the right\n        for (size_t k = N - 1; k > j; k--) {\n            x[k] = x[k - 1];\n        }\n        // Insert the current element at the correct index\n        x[j] = x[i];\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use the atomicMin function to find the smallest element in the array\n    // Hint: Use the atomicMax function to find the largest element in the array\n    // Hint: Use the atomicAdd function to find the sum of all elements in the array\n    // Hint: Use the atomicExch function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint: Use the atomicCAS function to swap two elements in the array\n    // Hint:",
            "// TODO: Implement this function\n}",
            "__shared__ cuDoubleComplex shared[BLOCK_SIZE];\n  __shared__ int sharedIndex[BLOCK_SIZE];\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int index = i;\n  cuDoubleComplex value = x[i];\n  if (i < N) {\n    shared[threadIdx.x] = value;\n    sharedIndex[threadIdx.x] = i;\n  }\n  __syncthreads();\n  // Perform a bitonic sort on the shared array.\n  for (int k = 2; k <= BLOCK_SIZE; k <<= 1) {\n    int j = threadIdx.x ^ (k >> 1);\n    if (j > threadIdx.x) {\n      if (abs(shared[threadIdx.x]) < abs(shared[j])) {\n        cuDoubleComplex temp = shared[threadIdx.x];\n        shared[threadIdx.x] = shared[j];\n        shared[j] = temp;\n        int tempIndex = sharedIndex[threadIdx.x];\n        sharedIndex[threadIdx.x] = sharedIndex[j];\n        sharedIndex[j] = tempIndex;\n      }\n    }\n    __syncthreads();\n  }\n  // Copy the sorted array back to the global memory.\n  if (i < N) {\n    x[i] = shared[threadIdx.x];\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    // Hint: Use the thrust::sort function\n}",
            "// TODO: Implement this function\n    // Hint: You can use the following functions:\n    //   cuCreal(cuDoubleComplex z)\n    //   cuCimag(cuDoubleComplex z)\n    //   cuCadd(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCsub(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCmul(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCdiv(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCsqrt(cuDoubleComplex z)\n    //   cuCabs(cuDoubleComplex z)\n    //   cuCexp(cuDoubleComplex z)\n    //   cuClog(cuDoubleComplex z)\n    //   cuCsin(cuDoubleComplex z)\n    //   cuCcos(cuDoubleComplex z)\n    //   cuCsinh(cuDoubleComplex z)\n    //   cuCcosh(cuDoubleComplex z)\n    //   cuCasin(cuDoubleComplex z)\n    //   cuCacos(cuDoubleComplex z)\n    //   cuCatan(cuDoubleComplex z)\n    //   cuCatan2(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCasinh(cuDoubleComplex z)\n    //   cuCacosh(cuDoubleComplex z)\n    //   cuCatanh(cuDoubleComplex z)\n    //   cuCpow(cuDoubleComplex a, cuDoubleComplex b)\n    //   cuCfma(cuDoubleComplex a, cuDoubleComplex b, cuDoubleComplex c)\n    //   cuCfmaf(cuFloatComplex a, cuFloatComplex b, cuFloatComplex c)\n    //   cuCproj(cuDoubleComplex z)\n    //   cuCconj(cuDoubleComplex z)\n    //   cuDoubleToFloat(double d)\n    //   cuFloatToDouble(float f)\n    //   cuCabsf(cuFloatComplex z)\n    //   cuCsqrtf(cuFloatComplex z)\n    //   cuCexpf(cuFloatComplex z)\n    //   cuClogf(cuFloatComplex z)\n    //   cuCsinf(cuFloatComplex z)\n    //   cuCcosf(cuFloatComplex z)\n    //   cuCsinhf(cuFloatComplex z)\n    //   cuCcoshf(cuFloatComplex z)\n    //   cuCasinf(cuFloatComplex z)\n    //   cuCacosf(cuFloatComplex z)\n    //   cuCatanf(cuFloatComplex z)\n    //   cuCatan2f(cuFloatComplex a, cuFloatComplex b)\n    //   cuCasinhf(cuFloatComplex z)\n    //   cuCacoshf(cuFloatComplex z)\n    //   cuCatanhf(cuFloatComplex z)\n    //   cuCpowf(cuFloatComplex a, cuFloatComplex b)\n    //   cuCfmaf(cuFloatComplex a, cuFloatComplex b, cuFloatComplex c)\n    //   cuCprojf(cuFloatComplex z)\n    //   cuCconjf(cuFloatComplex z)\n    //   cuCabsf(cuFloatComplex z)\n    //   cuCsqrtf(cuFloatComplex z)\n    //   cuCexpf(cuFloatComplex z)\n    //   cuClogf(cuFloatComplex z)\n    //   cuCsinf(cuFloatComplex z)\n    //   cuCcosf(cuFloatComplex z)\n    //   cuCsinhf(cuFloatComplex z)\n    //   cuCcoshf(cuFloatComplex z)\n    //   cuCasinf(cuFloatComplex z)\n    //   cuCacosf(cuFloatComplex z)\n    //   cuCatanf(cuFloatComplex z)\n    //   cuCatan2f(cuFloatComplex a, cuFloatComplex b)\n    //   cuCasinhf(cuFloatComplex z)\n    //   cuCacoshf(cuFloatComplex z)"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "36_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: sort x in parallel\n\n  // TODO: gather the results on rank 0\n\n  // TODO: broadcast the results to all ranks\n}",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: sort x by magnitude in ascending order\n  // Hint: use std::sort and std::abs\n  // Hint: use omp_get_thread_num() to get the thread id\n  // Hint: use MPI_Send and MPI_Recv to send and receive data\n  // Hint: use MPI_Reduce to combine the results\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here\n\n  // TODO: Your code here",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Find the k-th smallest element of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(Kokkos::View<const int*> const& x, int k) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*> y(\"y\", n);\n  Kokkos::parallel_for(\"copy\", n, KOKKOS_LAMBDA(int i) {\n    y(i) = x(i);\n  });\n  Kokkos::sort(y);\n  return y(k-1);\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// Create a Kokkos view for the result\n  Kokkos::View<int> result(\"result\", 1);\n\n  // Create a Kokkos parallel_for lambda function\n  Kokkos::parallel_for(\n    \"findKthSmallest\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 1),\n    KOKKOS_LAMBDA(const int&) {\n      // Set the result to the k-th smallest element of x\n      result(0) = Kokkos::Subview(x, Kokkos::make_pair(0, x.extent(0)))(k);\n    }\n  );\n\n  // Wait for the parallel_for to finish\n  Kokkos::fence();\n\n  // Return the result\n  return result(0);\n}",
            "// TODO: implement\n  return 0;\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*> y(\"y\", n);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(int i) {\n    y(i) = x(i);\n  });\n  Kokkos::sort(y);\n  return y(k-1);\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*> y(\"y\", n);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(int i) {\n    y(i) = x(i);\n  });\n  Kokkos::sort(y);\n  return y(k-1);\n}",
            "// Create a Kokkos::View to store the k-th smallest element.\n  Kokkos::View<int> kthSmallest(\"kthSmallest\", 1);\n\n  // Create a Kokkos::View to store the indices of the elements of x.\n  Kokkos::View<int*> indices(\"indices\", x.extent(0));\n\n  // Create a Kokkos::View to store the number of elements in x that are less than the k-th smallest element.\n  Kokkos::View<int*> numLess(\"numLess\", 1);\n\n  // Initialize the k-th smallest element to the maximum possible value.\n  Kokkos::deep_copy(kthSmallest, std::numeric_limits<int>::max());\n\n  // Initialize the number of elements in x that are less than the k-th smallest element to 0.\n  Kokkos::deep_copy(numLess, 0);\n\n  // Initialize the indices of the elements of x.\n  Kokkos::parallel_for(\n    \"InitializeIndices\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      indices(i) = i;\n    }\n  );\n\n  // Loop until the k-th smallest element is found.\n  while (true) {\n    // Find the median of the elements of x.\n    int medianIndex = x.extent(0) / 2;\n    if (x.extent(0) % 2 == 0) {\n      medianIndex -= 1;\n    }\n    int median = x(indices(medianIndex));\n\n    // Count the number of elements in x that are less than the median.\n    Kokkos::parallel_for(\n      \"CountLess\",\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i) {\n        if (x(indices(i)) < median) {\n          numLess(0) += 1;\n        }\n      }\n    );\n\n    // If the number of elements in x that are less than the median is equal to k, then the k-th smallest element is the median.\n    if (numLess(0) == k) {\n      Kokkos::deep_copy(kthSmallest, median);\n      break;\n    }\n    // If the number of elements in x that are less than the median is greater than k, then the k-th smallest element is in the first half of x.\n    else if (numLess(0) > k) {\n      // Update the indices of the elements of x to only include the first half of x.\n      Kokkos::parallel_for(\n        \"UpdateIndices\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(int i) {\n          if (i <= medianIndex) {\n            indices(i) = indices(i);\n          }\n          else {\n            indices(i) = indices(i - 1);\n          }\n        }\n      );\n    }\n    // If the number of elements in x that are less than the median is less than k, then the k-th smallest element is in the second half of x.\n    else {\n      // Update the indices of the elements of x to only include the second half of x.\n      Kokkos::parallel_for(\n        \"UpdateIndices\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(int i) {\n          if (i <= medianIndex) {\n            indices(i) = indices(i + 1);\n          }\n          else {\n            indices(i) = indices(i);\n          }\n        }\n      );\n\n      // Update k to be the number of elements in x that are less than the median minus k.\n      k = k - numLess(0);\n    }\n  }\n\n  // Copy the k-th smallest element from the Kokkos::View",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: Your code here\n  return 0;\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*> x_copy(\"x_copy\", n);\n  Kokkos::deep_copy(x_copy, x);\n  Kokkos::sort(x_copy);\n  return x_copy(k-1);\n}",
            "int n = x.extent(0);\n  Kokkos::View<int*> x_copy(\"x_copy\", n);\n  Kokkos::deep_copy(x_copy, x);\n  Kokkos::sort(x_copy);\n  return x_copy(k-1);\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    int *y = new int[n];\n    int *z = new int[n];\n    int *w = new int[n];\n    int *v = new int[n];\n    int *u = new int[n];\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        int chunk = n / nthreads;\n        int start = tid * chunk;\n        int end = (tid + 1) * chunk;\n        if (tid == nthreads - 1) end = n;\n\n        for (int i = start; i < end; i++) {\n            y[i] = x[i];\n        }\n\n        #pragma omp barrier\n\n        // Sort the chunk of the vector\n        for (int i = start; i < end; i++) {\n            for (int j = i + 1; j < end; j++) {\n                if (y[i] > y[j]) {\n                    int tmp = y[i];\n                    y[i] = y[j];\n                    y[j] = tmp;\n                }\n            }\n        }\n\n        #pragma omp barrier\n\n        // Merge the sorted chunks\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                z[i] = y[i];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                for (int j = i + 1; j < end; j++) {\n                    if (z[i] > z[j]) {\n                        int tmp = z[i];\n                        z[i] = z[j];\n                        z[j] = tmp;\n                    }\n                }\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                w[i] = z[i];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                for (int j = i + 1; j < end; j++) {\n                    if (w[i] > w[j]) {\n                        int tmp = w[i];\n                        w[i] = w[j];\n                        w[j] = tmp;\n                    }\n                }\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                v[i] = w[i];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                for (int j = i + 1; j < end; j++) {\n                    if (v[i] > v[j]) {\n                        int tmp = v[i];\n                        v[i] = v[j];\n                        v[j] = tmp;\n                    }\n                }\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                u[i] = v[i];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid > 0) {\n            for (int i = start; i < end; i++) {\n                for (int j = i + 1; j < end; j++) {\n                    if (u[i] > u[j]) {\n                        int tmp = u[i];\n                        u[i] = u[j];\n                        u[j] = tmp;\n                    }\n                }\n            }\n        }\n\n        #pragma omp barrier\n\n        if (tid == 0) {\n            for (int i = 0; i < n; i++) {\n                y[i] = u[i];",
            "int n = x.size();\n  std::vector<int> y(n);\n  int nthreads = omp_get_max_threads();\n  int nthreads_per_block = n / nthreads;\n  int nblocks = nthreads;\n  int nthreads_extra = n % nthreads;\n\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    int nthreads_this_block = nthreads_per_block;\n    if (tid < nthreads_extra) {\n      nthreads_this_block++;\n    }\n    int start = tid * nthreads_per_block;\n    int end = start + nthreads_this_block;\n    if (tid == nthreads - 1) {\n      end = n;\n    }\n    std::vector<int> z(nthreads_this_block);\n    for (int i = start; i < end; i++) {\n      z[i - start] = x[i];\n    }\n    std::sort(z.begin(), z.end());\n    for (int i = start; i < end; i++) {\n      y[i] = z[i - start];\n    }\n  }\n  std::sort(y.begin(), y.end());\n  return y[k - 1];\n}",
            "int n = x.size();\n    std::vector<int> y(n);\n    std::copy(x.begin(), x.end(), y.begin());\n\n    // TODO: implement this function\n    return 0;\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    y[i] = x[i];\n  }\n  int l = 0;\n  int r = n - 1;\n  while (l <= r) {\n    int m = (l + r) / 2;\n    int pivot = y[m];\n    int i = l;\n    int j = r;\n    while (i <= j) {\n      while (y[i] < pivot) i++;\n      while (y[j] > pivot) j--;\n      if (i <= j) {\n        std::swap(y[i], y[j]);\n        i++;\n        j--;\n      }\n    }\n    if (j < k - 1) {\n      l = i;\n    } else if (j > k - 1) {\n      r = j;\n    } else {\n      return pivot;\n    }\n  }\n  return -1;\n}",
            "int n = x.size();\n  int kthSmallest = 0;\n  std::vector<int> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    y[i] = x[i];\n  }\n  // TODO: implement this function\n  return kthSmallest;\n}",
            "int n = x.size();\n  int kth = 0;\n  int i = 0;\n  int j = n - 1;\n  while (i <= j) {\n    int pivot = x[i];\n    int i_new = i + 1;\n    int j_new = j;\n    while (i_new <= j_new) {\n      if (x[i_new] < pivot) {\n        i_new++;\n      } else if (x[i_new] > pivot) {\n        std::swap(x[i_new], x[j_new]);\n        j_new--;\n      } else {\n        i_new++;\n        j_new--;\n      }\n    }\n    std::swap(x[i], x[j_new]);\n    if (j_new == k - 1) {\n      kth = pivot;\n      break;\n    } else if (j_new < k - 1) {\n      i = j_new + 1;\n    } else {\n      j = j_new - 1;\n    }\n  }\n  return kth;\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    y[i] = x[i];\n  }\n  // TODO: sort y\n  return y[k-1];\n}",
            "int n = x.size();\n    int* y = new int[n];\n    std::copy(x.begin(), x.end(), y);\n    std::sort(y, y+n);\n    int result = y[k-1];\n    delete[] y;\n    return result;\n}",
            "int n = x.size();\n  int kthSmallest = 0;\n  std::vector<int> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    y[i] = x[i];\n  }\n  // TODO: implement this function\n  return kthSmallest;\n}",
            "int n = x.size();\n    int kthSmallest = 0;\n    // TODO: implement\n    return kthSmallest;\n}",
            "int n = x.size();\n  int kth = 0;\n  int i = 0;\n  int j = n - 1;\n  int mid = 0;\n  int pivot = 0;\n  int tmp = 0;\n  while (i <= j) {\n    mid = (i + j) / 2;\n    pivot = x[mid];\n    #pragma omp parallel for private(i, j, tmp)\n    for (i = 0; i < n; i++) {\n      if (x[i] < pivot) {\n        tmp = x[i];\n        x[i] = x[j];\n        x[j] = tmp;\n        j--;\n      }\n    }\n    if (k <= j) {\n      i = 0;\n      j = j - 1;\n    } else if (k > j + 1) {\n      i = j + 2;\n      j = n - 1;\n    } else {\n      kth = pivot;\n      break;\n    }\n  }\n  return kth;\n}",
            "int n = x.size();\n    std::vector<int> y(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        y[i] = x[i];\n    }\n    // sort y\n    //...\n    return y[k-1];\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  std::vector<int> z(n);\n  std::vector<int> w(n);\n  std::vector<int> v(n);\n  std::vector<int> u(n);\n  std::vector<int> t(n);\n  std::vector<int> s(n);\n  std::vector<int> r(n);\n  std::vector<int> q(n);\n  std::vector<int> p(n);\n  std::vector<int> o(n);\n  std::vector<int> nn(n);\n  std::vector<int> mm(n);\n  std::vector<int> ll(n);\n  std::vector<int> kk(n);\n  std::vector<int> jj(n);\n  std::vector<int> ii(n);\n  std::vector<int> hh(n);\n  std::vector<int> gg(n);\n  std::vector<int> ff(n);\n  std::vector<int> ee(n);\n  std::vector<int> dd(n);\n  std::vector<int> cc(n);\n  std::vector<int> bb(n);\n  std::vector<int> aa(n);\n\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      #pragma omp task\n      {\n        #pragma omp task\n        {\n          #pragma omp task\n          {\n            #pragma omp task\n            {\n              #pragma omp task\n              {\n                #pragma omp task\n                {\n                  #pragma omp task\n                  {\n                    #pragma omp task\n                    {\n                      #pragma omp task\n                      {\n                        #pragma omp task\n                        {\n                          #pragma omp task\n                          {\n                            #pragma omp task\n                            {\n                              #pragma omp task\n                              {\n                                #pragma omp task\n                                {\n                                  #pragma omp task\n                                  {\n                                    #pragma omp task\n                                    {\n                                      #pragma omp task\n                                      {\n                                        #pragma omp task\n                                        {\n                                          #pragma omp task\n                                          {\n                                            #pragma omp task\n                                            {\n                                              #pragma omp task\n                                              {\n                                                #pragma omp task\n                                                {\n                                                  #pragma omp task\n                                                  {\n                                                    #pragma omp task\n                                                    {\n                                                      #pragma omp task\n                                                      {\n                                                        #pragma omp task\n                                                        {\n                                                          #pragma omp task\n                                                          {\n                                                            #pragma omp task\n                                                            {\n                                                              #pragma omp task\n                                                              {\n                                                                #pragma omp task\n                                                                {\n                                                                  #pragma omp task\n                                                                  {\n                                                                    #pragma omp task\n                                                                    {\n                                                                      #pragma omp task\n                                                                      {\n                                                                        #pragma omp task\n                                                                        {\n                                                                          #pragma omp task\n                                                                          {\n                                                                            #pragma omp task\n                                                                            {\n                                                                              #pragma omp task\n                                                                              {\n                                                                                #pragma omp task\n                                                                                {\n                                                                                  #pragma omp task\n                                                                                  {\n                                                                                    #pragma omp task\n                                                                                    {\n                                                                                      #pragma omp task\n                                                                                      {\n                                                                                        #pragma omp task\n                                                                                        {\n                                                                                          #pragma omp task\n                                                                                          {\n                                                                                            #pragma omp task\n                                                                                            {\n                                                                                              #pragma omp task\n                                                                                              {\n                                                                                                #",
            "int n = x.size();\n    std::vector<int> y(n);\n    std::vector<int> z(n);\n    std::vector<int> w(n);\n    std::vector<int> v(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        y[i] = x[i];\n    }\n\n    for (int i = 1; i < k; ++i) {\n        #pragma omp parallel for\n        for (int j = 0; j < n; ++j) {\n            z[j] = y[j] + 1;\n        }\n        #pragma omp parallel for\n        for (int j = 0; j < n; ++j) {\n            y[j] = std::min(y[j], z[j]);\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        w[i] = y[i] + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        z[i] = std::max(y[i], w[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        v[i] = z[i] - 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        y[i] = std::max(y[i], v[i]);\n    }\n\n    return y[0];\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  std::copy(x.begin(), x.end(), y.begin());\n  std::sort(y.begin(), y.end());\n  return y[k-1];\n}",
            "int n = x.size();\n  int *y = new int[n];\n  for (int i = 0; i < n; i++) {\n    y[i] = x[i];\n  }\n  int *z = new int[n];\n  int *w = new int[n];\n  int *v = new int[n];\n  int *u = new int[n];\n  int *t = new int[n];\n  int *s = new int[n];\n  int *r = new int[n];\n  int *q = new int[n];\n  int *p = new int[n];\n  int *o = new int[n];\n  int *n = new int[n];\n  int *m = new int[n];\n  int *l = new int[n];\n  int *k = new int[n];\n  int *j = new int[n];\n  int *i = new int[n];\n  int *h = new int[n];\n  int *g = new int[n];\n  int *f = new int[n];\n  int *e = new int[n];\n  int *d = new int[n];\n  int *c = new int[n];\n  int *b = new int[n];\n  int *a = new int[n];\n  int *aa = new int[n];\n  int *ab = new int[n];\n  int *ac = new int[n];\n  int *ad = new int[n];\n  int *ae = new int[n];\n  int *af = new int[n];\n  int *ag = new int[n];\n  int *ah = new int[n];\n  int *ai = new int[n];\n  int *aj = new int[n];\n  int *ak = new int[n];\n  int *al = new int[n];\n  int *am = new int[n];\n  int *an = new int[n];\n  int *ao = new int[n];\n  int *ap = new int[n];\n  int *aq = new int[n];\n  int *ar = new int[n];\n  int *as = new int[n];\n  int *at = new int[n];\n  int *au = new int[n];\n  int *av = new int[n];\n  int *aw = new int[n];\n  int *ax = new int[n];\n  int *ay = new int[n];\n  int *az = new int[n];\n  int *ba = new int[n];\n  int *bb = new int[n];\n  int *bc = new int[n];\n  int *bd = new int[n];\n  int *be = new int[n];\n  int *bf = new int[n];\n  int *bg = new int[n];\n  int *bh = new int[n];\n  int *bi = new int[n];\n  int *bj = new int[n];\n  int *bk = new int[n];\n  int *bl = new int[n];\n  int *bm = new int[n];\n  int *bn = new int[n];\n  int *bo = new int[n];\n  int *bp = new int[n];\n  int *bq = new int[n];\n  int *br = new int[n];\n  int *bs = new int[n];\n  int *bt = new int[n];\n  int *bu = new int[n];\n  int *bv = new int[n];\n  int *bw = new int[n];\n  int *bx = new int[n];\n  int *by = new int[n];\n  int *bz = new int[n];\n  int *ca = new int[n];\n  int *cb = new int[n];\n  int *cc = new int[n];\n  int *cd = new int[n];\n  int *ce = new int[n];\n  int *cf = new int[n];\n  int *cg = new int[n];\n  int *ch = new int[n];\n  int *ci = new int[n];\n  int *cj = new int[n];\n  int *ck = new int[n];\n  int *cl",
            "int n = x.size();\n  int i = 0;\n  int j = n - 1;\n  int pivot = x[k];\n  while (i <= j) {\n    while (x[i] < pivot) i++;\n    while (x[j] > pivot) j--;\n    if (i <= j) {\n      std::swap(x[i], x[j]);\n      i++;\n      j--;\n    }\n  }\n  return x[k];\n}",
            "int n = x.size();\n  int kth = x[k];\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    if (x[i] < kth) {\n      kth = x[i];\n    }\n  }\n  return kth;\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    y[i] = x[i];\n  }\n  int kthSmallest = 0;\n  for (int i = 0; i < k; ++i) {\n    int minIndex = i;\n    for (int j = i + 1; j < n; ++j) {\n      if (y[j] < y[minIndex]) {\n        minIndex = j;\n      }\n    }\n    std::swap(y[i], y[minIndex]);\n  }\n  return y[k - 1];\n}",
            "int n = x.size();\n    std::vector<int> y(n);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        y[i] = x[i];\n    }\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "serial",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement this function\n    int n = x.size();\n    std::vector<int> y(n);\n    std::copy(x.begin(), x.end(), y.begin());\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
            "// TODO: implement\n  return 0;\n}",
            "// TODO: Implement this function\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
            "std::vector<int> y;\n    for (int i = 0; i < x.size(); ++i) {\n        y.push_back(x[i]);\n    }\n    std::sort(y.begin(), y.end());\n    return y[k - 1];\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k - 1];\n}",
            "// TODO: Implement this function\n  return 0;\n}",
            "std::vector<int> y = x;\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
            "std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k - 1];\n}",
            "std::vector<int> y = x;\n    std::sort(y.begin(), y.end());\n    return y[k - 1];\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement this function\n  return 0;\n}",
            "// TODO: implement\n    return 0;\n}",
            "// TODO: implement this function\n    return 0;\n}",
            "std::vector<int> y(x);\n  std::sort(y.begin(), y.end());\n  return y[k-1];\n}",
            "std::vector<int> y = x;\n    std::sort(y.begin(), y.end());\n    return y[k - 1];\n}",
            "// TODO: implement\n  return 0;\n}",
            "int n = x.size();\n    std::vector<int> y(n);\n    for (int i = 0; i < n; ++i) {\n        y[i] = x[i];\n    }\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
            "int n = x.size();\n  std::vector<int> y(n);\n  for (int i = 0; i < n; ++i) {\n    y[i] = x[i];\n  }\n  std::sort(y.begin(), y.end());\n  return y[k - 1];\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "hip",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nthreads = blockDim.x;\n    int nblocks = gridDim.x;\n    int n = N;\n    int *s = new int[nthreads];\n    int *t = new int[nthreads];\n    int *u = new int[nthreads];\n    int *v = new int[nthreads];\n    int *w = new int[nthreads];\n    int *r = new int[nthreads];\n    int *d = new int[nthreads];\n    int *e = new int[nthreads];\n    int *f = new int[nthreads];\n    int *g = new int[nthreads];\n    int *h = new int[nthreads];\n    int *i = new int[nthreads];\n    int *j = new int[nthreads];\n    int *kthSmallest_local = new int[nthreads];\n    int *kthSmallest_global = new int[nthreads];\n    int *kthSmallest_global_local = new int[nthreads];\n    int *kthSmallest_global_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];\n    int *kthSmallest_global_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = new int[nthreads];",
            "// TODO: implement this function\n}",
            "// TODO: implement this kernel\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this kernel\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this kernel\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int laneId = tid % warpSize;\n  int wid = tid / warpSize;\n  int numWarps = blockDim.x / warpSize;\n  int warpId = tid / warpSize;\n\n  __shared__ int sdata[2 * blockDim.x];\n  __shared__ int sdata2[2 * blockDim.x];\n  __shared__ int sdata3[2 * blockDim.x];\n  __shared__ int sdata4[2 * blockDim.x];\n\n  int localK = k;\n\n  if (localK > N) {\n    localK = N;\n  }\n\n  int localN = N;\n\n  if (localN > blockDim.x * 2) {\n    localN = blockDim.x * 2;\n  }\n\n  if (localN > 2 * blockDim.x) {\n    localN = 2 * blockDim.x;\n  }\n\n  if (localN > 4 * blockDim.x) {\n    localN = 4 * blockDim.x;\n  }\n\n  if (localN > 8 * blockDim.x) {\n    localN = 8 * blockDim.x;\n  }\n\n  if (localN > 16 * blockDim.x) {\n    localN = 16 * blockDim.x;\n  }\n\n  if (localN > 32 * blockDim.x) {\n    localN = 32 * blockDim.x;\n  }\n\n  if (localN > 64 * blockDim.x) {\n    localN = 64 * blockDim.x;\n  }\n\n  if (localN > 128 * blockDim.x) {\n    localN = 128 * blockDim.x;\n  }\n\n  if (localN > 256 * blockDim.x) {\n    localN = 256 * blockDim.x;\n  }\n\n  if (localN > 512 * blockDim.x) {\n    localN = 512 * blockDim.x;\n  }\n\n  if (localN > 1024 * blockDim.x) {\n    localN = 1024 * blockDim.x;\n  }\n\n  if (localN > 2048 * blockDim.x) {\n    localN = 2048 * blockDim.x;\n  }\n\n  if (localN > 4096 * blockDim.x) {\n    localN = 4096 * blockDim.x;\n  }\n\n  if (localN > 8192 * blockDim.x) {\n    localN = 8192 * blockDim.x;\n  }\n\n  if (localN > 16384 * blockDim.x) {\n    localN = 16384 * blockDim.x;\n  }\n\n  if (localN > 32768 * blockDim.x) {\n    localN = 32768 * blockDim.x;\n  }\n\n  if (localN > 65536 * blockDim.x) {\n    localN = 65536 * blockDim.x;\n  }\n\n  if (localN > 131072 * blockDim.x) {\n    localN = 131072 * blockDim.x;\n  }\n\n  if (localN > 262144 * blockDim.x) {\n    localN = 262144 * blockDim.x;\n  }\n\n  if (localN > 524288 * blockDim.x) {\n    localN = 524288 * blockDim.x;\n  }\n\n  if (localN > 1048576 * blockDim.x) {\n    localN = 1048576 * blockDim.x;\n  }\n\n  if (localN > 2097152 * blockDim.x) {\n    localN = 20",
            "// TODO: implement this kernel\n}",
            "// TODO: implement this function\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        // Copy the value of x[tid] to shared memory.\n        // This is necessary because the value of x[tid] may change in the future.\n        // If we don't copy it to shared memory, we may end up with a wrong result.\n        // We use the first element of shared memory to store the value of x[tid].\n        // We use the second element of shared memory to store the number of threads that have already finished their work.\n        // We use the third element of shared memory to store the value of the k-th smallest element.\n        // We use the fourth element of shared memory to store the number of threads that have already found the k-th smallest element.\n        extern __shared__ int shared[];\n        shared[0] = x[tid];\n        shared[1] = 1;\n        shared[2] = x[tid];\n        shared[3] = 1;\n        __syncthreads();\n\n        // Loop until all threads have finished their work.\n        while (shared[1] < blockDim.x) {\n            // Each thread compares its value with the value of the k-th smallest element.\n            if (shared[0] < shared[2]) {\n                // If the value of the current thread is smaller than the k-th smallest element,\n                // the current thread becomes the new k-th smallest element.\n                shared[2] = shared[0];\n                shared[3] = 1;\n            } else if (shared[0] == shared[2]) {\n                // If the value of the current thread is equal to the k-th smallest element,\n                // increment the number of threads that have already found the k-th smallest element.\n                shared[3]++;\n            }\n            __syncthreads();\n\n            // If the current thread has found the k-th smallest element,\n            // it can stop its work.\n            if (shared[3] == k) {\n                break;\n            }\n\n            // If the current thread has not found the k-th smallest element,\n            // it can stop its work.\n            if (shared[0] >= shared[2]) {\n                break;\n            }\n\n            // Increment the number of threads that have already finished their work.\n            shared[1]++;\n            __syncthreads();\n        }\n\n        // If the current thread has found the k-th smallest element,\n        // it can copy the value of the k-th smallest element to the output.\n        if (shared[3] == k) {\n            *kthSmallest = shared[2];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Use AMD HIP atomicMin to find the k-th smallest element of x.\n        atomicMin(kthSmallest, x[tid]);\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int kthSmallest = 0;\n  return kthSmallest;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement this function\n    return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int n = x.size();\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n_per_rank = n / size;\n  int n_extra = n % size;\n  int start = rank * n_per_rank;\n  int end = start + n_per_rank;\n  if (rank < n_extra) {\n    end++;\n  }\n  std::vector<int> x_local(x.begin() + start, x.begin() + end);\n  std::sort(x_local.begin(), x_local.end());\n  int k_local = k;\n  if (rank < n_extra) {\n    k_local += rank * n_per_rank;\n  } else {\n    k_local += n_extra * (n_per_rank + 1);\n  }\n  int result = x_local[k_local];\n  MPI_Reduce(&result, &result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int kthSmallest = 0;\n  return kthSmallest;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int kthSmallest = 0;\n  if (rank == 0) {\n    std::vector<int> x_copy(x);\n    for (int i = 1; i < size; i++) {\n      int kthSmallest_i;\n      MPI_Recv(&kthSmallest_i, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      if (kthSmallest_i < kthSmallest) {\n        kthSmallest = kthSmallest_i;\n      }\n    }\n  } else {\n    std::vector<int> x_copy(x);\n    std::nth_element(x_copy.begin(), x_copy.begin() + k, x_copy.end());\n    MPI_Send(&x_copy[k], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n  return kthSmallest;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: implement this function\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Your code here\n  int kthSmallest = 0;\n  int localKthSmallest = 0;\n  int localK = 0;\n  int localSize = x.size();\n  int localKthSmallestIndex = 0;\n  int localKthSmallestIndex2 = 0;\n  int localKthSmallestIndex3 = 0;\n  int localKthSmallestIndex4 = 0;\n  int localKthSmallestIndex5 = 0;\n  int localKthSmallestIndex6 = 0;\n  int localKthSmallestIndex7 = 0;\n  int localKthSmallestIndex8 = 0;\n  int localKthSmallestIndex9 = 0;\n  int localKthSmallestIndex10 = 0;\n  int localKthSmallestIndex11 = 0;\n  int localKthSmallestIndex12 = 0;\n  int localKthSmallestIndex13 = 0;\n  int localKthSmallestIndex14 = 0;\n  int localKthSmallestIndex15 = 0;\n  int localKthSmallestIndex16 = 0;\n  int localKthSmallestIndex17 = 0;\n  int localKthSmallestIndex18 = 0;\n  int localKthSmallestIndex19 = 0;\n  int localKthSmallestIndex20 = 0;\n  int localKthSmallestIndex21 = 0;\n  int localKthSmallestIndex22 = 0;\n  int localKthSmallestIndex23 = 0;\n  int localKthSmallestIndex24 = 0;\n  int localKthSmallestIndex25 = 0;\n  int localKthSmallestIndex26 = 0;\n  int localKthSmallestIndex27 = 0;\n  int localKthSmallestIndex28 = 0;\n  int localKthSmallestIndex29 = 0;\n  int localKthSmallestIndex30 = 0;\n  int localKthSmallestIndex31 = 0;\n  int localKthSmallestIndex32 = 0;\n  int localKthSmallestIndex33 = 0;\n  int localKthSmallestIndex34 = 0;\n  int localKthSmallestIndex35 = 0;\n  int localKthSmallestIndex36 = 0;\n  int localKthSmallestIndex37 = 0;\n  int localKthSmallestIndex38 = 0;\n  int localKthSmallestIndex39 = 0;\n  int localKthSmallestIndex40 = 0;\n  int localKthSmallestIndex41 = 0;\n  int localKthSmallestIndex42 = 0;\n  int localKthSmallestIndex43 = 0;\n  int localKthSmallestIndex44 = 0;\n  int localKthSmallestIndex45 = 0;\n  int localKthSmallestIndex46 = 0;\n  int localKthSmallestIndex47 = 0;\n  int localKthSmallestIndex48 = 0;\n  int localKthSmallestIndex49 = 0;\n  int localKthSmallestIndex50 = 0;\n  int localKthSmallestIndex51 = 0;\n  int localKthSmallestIndex52 = 0;\n  int localKthSmallestIndex53 = 0;\n  int localKthSmallestIndex54 = 0;\n  int localKthSmallestIndex55 = 0;\n  int localKthSmallestIndex56 = 0;\n  int localKthSmallestIndex57 = 0;\n  int localKthSmallestIndex58 = 0;\n  int local",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n  int kthSmallest = 0;\n  return kthSmallest;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int kth = -1;\n  if (rank == 0) {\n    // Sort the first k elements of x.\n    std::nth_element(x.begin(), x.begin() + k, x.end());\n    kth = x[k - 1];\n  }\n  // Broadcast the kth element to all ranks.\n  MPI_Bcast(&kth, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  return kth;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int kth = 0;\n  if (rank == 0) {\n    std::vector<int> x_copy(n);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(&x_copy[0], n, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < n; ++j) {\n        if (x[j] < x_copy[j]) {\n          x[j] = x_copy[j];\n        }\n      }\n    }\n    std::sort(x.begin(), x.end());\n    kth = x[k - 1];\n  } else {\n    MPI_Send(&x[0], n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n  return kth;\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "cuda",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function.\n    // Use the atomicMin function to find the k-th smallest element of x.\n    // You may assume that k is always valid.\n    // You may also assume that N is always a power of 2.\n    // You may not use shared memory.\n    // You may not use any other global memory than x and kthSmallest.\n    // You may not use any other variables than x and kthSmallest.\n    // You may not use any other functions than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.\n    // You may not use any other synchronization mechanisms than atomicMin.",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n  int tid = threadIdx.x;\n  int blockSize = blockDim.x;\n  int i = tid;\n  int j = tid + blockSize;\n  int kthSmallest_temp = 0;\n  int temp = 0;\n  while (i < N) {\n    if (j < N) {\n      if (x[i] > x[j]) {\n        temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n      }\n    }\n    i += blockSize;\n    j += blockSize;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    kthSmallest_temp = x[k - 1];\n  }\n  __syncthreads();\n  if (tid == 0) {\n    *kthSmallest = kthSmallest_temp;\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * blockDim.x + tid;\n\n    // Each thread compares its value to the k-th smallest element\n    // and swaps if necessary\n    if (i < N) {\n        if (x[i] < *kthSmallest) {\n            *kthSmallest = x[i];\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    __shared__ int sdata[BLOCK_SIZE];\n    int i = bid * BLOCK_SIZE + tid;\n    int j = i + BLOCK_SIZE;\n    int min = 0;\n    int min_index = 0;\n    if (i < N) {\n        sdata[tid] = x[i];\n    }\n    __syncthreads();\n    if (tid == 0) {\n        for (int i = 0; i < BLOCK_SIZE; i++) {\n            if (i == 0) {\n                min = sdata[i];\n                min_index = i;\n            } else {\n                if (sdata[i] < min) {\n                    min = sdata[i];\n                    min_index = i;\n                }\n            }\n        }\n        *kthSmallest = min;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __syncthreads() to synchronize all threads.\n  // Use __threadfence() to make sure that all global memory writes are visible to all threads.\n  // Use __threadfence_block() to make sure that all threads in the block have finished.\n  // Use __syncthreads() to synchronize all threads.\n  // Use atomicMin() to find the k-th smallest element of x.\n  // Use atomicAdd() to count the number of threads that have found the k-th smallest element.\n  // Use __",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "37_sort_k-th_smallest_element",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int n = x.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = n / size;\n    int remainder = n % size;\n    int start = rank * chunkSize;\n    int end = start + chunkSize;\n    if (rank == size - 1) {\n        end += remainder;\n    }\n    std::vector<int> localX(x.begin() + start, x.begin() + end);\n    int localK = k;\n    int localResult = 0;\n    if (localX.size() > 0) {\n        localResult = findKthSmallest(localX, localK);\n    }\n    int globalResult = 0;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalResult;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int n = x.size();\n    int kth = 0;\n    int *local_x = new int[n];\n    int *local_y = new int[n];\n    int *local_z = new int[n];\n    int *local_w = new int[n];\n    int *local_t = new int[n];\n    int *local_u = new int[n];\n    int *local_v = new int[n];\n    int *local_r = new int[n];\n    int *local_s = new int[n];\n    int *local_a = new int[n];\n    int *local_b = new int[n];\n    int *local_c = new int[n];\n    int *local_d = new int[n];\n    int *local_e = new int[n];\n    int *local_f = new int[n];\n    int *local_g = new int[n];\n    int *local_h = new int[n];\n    int *local_i = new int[n];\n    int *local_j = new int[n];\n    int *local_k = new int[n];\n    int *local_l = new int[n];\n    int *local_m = new int[n];\n    int *local_n = new int[n];\n    int *local_o = new int[n];\n    int *local_p = new int[n];\n    int *local_q = new int[n];\n    int *local_r = new int[n];\n    int *local_s = new int[n];\n    int *local_t = new int[n];\n    int *local_u = new int[n];\n    int *local_v = new int[n];\n    int *local_w = new int[n];\n    int *local_x = new int[n];\n    int *local_y = new int[n];\n    int *local_z = new int[n];\n    int *local_a = new int[n];\n    int *local_b = new int[n];\n    int *local_c = new int[n];\n    int *local_d = new int[n];\n    int *local_e = new int[n];\n    int *local_f = new int[n];\n    int *local_g = new int[n];\n    int *local_h = new int[n];\n    int *local_i = new int[n];\n    int *local_j = new int[n];\n    int *local_k = new int[n];\n    int *local_l = new int[n];\n    int *local_m = new int[n];\n    int *local_n = new int[n];\n    int *local_o = new int[n];\n    int *local_p = new int[n];\n    int *local_q = new int[n];\n    int *local_r = new int[n];\n    int *local_s = new int[n];\n    int *local_t = new int[n];\n    int *local_u = new int[n];\n    int *local_v = new int[n];\n    int *local_w = new int[n];\n    int *local_x = new int[n];\n    int *local_y = new int[n];\n    int *local_z = new int[n];\n    int *local_a = new int[n];\n    int *local_b = new int[n];\n    int *local_c = new int[n];\n    int *local_d = new int[n];\n    int *local_e = new int[n];\n    int *local_f = new int[n];\n    int *local_g = new int[n];\n    int *local_h = new int[n];\n    int *local_i = new int[n];\n    int *local_j = new int[n];\n    int *local_k = new int[n];\n    int *local_l = new int[n",
            "int n = x.size();\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int kth = 0;\n  int kth_rank = 0;\n  int kth_size = 0;\n  int kth_rank_local = 0;\n  int kth_size_local = 0;\n  int kth_local = 0;\n  int kth_local_rank = 0;\n  int kth_local_size = 0;\n  int kth_local_rank_local = 0;\n  int kth_local_size_local = 0;\n  int kth_local_rank_local_local = 0;\n  int kth_local_size_local_local = 0;\n  int kth_local_rank_local_local_local = 0;\n  int kth_local_size_local_local_local = 0;\n  int kth_local_rank_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_rank_local_local_local_local_local_local_local_local_local_local_local_local_local_local_local = 0;\n  int kth_local_size_local_local_local_local_local_local_local_local_local_local_local_local_local_",
            "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    int kth = 0;\n    return kth;\n}",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  return 0;\n}",
            "int n = x.size();\n  int rank = 0, size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int k_per_rank = k / size;\n  int k_remainder = k % size;\n  int k_start = rank * k_per_rank;\n  int k_end = k_start + k_per_rank;\n  if (rank == 0) {\n    k_start += k_remainder;\n    k_end += k_remainder;\n  }\n  if (rank == size - 1) {\n    k_end = n;\n  }\n  std::vector<int> x_local(x.begin() + k_start, x.begin() + k_end);\n  std::sort(x_local.begin(), x_local.end());\n  int result = x_local[k_per_rank];\n  MPI_Reduce(&result, &result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  return result;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement this function\n\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int kth = 0;\n  int start = rank * n / size;\n  int end = (rank + 1) * n / size;\n  std::vector<int> local_x(x.begin() + start, x.begin() + end);\n  std::sort(local_x.begin(), local_x.end());\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&kth, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      if (kth < local_x[k - 1]) {\n        local_x[k - 1] = kth;\n      }\n    }\n  } else {\n    MPI_Send(&local_x[k - 1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n  return local_x[k - 1];\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    int local_k = k;\n    int local_size = x.size();\n    int local_rank = rank;\n    int local_kth = 0;\n    int global_kth = 0;\n    int global_kth_rank = 0;\n    int global_kth_size = 0;\n    int global_kth_rank_size = 0;\n\n    // 1. Find the k-th smallest element of the local vector\n    // 2. Find the k-th smallest element of the global vector\n    // 3. Return the result on rank 0\n\n    return global_kth;\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int kth = 0;\n    int local_size = x.size()/size;\n    int local_k = k;\n    int local_kth = 0;\n    int local_kth_temp = 0;\n    int local_kth_temp2 = 0;\n    int local_kth_temp3 = 0;\n    int local_kth_temp4 = 0;\n    int local_kth_temp5 = 0;\n    int local_kth_temp6 = 0;\n    int local_kth_temp7 = 0;\n    int local_kth_temp8 = 0;\n    int local_kth_temp9 = 0;\n    int local_kth_temp10 = 0;\n    int local_kth_temp11 = 0;\n    int local_kth_temp12 = 0;\n    int local_kth_temp13 = 0;\n    int local_kth_temp14 = 0;\n    int local_kth_temp15 = 0;\n    int local_kth_temp16 = 0;\n    int local_kth_temp17 = 0;\n    int local_kth_temp18 = 0;\n    int local_kth_temp19 = 0;\n    int local_kth_temp20 = 0;\n    int local_kth_temp21 = 0;\n    int local_kth_temp22 = 0;\n    int local_kth_temp23 = 0;\n    int local_kth_temp24 = 0;\n    int local_kth_temp25 = 0;\n    int local_kth_temp26 = 0;\n    int local_kth_temp27 = 0;\n    int local_kth_temp28 = 0;\n    int local_kth_temp29 = 0;\n    int local_kth_temp30 = 0;\n    int local_kth_temp31 = 0;\n    int local_kth_temp32 = 0;\n    int local_kth_temp33 = 0;\n    int local_kth_temp34 = 0;\n    int local_kth_temp35 = 0;\n    int local_kth_temp36 = 0;\n    int local_kth_temp37 = 0;\n    int local_kth_temp38 = 0;\n    int local_kth_temp39 = 0;\n    int local_kth_temp40 = 0;\n    int local_kth_temp41 = 0;\n    int local_kth_temp42 = 0;\n    int local_kth_temp43 = 0;\n    int local_kth_temp44 = 0;\n    int local_kth_temp45 = 0;\n    int local_kth_temp46 = 0;\n    int local_kth_temp47 = 0;\n    int local_kth_temp48 = 0;\n    int local_kth_temp49 = 0;\n    int local_kth_temp50 = 0;\n    int local_kth_temp51 = 0;\n    int local_kth_temp52 = 0;\n    int local_kth_temp53 = 0;\n    int local_kth_temp54 = 0;\n    int local_kth_temp55 = 0;\n    int local_kth_temp56 = 0;\n    int local_kth_temp57 = 0;\n    int local_kth_temp58 = 0;\n    int local_kth_temp59 = 0;\n    int local_kth_temp60 = 0;\n    int local_kth_temp61 = 0;\n    int local_kth_temp62 = 0;\n    int local",
            "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  // You may use OpenMP to parallelize the computation\n  // You may use MPI to distribute the computation\n  // You may use both MPI and OpenMP\n  // You may assume that n is divisible by size\n\n  return 0;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n    int n = x.size();\n    int k_local = k;\n    int k_global = 0;\n    int k_local_rank = 0;\n    int k_global_rank = 0;\n    int k_local_rank_prev = 0;\n    int k_global_rank_prev = 0;\n    int k_local_rank_next = 0;\n    int k_global_rank_next = 0;\n    int k_local_rank_next_prev = 0;\n    int k_global_rank_next_prev = 0;\n    int k_local_rank_next_next = 0;\n    int k_global_rank_next_next = 0;\n    int k_local_rank_next_next_prev = 0;\n    int k_global_rank_next_next_prev = 0;\n    int k_local_rank_next_next_next = 0;\n    int k_global_rank_next_next_next = 0;\n    int k_local_rank_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next_next_next_next_next = 0;\n    int k_local_rank_next_next_next_next_next_next_next_next_next_prev = 0;\n    int k_global_rank_next_next_next_next_next_next_next_next_next_prev = 0;\n    int k_local_rank_next_next_next_next_next_next_next_next_next_next = 0;\n    int k_global_rank_next_next_next_next_next_next_next_next_next",
            "int n = x.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = n / size;\n    int remainder = n % size;\n    int start = rank * chunkSize + std::min(rank, remainder);\n    int end = (rank + 1) * chunkSize + std::min(rank + 1, remainder);\n    std::vector<int> localX(x.begin() + start, x.begin() + end);\n    int localK = k - start;\n    int localResult = 0;\n    if (localX.size() > 0) {\n        localResult = findKthSmallest(localX, localK);\n    }\n    int globalResult = 0;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalResult;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n  return 0;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int kthSmallest = 0;\n  return kthSmallest;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Implement this function\n  int local_k = k;\n  int local_size = x.size();\n  int local_rank = rank;\n  int local_kth_smallest = 0;\n  int global_kth_smallest = 0;\n\n  // TODO: Implement this function\n  if (rank == 0) {\n    local_kth_smallest = x[local_k - 1];\n  }\n\n  MPI_Bcast(&local_kth_smallest, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  return local_kth_smallest;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this function\n\n  return 0;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n\n  return 0;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    int n = x.size();\n    int kth = 0;\n    int local_kth = 0;\n    int local_n = n / size;\n    int local_k = k / size;\n    int local_k_rem = k % size;\n    int local_n_rem = n % size;\n    int local_n_rem_rank = 0;\n    int local_k_rem_rank = 0;\n    int local_k_rem_rank_n = 0;\n    int local_k_rem_rank_k = 0;\n    int local_k_rem_rank_n_rem = 0;\n    int local_k_rem_rank_k_rem = 0;\n    int local_k_rem_rank_n_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_n_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local_k_rem_rank_k_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem_rem = 0;\n    int local",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code here\n  int n = x.size();\n  int kth = 0;\n  int start = rank * n / size;\n  int end = (rank + 1) * n / size;\n  std::vector<int> local_x(x.begin() + start, x.begin() + end);\n  std::sort(local_x.begin(), local_x.end());\n  if (rank == 0) {\n    kth = local_x[k - 1];\n  }\n  MPI_Bcast(&kth, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  return kth;\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement me\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me!\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n  for (int i = 0; i < input.size(); i++) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == input.size() - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3;\n    }\n}",
            "int n = input.size();\n    int nthreads = omp_get_max_threads();\n    int chunk_size = n/nthreads;\n    int remainder = n%nthreads;\n\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int start = thread_id*chunk_size;\n        int end = start + chunk_size;\n        if (thread_id == nthreads-1) {\n            end += remainder;\n        }\n        for (int i = start; i < end; i++) {\n            if (i == 0) {\n                output[i] = (input[i+1] + input[i])/3;\n            } else if (i == n-1) {\n                output[i] = (input[i-1] + input[i])/3;\n            } else {\n                output[i] = (input[i-1] + input[i] + input[i+1])/3;\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n    int n = input.size();\n    int i;\n    double temp;\n    #pragma omp parallel for private(i, temp)\n    for (i = 1; i < n - 1; i++) {\n        temp = (input[i - 1] + input[i + 1]) / 3;\n        output[i] = temp;\n    }\n    output[0] = input[0];\n    output[n - 1] = input[n - 1];\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  output.resize(input.size());\n  for (int i = 0; i < input.size(); i++) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == input.size() - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int n = input.size();\n    output.resize(n);\n    for (int i = 0; i < n; i++) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == n - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this function\n    // Hint: You can use the `at` function to read from the `input` vector\n    //       and the `[]` operator to write to the `output` vector.\n    //       You can use the `size` function to get the size of the `input` vector.\n    //       You can use the `begin` and `end` functions to get iterators to the beginning and end of the `input` vector.\n    //       You can use the `begin` and `end` functions to get iterators to the beginning and end of the `output` vector.\n    //       You can use the `distance` function to get the distance between two iterators.\n    //       You can use the `advance` function to advance an iterator by a given number of steps.\n    //       You can use the `fill` function to fill a range with a given value.\n    //       You can use the `fill_n` function to fill a range with a given value.\n    //       You can use the `copy` function to copy a range to another range.\n    //       You can use the `copy_n` function to copy a range to another range.\n    //       You can use the `swap` function to swap two elements in a range.\n    //       You can use the `swap_ranges` function to swap two ranges.\n    //       You can use the `transform` function to apply a function to each element of a range.\n    //       You can use the `replace` function to replace elements in a range.\n    //       You can use the `replace_if` function to replace elements in a range.\n    //       You can use the `replace_copy` function to replace elements in a range.\n    //       You can use the `replace_copy_if` function to replace elements in a range.\n    //       You can use the `fill` function to fill a range with a given value.\n    //       You can use the `fill_n` function to fill a range with a given value.\n    //       You can use the `generate` function to generate a range.\n    //       You can use the `generate_n` function to generate a range.\n    //       You can use the `remove` function to remove elements from a range.\n    //       You can use the `remove_if` function to remove elements from a range.\n    //       You can use the `remove_copy` function to remove elements from a range.\n    //       You can use the `remove_copy_if` function to remove elements from a range.\n    //       You can use the `unique` function to remove consecutive duplicate elements from a range.\n    //       You can use the `unique_copy` function to remove consecutive duplicate elements from a range.\n    //       You can use the `reverse` function to reverse the order of elements in a range.\n    //       You can use the `reverse_copy` function to reverse the order of elements in a range.\n    //       You can use the `rotate` function to rotate the order of elements in a range.\n    //       You can use the `rotate_copy` function to rotate the order of elements in a range.\n    //       You can use the `random_shuffle` function to randomly shuffle the order of elements in a range.\n    //       You can use the `random_shuffle` function to randomly shuffle the order of elements in a range.\n    //       You can use the `random_sample` function to randomly sample elements from a range.\n    //       You can use the `random_sample_n` function to randomly sample elements from a range.\n    //       You can use the `sample` function to randomly sample elements from a range.\n    //       You can use the `sample_n` function to randomly sample elements from a range.\n    //       You can use the `partial_sum` function to compute the partial sums of a range.\n    //       You can use the `partial_sum_n` function to compute the partial sums of a range.\n    //       You can use the `adjacent_difference` function to compute the differences between adjacent elements in a range.\n    //       You can use the `adjacent_difference_n` function to compute the differences between adjacent elements in a range.\n    //       You can use the `inner_product` function to compute the inner product of two ranges.\n    //       You can use the `inner_product_n` function to compute the inner product of two ranges.\n    //       You can use the `accumulate` function to",
            "// TODO: Implement this function\n    // Hint: You can use the `at` function to access elements of a vector.\n    //       If you try to access an element that is out of bounds, `at` will throw an exception.\n    //       If you want to avoid this behavior, you can use the `[]` operator instead.\n    //       It will return a default value when accessing an element that is out of bounds.\n    //       Example:\n    //         std::vector<int> v(5);\n    //         v[0] = 1;\n    //         v[1] = 2;\n    //         v[2] = 3;\n    //         v[3] = 4;\n    //         v[4] = 5;\n    //         v[5] = 6; // This will not throw an exception.\n    //         v.at(5) = 6; // This will throw an exception.\n    //\n    //       You can use the `size` function to get the size of a vector.\n    //       Example:\n    //         std::vector<int> v(5);\n    //         v[0] = 1;\n    //         v[1] = 2;\n    //         v[2] = 3;\n    //         v[3] = 4;\n    //         v[4] = 5;\n    //         std::cout << v.size() << std::endl; // This will print 5.\n    //\n    //       You can use the `push_back` function to add an element to the end of a vector.\n    //       Example:\n    //         std::vector<int> v;\n    //         v.push_back(1);\n    //         v.push_back(2);\n    //         v.push_back(3);\n    //         v.push_back(4);\n    //         v.push_back(5);\n    //         std::cout << v.size() << std::endl; // This will print 5.\n    //\n    //       You can use the `clear` function to remove all elements from a vector.\n    //       Example:\n    //         std::vector<int> v;\n    //         v.push_back(1);\n    //         v.push_back(2);\n    //         v.push_back(3);\n    //         v.push_back(4);\n    //         v.push_back(5);\n    //         v.clear();\n    //         std::cout << v.size() << std::endl; // This will print 0.\n    //\n    //       You can use the `resize` function to change the size of a vector.\n    //       Example:\n    //         std::vector<int> v;\n    //         v.push_back(1);\n    //         v.push_back(2);\n    //         v.push_back(3);\n    //         v.push_back(4);\n    //         v.push_back(5);\n    //         v.resize(3);\n    //         std::cout << v.size() << std::endl; // This will print 3.\n    //\n    //       You can use the `insert` function to insert an element into a vector at a specific position.\n    //       Example:\n    //         std::vector<int> v;\n    //         v.push_back(1);\n    //         v.push_back(2);\n    //         v.push_back(3);\n    //         v.push_back(4);\n    //         v.push_back(5);\n    //         v.insert(v.begin() + 2, 10);\n    //         std::cout << v.size() << std::endl; // This will print 6.\n    //         std::cout << v[2] << std::endl; // This will print 10.\n    //\n    //       You can use the `erase` function to remove an element from a vector at a specific position.\n    //       Example:\n    //         std::vector<int> v;\n    //         v.push_back(1);\n    //         v.push_back(2);\n    //         v.push_back(3);\n    //         v.push_back(4);\n    //         v.push_back(5);",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int size = input.size();\n    for (int i = 0; i < size; i++) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == size - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "hip",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = input.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    std::vector<double> local_input(n_per_proc + 2);\n    std::vector<double> local_output(n_per_proc + 2);\n\n    MPI_Scatter(input.data(), n_per_proc + 2, MPI_DOUBLE, local_input.data(), n_per_proc + 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute local output\n    for (int i = 1; i < n_per_proc + 1; i++) {\n        local_output[i] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    // Compute output on rank 0\n    if (rank == 0) {\n        std::vector<double> global_output(n);\n        MPI_Gather(local_output.data(), n_per_proc + 2, MPI_DOUBLE, global_output.data(), n_per_proc + 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        output = global_output;\n    } else {\n        MPI_Gather(local_output.data(), n_per_proc + 2, MPI_DOUBLE, NULL, n_per_proc + 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = input.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n    if (rank == size - 1) {\n        end += n_rem;\n    }\n\n    std::vector<double> local_input(input.begin() + start, input.begin() + end);\n    std::vector<double> local_output(local_input.size());\n\n    for (int i = 1; i < local_input.size() - 1; i++) {\n        local_output[i] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        output.resize(n);\n    }\n\n    MPI_Gather(&local_output[0], local_output.size(), MPI_DOUBLE, &output[0], local_output.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Implement\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "cuda",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i - 1] + input[i]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "49_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int n_per_rank = n / size;\n    int n_extra = n % size;\n\n    // TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = input.size();\n  int n_per_rank = n / size;\n  int n_left = n - n_per_rank * size;\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&input[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  std::vector<double> local_input(n_per_rank + 2);\n  if (rank == 0) {\n    local_input[0] = input[0];\n    local_input[n_per_rank + 1] = input[n - 1];\n    std::copy(input.begin(), input.begin() + n_per_rank, local_input.begin() + 1);\n  } else {\n    MPI_Status status;\n    MPI_Recv(&local_input[1], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    local_input[0] = local_input[1] - 1;\n    local_input[n_per_rank + 1] = local_input[n_per_rank] + 1;\n  }\n\n  std::vector<double> local_output(n_per_rank + 2);\n  for (int i = 1; i < n_per_rank + 1; i++) {\n    local_output[i] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n  }\n\n  if (rank == 0) {\n    std::copy(local_output.begin() + 1, local_output.begin() + n_per_rank + 1, output.begin());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&output[i * n_per_rank], n_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Send(&local_output[1], n_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Fill in the code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = input.size();\n    int n_per_proc = n / size;\n    int n_rem = n % size;\n\n    std::vector<double> local_input(n_per_proc + 2);\n    std::vector<double> local_output(n_per_proc + 2);\n\n    MPI_Scatter(input.data(), n_per_proc + 2, MPI_DOUBLE, local_input.data(), n_per_proc + 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute the local output\n    for (int i = 1; i < n_per_proc + 1; i++) {\n        local_output[i] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    // Send the local output to rank 0\n    MPI_Gather(local_output.data(), n_per_proc + 2, MPI_DOUBLE, output.data(), n_per_proc + 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = input.size();\n  int n_per_rank = n/size;\n  int n_remainder = n%size;\n  int n_local = n_per_rank + (rank < n_remainder? 1 : 0);\n  int start = rank*n_per_rank + std::min(rank, n_remainder);\n\n  std::vector<double> local_input(n_local);\n  std::vector<double> local_output(n_local);\n  std::vector<double> local_output_temp(n_local);\n\n  MPI_Scatter(input.data(), n_local, MPI_DOUBLE, local_input.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 1; i < n_local - 1; ++i) {\n    local_output[i] = (local_input[i-1] + local_input[i] + local_input[i+1])/3;\n  }\n\n  // Boundary conditions\n  local_output[0] = (local_input[0] + local_input[1])/2;\n  local_output[n_local-1] = (local_input[n_local-2] + local_input[n_local-1])/2;\n\n  MPI_Gather(local_output.data(), n_local, MPI_DOUBLE, local_output_temp.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n      output[i] = local_output_temp[i];\n    }\n  }\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = input.size();\n    int n_local = n/size;\n    int n_local_start = rank*n_local;\n    int n_local_end = (rank+1)*n_local;\n\n    std::vector<double> local_input(n_local);\n    std::vector<double> local_output(n_local);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            local_input[i] = input[i];\n        }\n    } else {\n        MPI_Recv(&local_input[0], n_local, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == size-1) {\n        for (int i = 0; i < n_local; i++) {\n            local_input[i] = input[i+n_local_start];\n        }\n    } else {\n        MPI_Send(&local_input[0], n_local, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD);\n    }\n\n    #pragma omp parallel for\n    for (int i = 1; i < n_local-1; i++) {\n        local_output[i] = (local_input[i-1]+local_input[i]+local_input[i+1])/3;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            output[i] = local_output[i];\n        }\n    } else {\n        MPI_Send(&local_output[0], n_local, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == size-1) {\n        for (int i = 0; i < n_local; i++) {\n            output[i+n_local_start] = local_output[i];\n        }\n    } else {\n        MPI_Recv(&local_output[0], n_local, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement me!\n  // Hint: You can use Kokkos::parallel_for to parallelize the outer loop.\n  //       You can use Kokkos::parallel_reduce to parallelize the inner loop.\n  //       You can use Kokkos::atomic_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_compare_exchange to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_add to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_sub to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_and to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_or to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_xor to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_inc to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_dec to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_mul to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_div to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_mod to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_and_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_or_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_xor_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_add_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_sub_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_mul_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_div_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_mod_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_inc_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_dec_fetch to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_min to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_max to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_max to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_min to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_min_abs to atomically update the output.\n  //       You can use Kokkos::atomic_fetch_max_abs to atomically update the output.\n  //       You can use Kok",
            "// TODO: Implement me!\n}",
            "// TODO: Fill in the body of this function.\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me!\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n  // You can use the following functions:\n  // - Kokkos::parallel_for\n  // - Kokkos::parallel_reduce\n  // - Kokkos::parallel_scan\n  // - Kokkos::parallel_for_each\n  // - Kokkos::parallel_for_each_tile\n  // - Kokkos::parallel_reduce_tile\n  // - Kokkos::parallel_scan_tile\n  // - Kokkos::parallel_for_each_tile_1d\n  // - Kokkos::parallel_reduce_tile_1d\n  // - Kokkos::parallel_scan_tile_1d\n  // - Kokkos::parallel_for_each_tile_2d\n  // - Kokkos::parallel_reduce_tile_2d\n  // - Kokkos::parallel_scan_tile_2d\n  // - Kokkos::parallel_for_each_tile_3d\n  // - Kokkos::parallel_reduce_tile_3d\n  // - Kokkos::parallel_scan_tile_3d\n  // - Kokkos::parallel_for_each_tile_4d\n  // - Kokkos::parallel_reduce_tile_4d\n  // - Kokkos::parallel_scan_tile_4d\n  // - Kokkos::parallel_for_each_tile_5d\n  // - Kokkos::parallel_reduce_tile_5d\n  // - Kokkos::parallel_scan_tile_5d\n  // - Kokkos::parallel_for_each_tile_6d\n  // - Kokkos::parallel_reduce_tile_6d\n  // - Kokkos::parallel_scan_tile_6d\n  // - Kokkos::parallel_for_each_tile_7d\n  // - Kokkos::parallel_reduce_tile_7d\n  // - Kokkos::parallel_scan_tile_7d\n  // - Kokkos::parallel_for_each_tile_8d\n  // - Kokkos::parallel_reduce_tile_8d\n  // - Kokkos::parallel_scan_tile_8d\n  // - Kokkos::parallel_for_each_tile_9d\n  // - Kokkos::parallel_reduce_tile_9d\n  // - Kokkos::parallel_scan_tile_9d\n  // - Kokkos::parallel_for_each_tile_10d\n  // - Kokkos::parallel_reduce_tile_10d\n  // - Kokkos::parallel_scan_tile_10d\n  // - Kokkos::parallel_for_each_tile_11d\n  // - Kokkos::parallel_reduce_tile_11d\n  // - Kokkos::parallel_scan_tile_11d\n  // - Kokkos::parallel_for_each_tile_12d\n  // - Kokkos::parallel_reduce_tile_12d\n  // - Kokkos::parallel_scan_tile_12d\n  // - Kokkos::parallel_for_each_tile_13d\n  // - Kokkos::parallel_reduce_tile_13d\n  // - Kokkos::parallel_scan_tile_13d\n  // - Kokkos::parallel_for_each_tile_14d\n  // - Kokkos::parallel_reduce_tile_14d\n  // - Kokkos::parallel_scan_tile_14d\n  // - Kokkos::parallel_for_each_tile_15d\n  // - Kokkos::parallel_reduce_tile_15d\n  // - Kokkos::parallel_scan_tile_15d\n  // - Kokkos::parallel_for_each_tile_16d\n  // - Kokkos::parallel_reduce_tile_16d\n  // - Kokkos::parallel",
            "// TODO: Implement me\n}",
            "// TODO: Your code here\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](int i, int j) {\n    int alive_neighbors = 0;\n    if (i > 0 && j > 0) {\n      if (input(i - 1, j - 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (i > 0 && j < N - 1) {\n      if (input(i - 1, j + 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (i < N - 1 && j > 0) {\n      if (input(i + 1, j - 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (i < N - 1 && j < N - 1) {\n      if (input(i + 1, j + 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (i > 0) {\n      if (input(i - 1, j)) {\n        alive_neighbors++;\n      }\n    }\n    if (i < N - 1) {\n      if (input(i + 1, j)) {\n        alive_neighbors++;\n      }\n    }\n    if (j > 0) {\n      if (input(i, j - 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (j < N - 1) {\n      if (input(i, j + 1)) {\n        alive_neighbors++;\n      }\n    }\n    if (input(i, j)) {\n      if (alive_neighbors == 2 || alive_neighbors == 3) {\n        output(i, j) = 1;\n      } else {\n        output(i, j) = 0;\n      }\n    } else {\n      if (alive_neighbors == 3) {\n        output(i, j) = 1;\n      } else {\n        output(i, j) = 0;\n      }\n    }\n  });\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint liveNeighbors = 0;\n\t\t\tif (i > 0) {\n\t\t\t\tif (j > 0) {\n\t\t\t\t\tliveNeighbors += input[(i - 1) * N + j - 1];\n\t\t\t\t}\n\t\t\t\tliveNeighbors += input[(i - 1) * N + j];\n\t\t\t\tif (j < N - 1) {\n\t\t\t\t\tliveNeighbors += input[(i - 1) * N + j + 1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (j > 0) {\n\t\t\t\tliveNeighbors += input[i * N + j - 1];\n\t\t\t}\n\t\t\tif (j < N - 1) {\n\t\t\t\tliveNeighbors += input[i * N + j + 1];\n\t\t\t}\n\t\t\tif (i < N - 1) {\n\t\t\t\tif (j > 0) {\n\t\t\t\t\tliveNeighbors += input[(i + 1) * N + j - 1];\n\t\t\t\t}\n\t\t\t\tliveNeighbors += input[(i + 1) * N + j];\n\t\t\t\tif (j < N - 1) {\n\t\t\t\t\tliveNeighbors += input[(i + 1) * N + j + 1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (liveNeighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (liveNeighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (liveNeighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: implement\n\n}",
            "// TODO: Fill this in\n\n}",
            "// TODO: Implement this function\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count == 2 || count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tint i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, a, b, c, d, e, f, g, h, i2, j2, k2, l2, m2, n2, o2, p2, q2, r2, s2, t2, u2, v2, w2, x2, y2, z2, a2, b2, c2, d2, e2, f2, g2, h2, i3, j3, k3, l3, m3, n3, o3, p3, q3, r3, s3, t3, u3, v3, w3, x3, y3, z3, a3, b3, c3, d3, e3, f3, g3, h3, i4, j4, k4, l4, m4, n4, o4, p4, q4, r4, s4, t4, u4, v4, w4, x4, y4, z4, a4, b4, c4, d4, e4, f4, g4, h4, i5, j5, k5, l5, m5, n5, o5, p5, q5, r5, s5, t5, u5, v5, w5, x5, y5, z5, a5, b5, c5, d5, e5, f5, g5, h5, i6, j6, k6, l6, m6, n6, o6, p6, q6, r6, s6, t6, u6, v6, w6, x6, y6, z6, a6, b6, c6, d6, e6, f6, g6, h6, i7, j7, k7, l7, m7, n7, o7, p7, q7, r7, s7, t7, u7, v7, w7, x7, y7, z7, a7, b7, c7, d7, e7, f7, g7, h7, i8, j8, k8, l8, m8, n8, o8, p8, q8, r8, s8, t8, u8, v8, w8, x8, y8, z8, a8, b8, c8, d8, e8, f8, g8, h8, i9, j9, k9, l9, m9, n9, o9, p9, q9, r9, s9, t9, u9, v9, w9, x9, y9, z9, a9, b9, c9, d9, e9, f9, g9, h9, i10, j10, k10, l10, m10, n10, o10, p10, q10, r10, s10, t10, u10, v10, w10, x10, y10, z10, a10, b10, c10, d10, e10, f10, g10, h10, i11, j11, k11, l11, m11, n11, o11, p11, q11, r11, s11, t11, u11, v11, w11, x11, y11, z11, a11, b11, c11, d11, e11, f11, g11, h11, i12, j12, k12, l12, m12, n12, o12, p12, q12, r12, s12, t12, u12, v12, w12, x12, y12, z12, a12, b12, c12, d12, e12, f12, g12, h12, i13, j13, k13, l13, m13, n13",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint liveNeighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x >= 0 && x < N && y >= 0 && y < N) {\n\t\t\t\t\t\tif (input[x * N + y] == 1) {\n\t\t\t\t\t\t\t++liveNeighbors;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (liveNeighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (liveNeighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (liveNeighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint count = 0;\n\t\t\tfor (size_t k = 0; k < N; ++k) {\n\t\t\t\tfor (size_t l = 0; l < N; ++l) {\n\t\t\t\t\tif (i == k && j == l) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[k * N + l] == 1) {\n\t\t\t\t\t\t++count;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "int *input_ptr = &input[0];\n\tint *output_ptr = &output[0];\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < N; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input_ptr[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input_ptr[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput_ptr[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput_ptr[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput_ptr[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput_ptr[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement me!\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement me\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();\n\t// Hint: You can use the following code to get the thread id\n\t// int thread_id = omp_get_thread_num();\n\n\t// Hint: You can use the following code to get the number of threads\n\t// int num_threads = omp_get_num_threads();",
            "// TODO: Implement me\n\tint i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z;\n\tint count;\n\tint alive;\n\tint alive_neighbors;\n\tint alive_neighbors_count;\n\tint alive_neighbors_count_2;\n\tint alive_neighbors_count_3;\n\tint alive_neighbors_count_4;\n\tint alive_neighbors_count_5;\n\tint alive_neighbors_count_6;\n\tint alive_neighbors_count_7;\n\tint alive_neighbors_count_8;\n\tint alive_neighbors_count_9;\n\tint alive_neighbors_count_10;\n\tint alive_neighbors_count_11;\n\tint alive_neighbors_count_12;\n\tint alive_neighbors_count_13;\n\tint alive_neighbors_count_14;\n\tint alive_neighbors_count_15;\n\tint alive_neighbors_count_16;\n\tint alive_neighbors_count_17;\n\tint alive_neighbors_count_18;\n\tint alive_neighbors_count_19;\n\tint alive_neighbors_count_20;\n\tint alive_neighbors_count_21;\n\tint alive_neighbors_count_22;\n\tint alive_neighbors_count_23;\n\tint alive_neighbors_count_24;\n\tint alive_neighbors_count_25;\n\tint alive_neighbors_count_26;\n\tint alive_neighbors_count_27;\n\tint alive_neighbors_count_28;\n\tint alive_neighbors_count_29;\n\tint alive_neighbors_count_30;\n\tint alive_neighbors_count_31;\n\tint alive_neighbors_count_32;\n\tint alive_neighbors_count_33;\n\tint alive_neighbors_count_34;\n\tint alive_neighbors_count_35;\n\tint alive_neighbors_count_36;\n\tint alive_neighbors_count_37;\n\tint alive_neighbors_count_38;\n\tint alive_neighbors_count_39;\n\tint alive_neighbors_count_40;\n\tint alive_neighbors_count_41;\n\tint alive_neighbors_count_42;\n\tint alive_neighbors_count_43;\n\tint alive_neighbors_count_44;\n\tint alive_neighbors_count_45;\n\tint alive_neighbors_count_46;\n\tint alive_neighbors_count_47;\n\tint alive_neighbors_count_48;\n\tint alive_neighbors_count_49;\n\tint alive_neighbors_count_50;\n\tint alive_neighbors_count_51;\n\tint alive_neighbors_count_52;\n\tint alive_neighbors_count_53;\n\tint alive_neighbors_count_54;\n\tint alive_neighbors_count_55;\n\tint alive_neighbors_count_56;\n\tint alive_neighbors_count_57;\n\tint alive_neighbors_count_58;\n\tint alive_neighbors_count_59;\n\tint alive_neighbors_count_60;\n\tint alive_neighbors_count_61;\n\tint alive_neighbors_count_62;\n\tint alive_neighbors_count_63;\n\tint alive_neighbors_count_64;\n\tint alive_neigh",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int x = -1; x <= 1; x++) {\n\t\t\t\tfor (int y = -1; y <= 1; y++) {\n\t\t\t\t\tif (x == 0 && y == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tint x_ = i + x;\n\t\t\t\t\tint y_ = j + y;\n\t\t\t\t\tif (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[x_ * N + y_] == 1) {\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint alive = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\talive++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (alive < 2 || alive > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (alive == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint liveNeighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (k == 0 && l == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x >= 0 && x < N && y >= 0 && y < N) {\n\t\t\t\t\t\tif (input[x * N + y] == 1) {\n\t\t\t\t\t\t\tliveNeighbors++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (liveNeighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (liveNeighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (liveNeighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint liveNeighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tliveNeighbors++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (liveNeighbors < 2 || liveNeighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (liveNeighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement me\n\tint i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, a, b, c, d, e, f, g, h, i2, j2, k2, l2, m2, n2, o2, p2, q2, r2, s2, t2, u2, v2, w2, x2, y2, z2, a2, b2, c2, d2, e2, f2, g2, h2, i3, j3, k3, l3, m3, n3, o3, p3, q3, r3, s3, t3, u3, v3, w3, x3, y3, z3, a3, b3, c3, d3, e3, f3, g3, h3, i4, j4, k4, l4, m4, n4, o4, p4, q4, r4, s4, t4, u4, v4, w4, x4, y4, z4, a4, b4, c4, d4, e4, f4, g4, h4, i5, j5, k5, l5, m5, n5, o5, p5, q5, r5, s5, t5, u5, v5, w5, x5, y5, z5, a5, b5, c5, d5, e5, f5, g5, h5, i6, j6, k6, l6, m6, n6, o6, p6, q6, r6, s6, t6, u6, v6, w6, x6, y6, z6, a6, b6, c6, d6, e6, f6, g6, h6, i7, j7, k7, l7, m7, n7, o7, p7, q7, r7, s7, t7, u7, v7, w7, x7, y7, z7, a7, b7, c7, d7, e7, f7, g7, h7, i8, j8, k8, l8, m8, n8, o8, p8, q8, r8, s8, t8, u8, v8, w8, x8, y8, z8, a8, b8, c8, d8, e8, f8, g8, h8, i9, j9, k9, l9, m9, n9, o9, p9, q9, r9, s9, t9, u9, v9, w9, x9, y9, z9, a9, b9, c9, d9, e9, f9, g9, h9, i10, j10, k10, l10, m10, n10, o10, p10, q10, r10, s10, t10, u10, v10, w10, x10, y10, z10, a10, b10, c10, d10, e10, f10, g10, h10, i11, j11, k11, l11, m11, n11, o11, p11, q11, r11, s11, t11, u11, v11, w11, x11, y11, z11, a11, b11, c11, d11, e11, f11, g11, h11, i12, j12, k12, l12, m12, n12, o12, p12, q12, r12, s12, t12, u12, v12, w12, x12, y12, z12, a12, b12, c12, d12, e12, f12, g12, h12, i13, j13, k13, l13, m13, n13, o",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: implement\n\t// Hint: use OpenMP to parallelize the computation\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j+1]\n\t//                           + input[i+1][j-1] + input[i+1][j]\n\t//                           + input[i+1][j+1]\n\n\t// Hint: use the following formula to compute the number of neighbors\n\t//       of a cell at (i, j)\n\t//       n = input[i-1][j-1] + input[i-1][j] + input[i-1][j+1]\n\t//                           + input[i][j-1] + input[i][j",
            "// TODO: Implement this function\n\t// Hint: Use OpenMP to parallelize the computation\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a 2D array to represent the grid\n\t// Hint: Use a"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "serial",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\tstd::vector<int> temp(N*N);\n\tfor(size_t i = 0; i < N; i++){\n\t\tfor(size_t j = 0; j < N; j++){\n\t\t\tint count = 0;\n\t\t\tif(i > 0){\n\t\t\t\tif(j > 0){\n\t\t\t\t\tcount += input[(i-1)*N + j-1];\n\t\t\t\t}\n\t\t\t\tcount += input[(i-1)*N + j];\n\t\t\t\tif(j < N-1){\n\t\t\t\t\tcount += input[(i-1)*N + j+1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(j > 0){\n\t\t\t\tcount += input[i*N + j-1];\n\t\t\t}\n\t\t\tif(j < N-1){\n\t\t\t\tcount += input[i*N + j+1];\n\t\t\t}\n\t\t\tif(i < N-1){\n\t\t\t\tif(j > 0){\n\t\t\t\t\tcount += input[(i+1)*N + j-1];\n\t\t\t\t}\n\t\t\t\tcount += input[(i+1)*N + j];\n\t\t\t\tif(j < N-1){\n\t\t\t\t\tcount += input[(i+1)*N + j+1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(input[i*N + j] == 1){\n\t\t\t\tif(count < 2 || count > 3){\n\t\t\t\t\ttemp[i*N + j] = 0;\n\t\t\t\t}\n\t\t\t\telse{\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse{\n\t\t\t\tif(count == 3){\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t}\n\t\t\t\telse{\n\t\t\t\t\ttemp[i*N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}",
            "// TODO: implement\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint neighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\t++neighbors;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (neighbors < 2 || neighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif (neighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\t// Hint: You can use the helper function `countNeighbors`\n\t// Hint: You can use the helper function `isAlive`\n\t// Hint: You can use the helper function `isDead`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can use the helper function `isDeadNext`\n\t// Hint: You can use the helper function `isAliveNext`\n\t// Hint: You can",
            "// TODO: Implement\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint neighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tneighbors++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (neighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (neighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (neighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above\n\t// Hint: Use the helper functions above",
            "// TODO: Implement this function\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int x = -1; x <= 1; x++) {\n\t\t\t\tfor (int y = -1; y <= 1; y++) {\n\t\t\t\t\tif (x == 0 && y == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tint x_ = x + i;\n\t\t\t\t\tint y_ = y + j;\n\t\t\t\t\tif (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[x_ * N + y_] == 1) {\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tstd::vector<int> temp(N*N);\n\tfor(size_t i = 0; i < N; i++){\n\t\tfor(size_t j = 0; j < N; j++){\n\t\t\tint count = 0;\n\t\t\tfor(int k = -1; k < 2; k++){\n\t\t\t\tfor(int l = -1; l < 2; l++){\n\t\t\t\t\tif(i+k >= 0 && i+k < N && j+l >= 0 && j+l < N){\n\t\t\t\t\t\tif(input[(i+k)*N + j+l] == 1) count++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(input[i*N + j] == 1){\n\t\t\t\tif(count < 2 || count > 3) temp[i*N + j] = 0;\n\t\t\t\telse temp[i*N + j] = 1;\n\t\t\t}\n\t\t\telse if(count == 3) temp[i*N + j] = 1;\n\t\t\telse temp[i*N + j] = 0;\n\t\t}\n\t}\n\toutput = temp;\n}",
            "for (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint neighbors = 0;\n\t\t\tfor (int x = -1; x <= 1; ++x) {\n\t\t\t\tfor (int y = -1; y <= 1; ++y) {\n\t\t\t\t\tif (x == 0 && y == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tint x_ = i + x;\n\t\t\t\t\tint y_ = j + y;\n\t\t\t\t\tif (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tneighbors += input[x_ * N + y_];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (neighbors < 2 || neighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (neighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int alive_neighbors = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          if (k == 0 && l == 0) {\n            continue;\n          }\n          int x = i + k;\n          int y = j + l;\n          if (x < 0 || x >= N || y < 0 || y >= N) {\n            continue;\n          }\n          if (input[x * N + y] == 1) {\n            ++alive_neighbors;\n          }\n        }\n      }\n      if (input[i * N + j] == 1) {\n        if (alive_neighbors < 2) {\n          output[i * N + j] = 0;\n        } else if (alive_neighbors > 3) {\n          output[i * N + j] = 0;\n        } else {\n          output[i * N + j] = 1;\n        }\n      } else {\n        if (alive_neighbors == 3) {\n          output[i * N + j] = 1;\n        } else {\n          output[i * N + j] = 0;\n        }\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint live_neighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n\t\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t\tlive_neighbors++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (live_neighbors < 2 || live_neighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (live_neighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tstd::vector<int> temp(N*N, 0);\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x < 0 || x >= N || y < 0 || y >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[x*N + y] == 1) {\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i*N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\ttemp[i*N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}",
            "// TODO: Implement this function\n\tstd::vector<int> temp(N*N);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (k == 0 && l == 0) continue;\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x < 0 || x >= N || y < 0 || y >= N) continue;\n\t\t\t\t\tif (input[x*N + y] == 1) count++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i*N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) temp[i*N + j] = 0;\n\t\t\t\telse temp[i*N + j] = 1;\n\t\t\t} else {\n\t\t\t\tif (count == 3) temp[i*N + j] = 1;\n\t\t\t\telse temp[i*N + j] = 0;\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}",
            "// TODO: Implement this function\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint liveNeighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tif (i + k < 0 || i + k >= N || j + l < 0 || j + l >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (k == 0 && l == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\tliveNeighbors++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (liveNeighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (liveNeighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (liveNeighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n            if (input[(i + k) * N + j + l] == 1) {\n              ++liveNeighbors;\n            }\n          }\n        }\n      }\n      if (input[i * N + j] == 1) {\n        if (liveNeighbors < 2) {\n          output[i * N + j] = 0;\n        } else if (liveNeighbors > 3) {\n          output[i * N + j] = 0;\n        } else {\n          output[i * N + j] = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output[i * N + j] = 1;\n        } else {\n          output[i * N + j] = 0;\n        }\n      }\n    }\n  }\n}",
            "// TODO: implement\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      if (i > 0) {\n        if (j > 0) {\n          neighbors += input[(i - 1) * N + j - 1];\n        }\n        neighbors += input[(i - 1) * N + j];\n        if (j < N - 1) {\n          neighbors += input[(i - 1) * N + j + 1];\n        }\n      }\n      if (j > 0) {\n        neighbors += input[i * N + j - 1];\n      }\n      if (j < N - 1) {\n        neighbors += input[i * N + j + 1];\n      }\n      if (i < N - 1) {\n        if (j > 0) {\n          neighbors += input[(i + 1) * N + j - 1];\n        }\n        neighbors += input[(i + 1) * N + j];\n        if (j < N - 1) {\n          neighbors += input[(i + 1) * N + j + 1];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        if (neighbors < 2) {\n          output[i * N + j] = 0;\n        } else if (neighbors == 2 || neighbors == 3) {\n          output[i * N + j] = 1;\n        } else {\n          output[i * N + j] = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output[i * N + j] = 1;\n        } else {\n          output[i * N + j] = 0;\n        }\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint live_neighbors = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (k == 0 && l == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x < 0 || x >= N || y < 0 || y >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[x * N + y] == 1) {\n\t\t\t\t\t\t++live_neighbors;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (live_neighbors < 2) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else if (live_neighbors > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (live_neighbors == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "std::vector<int> temp(N*N);\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tint x = i + k;\n\t\t\t\t\tint y = j + l;\n\t\t\t\t\tif (x >= 0 && x < N && y >= 0 && y < N) {\n\t\t\t\t\t\tif (input[x*N + y] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i*N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\ttemp[i*N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\ttemp[i*N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\ttemp[i*N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}",
            "// TODO: Implement this function\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (i + k < 0 || i + k >= N || j + l < 0 || j + l >= N) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (input[(i + k) * N + j + l] == 1) {\n\t\t\t\t\t\t++count;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tstd::vector<int> temp(N*N, 0);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; ++k) {\n\t\t\t\tfor (int l = -1; l <= 1; ++l) {\n\t\t\t\t\tif (i + k < 0 || i + k >= N || j + l < 0 || j + l >= N) continue;\n\t\t\t\t\tif (i + k == i && j + l == j) continue;\n\t\t\t\t\tif (input[i * N + j + k * N + l]) ++count;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j]) {\n\t\t\t\tif (count == 2 || count == 3) temp[i * N + j] = 1;\n\t\t\t} else {\n\t\t\t\tif (count == 3) temp[i * N + j] = 1;\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "hip",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement me\n}",
            "// TODO: implement me\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int idx = x + y * N;\n  int count = 0;\n  if (x < N && y < N) {\n    for (int i = -1; i <= 1; i++) {\n      for (int j = -1; j <= 1; j++) {\n        int x1 = x + i;\n        int y1 = y + j;\n        if (x1 >= 0 && x1 < N && y1 >= 0 && y1 < N) {\n          int idx1 = x1 + y1 * N;\n          count += input[idx1];\n        }\n      }\n    }\n    if (input[idx] == 0) {\n      if (count == 3) {\n        output[idx] = 1;\n      } else {\n        output[idx] = 0;\n      }\n    } else {\n      if (count == 2 || count == 3) {\n        output[idx] = 1;\n      } else {\n        output[idx] = 0;\n      }\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int k = blockIdx.z * blockDim.z + threadIdx.z;\n  int index = i + j * N + k * N * N;\n  int neighbors = 0;\n\n  if (i > 0 && j > 0 && k > 0 && i < N - 1 && j < N - 1 && k < N - 1) {\n    if (input[index - N - 1] == 1) {\n      neighbors++;\n    }\n    if (input[index - N] == 1) {\n      neighbors++;\n    }\n    if (input[index - N + 1] == 1) {\n      neighbors++;\n    }\n    if (input[index - 1] == 1) {\n      neighbors++;\n    }\n    if (input[index + 1] == 1) {\n      neighbors++;\n    }\n    if (input[index + N - 1] == 1) {\n      neighbors++;\n    }\n    if (input[index + N] == 1) {\n      neighbors++;\n    }\n    if (input[index + N + 1] == 1) {\n      neighbors++;\n    }\n    if (input[index] == 1) {\n      if (neighbors == 2 || neighbors == 3) {\n        output[index] = 1;\n      } else {\n        output[index] = 0;\n      }\n    } else {\n      if (neighbors == 3) {\n        output[index] = 1;\n      } else {\n        output[index] = 0;\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t index = i + j * N;\n  int live_neighbors = 0;\n  if (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n    if (input[index - N - 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index - N] == 1) {\n      live_neighbors++;\n    }\n    if (input[index - N + 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index - 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index + 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index + N - 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index + N] == 1) {\n      live_neighbors++;\n    }\n    if (input[index + N + 1] == 1) {\n      live_neighbors++;\n    }\n    if (input[index] == 1) {\n      if (live_neighbors == 2 || live_neighbors == 3) {\n        output[index] = 1;\n      } else {\n        output[index] = 0;\n      }\n    } else {\n      if (live_neighbors == 3) {\n        output[index] = 1;\n      } else {\n        output[index] = 0;\n      }\n    }\n  }\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = i + j * N;\n  int neighbors = 0;\n  int i_min = (i == 0)? 0 : i - 1;\n  int i_max = (i == N - 1)? N - 1 : i + 1;\n  int j_min = (j == 0)? 0 : j - 1;\n  int j_max = (j == N - 1)? N - 1 : j + 1;\n\n  for (int x = i_min; x <= i_max; x++) {\n    for (int y = j_min; y <= j_max; y++) {\n      if (x == i && y == j)\n        continue;\n      neighbors += input[x + y * N];\n    }\n  }\n\n  if (input[index] == 1) {\n    if (neighbors < 2)\n      output[index] = 0;\n    else if (neighbors > 3)\n      output[index] = 0;\n    else\n      output[index] = 1;\n  } else {\n    if (neighbors == 3)\n      output[index] = 1;\n    else\n      output[index] = 0;\n  }\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int idx = i * N + j;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        if (x == 0 && y == 0)\n          continue;\n        int x1 = i + x;\n        int y1 = j + y;\n        if (x1 >= 0 && x1 < N && y1 >= 0 && y1 < N) {\n          count += input[x1 * N + y1];\n        }\n      }\n    }\n    if (input[idx] == 1) {\n      if (count < 2 || count > 3)\n        output[idx] = 0;\n      else\n        output[idx] = 1;\n    } else {\n      if (count == 3)\n        output[idx] = 1;\n      else\n        output[idx] = 0;\n    }\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  if (x >= N || y >= N) return;\n\n  int count = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      if (i == 0 && j == 0) continue;\n      int x2 = x + i;\n      int y2 = y + j;\n      if (x2 < 0 || x2 >= N || y2 < 0 || y2 >= N) continue;\n      count += input[y2 * N + x2];\n    }\n  }\n\n  if (input[y * N + x] == 1) {\n    if (count < 2 || count > 3) {\n      output[y * N + x] = 0;\n    } else {\n      output[y * N + x] = 1;\n    }\n  } else {\n    if (count == 3) {\n      output[y * N + x] = 1;\n    } else {\n      output[y * N + x] = 0;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int count = 0;\n  for (int di = -1; di <= 1; di++) {\n    for (int dj = -1; dj <= 1; dj++) {\n      if (di == 0 && dj == 0) continue;\n      size_t ii = i + di;\n      size_t jj = j + dj;\n      if (ii >= N || jj >= N) continue;\n      count += input[ii + jj * N];\n    }\n  }\n\n  if (input[i + j * N] == 1) {\n    if (count < 2 || count > 3) {\n      output[i + j * N] = 0;\n    } else {\n      output[i + j * N] = 1;\n    }\n  } else {\n    if (count == 3) {\n      output[i + j * N] = 1;\n    } else {\n      output[i + j * N] = 0;\n    }\n  }\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int n = 0;\n  if (i > 0) {\n    if (j > 0) n += input[(i - 1) * N + j - 1];\n    n += input[(i - 1) * N + j];\n    if (j < N - 1) n += input[(i - 1) * N + j + 1];\n  }\n  if (j > 0) n += input[i * N + j - 1];\n  n += input[i * N + j];\n  if (j < N - 1) n += input[i * N + j + 1];\n  if (i < N - 1) {\n    if (j > 0) n += input[(i + 1) * N + j - 1];\n    n += input[(i + 1) * N + j];\n    if (j < N - 1) n += input[(i + 1) * N + j + 1];\n  }\n\n  if (n == 3) {\n    output[i * N + j] = 1;\n  } else if (n == 2) {\n    output[i * N + j] = input[i * N + j];\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = x + y * N;\n  int count = 0;\n  if (x >= N || y >= N) return;\n  if (x > 0 && y > 0 && x < N - 1 && y < N - 1) {\n    if (input[index - N - 1] == 1) count++;\n    if (input[index - N] == 1) count++;\n    if (input[index - N + 1] == 1) count++;\n    if (input[index - 1] == 1) count++;\n    if (input[index + 1] == 1) count++;\n    if (input[index + N - 1] == 1) count++;\n    if (input[index + N] == 1) count++;\n    if (input[index + N + 1] == 1) count++;\n  } else {\n    if (x > 0 && y > 0 && x < N - 1 && y < N) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - N + 1] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n      if (input[index + N - 1] == 1) count++;\n      if (input[index + N] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N && y < N - 1) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n      if (input[index + N - 1] == 1) count++;\n      if (input[index + N] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N && y < N) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n      if (input[index + N - 1] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N - 1 && y < N) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - N + 1] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n      if (input[index + N - 1] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N && y < N) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N - 1 && y < N) {\n      if (input[index - N - 1] == 1) count++;\n      if (input[index - N] == 1) count++;\n      if (input[index - N + 1] == 1) count++;\n      if (input[index - 1] == 1) count++;\n      if (input[index + 1] == 1) count++;\n    } else if (x > 0 && y > 0 && x < N && y < N - 1) {\n      if (",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N)\n    return;\n\n  int neighbors = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int nx = x + i;\n      int ny = y + j;\n      if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n        neighbors += input[ny * N + nx];\n      }\n    }\n  }\n\n  int cell = input[y * N + x];\n  if (cell == 1) {\n    if (neighbors < 2 || neighbors > 3) {\n      output[y * N + x] = 0;\n    } else {\n      output[y * N + x] = 1;\n    }\n  } else {\n    if (neighbors == 3) {\n      output[y * N + x] = 1;\n    } else {\n      output[y * N + x] = 0;\n    }\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) {\n    return;\n  }\n\n  int index = x + y * N;\n  int neighbors = 0;\n\n  if (x > 0) {\n    if (y > 0) {\n      neighbors += input[(x - 1) + (y - 1) * N];\n    }\n    neighbors += input[(x - 1) + y * N];\n    if (y < N - 1) {\n      neighbors += input[(x - 1) + (y + 1) * N];\n    }\n  }\n\n  if (y > 0) {\n    neighbors += input[x + (y - 1) * N];\n  }\n  if (y < N - 1) {\n    neighbors += input[x + (y + 1) * N];\n  }\n\n  if (x < N - 1) {\n    if (y > 0) {\n      neighbors += input[(x + 1) + (y - 1) * N];\n    }\n    neighbors += input[(x + 1) + y * N];\n    if (y < N - 1) {\n      neighbors += input[(x + 1) + (y + 1) * N];\n    }\n  }\n\n  if (neighbors == 3) {\n    output[index] = 1;\n  } else if (neighbors == 2) {\n    output[index] = input[index];\n  } else {\n    output[index] = 0;\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) return;\n\n  int n = 0;\n  if (x > 0) {\n    if (y > 0) n += input[(y - 1) * N + x - 1];\n    n += input[(y) * N + x - 1];\n    if (y < N - 1) n += input[(y + 1) * N + x - 1];\n  }\n  if (y > 0) n += input[(y - 1) * N + x];\n  if (y < N - 1) n += input[(y + 1) * N + x];\n  if (x < N - 1) {\n    if (y > 0) n += input[(y - 1) * N + x + 1];\n    n += input[(y) * N + x + 1];\n    if (y < N - 1) n += input[(y + 1) * N + x + 1];\n  }\n\n  if (input[y * N + x] == 1) {\n    if (n < 2 || n > 3)\n      output[y * N + x] = 0;\n    else\n      output[y * N + x] = 1;\n  } else {\n    if (n == 3)\n      output[y * N + x] = 1;\n    else\n      output[y * N + x] = 0;\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) {\n    return;\n  }\n\n  int idx = x + y * N;\n\n  int neighbors = 0;\n\n  // Check the 8 neighbors of the cell\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      if (i == 0 && j == 0) {\n        continue;\n      }\n\n      int x_ = x + i;\n      int y_ = y + j;\n\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        continue;\n      }\n\n      int idx_ = x_ + y_ * N;\n\n      neighbors += input[idx_];\n    }\n  }\n\n  if (input[idx] == 1) {\n    if (neighbors < 2) {\n      output[idx] = 0;\n    } else if (neighbors > 3) {\n      output[idx] = 0;\n    } else {\n      output[idx] = 1;\n    }\n  } else {\n    if (neighbors == 3) {\n      output[idx] = 1;\n    } else {\n      output[idx] = 0;\n    }\n  }\n}",
            "// TODO: implement me\n}",
            "// TODO: Implement\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int idx = x + y * N;\n  int count = 0;\n  if (x < N && y < N) {\n    for (int i = -1; i <= 1; i++) {\n      for (int j = -1; j <= 1; j++) {\n        if (i == 0 && j == 0)\n          continue;\n        int x_ = x + i;\n        int y_ = y + j;\n        if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N)\n          continue;\n        count += input[x_ + y_ * N];\n      }\n    }\n    if (input[idx] == 1) {\n      if (count == 2 || count == 3)\n        output[idx] = 1;\n      else\n        output[idx] = 0;\n    } else {\n      if (count == 3)\n        output[idx] = 1;\n      else\n        output[idx] = 0;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_rem = N % size;\n\n\tint N_per_rank_plus_one = N_per_rank + 1;\n\tint N_per_rank_minus_one = N_per_rank - 1;\n\n\tint N_per_rank_plus_two = N_per_rank + 2;\n\tint N_per_rank_minus_two = N_per_rank - 2;\n\n\tint N_per_rank_plus_three = N_per_rank + 3;\n\tint N_per_rank_minus_three = N_per_rank - 3;\n\n\tint N_per_rank_plus_four = N_per_rank + 4;\n\tint N_per_rank_minus_four = N_per_rank - 4;\n\n\tint N_per_rank_plus_five = N_per_rank + 5;\n\tint N_per_rank_minus_five = N_per_rank - 5;\n\n\tint N_per_rank_plus_six = N_per_rank + 6;\n\tint N_per_rank_minus_six = N_per_rank - 6;\n\n\tint N_per_rank_plus_seven = N_per_rank + 7;\n\tint N_per_rank_minus_seven = N_per_rank - 7;\n\n\tint N_per_rank_plus_eight = N_per_rank + 8;\n\tint N_per_rank_minus_eight = N_per_rank - 8;\n\n\tint N_per_rank_plus_nine = N_per_rank + 9;\n\tint N_per_rank_minus_nine = N_per_rank - 9;\n\n\tint N_per_rank_plus_ten = N_per_rank + 10;\n\tint N_per_rank_minus_ten = N_per_rank - 10;\n\n\tint N_per_rank_plus_eleven = N_per_rank + 11;\n\tint N_per_rank_minus_eleven = N_per_rank - 11;\n\n\tint N_per_rank_plus_twelve = N_per_rank + 12;\n\tint N_per_rank_minus_twelve = N_per_rank - 12;\n\n\tint N_per_rank_plus_thirteen = N_per_rank + 13;\n\tint N_per_rank_minus_thirteen = N_per_rank - 13;\n\n\tint N_per_rank_plus_fourteen = N_per_rank + 14;\n\tint N_per_rank_minus_fourteen = N_per_rank - 14;\n\n\tint N_per_rank_plus_fifteen = N_per_rank + 15;\n\tint N_per_rank_minus_fifteen = N_per_rank - 15;\n\n\tint N_per_rank_plus_sixteen = N_per_rank + 16;\n\tint N_per_rank_minus_sixteen = N_per_rank - 16;\n\n\tint N_per_rank_plus_seventeen = N_per_rank + 17;\n\tint N_per_rank_minus_seventeen = N_per_rank - 17;\n\n\tint N_per_rank_plus_eighteen = N_per_rank + 18;\n\tint N_per_rank_minus_eighteen = N_per_rank - 18;\n\n\tint N_per_rank_plus_nineteen = N_per_rank + 19;\n\tint N_per_rank_minus_nineteen = N_per_rank - 19;\n\n\tint N_per_rank_plus_twenty = N_per_rank + 20;\n\tint N_per_rank",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = N/size;\n\tint m = N%size;\n\n\tint *input_arr = new int[N*N];\n\tint *output_arr = new int[N*N];\n\n\tfor (int i = 0; i < N*N; i++) {\n\t\tinput_arr[i] = input[i];\n\t\toutput_arr[i] = 0;\n\t}\n\n\tint *input_arr_rank = new int[n*n];\n\tint *output_arr_rank = new int[n*n];\n\n\tfor (int i = 0; i < n*n; i++) {\n\t\tinput_arr_rank[i] = input_arr[rank*n*n + i];\n\t\toutput_arr_rank[i] = 0;\n\t}\n\n\tint *input_arr_rank_m = new int[m*m];\n\tint *output_arr_rank_m = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_1 = new int[m*m];\n\tint *output_arr_rank_m_1 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_1[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m_1[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_2 = new int[m*m];\n\tint *output_arr_rank_m_2 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_2[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m_2[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_3 = new int[m*m];\n\tint *output_arr_rank_m_3 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_3[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m_3[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_4 = new int[m*m];\n\tint *output_arr_rank_m_4 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_4[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m_4[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_5 = new int[m*m];\n\tint *output_arr_rank_m_5 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_5[i] = input_arr[rank*n*n + n*n + i];\n\t\toutput_arr_rank_m_5[i] = 0;\n\t}\n\n\tint *input_arr_rank_m_6 = new int[m*m];\n\tint *output_arr_rank_m_6 = new int[m*m];\n\n\tfor (int i = 0; i < m*m; i++) {\n\t\tinput_arr_rank_m_6",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = N / size;\n\tint m = N % size;\n\n\tint *send_buf = new int[n * n];\n\tint *recv_buf = new int[n * n];\n\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = 0; j < n; j++) {\n\t\t\tsend_buf[i * n + j] = input[i * N + j];\n\t\t}\n\t}\n\n\tMPI_Scatter(send_buf, n * n, MPI_INT, recv_buf, n * n, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < n; i++) {\n\t\t\tfor (int j = 0; j < n; j++) {\n\t\t\t\toutput[i * N + j] = recv_buf[i * n + j];\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < n; i++) {\n\t\t\tfor (int j = 0; j < n; j++) {\n\t\t\t\toutput[i * N + j] = recv_buf[i * n + j];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = 0; j < n; j++) {\n\t\t\tint count = 0;\n\t\t\tif (i > 0 && j > 0) {\n\t\t\t\tif (output[i * N + j - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i - 1) * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i - 1) * N + j - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i > 0 && j < n - 1) {\n\t\t\t\tif (output[i * N + j + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i - 1) * N + j + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i - 1) * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i < n - 1 && j > 0) {\n\t\t\t\tif (output[i * N + j - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i + 1) * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i + 1) * N + j - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i < n - 1 && j < n - 1) {\n\t\t\t\tif (output[i * N + j + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i + 1) * N + j + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (output[(i + 1) * N + j] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i > 0) {\n\t\t\t\tif (output[(i - 1) * N + j] ==",
            "// TODO: Implement this function\n  // Hint: You can use std::vector<int>::at() to access the elements of the vector\n  // Hint: You can use std::vector<int>::size() to get the size of the vector\n  // Hint: You can use std::vector<int>::operator[] to access the elements of the vector\n  // Hint: You can use std::vector<int>::begin() and std::vector<int>::end() to iterate over the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::resize() to resize the vector\n  // Hint: You can use std::vector<int>::push_back() to add an element to the vector\n  // Hint: You can use std::vector<int>::insert() to add an element to the vector at a specific position\n  // Hint: You can use std::vector<int>::erase() to remove an element from the vector\n  // Hint: You can use std::vector<int>::clear() to remove all elements from the vector\n  // Hint: You can use std::vector<int>::empty() to check if the vector is empty\n  // Hint: You can use std::vector<int>::swap() to swap the contents of two vectors\n  // Hint: You can use std::vector<int>::reserve() to reserve space for the vector\n  // Hint: You can use std::vector<int>::capacity() to get the capacity of the vector\n  // Hint: You can use std::vector<int>::shrink_to_fit() to shrink the capacity of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use std::vector<int>::assign() to assign the elements of the vector\n  // Hint: You can use",
            "// TODO: Implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int N_per_rank = N/size;\n  int N_remainder = N%size;\n  int N_per_rank_plus_one = N_per_rank + 1;\n  int N_per_rank_plus_two = N_per_rank + 2;\n  int N_per_rank_minus_one = N_per_rank - 1;\n  int N_per_rank_minus_two = N_per_rank - 2;\n  int N_per_rank_plus_one_remainder = N_per_rank_plus_one + N_remainder;\n  int N_per_rank_plus_two_remainder = N_per_rank_plus_two + N_remainder;\n  int N_per_rank_minus_one_remainder = N_per_rank_minus_one + N_remainder;\n  int N_per_rank_minus_two_remainder = N_per_rank_minus_two + N_remainder;\n  int N_per_rank_plus_one_remainder_minus_one = N_per_rank_plus_one_remainder - 1;\n  int N_per_rank_plus_two_remainder_minus_one = N_per_rank_plus_two_remainder - 1;\n  int N_per_rank_minus_one_remainder_minus_one = N_per_rank_minus_one_remainder - 1;\n  int N_per_rank_minus_two_remainder_minus_one = N_per_rank_minus_two_remainder - 1;\n  int N_per_rank_plus_one_remainder_minus_two = N_per_rank_plus_one_remainder - 2;\n  int N_per_rank_plus_two_remainder_minus_two = N_per_rank_plus_two_remainder - 2;\n  int N_per_rank_minus_one_remainder_minus_two = N_per_rank_minus_one_remainder - 2;\n  int N_per_rank_minus_two_remainder_minus_two = N_per_rank_minus_two_remainder - 2;\n  int N_per_rank_plus_one_remainder_minus_three = N_per_rank_plus_one_remainder - 3;\n  int N_per_rank_plus_two_remainder_minus_three = N_per_rank_plus_two_remainder - 3;\n  int N_per_rank_minus_one_remainder_minus_three = N_per_rank_minus_one_remainder - 3;\n  int N_per_rank_minus_two_remainder_minus_three = N_per_rank_minus_two_remainder - 3;\n  int N_per_rank_plus_one_remainder_minus_four = N_per_rank_plus_one_remainder - 4;\n  int N_per_rank_plus_two_remainder_minus_four = N_per_rank_plus_two_remainder - 4;\n  int N_per_rank_minus_one_remainder_minus_four = N_per_rank_minus_one_remainder - 4;\n  int N_per_rank_minus_two_remainder_minus_four = N_per_rank_minus_two_remainder - 4;\n  int N_per_rank_plus_one_remainder_minus_five = N_per_rank_plus_one_remainder - 5;\n  int N_per_rank_plus_two_remainder_minus_five = N_per_rank_plus_two_remainder - 5;\n  int N_per_rank_minus_one_remainder_minus_five = N_per_rank_minus_one_remainder -",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint* input_arr = new int[N*N];\n\tint* output_arr = new int[N*N];\n\tint* input_arr_temp = new int[N*N];\n\tint* output_arr_temp = new int[N*N];\n\tint* input_arr_temp2 = new int[N*N];\n\tint* output_arr_temp2 = new int[N*N];\n\tint* input_arr_temp3 = new int[N*N];\n\tint* output_arr_temp3 = new int[N*N];\n\tint* input_arr_temp4 = new int[N*N];\n\tint* output_arr_temp4 = new int[N*N];\n\tint* input_arr_temp5 = new int[N*N];\n\tint* output_arr_temp5 = new int[N*N];\n\tint* input_arr_temp6 = new int[N*N];\n\tint* output_arr_temp6 = new int[N*N];\n\tint* input_arr_temp7 = new int[N*N];\n\tint* output_arr_temp7 = new int[N*N];\n\tint* input_arr_temp8 = new int[N*N];\n\tint* output_arr_temp8 = new int[N*N];\n\tint* input_arr_temp9 = new int[N*N];\n\tint* output_arr_temp9 = new int[N*N];\n\tint* input_arr_temp10 = new int[N*N];\n\tint* output_arr_temp10 = new int[N*N];\n\tint* input_arr_temp11 = new int[N*N];\n\tint* output_arr_temp11 = new int[N*N];\n\tint* input_arr_temp12 = new int[N*N];\n\tint* output_arr_temp12 = new int[N*N];\n\tint* input_arr_temp13 = new int[N*N];\n\tint* output_arr_temp13 = new int[N*N];\n\tint* input_arr_temp14 = new int[N*N];\n\tint* output_arr_temp14 = new int[N*N];\n\tint* input_arr_temp15 = new int[N*N];\n\tint* output_arr_temp15 = new int[N*N];\n\tint* input_arr_temp16 = new int[N*N];\n\tint* output_arr_temp16 = new int[N*N];\n\tint* input_arr_temp17 = new int[N*N];\n\tint* output_arr_temp17 = new int[N*N];\n\tint* input_arr_temp18 = new int[N*N];\n\tint* output_arr_temp18 = new int[N*N];\n\tint* input_arr_temp19 = new int[N*N];\n\tint* output_arr_temp19 = new int[N*N];\n\tint* input_arr_temp20 = new int[N*N];\n\tint* output_arr_temp20 = new int[N*N];\n\tint* input_arr_temp21 = new int[N*N];\n\tint* output_arr_temp21 = new int[N*N];\n\tint* input_arr_temp22 = new int[N*N];\n\tint* output_arr_temp22 = new int[N*N];\n\tint* input_arr_temp23 = new int[N*N];\n\tint* output_arr_temp23 = new int[N*N];\n\tint* input_arr_temp24 = new int[N*N];\n\tint* output_arr_temp24 = new int[N*N];\n\tint* input_arr_temp25 = new int[N*N];\n\tint* output_arr_temp25 = new int[N*N];\n\tint* input_arr",
            "// TODO: implement\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint *input_arr = new int[N*N];\n\tint *output_arr = new int[N*N];\n\n\tfor (int i = 0; i < N*N; i++) {\n\t\tinput_arr[i] = input[i];\n\t}\n\n\tint *send_arr = new int[N*N];\n\tint *recv_arr = new int[N*N];\n\n\tint *send_counts = new int[size];\n\tint *displs = new int[size];\n\n\tint send_count = N/size;\n\tint recv_count = N/size;\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tsend_counts[i] = send_count;\n\t\t\tdispls[i] = i*send_count;\n\t\t}\n\t\tsend_counts[0] = N - (size-1)*send_count;\n\t\tdispls[0] = 0;\n\t}\n\n\tMPI_Scatterv(input_arr, send_counts, displs, MPI_INT, recv_arr, recv_count, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < recv_count; i++) {\n\t\tfor (int j = 0; j < recv_count; j++) {\n\t\t\tint neighbors = 0;\n\t\t\tif (i > 0) {\n\t\t\t\tif (j > 0) {\n\t\t\t\t\tneighbors += recv_arr[i*recv_count + j - 1];\n\t\t\t\t}\n\t\t\t\tif (j < recv_count - 1) {\n\t\t\t\t\tneighbors += recv_arr[i*recv_count + j + 1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i < recv_count - 1) {\n\t\t\t\tif (j > 0) {\n\t\t\t\t\tneighbors += recv_arr[(i+1)*recv_count + j - 1];\n\t\t\t\t}\n\t\t\t\tif (j < recv_count - 1) {\n\t\t\t\t\tneighbors += recv_arr[(i+1)*recv_count + j + 1];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i > 0 && j > 0) {\n\t\t\t\tneighbors += recv_arr[(i-1)*recv_count + j - 1];\n\t\t\t}\n\t\t\tif (i > 0 && j < recv_count - 1) {\n\t\t\t\tneighbors += recv_arr[(i-1)*recv_count + j + 1];\n\t\t\t}\n\t\t\tif (recv_arr[i*recv_count + j] == 1) {\n\t\t\t\tif (neighbors < 2) {\n\t\t\t\t\toutput_arr[i*recv_count + j] = 0;\n\t\t\t\t} else if (neighbors == 2 || neighbors == 3) {\n\t\t\t\t\toutput_arr[i*recv_count + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput_arr[i*recv_count + j] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (neighbors == 3) {\n\t\t\t\t\toutput_arr[i*recv_count + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput_arr[i*recv_count + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tMPI_Gatherv(output_",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "cuda",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint index = i * N + j;\n\tint count = 0;\n\tif (i < N && j < N) {\n\t\tif (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n\t\t\tif (input[index - N - 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index - N] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index - N + 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index - 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index + 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index + N - 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index + N] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index + N + 1] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t\tif (input[index] == 1) {\n\t\t\t\tif (count == 2 || count == 3) {\n\t\t\t\t\toutput[index] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[index] = 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput[index] = 1;\n\t\t\t\t} else {\n\t\t\t\t\toutput[index] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n\t\t\t\tif (input[index - N - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index - N] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index - N + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index + N - 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index + N] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index + N + 1] == 1) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t\tif (input[index] == 1) {\n\t\t\t\t\tif (count == 2 || count == 3) {\n\t\t\t\t\t\toutput[index] = 1;\n\t\t\t\t\t} else {\n\t\t\t\t\t\toutput[index] = 0;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif (count == 3) {\n\t\t\t\t\t\toutput[index] = 1;\n\t\t\t\t\t} else {\n\t\t\t\t\t\toutput[index] = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n\t\t\t\t\tif (input[index - N - 1] == 1) {",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\tint index = x + y * N;\n\tint count = 0;\n\tif (x >= N || y >= N) return;\n\tif (x > 0) count += input[index - 1];\n\tif (x < N - 1) count += input[index + 1];\n\tif (y > 0) count += input[index - N];\n\tif (y < N - 1) count += input[index + N];\n\tif (x > 0 && y > 0) count += input[index - N - 1];\n\tif (x < N - 1 && y > 0) count += input[index - N + 1];\n\tif (x > 0 && y < N - 1) count += input[index + N - 1];\n\tif (x < N - 1 && y < N - 1) count += input[index + N + 1];\n\tif (input[index] == 1) {\n\t\tif (count < 2 || count > 3) output[index] = 0;\n\t\telse output[index] = 1;\n\t} else {\n\t\tif (count == 3) output[index] = 1;\n\t\telse output[index] = 0;\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = i * N + j;\n\tint count = 0;\n\tif (i >= N || j >= N)\n\t\treturn;\n\tif (i == 0 || i == N - 1 || j == 0 || j == N - 1) {\n\t\toutput[k] = 0;\n\t\treturn;\n\t}\n\tfor (int x = -1; x <= 1; x++) {\n\t\tfor (int y = -1; y <= 1; y++) {\n\t\t\tif (input[(i + x) * N + j + y] == 1)\n\t\t\t\tcount++;\n\t\t}\n\t}\n\tif (input[k] == 1) {\n\t\tif (count == 2 || count == 3)\n\t\t\toutput[k] = 1;\n\t\telse\n\t\t\toutput[k] = 0;\n\t} else {\n\t\tif (count == 3)\n\t\t\toutput[k] = 1;\n\t\telse\n\t\t\toutput[k] = 0;\n\t}\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) return;\n\n  int idx = y * N + x;\n  int count = 0;\n\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      if (i == 0 && j == 0) continue;\n      int x2 = x + i;\n      int y2 = y + j;\n      if (x2 >= 0 && x2 < N && y2 >= 0 && y2 < N) {\n        if (input[y2 * N + x2] == 1) count++;\n      }\n    }\n  }\n\n  if (input[idx] == 1) {\n    if (count < 2 || count > 3) {\n      output[idx] = 0;\n    } else {\n      output[idx] = 1;\n    }\n  } else {\n    if (count == 3) {\n      output[idx] = 1;\n    } else {\n      output[idx] = 0;\n    }\n  }\n}",
            "// TODO: implement\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int idx = i * N + j;\n  int count = 0;\n  if (i >= N || j >= N) {\n    return;\n  }\n  if (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n    if (input[idx - N - 1] == 1) {\n      count++;\n    }\n    if (input[idx - N] == 1) {\n      count++;\n    }\n    if (input[idx - N + 1] == 1) {\n      count++;\n    }\n    if (input[idx - 1] == 1) {\n      count++;\n    }\n    if (input[idx + 1] == 1) {\n      count++;\n    }\n    if (input[idx + N - 1] == 1) {\n      count++;\n    }\n    if (input[idx + N] == 1) {\n      count++;\n    }\n    if (input[idx + N + 1] == 1) {\n      count++;\n    }\n  } else {\n    if (i > 0 && j > 0 && i < N - 1 && j < N) {\n      if (input[idx - N - 1] == 1) {\n        count++;\n      }\n      if (input[idx - N] == 1) {\n        count++;\n      }\n      if (input[idx - N + 1] == 1) {\n        count++;\n      }\n      if (input[idx - 1] == 1) {\n        count++;\n      }\n      if (input[idx + 1] == 1) {\n        count++;\n      }\n      if (input[idx + N - 1] == 1) {\n        count++;\n      }\n      if (input[idx + N] == 1) {\n        count++;\n      }\n    } else if (i > 0 && j > 0 && i < N && j < N - 1) {\n      if (input[idx - N - 1] == 1) {\n        count++;\n      }\n      if (input[idx - N] == 1) {\n        count++;\n      }\n      if (input[idx - N + 1] == 1) {\n        count++;\n      }\n      if (input[idx - 1] == 1) {\n        count++;\n      }\n      if (input[idx + 1] == 1) {\n        count++;\n      }\n      if (input[idx + N - 1] == 1) {\n        count++;\n      }\n      if (input[idx + N] == 1) {\n        count++;\n      }\n    } else if (i > 0 && j > 0 && i < N && j < N) {\n      if (input[idx - N - 1] == 1) {\n        count++;\n      }\n      if (input[idx - N] == 1) {\n        count++;\n      }\n      if (input[idx - N + 1] == 1) {\n        count++;\n      }\n      if (input[idx - 1] == 1) {\n        count++;\n      }\n      if (input[idx + 1] == 1) {\n        count++;\n      }\n      if (input[idx + N - 1] == 1) {\n        count++;\n      }\n      if (input[idx + N] == 1) {\n        count++;\n      }\n    } else if (i > 0 && j > 0 && i < N - 1 && j < N) {\n      if (input[idx - N - 1] == 1) {\n        count++;\n      }\n      if (input[idx - N] == 1) {\n        count++;\n      }\n      if (input[idx - N + 1] == 1) {\n        count++;\n      }\n      if (input[idx - 1] == 1) {\n        count++;\n      }\n      if (input[idx + 1] == 1",
            "// TODO: Implement me\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int count = 0;\n  for (int x = i - 1; x <= i + 1; x++) {\n    for (int y = j - 1; y <= j + 1; y++) {\n      if (x == i && y == j) continue;\n      if (x < 0 || x >= N || y < 0 || y >= N) continue;\n      if (input[x * N + y]) count++;\n    }\n  }\n\n  if (input[i * N + j]) {\n    if (count == 2 || count == 3) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  } else {\n    if (count == 3) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "// TODO: Implement\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int idx = x + y * N;\n  int count = 0;\n  if (x >= N || y >= N)\n    return;\n  if (x == 0 || x == N - 1 || y == 0 || y == N - 1) {\n    output[idx] = 0;\n    return;\n  }\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      if (input[idx + i + j * N] == 1)\n        count++;\n    }\n  }\n  if (input[idx] == 1) {\n    if (count < 2 || count > 3)\n      output[idx] = 0;\n    else\n      output[idx] = 1;\n  } else {\n    if (count == 3)\n      output[idx] = 1;\n    else\n      output[idx] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int index = i * N + j;\n  int count = 0;\n  for (int x = -1; x <= 1; x++) {\n    for (int y = -1; y <= 1; y++) {\n      if (x == 0 && y == 0) {\n        continue;\n      }\n      int x_ = x + i;\n      int y_ = y + j;\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        continue;\n      }\n      int index_ = x_ * N + y_;\n      if (input[index_] == 1) {\n        count++;\n      }\n    }\n  }\n  if (input[index] == 1) {\n    if (count < 2 || count > 3) {\n      output[index] = 0;\n    } else {\n      output[index] = 1;\n    }\n  } else {\n    if (count == 3) {\n      output[index] = 1;\n    } else {\n      output[index] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int n = 0;\n  for (int x = i - 1; x <= i + 1; x++) {\n    for (int y = j - 1; y <= j + 1; y++) {\n      if (x == i && y == j) continue;\n      if (x >= 0 && x < N && y >= 0 && y < N) {\n        n += input[x * N + y];\n      }\n    }\n  }\n\n  if (input[i * N + j] == 1) {\n    if (n < 2 || n > 3) {\n      output[i * N + j] = 0;\n    } else {\n      output[i * N + j] = 1;\n    }\n  } else {\n    if (n == 3) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int k = i + j * N;\n  int neighbors = 0;\n  int alive = input[k];\n  if (i > 0 && i < N - 1 && j > 0 && j < N - 1) {\n    neighbors += input[k - 1 - N];\n    neighbors += input[k - 1];\n    neighbors += input[k - 1 + N];\n    neighbors += input[k + N];\n    neighbors += input[k + 1 + N];\n    neighbors += input[k + 1];\n    neighbors += input[k + 1 - N];\n    neighbors += input[k - N];\n  }\n  if (alive == 1) {\n    if (neighbors < 2) {\n      output[k] = 0;\n    } else if (neighbors > 3) {\n      output[k] = 0;\n    } else {\n      output[k] = 1;\n    }\n  } else {\n    if (neighbors == 3) {\n      output[k] = 1;\n    } else {\n      output[k] = 0;\n    }\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (x >= N || y >= N) return;\n\tint n = 0;\n\tif (x > 0) {\n\t\tif (y > 0) {\n\t\t\tn += input[(x - 1) * N + y - 1];\n\t\t}\n\t\tn += input[(x - 1) * N + y];\n\t\tif (y < N - 1) {\n\t\t\tn += input[(x - 1) * N + y + 1];\n\t\t}\n\t}\n\tif (y > 0) {\n\t\tn += input[x * N + y - 1];\n\t}\n\tif (y < N - 1) {\n\t\tn += input[x * N + y + 1];\n\t}\n\tif (x < N - 1) {\n\t\tif (y > 0) {\n\t\t\tn += input[(x + 1) * N + y - 1];\n\t\t}\n\t\tn += input[(x + 1) * N + y];\n\t\tif (y < N - 1) {\n\t\t\tn += input[(x + 1) * N + y + 1];\n\t\t}\n\t}\n\tif (input[x * N + y] == 1) {\n\t\tif (n < 2 || n > 3) {\n\t\t\toutput[x * N + y] = 0;\n\t\t} else {\n\t\t\toutput[x * N + y] = 1;\n\t\t}\n\t} else {\n\t\tif (n == 3) {\n\t\t\toutput[x * N + y] = 1;\n\t\t} else {\n\t\t\toutput[x * N + y] = 0;\n\t\t}\n\t}\n}",
            "// TODO: implement\n\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (x >= N || y >= N) return;\n\n\tint neighbors = 0;\n\tfor (int i = -1; i <= 1; i++) {\n\t\tfor (int j = -1; j <= 1; j++) {\n\t\t\tint nx = x + i;\n\t\t\tint ny = y + j;\n\t\t\tif (nx < 0 || nx >= N || ny < 0 || ny >= N) continue;\n\t\t\tif (i == 0 && j == 0) continue;\n\t\t\tif (input[ny * N + nx]) neighbors++;\n\t\t}\n\t}\n\n\tif (input[y * N + x]) {\n\t\tif (neighbors < 2) output[y * N + x] = 0;\n\t\telse if (neighbors > 3) output[y * N + x] = 0;\n\t\telse output[y * N + x] = 1;\n\t} else {\n\t\tif (neighbors == 3) output[y * N + x] = 1;\n\t\telse output[y * N + x] = 0;\n\t}\n}",
            "// TODO: implement\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (x >= N || y >= N) return;\n\n\tint index = x + y * N;\n\tint neighbors = 0;\n\n\t// Check the 8 neighbors\n\tfor (int i = -1; i <= 1; i++) {\n\t\tfor (int j = -1; j <= 1; j++) {\n\t\t\tif (i == 0 && j == 0) continue;\n\n\t\t\tint nx = x + i;\n\t\t\tint ny = y + j;\n\n\t\t\tif (nx < 0 || nx >= N || ny < 0 || ny >= N) continue;\n\n\t\t\tint nIndex = nx + ny * N;\n\t\t\tneighbors += input[nIndex];\n\t\t}\n\t}\n\n\tif (input[index] == 1) {\n\t\tif (neighbors < 2 || neighbors > 3) {\n\t\t\toutput[index] = 0;\n\t\t} else {\n\t\t\toutput[index] = 1;\n\t\t}\n\t} else {\n\t\tif (neighbors == 3) {\n\t\t\toutput[index] = 1;\n\t\t} else {\n\t\t\toutput[index] = 0;\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  int idx = i * N + j;\n  int count = 0;\n  for (int x = i - 1; x <= i + 1; x++) {\n    for (int y = j - 1; y <= j + 1; y++) {\n      if (x == i && y == j) continue;\n      if (x < 0 || x >= N || y < 0 || y >= N) continue;\n      if (input[x * N + y]) count++;\n    }\n  }\n  if (input[idx]) {\n    if (count < 2 || count > 3) output[idx] = 0;\n    else output[idx] = 1;\n  } else {\n    if (count == 3) output[idx] = 1;\n    else output[idx] = 0;\n  }\n}",
            "// TODO: Implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint idx = i + j * N;\n\tint count = 0;\n\tif (i < N && j < N) {\n\t\tfor (int x = -1; x <= 1; x++) {\n\t\t\tfor (int y = -1; y <= 1; y++) {\n\t\t\t\tint x_ = i + x;\n\t\t\t\tint y_ = j + y;\n\t\t\t\tif (x_ >= 0 && x_ < N && y_ >= 0 && y_ < N) {\n\t\t\t\t\tif (input[x_ + y_ * N] == 1) {\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (input[idx] == 1) {\n\t\t\tif (count == 2 || count == 3) {\n\t\t\t\toutput[idx] = 1;\n\t\t\t} else {\n\t\t\t\toutput[idx] = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tif (count == 3) {\n\t\t\t\toutput[idx] = 1;\n\t\t\t} else {\n\t\t\t\toutput[idx] = 0;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint index = i + j * N;\n\tint neighbors = 0;\n\tif (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n\t\tif (input[index - N - 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index - N] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index - N + 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index - 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index + 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index + N - 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index + N] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index + N + 1] == 1) {\n\t\t\tneighbors++;\n\t\t}\n\t\tif (input[index] == 1) {\n\t\t\tif (neighbors < 2) {\n\t\t\t\toutput[index] = 0;\n\t\t\t} else if (neighbors > 3) {\n\t\t\t\toutput[index] = 0;\n\t\t\t} else {\n\t\t\t\toutput[index] = 1;\n\t\t\t}\n\t\t} else {\n\t\t\tif (neighbors == 3) {\n\t\t\t\toutput[index] = 1;\n\t\t\t} else {\n\t\t\t\toutput[index] = 0;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Implement this function\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tint k = i * N + j;\n\tint count = 0;\n\tif (i < N && j < N) {\n\t\tif (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n\t\t\tif (input[k - N - 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k - N] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k - N + 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k - 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k + 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k + N - 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k + N] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k + N + 1] == 1)\n\t\t\t\tcount++;\n\t\t\tif (input[k] == 1)\n\t\t\t\tcount++;\n\t\t\tif (count == 3)\n\t\t\t\toutput[k] = 1;\n\t\t\telse if (count == 2)\n\t\t\t\toutput[k] = input[k];\n\t\t\telse\n\t\t\t\toutput[k] = 0;\n\t\t}\n\t}\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_game_of_life",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_remainder = N % size;\n\n\tint N_per_rank_rank = N_per_rank + 1;\n\tint N_per_rank_other = N_per_rank;\n\n\tif (rank == 0) {\n\t\tN_per_rank_rank = N_per_rank + N_remainder;\n\t}\n\telse if (rank == size - 1) {\n\t\tN_per_rank_other = N_per_rank + N_remainder;\n\t}\n\n\tint N_per_rank_rank_per_thread = N_per_rank_rank / omp_get_max_threads();\n\tint N_per_rank_other_per_thread = N_per_rank_other / omp_get_max_threads();\n\n\tint N_per_rank_rank_per_thread_remainder = N_per_rank_rank % omp_get_max_threads();\n\tint N_per_rank_other_per_thread_remainder = N_per_rank_other % omp_get_max_threads();\n\n\tint N_per_rank_rank_per_thread_rank = N_per_rank_rank_per_thread + 1;\n\tint N_per_rank_other_per_thread_rank = N_per_rank_other_per_thread + 1;\n\n\tif (rank == 0) {\n\t\tN_per_rank_rank_per_thread_rank = N_per_rank_rank_per_thread + N_per_rank_rank_per_thread_remainder;\n\t}\n\telse if (rank == size - 1) {\n\t\tN_per_rank_other_per_thread_rank = N_per_rank_other_per_thread + N_per_rank_other_per_thread_remainder;\n\t}\n\n\tint N_per_rank_rank_per_thread_rank_per_thread = N_per_rank_rank_per_thread_rank / omp_get_max_threads();\n\tint N_per_rank_other_per_thread_rank_per_thread = N_per_rank_other_per_thread_rank / omp_get_max_threads();\n\n\tint N_per_rank_rank_per_thread_rank_per_thread_remainder = N_per_rank_rank_per_thread_rank % omp_get_max_threads();\n\tint N_per_rank_other_per_thread_rank_per_thread_remainder = N_per_rank_other_per_thread_rank % omp_get_max_threads();\n\n\tint N_per_rank_rank_per_thread_rank_per_thread_rank = N_per_rank_rank_per_thread_rank_per_thread + 1;\n\tint N_per_rank_other_per_thread_rank_per_thread_rank = N_per_rank_other_per_thread_rank_per_thread + 1;\n\n\tif (rank == 0) {\n\t\tN_per_rank_rank_per_thread_rank_per_thread_rank = N_per_rank_rank_per_thread_rank_per_thread + N_per_rank_rank_per_thread_rank_per_thread_remainder;\n\t}\n\telse if (rank == size - 1) {\n\t\tN_per_rank_other_per_thread_rank_per_thread_rank = N_per_rank_other_per_thread_rank_per_thread + N_per_rank_other_per_thread_rank_per_thread_remainder;\n\t}\n\n\tint N_per_rank_rank_per_thread_rank_per_thread_rank_per_thread = N_per_rank_rank_per_thread_rank_per_thread_rank / omp_get_max_threads();\n\tint N_per_rank_other_per_thread_rank_per_thread_rank",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement me\n\n}",
            "// TODO: Implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int *input_rank = new int[N*N];\n  int *output_rank = new int[N*N];\n\n  MPI_Scatter(input.data(), N*N, MPI_INT, input_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int *input_rank_copy = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy[i] = input_rank[i];\n  }\n\n  int *output_rank_copy = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_2 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_2[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_2 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_2[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_3 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_3[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_3 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_3[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_4 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_4[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_4 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_4[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_5 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_5[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_5 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_5[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_6 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_6[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_6 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_6[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_7 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    input_rank_copy_7[i] = input_rank[i];\n  }\n\n  int *output_rank_copy_7 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {\n    output_rank_copy_7[i] = output_rank[i];\n  }\n\n  int *input_rank_copy_8 = new int[N*N];\n  for (int i = 0; i < N*N; i++) {",
            "// TODO: Your code here\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint *input_array = new int[N*N];\n\tint *output_array = new int[N*N];\n\n\tfor (size_t i = 0; i < N*N; i++) {\n\t\tinput_array[i] = input[i];\n\t}\n\n\tint *input_array_local = new int[N*N];\n\tint *output_array_local = new int[N*N];\n\n\tint *input_array_local_temp = new int[N*N];\n\tint *output_array_local_temp = new int[N*N];\n\n\tint *input_array_local_temp2 = new int[N*N];\n\tint *output_array_local_temp2 = new int[N*N];\n\n\tint *input_array_local_temp3 = new int[N*N];\n\tint *output_array_local_temp3 = new int[N*N];\n\n\tint *input_array_local_temp4 = new int[N*N];\n\tint *output_array_local_temp4 = new int[N*N];\n\n\tint *input_array_local_temp5 = new int[N*N];\n\tint *output_array_local_temp5 = new int[N*N];\n\n\tint *input_array_local_temp6 = new int[N*N];\n\tint *output_array_local_temp6 = new int[N*N];\n\n\tint *input_array_local_temp7 = new int[N*N];\n\tint *output_array_local_temp7 = new int[N*N];\n\n\tint *input_array_local_temp8 = new int[N*N];\n\tint *output_array_local_temp8 = new int[N*N];\n\n\tint *input_array_local_temp9 = new int[N*N];\n\tint *output_array_local_temp9 = new int[N*N];\n\n\tint *input_array_local_temp10 = new int[N*N];\n\tint *output_array_local_temp10 = new int[N*N];\n\n\tint *input_array_local_temp11 = new int[N*N];\n\tint *output_array_local_temp11 = new int[N*N];\n\n\tint *input_array_local_temp12 = new int[N*N];\n\tint *output_array_local_temp12 = new int[N*N];\n\n\tint *input_array_local_temp13 = new int[N*N];\n\tint *output_array_local_temp13 = new int[N*N];\n\n\tint *input_array_local_temp14 = new int[N*N];\n\tint *output_array_local_temp14 = new int[N*N];\n\n\tint *input_array_local_temp15 = new int[N*N];\n\tint *output_array_local_temp15 = new int[N*N];\n\n\tint *input_array_local_temp16 = new int[N*N];\n\tint *output_array_local_temp16 = new int[N*N];\n\n\tint *input_array_local_temp17 = new int[N*N];\n\tint *output_array_local_temp17 = new int[N*N];\n\n\tint *input_array_local_temp18 = new int[N*N];\n\tint *output_array_local_temp18 = new int[N*N];\n\n\tint *input_array_local_temp19 = new int[N*N];\n\tint *output_array_local_temp19 = new int[N*N];\n\n\tint *input_array_local_temp20 = new int[N*N];\n\tint *output_array_local_temp20 = new int[N*N];\n\n\tint *input_array_local_temp21 = new int[N*N];\n\tint *output_array",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_remainder = N % size;\n\n\tint N_per_rank_rank = N_per_rank + 1;\n\tint N_per_rank_other = N_per_rank;\n\n\tif (rank == 0) {\n\t\tN_per_rank_rank = N_per_rank + N_remainder;\n\t}\n\n\tif (rank == size - 1) {\n\t\tN_per_rank_other = N_per_rank + N_remainder;\n\t}\n\n\tint N_per_rank_rank_N = N_per_rank_rank * N;\n\tint N_per_rank_other_N = N_per_rank_other * N;\n\n\tint N_per_rank_rank_N_per_rank_other = N_per_rank_rank * N_per_rank_other;\n\n\tstd::vector<int> input_rank(N_per_rank_rank_N);\n\tstd::vector<int> output_rank(N_per_rank_rank_N);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < N_per_rank_rank_N; i++) {\n\t\t\tinput_rank[i] = input[i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < N_per_rank_rank_N; i++) {\n\t\t\tinput_rank[i] = 0;\n\t\t}\n\t}\n\n\tMPI_Scatter(input_rank.data(), N_per_rank_rank_N, MPI_INT, output_rank.data(), N_per_rank_rank_N, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tstd::vector<int> input_rank_other(N_per_rank_other_N);\n\tstd::vector<int> output_rank_other(N_per_rank_other_N);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < N_per_rank_other_N; i++) {\n\t\t\tinput_rank_other[i] = input[N_per_rank_rank_N + i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < N_per_rank_other_N; i++) {\n\t\t\tinput_rank_other[i] = 0;\n\t\t}\n\t}\n\n\tMPI_Scatter(input_rank_other.data(), N_per_rank_other_N, MPI_INT, output_rank_other.data(), N_per_rank_other_N, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tint N_per_rank_rank_N_per_rank_other_N = N_per_rank_rank_N_per_rank_other * N;\n\n\tstd::vector<int> input_rank_other_N(N_per_rank_rank_N_per_rank_other_N);\n\tstd::vector<int> output_rank_other_N(N_per_rank_rank_N_per_rank_other_N);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < N_per_rank_rank_N_per_rank_other_N; i++) {\n\t\t\tinput_rank_other_N[i] = input[N_per_rank_rank_N + N_per_rank_other_N + i];\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < N_per_rank_rank_N_per_rank_other_N; i++) {\n\t\t\tinput_rank_other_N[i] = 0;\n\t\t}\n\t}\n\n\tMPI_Scatter(input_rank_other_N.data(), N_per_rank_rank_N_",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int N_per_rank = N / size;\n  int N_per_rank_rem = N % size;\n  int N_per_rank_rank0 = N_per_rank + N_per_rank_rem;\n  int N_per_rank_rank_rest = N_per_rank;\n  int N_per_rank_rank_rest_rem = 0;\n  if (rank!= 0) {\n    N_per_rank_rank_rest = N_per_rank + 1;\n    N_per_rank_rank_rest_rem = 1;\n  }\n  int N_per_rank_rank_rest_last = N_per_rank_rank_rest + N_per_rank_rank_rest_rem;\n  int N_per_rank_rank_rest_last_rem = 0;\n  if (rank!= size - 1) {\n    N_per_rank_rank_rest_last = N_per_rank_rank_rest + 1;\n    N_per_rank_rank_rest_last_rem = 1;\n  }\n  int N_per_rank_rank_rest_last_last = N_per_rank_rank_rest_last + N_per_rank_rank_rest_last_rem;\n  int N_per_rank_rank_rest_last_last_rem = 0;\n  if (rank!= size - 2) {\n    N_per_rank_rank_rest_last_last = N_per_rank_rank_rest_last + 1;\n    N_per_rank_rank_rest_last_last_rem = 1;\n  }\n  int N_per_rank_rank_rest_last_last_last = N_per_rank_rank_rest_last_last + N_per_rank_rank_rest_last_last_rem;\n  int N_per_rank_rank_rest_last_last_last_rem = 0;\n  if (rank!= size - 3) {\n    N_per_rank_rank_rest_last_last_last = N_per_rank_rank_rest_last_last + 1;\n    N_per_rank_rank_rest_last_last_last_rem = 1;\n  }\n  int N_per_rank_rank_rest_last_last_last_last = N_per_rank_rank_rest_last_last_last + N_per_rank_rank_rest_last_last_last_rem;\n  int N_per_rank_rank_rest_last_last_last_last_rem = 0;\n  if (rank!= size - 4) {\n    N_per_rank_rank_rest_last_last_last_last = N_per_rank_rank_rest_last_last_last + 1;\n    N_per_rank_rank_rest_last_last_last_last_rem = 1;\n  }\n  int N_per_rank_rank_rest_last_last_last_last_last = N_per_rank_rank_rest_last_last_last_last + N_per_rank_rank_rest_last_last_last_last_rem;\n  int N_per_rank_rank_rest_last_last_last_last_last_rem = 0;\n  if (rank!= size - 5) {\n    N_per_rank_rank_rest_last_last_last_last_last = N_per_rank_rank_rest_last_last_last_last + 1;\n    N_per_rank_rank_rest_last_last_last_last_last_rem = 1;\n  }\n  int N_per_rank_rank_rest_last_last_last_last_last_last = N_per_rank_rank_rest_last_last_last_last_last + N_per_rank_rank_rest_last_last_last_last_last_rem;\n  int N_per_rank_rank_rest_last_last_last_last_last_last_rem =",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_remainder = N % size;\n\n\tint N_per_rank_rank = N_per_rank + (rank < N_remainder);\n\tint N_per_rank_rank_plus_one = N_per_rank + (rank < N_remainder);\n\n\tint N_per_rank_rank_plus_one_start = rank * N_per_rank + (rank < N_remainder);\n\tint N_per_rank_rank_plus_one_end = N_per_rank_rank_plus_one_start + N_per_rank_rank_plus_one;\n\n\tint N_per_rank_rank_start = rank * N_per_rank + (rank < N_remainder);\n\tint N_per_rank_rank_end = N_per_rank_rank_start + N_per_rank_rank;\n\n\tint N_per_rank_rank_plus_one_start_row = N_per_rank_rank_plus_one_start / N;\n\tint N_per_rank_rank_plus_one_start_col = N_per_rank_rank_plus_one_start % N;\n\n\tint N_per_rank_rank_start_row = N_per_rank_rank_start / N;\n\tint N_per_rank_rank_start_col = N_per_rank_rank_start % N;\n\n\tint N_per_rank_rank_plus_one_end_row = N_per_rank_rank_plus_one_end / N;\n\tint N_per_rank_rank_plus_one_end_col = N_per_rank_rank_plus_one_end % N;\n\n\tint N_per_rank_rank_end_row = N_per_rank_rank_end / N;\n\tint N_per_rank_rank_end_col = N_per_rank_rank_end % N;\n\n\tstd::vector<int> input_rank(N_per_rank_rank);\n\tstd::vector<int> output_rank(N_per_rank_rank);\n\n\tfor (int i = 0; i < N_per_rank_rank; i++) {\n\t\tinput_rank[i] = input[N_per_rank_rank_start + i];\n\t}\n\n\tint num_threads = omp_get_max_threads();\n\tint num_threads_rank = omp_get_max_threads();\n\tint num_threads_rank_plus_one = omp_get_max_threads();\n\n\tint num_threads_rank_plus_one_start = rank * num_threads + (rank < N_remainder);\n\tint num_threads_rank_plus_one_end = num_threads_rank_plus_one_start + num_threads_rank_plus_one;\n\n\tint num_threads_rank_start = rank * num_threads + (rank < N_remainder);\n\tint num_threads_rank_end = num_threads_rank_start + num_threads_rank;\n\n\tint num_threads_rank_plus_one_start_row = num_threads_rank_plus_one_start / N;\n\tint num_threads_rank_plus_one_start_col = num_threads_rank_plus_one_start % N;\n\n\tint num_threads_rank_start_row = num_threads_rank_start / N;\n\tint num_threads_rank_start_col = num_threads_rank_start % N;\n\n\tint num_threads_rank_plus_one_end_row = num_threads_rank_plus_one_end / N;\n\tint num_threads_rank_plus_one_end_col = num_threads_rank_plus_one_end % N;\n\n\tint num_threads_rank_end_row = num_threads_rank_end / N;\n\tint num_threads_rank_end_col = num_threads_rank_end % N;\n\n\tint num_threads_rank",
            "// TODO: Implement me\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = N/size;\n  int m = N%size;\n\n  int *input_rank = new int[n*n];\n  int *output_rank = new int[n*n];\n\n  if(rank == 0){\n    for(int i = 0; i < n*n; i++){\n      input_rank[i] = input[i];\n    }\n  }\n\n  MPI_Scatter(input_rank, n*n, MPI_INT, output_rank, n*n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for(int i = 0; i < n*n; i++){\n    int x = i/n;\n    int y = i%n;\n    int sum = 0;\n    for(int j = -1; j <= 1; j++){\n      for(int k = -1; k <= 1; k++){\n        if(x+j >= 0 && x+j < n && y+k >= 0 && y+k < n){\n          sum += output_rank[(x+j)*n + y+k];\n        }\n      }\n    }\n    if(sum == 3 || (sum == 2 && output_rank[i] == 1)){\n      output_rank[i] = 1;\n    }\n    else{\n      output_rank[i] = 0;\n    }\n  }\n\n  MPI_Gather(output_rank, n*n, MPI_INT, input_rank, n*n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if(rank == 0){\n    for(int i = 0; i < n*n; i++){\n      output[i] = input_rank[i];\n    }\n  }\n\n  delete[] input_rank;\n  delete[] output_rank;\n}",
            "// TODO: implement\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_remainder = N % size;\n\n\tint N_per_rank_plus_one = N_per_rank + 1;\n\tint N_per_rank_minus_one = N_per_rank - 1;\n\n\tint N_per_rank_plus_one_remainder = N_per_rank_plus_one + N_remainder;\n\tint N_per_rank_minus_one_remainder = N_per_rank_minus_one + N_remainder;\n\n\tint N_per_rank_plus_one_remainder_times_N = N_per_rank_plus_one_remainder * N;\n\tint N_per_rank_minus_one_remainder_times_N = N_per_rank_minus_one_remainder * N;\n\n\tint N_per_rank_plus_one_remainder_times_N_plus_one = N_per_rank_plus_one_remainder_times_N + 1;\n\tint N_per_rank_minus_one_remainder_times_N_plus_one = N_per_rank_minus_one_remainder_times_N + 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_minus_one = N_per_rank_plus_one_remainder_times_N - 1;\n\tint N_per_rank_minus_one_remainder_times_N_minus_one = N_per_rank_minus_one_remainder_times_N - 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_plus_one_minus_one = N_per_rank_plus_one_remainder_times_N_plus_one - 1;\n\tint N_per_rank_minus_one_remainder_times_N_plus_one_minus_one = N_per_rank_minus_one_remainder_times_N_plus_one - 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_minus_one_minus_one = N_per_rank_plus_one_remainder_times_N_minus_one - 1;\n\tint N_per_rank_minus_one_remainder_times_N_minus_one_minus_one = N_per_rank_minus_one_remainder_times_N_minus_one - 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_minus_one_minus_one_plus_one = N_per_rank_plus_one_remainder_times_N_minus_one_minus_one + 1;\n\tint N_per_rank_minus_one_remainder_times_N_minus_one_minus_one_plus_one = N_per_rank_minus_one_remainder_times_N_minus_one_minus_one + 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_minus_one_minus_one_plus_one_plus_one = N_per_rank_plus_one_remainder_times_N_minus_one_minus_one_plus_one + 1;\n\tint N_per_rank_minus_one_remainder_times_N_minus_one_minus_one_plus_one_plus_one = N_per_rank_minus_one_remainder_times_N_minus_one_minus_one_plus_one + 1;\n\n\tint N_per_rank_plus_one_remainder_times_N_minus_one_minus_one_plus_one_plus_one_plus_one = N_per_rank_plus_one_remainder_times_N_minus_one_minus_one_plus_one_plus_one + 1;\n\tint N_per_rank_minus_",
            "// TODO: implement\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint N_per_rank = N / size;\n\tint N_rem = N % size;\n\tint N_per_rank_plus_one = N_per_rank + 1;\n\n\tint N_per_rank_plus_one_rem = N_per_rank_plus_one % size;\n\tint N_per_rank_plus_one_div = N_per_rank_plus_one / size;\n\n\tint N_per_rank_plus_one_rem_rank = N_per_rank_plus_one_rem;\n\tint N_per_rank_plus_one_div_rank = N_per_rank_plus_one_div;\n\n\tint N_per_rank_plus_one_rem_rank_start = 0;\n\tint N_per_rank_plus_one_div_rank_start = 0;\n\n\tfor (int i = 0; i < rank; i++) {\n\t\tN_per_rank_plus_one_rem_rank_start += N_per_rank_plus_one_rem;\n\t\tN_per_rank_plus_one_div_rank_start += N_per_rank_plus_one_div;\n\t}\n\n\tint N_per_rank_plus_one_rem_rank_end = N_per_rank_plus_one_rem_rank_start + N_per_rank_plus_one_rem_rank;\n\tint N_per_rank_plus_one_div_rank_end = N_per_rank_plus_one_div_rank_start + N_per_rank_plus_one_div_rank;\n\n\tint N_per_rank_plus_one_rem_rank_start_local = N_per_rank_plus_one_rem_rank_start - N_per_rank_plus_one_rem_rank;\n\tint N_per_rank_plus_one_div_rank_start_local = N_per_rank_plus_one_div_rank_start - N_per_rank_plus_one_div_rank;\n\n\tint N_per_rank_plus_one_rem_rank_end_local = N_per_rank_plus_one_rem_rank_end - N_per_rank_plus_one_rem_rank;\n\tint N_per_rank_plus_one_div_rank_end_local = N_per_rank_plus_one_div_rank_end - N_per_rank_plus_one_div_rank;\n\n\tint N_per_rank_plus_one_rem_rank_start_local_plus_one = N_per_rank_plus_one_rem_rank_start_local + 1;\n\tint N_per_rank_plus_one_div_rank_start_local_plus_one = N_per_rank_plus_one_div_rank_start_local + 1;\n\n\tint N_per_rank_plus_one_rem_rank_end_local_minus_one = N_per_rank_plus_one_rem_rank_end_local - 1;\n\tint N_per_rank_plus_one_div_rank_end_local_minus_one = N_per_rank_plus_one_div_rank_end_local - 1;\n\n\tint N_per_rank_plus_one_rem_rank_end_local_plus_one = N_per_rank_plus_one_rem_rank_end_local + 1;\n\tint N_per_rank_plus_one_div_rank_end_local_plus_one = N_per_rank_plus_one_div_rank_end_local + 1;\n\n\tint N_per_rank_plus_one_rem_rank_end_local_plus_two = N_per_rank_plus_one_rem_rank_end_local + 2;\n\tint N_per_rank_plus_one_div_rank_end_local_plus_two = N_per_rank_plus_one_div_rank_end_local + 2;\n\n\tint N_per_",
            "// TODO: Your code here\n\t// Hint: Use OpenMP to parallelize the computation\n\t// Hint: Use MPI to distribute the computation\n\t// Hint: Use std::vector<int>::at() to access the elements of input and output\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n  // Hint: Use OpenMP to parallelize the for loop over the rows.\n  // Hint: Use MPI to distribute the work across the ranks.\n  // Hint: Use MPI_Reduce to collect the results on rank 0.\n}",
            "// TODO: Implement this function\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint num_rows = N / size;\n\tint num_cols = N;\n\n\tint num_rows_rank = num_rows;\n\tint num_cols_rank = num_cols;\n\tif (rank == size - 1) {\n\t\tnum_rows_rank = N - (size - 1) * num_rows;\n\t}\n\n\tstd::vector<int> input_rank(num_rows_rank * num_cols_rank);\n\tstd::vector<int> output_rank(num_rows_rank * num_cols_rank);\n\n\tMPI_Scatter(input.data(), num_rows_rank * num_cols_rank, MPI_INT, input_rank.data(), num_rows_rank * num_cols_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t// Compute\n\tfor (int i = 0; i < num_rows_rank; i++) {\n\t\tfor (int j = 0; j < num_cols_rank; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (int k = -1; k <= 1; k++) {\n\t\t\t\tfor (int l = -1; l <= 1; l++) {\n\t\t\t\t\tint row = i + k;\n\t\t\t\t\tint col = j + l;\n\t\t\t\t\tif (row >= 0 && row < num_rows_rank && col >= 0 && col < num_cols_rank) {\n\t\t\t\t\t\tif (input_rank[row * num_cols_rank + col] == 1) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input_rank[i * num_cols_rank + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\toutput_rank[i * num_cols_rank + j] = 0;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput_rank[i * num_cols_rank + j] = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\toutput_rank[i * num_cols_rank + j] = 1;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\toutput_rank[i * num_cols_rank + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Gather\n\tMPI_Gather(output_rank.data(), num_rows_rank * num_cols_rank, MPI_INT, output.data(), num_rows_rank * num_cols_rank, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int N_per_rank = N / size;\n  int N_remainder = N % size;\n\n  int N_per_rank_rank = N_per_rank + 1;\n  int N_per_rank_rank_remainder = N_per_rank + 1;\n\n  if (rank == 0) {\n    N_per_rank_rank = N_per_rank + N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder = N_per_rank + N_remainder;\n  }\n\n  int N_per_rank_rank_remainder_offset = 0;\n  int N_per_rank_rank_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset = 0;\n  int N_per_rank_rank_remainder_offset_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset_offset = 0;\n  int N_per_rank_rank_remainder_offset_offset_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset_offset_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset_offset_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset_offset_offset = 0;\n  int N_per_rank_rank_remainder_offset_offset_offset_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset_offset_offset_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset_offset_offset_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset_offset_offset_offset = 0;\n  int N_per_rank_rank_remainder_offset_offset_offset_offset_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset_offset_offset_offset_offset_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset_offset_offset_offset_offset_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset_offset_offset_offset_offset_offset = 0;\n  int N_per_rank_rank_remainder_offset_offset_offset_offset_offset_offset_offset = 0;\n\n  if (rank == 0) {\n    N_per_rank_rank_offset_offset_offset_offset_offset_offset_offset_offset = N_remainder;\n  }\n\n  if (rank == size - 1) {\n    N_per_rank_rank_remainder_offset_offset_offset_offset_offset_offset_offset_offset = N_remainder;\n  }\n\n  int N_per_rank_rank_offset_offset_offset_offset_offset_offset_offset_offset_offset = 0;\n  int N_per_rank_rank_remain"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n  // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](const int i, const int j) {\n  //   imageOut(i, j) = 0;\n  //   for (int k = -1; k <= 1; k++) {\n  //     for (int l = -1; l <= 1; l++) {\n  //       imageOut(i, j) += imageIn(i + k, j + l) * edgeKernel[k + 1][l + 1];\n  //     }\n  //   }\n  //   imageOut(i, j) = std::max(0, std::min(255, imageOut(i, j)));\n  // });\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](const int i, const int j) {\n    imageOut(i, j) = 0;\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        imageOut(i, j) += imageIn(i + k, j + l) * edgeKernel[k + 1][l + 1];\n      }\n    }\n    imageOut(i, j) = std::max(0, std::min(255, imageOut(i, j)));\n  });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n  // You may need to use Kokkos::parallel_for\n  // You may need to use Kokkos::subview\n  // You may need to use Kokkos::subview(imageIn, i, j)\n  // You may need to use Kokkos::subview(imageOut, i, j)\n  // You may need to use Kokkos::subview(imageIn, i, j)[k]\n  // You may need to use Kokkos::subview(imageOut, i, j)[k]\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageOut, i, j)[k] =...\n  // You may need to use Kokkos::subview(imageIn, i, j)[k",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_for to iterate over the image\n  // Hint: Use Kokkos::subview to access the image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use Kokkos::subview to access the edge kernel\n  // Hint: Use Kokkos::subview to access the output image\n  // Hint: Use Kokkos::subview to access the input image\n  // Hint: Use K",
            "// TODO: Implement me!\n\n}",
            "// TODO: Implement me\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement me!\n  // Use Kokkos::parallel_for to iterate over the image.\n  // Use Kokkos::subview to access the image's pixels.\n  // Use Kokkos::subview to access the edge kernel.\n  // Use Kokkos::atomic_fetch_add to compute the dot product.\n  // Use Kokkos::atomic_fetch_max to clip the output between 0 and 255.\n}",
            "// TODO: Implement this function\n  // You may need to use the following Kokkos functions:\n  // Kokkos::parallel_for\n  // Kokkos::RangePolicy\n  // Kokkos::MDRangePolicy\n  // Kokkos::Experimental::require\n  // Kokkos::Experimental::work_group_collective\n  // Kokkos::Experimental::work_group_reduce\n  // Kokkos::Experimental::work_group_scan\n  // Kokkos::Experimental::shfl_down\n  // Kokkos::Experimental::shfl_up\n  // Kokkos::Experimental::shfl_xor\n  // Kokkos::Experimental::shfl\n  // Kokkos::Experimental::sync_warp\n  // Kokkos::Experimental::warp_reduce\n  // Kokkos::Experimental::warp_scan\n  // Kokkos::Experimental::warp_shuffle_down\n  // Kokkos::Experimental::warp_shuffle_up\n  // Kokkos::Experimental::warp_shuffle_xor\n  // Kokkos::Experimental::warp_shuffle\n  // Kokkos::Experimental::warp_sync\n\n  // You may need to use the following Kokkos views:\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::HostSpace>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaUVMSpace>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaUVMSpace, Kokkos::MemoryUnmanaged>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryUnmanaged>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryManaged>\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Managed> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged|Kokkos::RandomAccess> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Managed|Kokkos::RandomAccess> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged|Kokkos::RandomAccess,Kokkos::Restrict> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Managed|Kokkos::RandomAccess,Kokkos::Restrict> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged|Kokkos::RandomAccess,Kokkos::Restrict,Kokkos::Atomic> >\n  // Kokkos::View<int**, Kokkos::LayoutRight, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Managed|Kokkos::RandomAccess,Kokkos::Restrict,Kokkos::Atomic> >\n  // Kokkos::View<int",
            "// TODO: Implement me!\n  // Kokkos::parallel_for(\"convolveKernel\", Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](int i, int j) {\n  //   int sum = 0;\n  //   for (int ii = -1; ii <= 1; ii++) {\n  //     for (int jj = -1; jj <= 1; jj++) {\n  //       int iii = i + ii;\n  //       int jjj = j + jj;\n  //       if (iii >= 0 && iii < N && jjj >= 0 && jjj < N) {\n  //         sum += imageIn(iii, jjj) * edgeKernel[ii + 1][jj + 1];\n  //       }\n  //     }\n  //   }\n  //   imageOut(i, j) = std::min(255, std::max(0, sum));\n  // });\n  Kokkos::parallel_for(\"convolveKernel\", Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](int i, int j) {\n    int sum = 0;\n    for (int ii = -1; ii <= 1; ii++) {\n      for (int jj = -1; jj <= 1; jj++) {\n        int iii = i + ii;\n        int jjj = j + jj;\n        if (iii >= 0 && iii < N && jjj >= 0 && jjj < N) {\n          sum += imageIn(iii, jjj) * edgeKernel[ii + 1][jj + 1];\n        }\n      }\n    }\n    imageOut(i, j) = std::min(255, std::max(0, sum));\n  });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Fill in the body of this function\n}",
            "// TODO: Implement this function\n  Kokkos::parallel_for(\n    \"convolveKernel\",\n    Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N),\n    KOKKOS_LAMBDA(const int i, const int j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int ii = i + k;\n          int jj = j + l;\n          if (ii < 0 || ii >= N || jj < 0 || jj >= N) {\n            sum += 0;\n          } else {\n            sum += imageIn(ii, jj) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum > 255? 255 : sum < 0? 0 : sum;\n    }\n  );\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n  // Hint: Use Kokkos::parallel_for to iterate over the image.\n  // Hint: Use Kokkos::subview to access the image's pixels.\n  // Hint: Use Kokkos::subview to access the edge kernel.\n  // Hint: Use Kokkos::atomic_fetch_add to update the output image.\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(sum, 255));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(255, std::max(0, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    int i, j, k, l;\n    int sum;\n    int imageIn_size = imageIn.size();\n    int imageOut_size = imageOut.size();\n    int imageIn_N = imageIn_size / N;\n    int imageOut_N = imageOut_size / N;\n    int imageIn_N_1 = imageIn_N - 1;\n    int imageOut_N_1 = imageOut_size - 1;\n\n    #pragma omp parallel for private(i, j, k, l, sum)\n    for (i = 0; i < imageOut_N; i++) {\n        for (j = 0; j < imageOut_N; j++) {\n            sum = 0;\n            for (k = -1; k <= 1; k++) {\n                for (l = -1; l <= 1; l++) {\n                    if (i + k < 0 || i + k > imageIn_N_1 || j + l < 0 || j + l > imageIn_N_1) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[i + k * imageIn_N + l] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * imageOut_N + j] = sum > 255? 255 : sum < 0? 0 : sum;\n        }\n    }\n}",
            "// TODO: Fill this in\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(255, std::max(0, sum));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; ++k) {\n                for (size_t l = 0; l < 3; ++l) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n\n    // TODO: Implement this function\n    // Hint:",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize this function\n    // Hint: Use the edgeKernel defined above\n    // Hint: Use the following formula to compute the convolution:\n    //       output[i, j] = sum(imageIn[i + k, j + l] * edgeKernel[k, l])\n    //       where the sum is over all k, l\n    //       and the indices are clamped to the image boundaries\n    //       and the output is clipped to the range [0, 255]\n    // Hint: You can use the following code to compute the convolution for a single pixel:\n    //       int output = 0;\n    //       for (int k = -1; k <= 1; k++) {\n    //           for (int l = -1; l <= 1; l++) {\n    //               int i = i + k;\n    //               int j = j + l;\n    //               if (i >= 0 && i < N && j >= 0 && j < N) {\n    //                   output += imageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n    //               }\n    //           }\n    //       }\n    //       output = std::max(0, std::min(output, 255));\n    //       imageOut[i * N + j] = output;\n\n    int i, j, k, l;\n    int output;\n    int imageInSize = N * N;\n    int imageOutSize = N * N;\n\n    #pragma omp parallel for private(i, j, k, l, output)\n    for (i = 0; i < imageInSize; i++) {\n        output = 0;\n        for (k = -1; k <= 1; k++) {\n            for (l = -1; l <= 1; l++) {\n                j = i + k + l * N;\n                if (j >= 0 && j < imageInSize) {\n                    output += imageIn[j] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        output = std::max(0, std::min(output, 255));\n        imageOut[i] = output;\n    }\n}",
            "// TODO: Implement this function\n    int i, j, k, l;\n    int sum;\n    int x, y;\n    int x1, y1;\n    int x2, y2;\n    int x3, y3;\n    int x4, y4;\n    int x5, y5;\n    int x6, y6;\n    int x7, y7;\n    int x8, y8;\n    int x9, y9;\n    int x10, y10;\n    int x11, y11;\n    int x12, y12;\n    int x13, y13;\n    int x14, y14;\n    int x15, y15;\n    int x16, y16;\n    int x17, y17;\n    int x18, y18;\n    int x19, y19;\n    int x20, y20;\n    int x21, y21;\n    int x22, y22;\n    int x23, y23;\n    int x24, y24;\n    int x25, y25;\n    int x26, y26;\n    int x27, y27;\n    int x28, y28;\n    int x29, y29;\n    int x30, y30;\n    int x31, y31;\n    int x32, y32;\n    int x33, y33;\n    int x34, y34;\n    int x35, y35;\n    int x36, y36;\n    int x37, y37;\n    int x38, y38;\n    int x39, y39;\n    int x40, y40;\n    int x41, y41;\n    int x42, y42;\n    int x43, y43;\n    int x44, y44;\n    int x45, y45;\n    int x46, y46;\n    int x47, y47;\n    int x48, y48;\n    int x49, y49;\n    int x50, y50;\n    int x51, y51;\n    int x52, y52;\n    int x53, y53;\n    int x54, y54;\n    int x55, y55;\n    int x56, y56;\n    int x57, y57;\n    int x58, y58;\n    int x59, y59;\n    int x60, y60;\n    int x61, y61;\n    int x62, y62;\n    int x63, y63;\n    int x64, y64;\n    int x65, y65;\n    int x66, y66;\n    int x67, y67;\n    int x68, y68;\n    int x69, y69;\n    int x70, y70;\n    int x71, y71;\n    int x72, y72;\n    int x73, y73;\n    int x74, y74;\n    int x75, y75;\n    int x76, y76;\n    int x77, y77;\n    int x78, y78;\n    int x79, y79;\n    int x80, y80;\n    int x81, y81;\n    int x82, y82;\n    int x83, y83;\n    int x84, y84;\n    int x85, y85;\n    int x86, y86;\n    int x87, y87;\n    int x88, y88;\n    int x89, y89;\n    int x90, y90;\n    int x91, y91;\n    int x92, y9",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Fill in this function\n    int i, j, k, l;\n    int sum;\n    int width = N;\n    int height = N;\n    int kernel_width = 3;\n    int kernel_height = 3;\n\n    #pragma omp parallel for private(i, j, k, l, sum)\n    for (i = 0; i < height; i++) {\n        for (j = 0; j < width; j++) {\n            sum = 0;\n            for (k = -1; k <= 1; k++) {\n                for (l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= height || y < 0 || y >= width) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * width + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * width + j] = sum;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Replace this line with your code\n    // You can use the edge kernel defined above\n    // You can use the following variables:\n    //   imageIn: the input image\n    //   imageOut: the output image\n    //   N: the size of the image\n    //   edgeKernel: the edge kernel\n\n    int row, col;\n    int row_start, row_end, col_start, col_end;\n    int sum;\n\n    #pragma omp parallel for private(row, col, row_start, row_end, col_start, col_end, sum)\n    for (row = 0; row < N; row++) {\n        row_start = row - 1;\n        row_end = row + 1;\n        for (col = 0; col < N; col++) {\n            col_start = col - 1;\n            col_end = col + 1;\n            sum = 0;\n            for (int i = row_start; i <= row_end; i++) {\n                for (int j = col_start; j <= col_end; j++) {\n                    if (i < 0 || i >= N || j < 0 || j >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[i * N + j] * edgeKernel[i - row_start][j - col_start];\n                    }\n                }\n            }\n            imageOut[row * N + col] = std::max(0, std::min(sum, 255));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; ++k) {\n                for (size_t l = 0; l < 3; ++l) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; ++k) {\n                for (size_t l = 0; l < 3; ++l) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "serial",
        "prompt": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Replace this code with your solution\n    // The size of the image is NxN\n    // The edge kernel is 3x3\n    // The edge kernel is symmetric\n    // The edge kernel is [-1, -1, -1, -1, 8, -1, -1, -1, -1]\n    // The edge kernel is applied to each pixel in the image\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //   top left, top middle, top right,\n    //   middle left, middle middle, middle right,\n    //   bottom left, bottom middle, bottom right\n    // The edge kernel is applied to the pixels in the image in the following order:\n    //",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    imageOut.resize(N * N);\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; ++k) {\n                for (size_t l = 0; l < 3; ++l) {\n                    size_t row = i + k - 1;\n                    size_t col = j + l - 1;\n                    if (row < N && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(sum, 255));\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Replace this comment with your code\n    for (size_t i = 0; i < imageIn.size(); i++) {\n        int sum = 0;\n        int x = i % N;\n        int y = i / N;\n        for (int j = -1; j <= 1; j++) {\n            for (int k = -1; k <= 1; k++) {\n                int x1 = x + k;\n                int y1 = y + j;\n                if (x1 < 0 || x1 >= N || y1 < 0 || y1 >= N) {\n                    sum += 0;\n                } else {\n                    sum += imageIn[x1 + y1 * N] * edgeKernel[j + 1][k + 1];\n                }\n            }\n        }\n        imageOut[i] = std::max(0, std::min(255, sum));\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    size_t x = i + k - 1;\n                    size_t y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use nested for loops to iterate over the image\n    // Hint: Use nested if statements to check for boundaries\n    // Hint: Use the edgeKernel to perform the convolution\n    // Hint: Use the imageOut vector to store the output\n    // Hint: Use the imageIn vector to access the input\n    // Hint: Use the N variable to access the image size\n    // Hint: Use the std::max and std::min functions to clip the output\n    // Hint: Use the std::vector::at function to access the image\n    // Hint: Use the std::vector::size function to access the image size\n    // Hint: Use the std::vector::operator[] function to access the image\n    // Hint: Use the std::vector::begin and std::vector::end functions to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access the image\n    // Hint: Use the std::vector::iterator to access the image\n    // Hint: Use the std::vector::const_iterator to access",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Replace this code with your solution\n    for (size_t i = 0; i < imageIn.size(); i++) {\n        imageOut[i] = 0;\n        for (size_t j = 0; j < 3; j++) {\n            for (size_t k = 0; k < 3; k++) {\n                size_t index = i + (j - 1) * N + (k - 1);\n                if (index < imageIn.size()) {\n                    imageOut[i] += imageIn[index] * edgeKernel[j][k];\n                }\n            }\n        }\n        imageOut[i] = std::max(0, std::min(255, imageOut[i]));\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
            "// TODO: Replace this code with your solution\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    int sum;\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    if (i + k - 1 < 0 || i + k - 1 >= N || j + l - 1 < 0 || j + l - 1 >= N) {\n                        sum += 0 * edgeKernel[k][l];\n                    } else {\n                        sum += imageIn[i + k - 1 + (j + l - 1) * N] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i + j * N] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < imageIn.size(); i++) {\n        imageOut[i] = 0;\n    }\n    for (size_t i = 0; i < imageIn.size(); i++) {\n        int x = i % N;\n        int y = i / N;\n        for (int j = -1; j <= 1; j++) {\n            for (int k = -1; k <= 1; k++) {\n                int x1 = x + j;\n                int y1 = y + k;\n                if (x1 < 0 || x1 >= N || y1 < 0 || y1 >= N) {\n                    continue;\n                }\n                imageOut[i] += imageIn[x1 + y1 * N] * edgeKernel[j + 1][k + 1];\n            }\n        }\n        imageOut[i] = std::max(0, std::min(255, imageOut[i]));\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "hip",
        "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (x >= N || y >= N) {\n        return;\n    }\n\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int xi = x + i;\n            int yj = y + j;\n            if (xi < 0 || xi >= N || yj < 0 || yj >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[xi + yj * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n\n    imageOut[x + y * N] = max(0, min(sum, 255));\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  if (x >= N || y >= N) return;\n\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int x2 = x + i;\n      int y2 = y + j;\n      if (x2 < 0 || x2 >= N || y2 < 0 || y2 >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x2 + y2 * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n  imageOut[x + y * N] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    if (x >= N || y >= N) return;\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int xi = x + i;\n            int yj = y + j;\n            if (xi < 0 || xi >= N || yj < 0 || yj >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[yi * N + xi] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[y * N + x] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int sum = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      int x = i + k;\n      int y = j + l;\n      if (x < 0 || x >= N || y < 0 || y >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x + y * N] * edgeKernel[k + 1][l + 1];\n      }\n    }\n  }\n  imageOut[i + j * N] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Replace this code with your implementation.\n  // You may use the following variables:\n  //   int i, j: the row and column of the pixel being processed\n  //   int N: the width and height of the image\n  //   int imageIn[N][N]: the input image\n  //   int imageOut[N][N]: the output image\n  //   int edgeKernel[3][3]: the edge kernel\n  //   int sum: the sum of the dot product of the pixel and its neighbors\n  //   int pixel: the value of the pixel being processed\n  //   int neighbor: the value of a neighboring pixel\n  //   int neighborX, neighborY: the row and column of a neighboring pixel\n  //   int neighborIndex: the index of a neighboring pixel\n  //   int neighborRow, neighborCol: the row and column of a neighboring pixel\n  //   int neighborRowIndex, neighborColIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //   int neighborRowOffsetIndex, neighborColOffsetIndex: the index of a neighboring pixel\n  //   int neighborRowOffset, neighborColOffset: the offset of a neighboring pixel\n  //",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    if (x >= N || y >= N) return;\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int xx = x + i;\n            int yy = y + j;\n            if (xx >= 0 && xx < N && yy >= 0 && yy < N) {\n                sum += imageIn[xx + yy * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[x + y * N] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int sum = 0;\n  if (i >= N || j >= N) {\n    return;\n  }\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      int ii = i + k;\n      int jj = j + l;\n      if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n        sum += imageIn[ii * N + jj] * edgeKernel[k + 1][l + 1];\n      }\n    }\n  }\n  imageOut[i * N + j] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = 0; k < 3; k++) {\n            for (int l = 0; l < 3; l++) {\n                int x = i + k - 1;\n                int y = j + l - 1;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k][l];\n                }\n            }\n        }\n        imageOut[i * N + j] = sum < 0? 0 : sum > 255? 255 : sum;\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (x >= N || y >= N) {\n        return;\n    }\n\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int xi = x + i;\n            int yj = y + j;\n            if (xi < 0 || xi >= N || yj < 0 || yj >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[xi + yj * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n\n    imageOut[x + y * N] = min(max(sum, 0), 255);\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) {\n    return;\n  }\n\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int x_ = x + i;\n      int y_ = y + j;\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x_ + y_ * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n\n  imageOut[x + y * N] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int x1 = x + i;\n            int y1 = y + j;\n            if (x1 >= 0 && x1 < N && y1 >= 0 && y1 < N) {\n                sum += imageIn[x1 + y1 * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[x + y * N] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = y * N + x;\n  int sum = 0;\n  for (int i = 0; i < 3; i++) {\n    for (int j = 0; j < 3; j++) {\n      int x_ = x - 1 + j;\n      int y_ = y - 1 + i;\n      if (x_ >= 0 && x_ < N && y_ >= 0 && y_ < N) {\n        sum += imageIn[y_ * N + x_] * edgeKernel[i][j];\n      }\n    }\n  }\n  imageOut[index] = sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) {\n    return;\n  }\n\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int xi = x + i;\n      int yj = y + j;\n      if (xi < 0 || xi >= N || yj < 0 || yj >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[xi + yj * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n  imageOut[x + y * N] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (x >= N || y >= N) {\n        return;\n    }\n\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int x_ = x + i;\n            int y_ = y + j;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x_ + y_ * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n\n    imageOut[x + y * N] = max(0, min(255, sum));\n}",
            "// TODO: Replace this with your code\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = x + y * N;\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int x_ = x + i;\n      int y_ = y + j;\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x_ + y_ * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n  imageOut[index] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Replace this code with your own implementation\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int index = x + y * N;\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int x_ = x + i;\n      int y_ = y + j;\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x_ + y_ * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n  imageOut[index] = sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x >= N || y >= N) {\n    return;\n  }\n\n  int sum = 0;\n  for (int i = -1; i <= 1; i++) {\n    for (int j = -1; j <= 1; j++) {\n      int x_ = x + i;\n      int y_ = y + j;\n      if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[x_ + y_ * N] * edgeKernel[i + 1][j + 1];\n      }\n    }\n  }\n\n  imageOut[x + y * N] = sum > 255? 255 : sum;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int sum = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      int ii = i + k;\n      int jj = j + l;\n      if (ii < 0 || ii >= N || jj < 0 || jj >= N) {\n        sum += 0;\n      } else {\n        sum += imageIn[ii * N + jj] * edgeKernel[k + 1][l + 1];\n      }\n    }\n  }\n  imageOut[i * N + j] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    int index = y * N + x;\n    int sum = 0;\n    if (x >= N || y >= N) {\n        imageOut[index] = 0;\n        return;\n    }\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int x_ = x + i;\n            int y_ = y + j;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[y_ * N + x_] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[index] = sum > 255? 255 : sum;\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *imageIn_rank = new int[N*N];\n    int *imageOut_rank = new int[N*N];\n    int *imageOut_rank_temp = new int[N*N];\n\n    MPI_Scatter(imageIn.data(), N*N, MPI_INT, imageIn_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x*N + y] * edgeKernel[k+1][l+1];\n                    }\n                }\n            }\n            imageOut_rank[i*N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(imageOut_rank, N*N, MPI_INT, imageOut_rank_temp, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < N*N; i++) {\n            imageOut[i] = imageOut_rank_temp[i];\n        }\n    }\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n    delete[] imageOut_rank_temp;\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n    int sum = 0;\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    if (i - 1 + k < 0 || i - 1 + k >= N || j - 1 + l < 0 || j - 1 + l >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[i - 1 + k + (j - 1 + l) * N] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut[i + j * N] = std::max(0, std::min(sum, 255));\n        }\n    }\n    MPI_Gather(imageOut.data() + start, N, MPI_INT, imageOut.data(), N, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int imageSize = N * N;\n    int imageSizePerRank = imageSize / size;\n    int imageSizeRemainder = imageSize % size;\n\n    int imageSizePerRankWithRemainder = imageSizePerRank + 1;\n\n    int imageSizePerRankWithoutRemainder = imageSizePerRank;\n\n    int imageSizePerRankWithRemainderStart = rank * imageSizePerRankWithRemainder;\n    int imageSizePerRankWithoutRemainderStart = rank * imageSizePerRankWithoutRemainder;\n\n    int imageSizePerRankWithRemainderEnd = imageSizePerRankWithRemainderStart + imageSizePerRankWithRemainder;\n    int imageSizePerRankWithoutRemainderEnd = imageSizePerRankWithoutRemainderStart + imageSizePerRankWithoutRemainder;\n\n    int imageSizePerRankWithRemainderEndLastRank = imageSizePerRankWithRemainderEnd + imageSizeRemainder;\n\n    int imageSizePerRankWithoutRemainderEndLastRank = imageSizePerRankWithoutRemainderEnd + imageSizeRemainder;\n\n    std::vector<int> imageInPerRank(imageSizePerRankWithRemainder);\n    std::vector<int> imageOutPerRank(imageSizePerRankWithRemainder);\n\n    if (rank == 0) {\n        imageOut.resize(imageSize);\n    }\n\n    MPI_Scatter(imageIn.data(), imageSizePerRankWithRemainder, MPI_INT, imageInPerRank.data(), imageSizePerRankWithRemainder, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < imageSizePerRankWithRemainder; i++) {\n        imageOutPerRank[i] = 0;\n    }\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int index = i * N + j;\n            int indexPerRank = index - imageSizePerRankWithoutRemainderStart;\n            int indexPerRankWithRemainder = index - imageSizePerRankWithRemainderStart;\n            int indexPerRankWithoutRemainder = index - imageSizePerRankWithoutRemainderStart;\n\n            int sum = 0;\n\n            if (indexPerRankWithRemainder >= 0 && indexPerRankWithRemainder < imageSizePerRankWithRemainder) {\n                sum += imageInPerRank[indexPerRankWithRemainder] * edgeKernel[0][0];\n            }\n\n            if (indexPerRankWithRemainder >= 1 && indexPerRankWithRemainder < imageSizePerRankWithRemainder - 1) {\n                sum += imageInPerRank[indexPerRankWithRemainder - 1] * edgeKernel[0][1];\n                sum += imageInPerRank[indexPerRankWithRemainder + 1] * edgeKernel[0][2];\n            }\n\n            if (indexPerRankWithRemainder >= 2 && indexPerRankWithRemainder < imageSizePerRankWithRemainder - 2) {\n                sum += imageInPerRank[indexPerRankWithRemainder - 2] * edgeKernel[1][0];\n                sum += imageInPerRank[indexPerRankWithRemainder - 1] * edgeKernel[1][1];\n                sum += imageInPerRank[indexPerRankWithRemainder + 1] * edgeKernel[1][2];\n                sum += imageInPerRank[indexPerRankWithRemainder + 2] * edgeKernel[2][0];\n            }\n\n            if (indexPerRankWithRemainder >= 3 && indexPerRankWithRemainder < imageSizePerRankWithRemain",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here\n    // You may assume that imageIn and imageOut have the same size\n    // You may assume that imageIn and imageOut are square\n    // You may assume that N is the size of the image\n    // You may assume that imageIn and imageOut are row-major\n    // You may assume that imageIn and imageOut are grayscale\n    // You may assume that imageIn and imageOut are stored as a vector of ints\n    // You may assume that imageIn and imageOut are stored in rank 0\n    // You may assume that the edge kernel is stored in edgeKernel\n\n    // TODO: Your code here",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *imageIn_rank = new int[N*N];\n    int *imageOut_rank = new int[N*N];\n\n    MPI_Scatter(imageIn.data(), N*N, MPI_INT, imageIn_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x*N + y] * edgeKernel[k+1][l+1];\n                    }\n                }\n            }\n            imageOut_rank[i*N + j] = std::max(0, std::min(sum, 255));\n        }\n    }\n\n    MPI_Gather(imageOut_rank, N*N, MPI_INT, imageOut.data(), N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_remainder = N % size;\n\n    int start_row = rank * N_per_rank;\n    int end_row = (rank + 1) * N_per_rank;\n    if (rank == size - 1) {\n        end_row += N_remainder;\n    }\n\n    std::vector<int> local_image_in(N_per_rank * N, 0);\n    std::vector<int> local_image_out(N_per_rank * N, 0);\n\n    for (int i = 0; i < N_per_rank; i++) {\n        for (int j = 0; j < N; j++) {\n            local_image_in[i * N + j] = imageIn[(start_row + i) * N + j];\n        }\n    }\n\n    for (int i = 0; i < N_per_rank; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int row = i + k - 1;\n                    int col = j + l - 1;\n                    if (row < 0 || row >= N_per_rank || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += local_image_in[row * N + col] * edgeKernel[k][l];\n                    }\n                }\n            }\n            local_image_out[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < N_per_rank; i++) {\n            for (int j = 0; j < N; j++) {\n                imageOut[(start_row + i) * N + j] = local_image_out[i * N + j];\n            }\n        }\n    } else {\n        MPI_Send(local_image_out.data(), N_per_rank * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(local_image_in.data(), N_per_rank * N, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int i = 0; i < N_per_rank; i++) {\n            for (int j = 0; j < N; j++) {\n                imageOut[(start_row + i) * N + j] = local_image_in[i * N + j];\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n    // You may assume that N is even\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_remainder = N % size;\n\n    int N_per_rank_p1 = N_per_rank + 1;\n    int N_per_rank_m1 = N_per_rank - 1;\n\n    int N_per_rank_p1_remainder = N_per_rank_p1 + N_remainder;\n    int N_per_rank_m1_remainder = N_per_rank_m1 + N_remainder;\n\n    int N_per_rank_p1_remainder_p1 = N_per_rank_p1_remainder + 1;\n    int N_per_rank_m1_remainder_m1 = N_per_rank_m1_remainder - 1;\n\n    int N_per_rank_p1_remainder_p1_m1 = N_per_rank_p1_remainder_p1 - 1;\n    int N_per_rank_m1_remainder_m1_p1 = N_per_rank_m1_remainder_m1 + 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1 = N_per_rank_p1_remainder_p1_m1 + 1;\n    int N_per_rank_m1_remainder_m1_p1_m1 = N_per_rank_m1_remainder_m1_p1 - 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1_m1 = N_per_rank_p1_remainder_p1_m1_p1 - 1;\n    int N_per_rank_m1_remainder_m1_p1_m1_p1 = N_per_rank_m1_remainder_m1_p1_m1 + 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1_m1_p1 = N_per_rank_p1_remainder_p1_m1_p1_m1 + 1;\n    int N_per_rank_m1_remainder_m1_p1_m1_p1_m1 = N_per_rank_m1_remainder_m1_p1_m1_p1 - 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1_m1_p1_m1 = N_per_rank_p1_remainder_p1_m1_p1_m1_p1 - 1;\n    int N_per_rank_m1_remainder_m1_p1_m1_p1_m1_m1 = N_per_rank_m1_remainder_m1_p1_m1_p1_m1 + 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1_m1_p1_m1_m1 = N_per_rank_p1_remainder_p1_m1_p1_m1_p1_m1 - 1;\n    int N_per_rank_m1_remainder_m1_p1_m1_p1_m1_m1_p1 = N_per_rank_m1_remainder_m1_p1_m1_p1_m1_m1 + 1;\n\n    int N_per_rank_p1_remainder_p1_m1_p1_m1_p1_m1_m1_p1 = N_per_rank_p1_remainder_p1_m1_p1_m1_p1_m1_m1 +",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int* imageIn_ = new int[N*N];\n    int* imageOut_ = new int[N*N];\n    for (int i = 0; i < N*N; i++) {\n        imageIn_[i] = imageIn[i];\n        imageOut_[i] = 0;\n    }\n\n    int* imageIn_temp = new int[N*N];\n    int* imageOut_temp = new int[N*N];\n\n    int* imageIn_temp_ = new int[N*N];\n    int* imageOut_temp_ = new int[N*N];\n\n    int* imageIn_temp__ = new int[N*N];\n    int* imageOut_temp__ = new int[N*N];\n\n    int* imageIn_temp___ = new int[N*N];\n    int* imageOut_temp___ = new int[N*N];\n\n    int* imageIn_temp____ = new int[N*N];\n    int* imageOut_temp____ = new int[N*N];\n\n    int* imageIn_temp_____ = new int[N*N];\n    int* imageOut_temp_____ = new int[N*N];\n\n    int* imageIn_temp______ = new int[N*N];\n    int* imageOut_temp______ = new int[N*N];\n\n    int* imageIn_temp_______ = new int[N*N];\n    int* imageOut_temp_______ = new int[N*N];\n\n    int* imageIn_temp________ = new int[N*N];\n    int* imageOut_temp________ = new int[N*N];\n\n    int* imageIn_temp_________ = new int[N*N];\n    int* imageOut_temp_________ = new int[N*N];\n\n    int* imageIn_temp__________ = new int[N*N];\n    int* imageOut_temp__________ = new int[N*N];\n\n    int* imageIn_temp___________ = new int[N*N];\n    int* imageOut_temp___________ = new int[N*N];\n\n    int* imageIn_temp____________ = new int[N*N];\n    int* imageOut_temp____________ = new int[N*N];\n\n    int* imageIn_temp_____________ = new int[N*N];\n    int* imageOut_temp_____________ = new int[N*N];\n\n    int* imageIn_temp______________ = new int[N*N];\n    int* imageOut_temp______________ = new int[N*N];\n\n    int* imageIn_temp_______________ = new int[N*N];\n    int* imageOut_temp_______________ = new int[N*N];\n\n    int* imageIn_temp________________ = new int[N*N];\n    int* imageOut_temp________________ = new int[N*N];\n\n    int* imageIn_temp_________________ = new int[N*N];\n    int* imageOut_temp_________________ = new int[N*N];\n\n    int* imageIn_temp__________________ = new int[N*N];\n    int* imageOut_temp__________________ = new int[N*N];\n\n    int* imageIn_temp___________________ = new int[N*N];\n    int* imageOut_temp___________________ = new int[N*N];\n\n    int* imageIn_temp____________________ = new int[N*N];\n    int* imageOut_temp____________________ = new int[N*N];\n\n    int* imageIn_temp_____________________ = new int[N*N];\n    int* imageOut_temp_____________________ = new int[N*N];\n\n    int* imageIn_temp______________________ = new int[N*N];\n    int* imageOut_temp______________________ = new int[N*N];\n\n    int* imageIn_temp_______________________ =",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n    // Hint: You may want to use MPI_Scatter and MPI_Gather\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int* imageIn_rank = new int[N*N];\n    int* imageOut_rank = new int[N*N];\n\n    MPI_Scatter(imageIn.data(), N*N, MPI_INT, imageIn_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x*N + y] * edgeKernel[k+1][l+1];\n                    }\n                }\n            }\n            imageOut_rank[i*N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(imageOut_rank, N*N, MPI_INT, imageOut.data(), N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_proc = N / size;\n    int N_rem = N % size;\n\n    int N_per_proc_plus = N_per_proc + 1;\n    int N_rem_plus = N_rem + 1;\n\n    int N_per_proc_minus = N_per_proc - 1;\n    int N_rem_minus = N_rem - 1;\n\n    int N_per_proc_plus_rem = N_per_proc + N_rem;\n    int N_per_proc_minus_rem = N_per_proc - N_rem;\n\n    int N_per_proc_plus_rem_plus = N_per_proc + N_rem + 1;\n    int N_per_proc_minus_rem_minus = N_per_proc - N_rem - 1;\n\n    int N_per_proc_plus_rem_minus = N_per_proc + N_rem - 1;\n    int N_per_proc_minus_rem_plus = N_per_proc - N_rem + 1;\n\n    int N_per_proc_plus_rem_plus_rem = N_per_proc + N_rem + N_rem;\n    int N_per_proc_minus_rem_minus_rem = N_per_proc - N_rem - N_rem;\n\n    int N_per_proc_plus_rem_minus_rem = N_per_proc + N_rem - N_rem;\n    int N_per_proc_minus_rem_plus_rem = N_per_proc - N_rem + N_rem;\n\n    int N_per_proc_plus_rem_plus_rem_plus = N_per_proc + N_rem + N_rem + 1;\n    int N_per_proc_minus_rem_minus_rem_minus = N_per_proc - N_rem - N_rem - 1;\n\n    int N_per_proc_plus_rem_plus_rem_minus = N_per_proc + N_rem + N_rem - 1;\n    int N_per_proc_minus_rem_minus_rem_plus = N_per_proc - N_rem - N_rem + 1;\n\n    int N_per_proc_plus_rem_minus_rem_plus = N_per_proc + N_rem - N_rem + 1;\n    int N_per_proc_minus_rem_plus_rem_minus = N_per_proc - N_rem + N_rem - 1;\n\n    int N_per_proc_plus_rem_minus_rem_minus = N_per_proc + N_rem - N_rem - 1;\n    int N_per_proc_minus_rem_minus_rem_plus = N_per_proc - N_rem - N_rem + 1;\n\n    int N_per_proc_plus_rem_minus_rem_minus_rem = N_per_proc + N_rem - N_rem - N_rem;\n    int N_per_proc_minus_rem_minus_rem_minus_rem = N_per_proc - N_rem - N_rem - N_rem;\n\n    int N_per_proc_plus_rem_minus_rem_minus_rem_minus = N_per_proc + N_rem - N_rem - N_rem - 1;\n    int N_per_proc_minus_rem_minus_rem_minus_rem_minus = N_per_proc - N_rem - N_rem - N_rem - 1;\n\n    int N_per_proc_plus_rem_minus_rem_minus_rem_minus_rem = N_per_proc + N_rem - N_rem - N_rem - N_rem;\n    int N_per_proc_minus_rem_minus_rem_minus_rem_minus_rem = N_per_proc - N_rem - N_rem - N_rem - N_rem;\n\n    int N_per_proc_plus_rem_minus_rem_minus_rem_minus_rem",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *imageIn_rank = new int[N*N];\n    int *imageOut_rank = new int[N*N];\n\n    if (rank == 0) {\n        for (int i = 0; i < N*N; i++) {\n            imageIn_rank[i] = imageIn[i];\n        }\n    }\n\n    MPI_Bcast(imageIn_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x*N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut_rank[i*N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n\n    MPI_Gather(imageOut_rank, N*N, MPI_INT, imageOut.data(), N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_proc = N / size;\n    int N_rem = N % size;\n\n    int start_row = rank * N_per_proc;\n    int end_row = start_row + N_per_proc;\n\n    if (rank == 0) {\n        end_row += N_rem;\n    }\n\n    std::vector<int> image_in_proc(N_per_proc * N);\n    std::vector<int> image_out_proc(N_per_proc * N);\n\n    for (int i = 0; i < N_per_proc; i++) {\n        for (int j = 0; j < N; j++) {\n            image_in_proc[i * N + j] = imageIn[(start_row + i) * N + j];\n        }\n    }\n\n    for (int i = 0; i < N_per_proc; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int row = i + k - 1;\n                    int col = j + l - 1;\n                    if (row < 0 || row >= N_per_proc || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += image_in_proc[row * N + col] * edgeKernel[k][l];\n                    }\n                }\n            }\n            image_out_proc[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < N_per_proc; i++) {\n            for (int j = 0; j < N; j++) {\n                imageOut[(start_row + i) * N + j] = image_out_proc[i * N + j];\n            }\n        }\n    } else {\n        MPI_Send(&image_out_proc[0], N_per_proc * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&imageOut[end_row * N], N_per_proc * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            end_row += N_per_proc;\n            if (i < size - 1) {\n                end_row += N_rem;\n            }\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n\n    // TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "cuda",
        "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = 0; k < 3; k++) {\n            for (int l = 0; l < 3; l++) {\n                int x = i + k - 1;\n                int y = j + l - 1;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k][l];\n                } else {\n                    sum += 0 * edgeKernel[k][l];\n                }\n            }\n        }\n        imageOut[i * N + j] = sum < 0? 0 : sum > 255? 255 : sum;\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    if (i < N && j < N) {\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                int x_ = i + x;\n                int y_ = j + y;\n                if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                    sum += 0;\n                } else {\n                    sum += imageIn[x_ * N + y_] * edgeKernel[x + 1][y + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = sum > 255? 255 : sum;\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (x >= N || y >= N) {\n        return;\n    }\n\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int xi = x + i;\n            int yj = y + j;\n            if (xi < 0 || xi >= N || yj < 0 || yj >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[xi + yj * N] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n\n    imageOut[x + y * N] = max(0, min(sum, 255));\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    int k, l;\n    if (i < N && j < N) {\n        for (k = -1; k <= 1; k++) {\n            for (l = -1; l <= 1; l++) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int idx = i + j * N;\n    int sum = 0;\n    for (int k = 0; k < 3; k++) {\n        for (int l = 0; l < 3; l++) {\n            int ii = i + k - 1;\n            int jj = j + l - 1;\n            if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n                sum += imageIn[ii + jj * N] * edgeKernel[k][l];\n            }\n        }\n    }\n    imageOut[idx] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    int index = y * N + x;\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int x_ = x + j;\n            int y_ = y + i;\n            if (x_ >= 0 && x_ < N && y_ >= 0 && y_ < N) {\n                sum += imageIn[y_ * N + x_] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[index] = sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int index = i + j * N;\n    int sum = 0;\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            int x = i + k;\n            int y = j + l;\n            if (x >= 0 && x < N && y >= 0 && y < N) {\n                sum += imageIn[x + y * N] * edgeKernel[k + 1][l + 1];\n            }\n        }\n    }\n    imageOut[index] = max(0, min(sum, 255));\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = sum < 0? 0 : sum > 255? 255 : sum;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    int sum = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            int x_ = i + x;\n            int y_ = j + y;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x_ + y_ * N] * edgeKernel[x + 1][y + 1];\n            }\n        }\n    }\n    imageOut[i + j * N] = sum < 0? 0 : sum > 255? 255 : sum;\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    int count = 0;\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            int x = i + k;\n            int y = j + l;\n            if (x >= 0 && x < N && y >= 0 && y < N) {\n                sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                count++;\n            }\n        }\n    }\n    imageOut[i * N + j] = sum / count;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            int x = i + k;\n            int y = j + l;\n            if (x < 0 || x >= N || y < 0 || y >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x + y * N] * edgeKernel[k + 1][l + 1];\n            }\n        }\n    }\n    imageOut[i + j * N] = sum > 255? 255 : sum;\n}",
            "// TODO: Fill this in\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            int x_ = x + i;\n            int y_ = y + j;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x_ * N + y_] * edgeKernel[x + 1][y + 1];\n            }\n        }\n    }\n    imageOut[i * N + j] = max(0, min(255, sum));\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    if (i < N && j < N) {\n        for (int k = 0; k < 3; k++) {\n            for (int l = 0; l < 3; l++) {\n                int x = i - 1 + k;\n                int y = j - 1 + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k][l];\n                } else {\n                    sum += 0 * edgeKernel[k][l];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}",
            "// TODO: Fill this in\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    if (x >= N || y >= N) return;\n    int sum = 0;\n    for (int i = -1; i <= 1; i++) {\n        for (int j = -1; j <= 1; j++) {\n            int x_ = x + i;\n            int y_ = y + j;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x_ * N + y_] * edgeKernel[i + 1][j + 1];\n            }\n        }\n    }\n    imageOut[x * N + y] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "// TODO: Implement this function\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    int i = y * N + x;\n    int sum = 0;\n    if (x >= N || y >= N) return;\n    for (int j = 0; j < 3; j++) {\n        for (int k = 0; k < 3; k++) {\n            int x_ = x + k - 1;\n            int y_ = y + j - 1;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[y_ * N + x_] * edgeKernel[j][k];\n            }\n        }\n    }\n    imageOut[i] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            int x_ = i + x;\n            int y_ = j + y;\n            if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x_ * N + y_] * edgeKernel[x + 1][y + 1];\n            }\n        }\n    }\n    imageOut[i * N + j] = sum > 255? 255 : sum < 0? 0 : sum;\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int sum = 0;\n    int index = i * N + j;\n    if (i >= N || j >= N) {\n        imageOut[index] = 0;\n        return;\n    }\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            int x = i + k;\n            int y = j + l;\n            if (x < 0 || x >= N || y < 0 || y >= N) {\n                sum += 0;\n            } else {\n                sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n            }\n        }\n    }\n    imageOut[index] = sum > 255? 255 : sum;\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (x < N && y < N) {\n        int sum = 0;\n        for (int i = -1; i <= 1; i++) {\n            for (int j = -1; j <= 1; j++) {\n                int x_ = x + i;\n                int y_ = y + j;\n                if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                    sum += 0;\n                } else {\n                    sum += imageIn[y_ * N + x_] * edgeKernel[i + 1][j + 1];\n                }\n            }\n        }\n        imageOut[y * N + x] = min(max(sum, 0), 255);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int sum = 0;\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        int ii = i + x;\n        int jj = j + y;\n        if (ii < 0 || jj < 0 || ii >= N || jj >= N) {\n          sum += 0;\n        } else {\n          sum += imageIn[ii * N + jj] * edgeKernel[x + 1][y + 1];\n        }\n      }\n    }\n    imageOut[i * N + j] = max(0, min(sum, 255));\n  }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "48_stencil_edge_kernel",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int const n = N;\n    int const n_local = n / size;\n    int const n_local_padded = n_local + 2;\n    int const n_local_padded_sq = n_local_padded * n_local_padded;\n\n    std::vector<int> image_local(n_local_padded_sq);\n    std::vector<int> image_local_out(n_local_padded_sq);\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local_padded_sq; i++) {\n            image_local[i] = imageIn[i];\n        }\n    }\n\n    MPI_Bcast(image_local.data(), n_local_padded_sq, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int const start_row = rank * n_local;\n    int const end_row = start_row + n_local;\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_local_padded; i++) {\n        for (int j = 0; j < n_local_padded; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int row = i + k - 1;\n                    int col = j + l - 1;\n                    if (row < 0 || row >= n_local_padded || col < 0 || col >= n_local_padded) {\n                        sum += 0;\n                    } else {\n                        sum += image_local[row * n_local_padded + col] * edgeKernel[k][l];\n                    }\n                }\n            }\n            image_local_out[i * n_local_padded + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(image_local_out.data(), n_local_padded_sq, MPI_INT, imageOut.data(), n_local_padded_sq, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N / size;\n    int n_rem = N % size;\n\n    int *imageIn_rank = new int[n * n];\n    int *imageOut_rank = new int[n * n];\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                imageIn_rank[i * n + j] = imageIn[i * N + j];\n            }\n        }\n    }\n\n    MPI_Scatter(imageIn_rank, n * n, MPI_INT, imageOut_rank, n * n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= n || y < 0 || y >= n) {\n                        sum += 0;\n                    } else {\n                        sum += imageOut_rank[x * n + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut_rank[i * n + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(imageOut_rank, n * n, MPI_INT, imageIn_rank, n * n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                imageOut[i * n + j] = imageIn_rank[i * n + j];\n            }\n        }\n        for (int i = 0; i < n_rem; i++) {\n            for (int j = 0; j < n_rem; j++) {\n                imageOut[n * n + i * N + j] = imageIn[n * n + i * N + j];\n            }\n        }\n    }\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n    if (rank == size - 1) {\n        end = N;\n    }\n    std::vector<int> localImageIn(imageIn.begin() + start * N, imageIn.begin() + end * N);\n    std::vector<int> localImageOut(localImageIn.size());\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = 0; k < 3; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int x = i - 1 + k;\n                    int y = j - 1 + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n    MPI_Gather(localImageOut.data(), N * (end - start), MPI_INT, imageOut.data(), N * (end - start), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n\n    int *imageIn_rank = new int[N * N];\n    int *imageOut_rank = new int[N * N];\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            imageIn_rank[i * N + j] = imageIn[i * N + j];\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut_rank[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                imageOut[i * N + j] = imageOut_rank[i * N + j];\n            }\n        }\n    }\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int i, j, k, l, m, n;\n    int x, y, z, w, v;\n    int a, b, c, d, e;\n    int sum;\n    int temp[N][N];\n    int temp2[N][N];\n    int temp3[N][N];\n    int temp4[N][N];\n    int temp5[N][N];\n    int temp6[N][N];\n    int temp7[N][N];\n    int temp8[N][N];\n    int temp9[N][N];\n    int temp10[N][N];\n    int temp11[N][N];\n    int temp12[N][N];\n    int temp13[N][N];\n    int temp14[N][N];\n    int temp15[N][N];\n    int temp16[N][N];\n    int temp17[N][N];\n    int temp18[N][N];\n    int temp19[N][N];\n    int temp20[N][N];\n    int temp21[N][N];\n    int temp22[N][N];\n    int temp23[N][N];\n    int temp24[N][N];\n    int temp25[N][N];\n    int temp26[N][N];\n    int temp27[N][N];\n    int temp28[N][N];\n    int temp29[N][N];\n    int temp30[N][N];\n    int temp31[N][N];\n    int temp32[N][N];\n    int temp33[N][N];\n    int temp34[N][N];\n    int temp35[N][N];\n    int temp36[N][N];\n    int temp37[N][N];\n    int temp38[N][N];\n    int temp39[N][N];\n    int temp40[N][N];\n    int temp41[N][N];\n    int temp42[N][N];\n    int temp43[N][N];\n    int temp44[N][N];\n    int temp45[N][N];\n    int temp46[N][N];\n    int temp47[N][N];\n    int temp48[N][N];\n    int temp49[N][N];\n    int temp50[N][N];\n    int temp51[N][N];\n    int temp52[N][N];\n    int temp53[N][N];\n    int temp54[N][N];\n    int temp55[N][N];\n    int temp56[N][N];\n    int temp57[N][N];\n    int temp58[N][N];\n    int temp59[N][N];\n    int temp60[N][N];\n    int temp61[N][N];\n    int temp62[N][N];\n    int temp63[N][N];\n    int temp64[N][N];\n    int temp65[N][N];\n    int temp66[N][N];\n    int temp67[N][N];\n    int temp68[N][N];\n    int temp69[N][N];\n    int temp70[N][N];\n    int temp71[N][N];\n    int temp72[N][N];\n    int temp73[N][N];\n    int temp74[N][N];\n    int temp75[N][N];\n    int temp76[N][N];\n    int temp77[N][N];\n    int temp78[N][N];\n    int temp79[N][N];\n    int temp80[N][N];\n    int temp81[N][N];\n    int temp82[N][N];\n    int temp83[N][N];\n    int temp84[N][N];\n    int temp85",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n\n    std::vector<int> localImageIn(imageIn.begin() + start * N, imageIn.begin() + end * N);\n    std::vector<int> localImageOut(localImageIn.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < localImageIn.size(); i++) {\n        int row = i / N;\n        int col = i % N;\n\n        int sum = 0;\n        for (int r = -1; r <= 1; r++) {\n            for (int c = -1; c <= 1; c++) {\n                int row2 = row + r;\n                int col2 = col + c;\n\n                if (row2 < 0 || row2 >= N || col2 < 0 || col2 >= N) {\n                    sum += 0;\n                } else {\n                    sum += localImageIn[row2 * N + col2] * edgeKernel[r + 1][c + 1];\n                }\n            }\n        }\n\n        localImageOut[i] = std::min(std::max(sum, 0), 255);\n    }\n\n    if (rank == 0) {\n        imageOut.resize(imageIn.size());\n    }\n\n    MPI_Gather(localImageOut.data(), localImageOut.size(), MPI_INT, imageOut.data(), localImageOut.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num_threads;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n    int num_rows = N / size;\n    int num_cols = N;\n    int num_rows_per_thread = num_rows / num_threads;\n    int num_cols_per_thread = num_cols;\n    int start_row = rank * num_rows_per_thread;\n    int end_row = (rank + 1) * num_rows_per_thread;\n    int start_col = 0;\n    int end_col = num_cols;\n    if (rank == 0) {\n        start_row = 0;\n        end_row = num_rows;\n    }\n    if (rank == size - 1) {\n        end_row = num_rows;\n    }\n    if (rank == 0) {\n        start_col = 0;\n        end_col = num_cols / 2;\n    }\n    if (rank == size - 1) {\n        start_col = num_cols / 2;\n        end_col = num_cols;\n    }\n    int num_pixels = num_rows_per_thread * num_cols;\n    std::vector<int> image_in_thread(num_pixels);\n    std::vector<int> image_out_thread(num_pixels);\n    for (int i = start_row; i < end_row; i++) {\n        for (int j = start_col; j < end_col; j++) {\n            int index = i * num_cols + j;\n            image_in_thread[index] = imageIn[index];\n        }\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < num_pixels; i++) {\n        int sum = 0;\n        for (int j = 0; j < 3; j++) {\n            for (int k = 0; k < 3; k++) {\n                int row = i / num_cols_per_thread + j - 1;\n                int col = i % num_cols_per_thread + k - 1;\n                if (row >= 0 && row < num_rows_per_thread && col >= 0 && col < num_cols_per_thread) {\n                    int index = row * num_cols_per_thread + col;\n                    sum += image_in_thread[index] * edgeKernel[j][k];\n                }\n            }\n        }\n        image_out_thread[i] = std::max(0, std::min(255, sum));\n    }\n    MPI_Gather(image_out_thread.data(), num_pixels, MPI_INT, imageOut.data(), num_pixels, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N / size;\n    int m = N % size;\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                int sum = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        int x = i + k;\n                        int y = j + l;\n                        if (x < 0 || x >= n || y < 0 || y >= n) {\n                            sum += 0;\n                        } else {\n                            sum += imageIn[x * n + y];\n                        }\n                    }\n                }\n                imageOut[i * n + j] = std::min(255, std::max(0, sum));\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < m; i++) {\n            for (int j = 0; j < n; j++) {\n                int sum = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        int x = i + k;\n                        int y = j + l;\n                        if (x < 0 || x >= m || y < 0 || y >= n) {\n                            sum += 0;\n                        } else {\n                            sum += imageIn[x * n + y];\n                        }\n                    }\n                }\n                imageOut[i * n + j] = std::min(255, std::max(0, sum));\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < m; j++) {\n                int sum = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        int x = i + k;\n                        int y = j + l;\n                        if (x < 0 || x >= n || y < 0 || y >= m) {\n                            sum += 0;\n                        } else {\n                            sum += imageIn[x * n + y];\n                        }\n                    }\n                }\n                imageOut[i * n + j] = std::min(255, std::max(0, sum));\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < m; i++) {\n            for (int j = 0; j < m; j++) {\n                int sum = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        int x = i + k;\n                        int y = j + l;\n                        if (x < 0 || x >= m || y < 0 || y >= m) {\n                            sum += 0;\n                        } else {\n                            sum += imageIn[x * n + y];\n                        }\n                    }\n                }\n                imageOut[i * n + j] = std::min(255, std::max(0, sum));\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_remainder = N % size;\n\n    int N_per_rank_rank0 = N_per_rank + N_remainder;\n    int N_per_rank_other_ranks = N_per_rank;\n\n    int N_per_rank_rank0_start = 0;\n    int N_per_rank_rank0_end = N_per_rank_rank0 - 1;\n    int N_per_rank_other_ranks_start = N_per_rank_rank0;\n    int N_per_rank_other_ranks_end = N_per_rank_rank0 + N_per_rank_other_ranks - 1;\n\n    int N_per_rank_rank0_start_row = N_per_rank_rank0_start / N;\n    int N_per_rank_rank0_end_row = N_per_rank_rank0_end / N;\n    int N_per_rank_other_ranks_start_row = N_per_rank_other_ranks_start / N;\n    int N_per_rank_other_ranks_end_row = N_per_rank_other_ranks_end / N;\n\n    int N_per_rank_rank0_start_col = N_per_rank_rank0_start % N;\n    int N_per_rank_rank0_end_col = N_per_rank_rank0_end % N;\n    int N_per_rank_other_ranks_start_col = N_per_rank_other_ranks_start % N;\n    int N_per_rank_other_ranks_end_col = N_per_rank_other_ranks_end % N;\n\n    int N_per_rank_rank0_start_row_offset = N_per_rank_rank0_start_row * N;\n    int N_per_rank_rank0_end_row_offset = N_per_rank_rank0_end_row * N;\n    int N_per_rank_other_ranks_start_row_offset = N_per_rank_other_ranks_start_row * N;\n    int N_per_rank_other_ranks_end_row_offset = N_per_rank_other_ranks_end_row * N;\n\n    int N_per_rank_rank0_start_col_offset = N_per_rank_rank0_start_col;\n    int N_per_rank_rank0_end_col_offset = N_per_rank_rank0_end_col;\n    int N_per_rank_other_ranks_start_col_offset = N_per_rank_other_ranks_start_col;\n    int N_per_rank_other_ranks_end_col_offset = N_per_rank_other_ranks_end_col;\n\n    int N_per_rank_rank0_start_offset = N_per_rank_rank0_start_row_offset + N_per_rank_rank0_start_col_offset;\n    int N_per_rank_rank0_end_offset = N_per_rank_rank0_end_row_offset + N_per_rank_rank0_end_col_offset;\n    int N_per_rank_other_ranks_start_offset = N_per_rank_other_ranks_start_row_offset + N_per_rank_other_ranks_start_col_offset;\n    int N_per_rank_other_ranks_end_offset = N_per_rank_other_ranks_end_row_offset + N_per_rank_other_ranks_end_col_offset;\n\n    int N_per_rank_rank0_start_offset_row = N_per_rank_rank0_start_row_offset;\n    int N_per_rank_rank0_end_offset_row = N_per_rank_rank0_end_row_offset;\n    int N_per",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    int localN_1 = localN - 1;\n    int localN_2 = localN - 2;\n\n    int start = rank * localN;\n    int end = start + localN;\n\n    std::vector<int> localImageIn(localN * localN);\n    std::vector<int> localImageOut(localN * localN);\n\n    for (int i = 0; i < localN; i++) {\n        for (int j = 0; j < localN; j++) {\n            localImageIn[i * localN + j] = imageIn[start + i * N + j];\n        }\n    }\n\n    for (int i = 0; i < localN; i++) {\n        for (int j = 0; j < localN; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x > localN_1 || y < 0 || y > localN_1) {\n                        sum += 0;\n                    } else {\n                        sum += localImageIn[x * localN + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * localN + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < localN; i++) {\n            for (int j = 0; j < localN; j++) {\n                imageOut[i * N + j] = localImageOut[i * localN + j];\n            }\n        }\n    } else {\n        MPI_Send(&localImageOut[0], localN * localN, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&localImageOut[0], localN * localN, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < localN; j++) {\n                for (int k = 0; k < localN; k++) {\n                    imageOut[(i * localN + j) * N + k] = localImageOut[j * localN + k];\n                }\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *imageIn_rank = new int[N*N];\n    int *imageOut_rank = new int[N*N];\n\n    MPI_Scatter(imageIn.data(), N*N, MPI_INT, imageIn_rank, N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (size_t k = 0; k < 3; k++) {\n                for (size_t l = 0; l < 3; l++) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_rank[x*N + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            imageOut_rank[i*N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(imageOut_rank, N*N, MPI_INT, imageOut.data(), N*N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] imageIn_rank;\n    delete[] imageOut_rank;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = N / size;\n    int start = rank * chunkSize;\n    int end = (rank + 1) * chunkSize;\n    if (rank == size - 1) {\n        end = N;\n    }\n    std::vector<int> localImage(N * N);\n    std::vector<int> localImageOut(N * N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            localImage[i * N + j] = imageIn[i * N + j];\n        }\n    }\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += localImage[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = sum;\n        }\n    }\n    MPI_Reduce(localImageOut.data(), imageOut.data(), N * N, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                imageOut[i * N + j] = std::max(0, std::min(255, imageOut[i * N + j]));\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_rem = N % size;\n\n    int N_per_rank_p = N_per_rank + 1;\n    int N_per_rank_m = N_per_rank - 1;\n\n    int N_per_rank_p_rem = N_per_rank_p % size;\n    int N_per_rank_m_rem = N_per_rank_m % size;\n\n    int N_per_rank_p_div = N_per_rank_p / size;\n    int N_per_rank_m_div = N_per_rank_m / size;\n\n    int N_per_rank_p_div_rem = N_per_rank_p_div % size;\n    int N_per_rank_m_div_rem = N_per_rank_m_div % size;\n\n    int N_per_rank_p_div_rem_p = N_per_rank_p_div_rem + 1;\n    int N_per_rank_m_div_rem_p = N_per_rank_m_div_rem + 1;\n\n    int N_per_rank_p_div_rem_m = N_per_rank_p_div_rem - 1;\n    int N_per_rank_m_div_rem_m = N_per_rank_m_div_rem - 1;\n\n    int N_per_rank_p_div_rem_m_rem = N_per_rank_p_div_rem_m % size;\n    int N_per_rank_m_div_rem_m_rem = N_per_rank_m_div_rem_m % size;\n\n    int N_per_rank_p_div_rem_m_div = N_per_rank_p_div_rem_m / size;\n    int N_per_rank_m_div_rem_m_div = N_per_rank_m_div_rem_m / size;\n\n    int N_per_rank_p_div_rem_m_div_rem = N_per_rank_p_div_rem_m_div % size;\n    int N_per_rank_m_div_rem_m_div_rem = N_per_rank_m_div_rem_m_div % size;\n\n    int N_per_rank_p_div_rem_m_div_rem_p = N_per_rank_p_div_rem_m_div_rem + 1;\n    int N_per_rank_m_div_rem_m_div_rem_p = N_per_rank_m_div_rem_m_div_rem + 1;\n\n    int N_per_rank_p_div_rem_m_div_rem_m = N_per_rank_p_div_rem_m_div_rem - 1;\n    int N_per_rank_m_div_rem_m_div_rem_m = N_per_rank_m_div_rem_m_div_rem - 1;\n\n    int N_per_rank_p_div_rem_m_div_rem_m_rem = N_per_rank_p_div_rem_m_div_rem_m % size;\n    int N_per_rank_m_div_rem_m_div_rem_m_rem = N_per_rank_m_div_rem_m_div_rem_m % size;\n\n    int N_per_rank_p_div_rem_m_div_rem_m_div = N_per_rank_p_div_rem_m_div_rem_m / size;\n    int N_per_rank_m_div_rem_m_div_rem_m_div = N_per_rank_m_div_rem_m_div_rem_m / size;\n\n    int N_per_rank_p_div_rem_m_div_rem_m_div_rem = N_per_rank_p_div_rem_m",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n    int *imageIn_ptr = imageIn.data();\n    int *imageOut_ptr = imageOut.data();\n\n    int *imageIn_rank = new int[N * N];\n    int *imageOut_rank = new int[N * N];\n\n    int *imageIn_rank_ptr = imageIn_rank;\n    int *imageOut_rank_ptr = imageOut_rank;\n\n    int *imageIn_rank_ptr_start = imageIn_rank_ptr;\n    int *imageOut_rank_ptr_start = imageOut_rank_ptr;\n\n    int *imageIn_rank_ptr_end = imageIn_rank_ptr + N * N;\n    int *imageOut_rank_ptr_end = imageOut_rank_ptr + N * N;\n\n    int *imageIn_ptr_start = imageIn_ptr;\n    int *imageOut_ptr_start = imageOut_ptr;\n\n    int *imageIn_ptr_end = imageIn_ptr + N * N;\n    int *imageOut_ptr_end = imageOut_ptr + N * N;\n\n    int *imageIn_ptr_start_rank = imageIn_ptr_start;\n    int *imageOut_ptr_start_rank = imageOut_ptr_start;\n\n    int *imageIn_ptr_end_rank = imageIn_ptr_end;\n    int *imageOut_ptr_end_rank = imageOut_ptr_end;\n\n    int *imageIn_ptr_start_rank_rank = imageIn_ptr_start_rank;\n    int *imageOut_ptr_start_rank_rank = imageOut_ptr_start_rank;\n\n    int *imageIn_ptr_end_rank_rank = imageIn_ptr_end_rank;\n    int *imageOut_ptr_end_rank_rank = imageOut_ptr_end_rank;\n\n    int *imageIn_ptr_start_rank_rank_rank = imageIn_ptr_start_rank_rank;\n    int *imageOut_ptr_start_rank_rank_rank = imageOut_ptr_start_rank_rank;\n\n    int *imageIn_ptr_end_rank_rank_rank = imageIn_ptr_end_rank_rank;\n    int *imageOut_ptr_end_rank_rank_rank = imageOut_ptr_end_rank_rank;\n\n    int *imageIn_ptr_start_rank_rank_rank_rank = imageIn_ptr_start_rank_rank_rank;\n    int *imageOut_ptr_start_rank_rank_rank_rank = imageOut_ptr_start_rank_rank_rank;\n\n    int *imageIn_ptr_end_rank_rank_rank_rank = imageIn_ptr_end_rank_rank_rank;\n    int *imageOut_ptr_end_rank_rank_rank_rank = imageOut_ptr_end_rank_rank_rank;\n\n    int *imageIn_ptr_start_rank_rank_rank_rank_rank = imageIn_ptr_start_rank_rank_rank_rank;\n    int *imageOut_ptr_start_rank_rank_rank_rank_rank = imageOut_ptr_start_rank_rank_rank_rank;\n\n    int *imageIn_ptr_end_rank_rank_rank_rank_rank = imageIn_ptr_end_rank_rank_rank_rank;\n    int *imageOut_ptr_end_rank_rank_rank_rank_rank = imageOut_ptr_end_rank_rank_rank_rank;\n\n    int *imageIn_ptr_start_rank_rank_rank_rank_rank_rank = imageIn_ptr_start_rank_rank_rank_rank_rank;\n    int *imageOut_ptr_start_rank_rank_rank_rank_rank_rank = imageOut_ptr_start_rank_rank_rank_rank_rank;\n\n    int *imageIn_ptr_end_rank_rank_rank_rank_rank_rank = imageIn_ptr_end_rank_rank_rank_rank_rank;\n    int *imageOut_ptr_end_rank_rank_rank",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_rows = N / size;\n    int num_cols = N;\n    int start_row = rank * num_rows;\n    int end_row = start_row + num_rows;\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n    // Hint: Use MPI to distribute the computation\n    // Hint: Use MPI_Gather to collect the results\n\n    // TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n    // Hint: Use MPI to distribute the computation\n    // Hint: Use MPI_Gather to collect the results\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n    int N_local = end - start;\n    std::vector<int> imageIn_local(N_local * N_local);\n    std::vector<int> imageOut_local(N_local * N_local);\n    for (int i = 0; i < N_local; i++) {\n        for (int j = 0; j < N_local; j++) {\n            imageIn_local[i * N_local + j] = imageIn[(i + start) * N + j + start];\n        }\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < N_local; i++) {\n        for (int j = 0; j < N_local; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N_local || y < 0 || y >= N_local) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn_local[x * N_local + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut_local[i * N_local + j] = sum;\n        }\n    }\n    MPI_Gather(imageOut_local.data(), N_local * N_local, MPI_INT, imageOut.data(), N_local * N_local, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_remainder = N % size;\n\n    int N_start = rank * N_per_rank;\n    int N_end = N_start + N_per_rank;\n    if (rank == size - 1) {\n        N_end += N_remainder;\n    }\n\n    std::vector<int> image_rank(N_per_rank * N_per_rank);\n    std::vector<int> image_rank_out(N_per_rank * N_per_rank);\n\n    if (rank == 0) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                image_rank[i * N_per_rank + j] = imageIn[i * N + j];\n            }\n        }\n    }\n\n    MPI_Bcast(image_rank.data(), N_per_rank * N_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N_per_rank; ++i) {\n        for (int j = 0; j < N_per_rank; ++j) {\n            int sum = 0;\n            for (int k = 0; k < 3; ++k) {\n                for (int l = 0; l < 3; ++l) {\n                    int x = i + k - 1;\n                    int y = j + l - 1;\n                    if (x < 0 || x >= N_per_rank || y < 0 || y >= N_per_rank) {\n                        sum += 0;\n                    } else {\n                        sum += image_rank[x * N_per_rank + y] * edgeKernel[k][l];\n                    }\n                }\n            }\n            image_rank_out[i * N_per_rank + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    MPI_Gather(image_rank_out.data(), N_per_rank * N_per_rank, MPI_INT, imageOut.data(), N_per_rank * N_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N_per_rank = N / size;\n    int N_rem = N % size;\n\n    int start_row = rank * N_per_rank;\n    int end_row = start_row + N_per_rank;\n    if (rank == size - 1) {\n        end_row += N_rem;\n    }\n\n    int start_col = 0;\n    int end_col = N;\n\n    std::vector<int> image_rank(N * N);\n    for (int i = start_row; i < end_row; i++) {\n        for (int j = start_col; j < end_col; j++) {\n            image_rank[i * N + j] = imageIn[i * N + j];\n        }\n    }\n\n    int image_rank_size = image_rank.size();\n    std::vector<int> image_rank_out(image_rank_size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < image_rank_size; i++) {\n        int row = i / N;\n        int col = i % N;\n        int sum = 0;\n        for (int r = -1; r <= 1; r++) {\n            for (int c = -1; c <= 1; c++) {\n                int row_neighbor = row + r;\n                int col_neighbor = col + c;\n                if (row_neighbor >= 0 && row_neighbor < N && col_neighbor >= 0 && col_neighbor < N) {\n                    sum += image_rank[row_neighbor * N + col_neighbor] * edgeKernel[r + 1][c + 1];\n                }\n            }\n        }\n        image_rank_out[i] = std::max(0, std::min(255, sum));\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < image_rank_size; i++) {\n            imageOut[i] = image_rank_out[i];\n        }\n    }\n\n    MPI_Gather(image_rank_out.data(), image_rank_size, MPI_INT, imageOut.data(), image_rank_size, MPI_INT, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement the Jacobi iteration\n  // Hint: Use a parallel_for loop over the grid\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use a lambda function to compute the average of a grid element\n  // Hint: Use",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == 0) {\n                if (j == 0) {\n                    output[i * N + j] = (input[i * N + j + 1] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j + 1]) / 4;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[i * N + j - 1] + input[(i + 1) * N + j - 1] + input[i * N + j - 1] + input[(i + 1) * N + j]) / 4;\n                } else {\n                    output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 5;\n                }\n            } else if (i == N - 1) {\n                if (j == 0) {\n                    output[i * N + j] = (input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[i * N + j] + input[i * N + j + 1]) / 4;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[i * N + j - 1] + input[i * N + j]) / 4;\n                } else {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j + 1]) / 5;\n                }\n            } else {\n                if (j == 0) {\n                    output[i * N + j] = (input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[i * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 6;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[i * N + j - 1] + input[i * N + j] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j]) / 6;\n                } else {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 9;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Fill in the body of this function\n\n    // Hint:\n    // You can use the following code to access the (i,j)th element of the input and output\n    // double input_ij = input[i*N + j];\n    // double output_ij = output[i*N + j];\n\n    // You can use the following code to access the (i-1,j)th element of the input and output\n    // double input_im1j = input[(i-1)*N + j];\n    // double output_im1j = output[(i-1)*N + j];\n\n    // You can use the following code to access the (i+1,j)th element of the input and output\n    // double input_ip1j = input[(i+1)*N + j];\n    // double output_ip1j = output[(i+1)*N + j];\n\n    // You can use the following code to access the (i,j-1)th element of the input and output\n    // double input_ijm1 = input[i*N + j-1];\n    // double output_ijm1 = output[i*N + j-1];\n\n    // You can use the following code to access the (i,j+1)th element of the input and output\n    // double input_ijp1 = input[i*N + j+1];\n    // double output_ijp1 = output[i*N + j+1];\n\n    // You can use the following code to access the (i-1,j-1)th element of the input and output\n    // double input_im1jm1 = input[(i-1)*N + j-1];\n    // double output_im1jm1 = output[(i-1)*N + j-1];\n\n    // You can use the following code to access the (i-1,j+1)th element of the input and output\n    // double input_im1jp1 = input[(i-1)*N + j+1];\n    // double output_im1jp1 = output[(i-1)*N + j+1];\n\n    // You can use the following code to access the (i+1,j-1)th element of the input and output\n    // double input_ip1jm1 = input[(i+1)*N + j-1];\n    // double output_ip1jm1 = output[(i+1)*N + j-1];\n\n    // You can use the following code to access the (i+1,j+1)th element of the input and output\n    // double input_ip1jp1 = input[(i+1)*N + j+1];\n    // double output_ip1jp1 = output[(i+1)*N + j+1];\n\n    // You can use the following code to access the (i-1,j-1)th element of the input and output\n    // double input_im1jm1 = input[(i-1)*N + j-1];\n    // double output_im1jm1 = output[(i-1)*N + j-1];\n\n    // You can use the following code to access the (i-1,j+1)th element of the input and output\n    // double input_im1jp1 = input[(i-1)*N + j+1];\n    // double output_im1jp1 = output[(i-1)*N + j+1];\n\n    // You can use the following code to access the (i+1,j-1)th element of the input and output\n    // double input_ip1jm1 = input[(i+1)*N + j-1];\n    // double output_ip1jm1 = output[(i+1)*N + j-1];\n\n    // You can use the following code to access the (i+1,j+1)th element of the input and output\n    // double input_ip1jp1 = input[(i+1)*N + j+1];\n    // double output_ip1jp1 = output[(i+1)*N + j+1];\n\n}",
            "// TODO: Implement this function\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp_get_num_threads() to get the number of threads\n    // Hint: Use omp_get_thread_num() to get the thread ID\n    // Hint: Use omp_get_num_procs() to get the number of processors\n    // Hint: Use omp",
            "// TODO: Implement this function\n    // You can use the following variables:\n    // - N: the size of the grid\n    // - input: the input grid\n    // - output: the output grid\n    // - i, j: the indices of the current element\n    // - i_n, j_n: the indices of the neighboring element\n    // - input[i_n][j_n]: the value of the neighboring element\n    // - output[i][j]: the value of the current element\n    // - num_threads: the number of threads in the current team\n    // - thread_num: the number of the current thread\n    // - num_teams: the number of teams\n    // - team_num: the number of the current team\n    // - omp_get_thread_num(): the number of the current thread\n    // - omp_get_num_threads(): the number of threads in the current team\n    // - omp_get_num_teams(): the number of teams\n    // - omp_get_team_num(): the number of the current team\n    // - omp_in_parallel(): true if in parallel region\n    // - omp_get_dynamic(): true if dynamic teams are enabled\n    // - omp_get_cancellation(): true if cancellation is enabled\n    // - omp_get_nested(): true if nested parallelism is enabled\n    // - omp_get_max_threads(): the maximum number of threads\n    // - omp_get_thread_limit(): the maximum number of threads allowed\n    // - omp_get_num_procs(): the number of processors\n    // - omp_get_num_places(): the number of places\n    // - omp_in_final(): true if in final region\n    // - omp_get_proc_bind(): the proc_bind value\n    // - omp_get_place_num_procs(): the number of processors in the place\n    // - omp_get_place_num(): the number of the place\n    // - omp_get_place_proc_ids(): the IDs of the processors in the place\n    // - omp_get_place_num_places(): the number of places in the place\n    // - omp_get_place_places(): the IDs of the places in the place\n    // - omp_get_partition_num_places(): the number of places in the partition\n    // - omp_get_partition_place_nums(): the IDs of the places in the partition\n    // - omp_get_default_device(): the default device\n    // - omp_get_num_devices(): the number of devices\n    // - omp_get_num_teams(): the number of teams\n    // - omp_get_team_num(): the number of the current team\n    // - omp_is_initial_device(): true if the current device is the initial device\n    // - omp_get_initial_device(): the initial device\n    // - omp_get_max_task_priority(): the maximum task priority\n    // - omp_pause_resource(omp_event_handle_t, int): pause the current thread until the event is set\n    // - omp_pause_resource_all(omp_event_handle_t, int): pause all threads until the event is set\n    // - omp_set_event(omp_event_handle_t): set the event\n    // - omp_unset_event(omp_event_handle_t): unset the event\n    // - omp_test_event(omp_event_handle_t): test if the event is set\n    // - omp_get_wtime(): the wall clock time\n    // - omp_get_wtick(): the time in seconds of a clock tick\n    // - omp_get_num_places(): the number of places\n    // - omp_get_place_num_procs(): the number of processors in the place\n    // - omp_get_place_proc_ids(): the IDs of the processors in the place\n    // - omp_get_place_num(): the number of the place\n    // - omp_get_place_num_places(): the number of places in the place\n    // - omp_get_place_places(): the IDs of the places in the place\n    // - omp_get_",
            "// TODO: Implement this function\n    // You may need to use omp_get_thread_num() to determine which thread you are in.\n    // You may need to use omp_get_num_threads() to determine how many threads are used.\n    // You may need to use omp_get_num_procs() to determine how many processors are available.\n    // You may need to use omp_get_max_threads() to determine how many threads can be used.\n    // You may need to use omp_get_thread_limit() to determine how many threads can be used.\n    // You may need to use omp_get_dynamic() to determine if threads can be added or removed.\n    // You may need to use omp_get_nested() to determine if nested parallelism is enabled.\n    // You may need to use omp_in_parallel() to determine if you are in a parallel region.\n    // You may need to use omp_set_num_threads() to set the number of threads.\n    // You may need to use omp_set_dynamic() to set if threads can be added or removed.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_num_threads() to set the number of threads.\n    // You may need to use omp_set_dynamic() to set if threads can be added or removed.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_num_threads() to set the number of threads.\n    // You may need to use omp_set_dynamic() to set if threads can be added or removed.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_num_threads() to set the number of threads.\n    // You may need to use omp_set_dynamic() to set if threads can be added or removed.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_num_threads() to set the number of threads.\n    // You may need to use omp_set_dynamic() to set if threads can be added or removed.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.\n    // You may need to use omp_set_max_active_levels() to set the maximum number of nested parallelism levels.\n    // You may need to use omp_set_schedule() to set the scheduling type.\n    // You may need to use omp_set_nested() to set if nested parallelism is enabled.",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == 0) {\n                if (j == 0) {\n                    output[i * N + j] = (input[i * N + j + 1] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j + 1]) / 4;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[i * N + j - 1] + input[(i + 1) * N + j - 1] + input[i * N + j - 1] + input[(i + 1) * N + j]) / 4;\n                } else {\n                    output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 5;\n                }\n            } else if (i == N - 1) {\n                if (j == 0) {\n                    output[i * N + j] = (input[(i - 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1] + input[i * N + j]) / 4;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[i * N + j - 1] + input[i * N + j]) / 4;\n                } else {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j + 1]) / 5;\n                }\n            } else {\n                if (j == 0) {\n                    output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j + 1]) / 5;\n                } else if (j == N - 1) {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i + 1) * N + j - 1] + input[i * N + j - 1] + input[(i - 1) * N + j] + input[(i + 1) * N + j]) / 5;\n                } else {\n                    output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j + 1]) / 9;\n                }\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    // You can use the following variables:\n    // - N: size of the grid\n    // - input: input grid\n    // - output: output grid\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == 0) {\n                if (j == 0) {\n                    output[i*N + j] = (input[i*N + j + 1] + input[(i+1)*N + j] + input[i*N + j] + input[(i+1)*N + j + 1])/4;\n                } else if (j == N-1) {\n                    output[i*N + j] = (input[i*N + j - 1] + input[(i+1)*N + j] + input[i*N + j] + input[(i+1)*N + j - 1])/4;\n                } else {\n                    output[i*N + j] = (input[i*N + j - 1] + input[i*N + j + 1] + input[(i+1)*N + j] + input[(i+1)*N + j - 1] + input[(i+1)*N + j + 1])/5;\n                }\n            } else if (i == N-1) {\n                if (j == 0) {\n                    output[i*N + j] = (input[i*N + j + 1] + input[(i-1)*N + j] + input[i*N + j] + input[(i-1)*N + j + 1])/4;\n                } else if (j == N-1) {\n                    output[i*N + j] = (input[i*N + j - 1] + input[(i-1)*N + j] + input[i*N + j] + input[(i-1)*N + j - 1])/4;\n                } else {\n                    output[i*N + j] = (input[i*N + j - 1] + input[i*N + j + 1] + input[(i-1)*N + j] + input[(i-1)*N + j - 1] + input[(i-1)*N + j + 1])/5;\n                }\n            } else {\n                if (j == 0) {\n                    output[i*N + j] = (input[i*N + j + 1] + input[(i-1)*N + j] + input[(i+1)*N + j] + input[(i-1)*N + j + 1] + input[(i+1)*N + j])/5;\n                } else if (j == N-1) {\n                    output[i*N + j] = (input[i*N + j - 1] + input[(i-1)*N + j] + input[(i+1)*N + j] + input[(i-1)*N + j - 1] + input[(i+1)*N + j])/5;\n                } else {\n                    output[i*N + j] = (input[i*N + j - 1] + input[i*N + j + 1] + input[(i-1)*N + j] + input[(i-1)*N + j - 1] + input[(i-1)*N + j + 1] + input[(i+1)*N + j] + input[(i+1)*N + j - 1] + input[(i+1)*N + j + 1])/9;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n    //\n    // Hint:\n    //  - Use OpenMP to parallelize the computation\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of processors\n    //  - Use the `omp_get_max_threads()` function to get the maximum number of threads\n    //  - Use the `omp_get_dynamic()` function to get the dynamic thread adjustment\n    //  - Use the `omp_set_dynamic()` function to set the dynamic thread adjustment\n    //  - Use the `omp_set_num_threads()` function to set the number of threads\n    //  - Use the `omp_in_parallel()` function to check if the code is running in parallel\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of processors\n    //  - Use the `omp_get_max_threads()` function to get the maximum number of threads\n    //  - Use the `omp_get_dynamic()` function to get the dynamic thread adjustment\n    //  - Use the `omp_set_dynamic()` function to set the dynamic thread adjustment\n    //  - Use the `omp_set_num_threads()` function to set the number of threads\n    //  - Use the `omp_in_parallel()` function to check if the code is running in parallel\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of processors\n    //  - Use the `omp_get_max_threads()` function to get the maximum number of threads\n    //  - Use the `omp_get_dynamic()` function to get the dynamic thread adjustment\n    //  - Use the `omp_set_dynamic()` function to set the dynamic thread adjustment\n    //  - Use the `omp_set_num_threads()` function to set the number of threads\n    //  - Use the `omp_in_parallel()` function to check if the code is running in parallel\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of processors\n    //  - Use the `omp_get_max_threads()` function to get the maximum number of threads\n    //  - Use the `omp_get_dynamic()` function to get the dynamic thread adjustment\n    //  - Use the `omp_set_dynamic()` function to set the dynamic thread adjustment\n    //  - Use the `omp_set_num_threads()` function to set the number of threads\n    //  - Use the `omp_in_parallel()` function to check if the code is running in parallel\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of processors\n    //  - Use the `omp_get_max_threads()` function to get the maximum number of threads\n    //  - Use the `omp_get_dynamic()` function to get the dynamic thread adjustment\n    //  - Use the `omp_set_dynamic()` function to set the dynamic thread adjustment\n    //  - Use the `omp_set_num_threads()` function to set the number of threads\n    //  - Use the `omp_in_parallel()` function to check if the code is running in parallel\n    //  - Use the `omp_get_thread_num()` function to get the thread number\n    //  - Use the `omp_get_num_threads()` function to get the number of threads\n    //  - Use the `omp_get_num_procs()` function to get the number of",
            "// TODO: Fill in the body of this function\n    // You can use the following variables:\n    // - input: the input grid\n    // - output: the output grid\n    // - N: the size of the grid\n\n    // You can use the following functions:\n    // - std::min(a, b)\n    // - std::max(a, b)\n    // - std::abs(a)\n    // - std::sqrt(a)\n    // - std::pow(a, b)\n    // - std::exp(a)\n    // - std::log(a)\n    // - std::sin(a)\n    // - std::cos(a)\n    // - std::tan(a)\n    // - std::atan(a)\n    // - std::atan2(a, b)\n    // - std::round(a)\n    // - std::floor(a)\n    // - std::ceil(a)\n    // - std::fmod(a, b)\n    // - std::fmax(a, b)\n    // - std::fmin(a, b)\n    // - std::fdim(a, b)\n    // - std::trunc(a)\n    // - std::modf(a, b)\n    // - std::remainder(a, b)\n    // - std::copysign(a, b)\n    // - std::nextafter(a, b)\n    // - std::nexttoward(a, b)\n    // - std::fma(a, b, c)\n    // - std::fabs(a)\n    // - std::fputc(a, b)\n    // - std::fputs(a, b)\n    // - std::fwrite(a, b, c, d)\n    // - std::fread(a, b, c, d)\n    // - std::fgetc(a)\n    // - std::fgets(a, b, c)\n    // - std::fprintf(a, b,...)\n    // - std::fscanf(a, b,...)\n    // - std::freopen(a, b, c)\n    // - std::setbuf(a, b)\n    // - std::setvbuf(a, b, c, d)\n    // - std::tmpfile()\n    // - std::tmpnam(a)\n    // - std::fclose(a)\n    // - std::fflush(a)\n    // - std::fgetpos(a, b)\n    // - std::fseek(a, b, c)\n    // - std::fsetpos(a, b)\n    // - std::ftell(a)\n    // - std::rewind(a)\n    // - std::clearerr(a)\n    // - std::feof(a)\n    // - std::ferror(a)\n    // - std::perror(a)\n    // - std::fileno(a)\n    // - std::popen(a, b)\n    // - std::pclose(a)\n    // - std::flockfile(a)\n    // - std::ftrylockfile(a)\n    // - std::funlockfile(a)\n    // - std::getc(a)\n    // - std::getchar()\n    // - std::gets(a)\n    // - std::putc(a, b)\n    // - std::putchar(a)\n    // - std::puts(a)\n    // - std::ungetc(a, b)\n    // - std::fscanf(a, b,...)\n    // - std::fprintf(a, b,...)\n    // - std::vfscanf(a, b, c,...)\n    // - std::vfprintf(a, b, c,...)\n    // - std::fgetc(a)\n    // - std::fgets(a, b, c)\n    // - std::fputc(a, b)\n    // - std::fputs(a, b)\n    // - std::fread(a, b, c, d)\n    // - std::fwrite(a, b, c, d)\n    // - std::fgetpos(a, b)\n    // - std::fse",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Fill in the code to implement the 2D Jacobi stencil.\n    // You may use the following variables:\n    // - input: the input grid\n    // - output: the output grid\n    // - N: the size of the grid\n    // - i, j: the indices of the current element\n    // - i_n, j_n: the indices of the neighboring element\n    // - input[i_n + N*j_n]: the value of the neighboring element\n    // - output[i + N*j]: the value of the current element\n\n    // You may use the following OpenMP directives:\n    // - #pragma omp parallel for\n    // - #pragma omp parallel for collapse(2)\n\n    // You may use the following OpenMP functions:\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n\n    // You may use the following OpenMP variables:\n    // - omp_get_num_threads()\n    // - omp_get_thread_num()\n\n    // You may use the following OpenMP clauses:\n    // - private(i, j)\n    // - shared(input, output, N)\n    // - firstprivate(i, j)\n    // - lastprivate(i, j)\n    // - reduction(+: sum)\n    // - schedule(static)\n    // - schedule(dynamic)\n    // - schedule(guided)\n    // - schedule(runtime)\n    // - schedule(auto)\n    // - num_threads(4)\n\n    // You may use the following OpenMP constructs:\n    // - omp_set_num_threads(4)\n    // - omp_get_num_threads()\n    // - omp_set_dynamic(0)\n    // - omp_set_num_threads(omp_get_num_procs())\n    // - omp_set_nested(1)\n    // - omp_set_dynamic(1)\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp_get_max_threads()\n    // - omp_get_thread_num()\n    // - omp_get_num_threads()\n    // - omp_get_num_procs()\n    // - omp",
            "// TODO: Fill in the body of this function\n\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n    // Hint: Use OpenMP to parallelize the computation\n    // Hint: Use the `omp_get_thread_num()` function to get the thread number\n    // Hint: Use the `omp_get_num_threads()` function to get the number of threads\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            if (i > 0 && j > 0) {\n                sum += input[(i - 1) * N + j - 1];\n            }\n            if (i > 0 && j < N - 1) {\n                sum += input[(i - 1) * N + j + 1];\n            }\n            if (i < N - 1 && j > 0) {\n                sum += input[(i + 1) * N + j - 1];\n            }\n            if (i < N - 1 && j < N - 1) {\n                sum += input[(i + 1) * N + j + 1];\n            }\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement me\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (i == 0 && j == 0) {\n        output[i * N + j] = (input[i * N + j + 1] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j + 1]) / 4;\n      } else if (i == 0 && j == N - 1) {\n        output[i * N + j] = (input[i * N + j - 1] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[(i + 1) * N + j - 1]) / 4;\n      } else if (i == N - 1 && j == 0) {\n        output[i * N + j] = (input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1]) / 4;\n      } else if (i == N - 1 && j == N - 1) {\n        output[i * N + j] = (input[i * N + j - 1] + input[(i - 1) * N + j] + input[i * N + j - 1] + input[(i - 1) * N + j - 1]) / 4;\n      } else if (i == 0) {\n        output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 5;\n      } else if (i == N - 1) {\n        output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1]) / 5;\n      } else if (j == 0) {\n        output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j + 1]) / 5;\n      } else if (j == N - 1) {\n        output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[(i - 1) * N + j - 1] + input[(i + 1) * N + j - 1]) / 5;\n      } else {\n        output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i + 1) * N + j - 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j] + input[(i + 1) * N + j]) / 9;\n      }\n    }\n  }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5.0;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0;\n      if (i > 0) {\n        sum += input[i * N + j - 1];\n      }\n      if (i < N - 1) {\n        sum += input[i * N + j + 1];\n      }\n      if (j > 0) {\n        sum += input[(i - 1) * N + j];\n      }\n      if (j < N - 1) {\n        sum += input[(i + 1) * N + j];\n      }\n      sum += input[i * N + j];\n      output[i * N + j] = sum / 5;\n    }\n  }\n}",
            "// TODO: Implement this function\n    // Note: You can assume that N is always odd\n    // Note: You can assume that input.size() == N*N\n    // Note: You can assume that output.size() == N*N\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same vector\n    // Note: You can assume that input and output are not the same",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == 0 || i == N - 1 || j == 0 || j == N - 1) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j + 1] + input[i * N + j]) / 5;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    double sum = 0;\n    double temp = 0;\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            temp = sum / 5;\n            output[i * N + j] = temp;\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == 0 && j == 0) {\n                output[i * N + j] = (input[i * N + j + 1] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i + 1) * N + j + 1]) / 4;\n            } else if (i == 0 && j == N - 1) {\n                output[i * N + j] = (input[i * N + j - 1] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[(i + 1) * N + j - 1]) / 4;\n            } else if (i == N - 1 && j == 0) {\n                output[i * N + j] = (input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1]) / 4;\n            } else if (i == N - 1 && j == N - 1) {\n                output[i * N + j] = (input[i * N + j - 1] + input[(i - 1) * N + j] + input[i * N + j - 1] + input[(i - 1) * N + j - 1]) / 4;\n            } else if (i == 0) {\n                output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1]) / 5;\n            } else if (i == N - 1) {\n                output[i * N + j] = (input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1]) / 5;\n            } else if (j == 0) {\n                output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j + 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j + 1]) / 5;\n            } else if (j == N - 1) {\n                output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[(i - 1) * N + j - 1] + input[(i + 1) * N + j - 1]) / 5;\n            } else {\n                output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j] + input[(i + 1) * N + j + 1] + input[i * N + j - 1] + input[i * N + j + 1]) / 9;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i == 0 || i == N - 1 || j == 0 || j == N - 1) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = (input[(i - 1) * N + j - 1] + input[(i - 1) * N + j + 1] + input[(i + 1) * N + j - 1] + input[(i + 1) * N + j + 1] + input[i * N + j]) / 5;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            if (i > 0) {\n                sum += input[i * N + j - 1];\n            }\n            if (i < N - 1) {\n                sum += input[i * N + j + 1];\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n            }\n            sum += input[i * N + j];\n            output[i * N + j] = sum / 5;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "hip",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i < N && j < N) {\n    double sum = 0;\n    if (i > 0) {\n      sum += input[(i - 1) * N + j];\n    }\n    if (i < N - 1) {\n      sum += input[(i + 1) * N + j];\n    }\n    if (j > 0) {\n      sum += input[i * N + j - 1];\n    }\n    if (j < N - 1) {\n      sum += input[i * N + j + 1];\n    }\n    sum += input[i * N + j];\n    output[i * N + j] = sum / 5;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0)\n            sum += input[(i - 1) * N + j];\n        if (i < N - 1)\n            sum += input[(i + 1) * N + j];\n        if (j > 0)\n            sum += input[i * N + j - 1];\n        if (j < N - 1)\n            sum += input[i * N + j + 1];\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  double sum = 0;\n  if (i > 0) sum += input[(i - 1) * N + j];\n  if (i < N - 1) sum += input[(i + 1) * N + j];\n  if (j > 0) sum += input[i * N + j - 1];\n  if (j < N - 1) sum += input[i * N + j + 1];\n  sum += input[i * N + j];\n  output[i * N + j] = sum / 5;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  double sum = 0.0;\n  if (i > 0) sum += input[(i - 1) * N + j];\n  if (i < N - 1) sum += input[(i + 1) * N + j];\n  if (j > 0) sum += input[i * N + j - 1];\n  if (j < N - 1) sum += input[i * N + j + 1];\n  sum += input[i * N + j];\n  output[i * N + j] = sum / 5.0;\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    double sum = 0;\n    if (i > 0) sum += input[(i - 1) * N + j];\n    if (i < N - 1) sum += input[(i + 1) * N + j];\n    if (j > 0) sum += input[i * N + j - 1];\n    if (j < N - 1) sum += input[i * N + j + 1];\n    output[i * N + j] = sum / 5;\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  double sum = 0;\n  if (i > 0) sum += input[(i-1)*N + j];\n  if (i < N-1) sum += input[(i+1)*N + j];\n  if (j > 0) sum += input[i*N + j-1];\n  if (j < N-1) sum += input[i*N + j+1];\n  output[i*N + j] = sum/5;\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    double sum = 0;\n    if (i > 0) {\n      sum += input[(i - 1) * N + j];\n    }\n    if (i < N - 1) {\n      sum += input[(i + 1) * N + j];\n    }\n    if (j > 0) {\n      sum += input[i * N + j - 1];\n    }\n    if (j < N - 1) {\n      sum += input[i * N + j + 1];\n    }\n    sum += input[i * N + j];\n    output[i * N + j] = sum / 5;\n  }\n}",
            "// TODO: Implement this function\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  double sum = 0.0;\n  if (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n    sum = input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j];\n    sum /= 5.0;\n  }\n  output[i * N + j] = sum;\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  double sum = 0;\n  if (i > 0) sum += input[(i - 1) * N + j];\n  if (i < N - 1) sum += input[(i + 1) * N + j];\n  if (j > 0) sum += input[i * N + j - 1];\n  if (j < N - 1) sum += input[i * N + j + 1];\n  output[i * N + j] = sum / 5;\n}",
            "// TODO: Implement\n}",
            "// TODO: Implement this function\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i >= N || j >= N) {\n        return;\n    }\n\n    double sum = 0;\n    if (i > 0) {\n        sum += input[(i - 1) * N + j];\n    }\n    if (i < N - 1) {\n        sum += input[(i + 1) * N + j];\n    }\n    if (j > 0) {\n        sum += input[i * N + j - 1];\n    }\n    if (j < N - 1) {\n        sum += input[i * N + j + 1];\n    }\n    sum += input[i * N + j];\n    output[i * N + j] = sum / 5;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i >= N || j >= N) return;\n\n  double sum = 0;\n  if (i > 0) sum += input[(i - 1) * N + j];\n  if (i < N - 1) sum += input[(i + 1) * N + j];\n  if (j > 0) sum += input[i * N + j - 1];\n  if (j < N - 1) sum += input[i * N + j + 1];\n  sum += input[i * N + j];\n  output[i * N + j] = sum / 5;\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = N/size;\n  int n_rem = N%size;\n\n  int n_local = n;\n  int n_local_rem = n_rem;\n\n  if (rank == 0) {\n    n_local += n_local_rem;\n  }\n  else if (rank < n_rem) {\n    n_local += 1;\n  }\n\n  std::vector<double> local_input(n_local*n_local);\n  std::vector<double> local_output(n_local*n_local);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_local; i++) {\n      for (int j = 0; j < n_local; j++) {\n        local_input[i*n_local+j] = input[i*n+j];\n      }\n    }\n  }\n  else {\n    for (int i = 0; i < n_local; i++) {\n      for (int j = 0; j < n_local; j++) {\n        local_input[i*n_local+j] = 0;\n      }\n    }\n  }\n\n  MPI_Scatter(local_input.data(), n_local*n_local, MPI_DOUBLE, local_output.data(), n_local*n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 1; i < n_local-1; i++) {\n    for (int j = 1; j < n_local-1; j++) {\n      local_output[i*n_local+j] = (local_output[(i-1)*n_local+j] + local_output[(i+1)*n_local+j] + local_output[i*n_local+j-1] + local_output[i*n_local+j+1] + local_output[i*n_local+j])/5;\n    }\n  }\n\n  MPI_Gather(local_output.data(), n_local*n_local, MPI_DOUBLE, local_input.data(), n_local*n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_local; i++) {\n      for (int j = 0; j < n_local; j++) {\n        output[i*n+j] = local_input[i*n_local+j];\n      }\n    }\n  }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N;\n    int n_local = n/size;\n    int n_local_start = n_local*rank;\n    int n_local_end = n_local*(rank+1);\n\n    std::vector<double> input_local(n_local*n);\n    std::vector<double> output_local(n_local*n);\n\n    for (int i = 0; i < n_local*n; i++) {\n        input_local[i] = input[i];\n    }\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n; j++) {\n            if (i == 0) {\n                if (j == 0) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j + 1] + input_local[(i + 1)*n + j] + input_local[(i + 1)*n + j + 1])/4;\n                } else if (j == n - 1) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j - 1] + input_local[(i + 1)*n + j] + input_local[(i + 1)*n + j - 1])/4;\n                } else {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j - 1] + input_local[i*n + j + 1] + input_local[(i + 1)*n + j] + input_local[(i + 1)*n + j - 1] + input_local[(i + 1)*n + j + 1])/6;\n                }\n            } else if (i == n_local - 1) {\n                if (j == 0) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j + 1] + input_local[(i - 1)*n + j] + input_local[(i - 1)*n + j + 1])/4;\n                } else if (j == n - 1) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j - 1] + input_local[(i - 1)*n + j] + input_local[(i - 1)*n + j - 1])/4;\n                } else {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j - 1] + input_local[i*n + j + 1] + input_local[(i - 1)*n + j] + input_local[(i - 1)*n + j - 1] + input_local[(i - 1)*n + j + 1])/6;\n                }\n            } else {\n                if (j == 0) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j + 1] + input_local[(i - 1)*n + j] + input_local[(i - 1)*n + j + 1] + input_local[(i + 1)*n + j] + input_local[(i + 1)*n + j + 1])/6;\n                } else if (j == n - 1) {\n                    output_local[i*n + j] = (input_local[i*n + j] + input_local[i*n + j - 1] + input_local[(i - 1)*n + j] + input_local[(i - 1)*n + j - 1] + input_local[(i + 1)*",
            "// TODO: Implement\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = N/size;\n    int m = N%size;\n    int start = rank*n;\n    int end = start + n;\n    if(rank == size-1)\n        end += m;\n    std::vector<double> temp(n*n);\n    for(int i = start; i < end; i++) {\n        for(int j = 0; j < n; j++) {\n            int x = i - start;\n            int y = j;\n            if(i == 0) {\n                temp[x*n + y] = (input[i*n + y] + input[i*n + y + 1] + input[i*n + y + 2] + input[i*n + y + 3] + input[i*n + y + 4])/5;\n            }\n            else if(i == N-1) {\n                temp[x*n + y] = (input[(i-1)*n + y] + input[(i-1)*n + y + 1] + input[(i-1)*n + y + 2] + input[(i-1)*n + y + 3] + input[(i-1)*n + y + 4])/5;\n            }\n            else {\n                temp[x*n + y] = (input[(i-1)*n + y] + input[(i-1)*n + y + 1] + input[(i-1)*n + y + 2] + input[(i-1)*n + y + 3] + input[(i-1)*n + y + 4] + input[i*n + y] + input[i*n + y + 1] + input[i*n + y + 2] + input[i*n + y + 3] + input[i*n + y + 4])/5;\n            }\n        }\n    }\n    if(rank == 0) {\n        for(int i = 0; i < n; i++) {\n            for(int j = 0; j < n; j++) {\n                output[i*n + j] = temp[i*n + j];\n            }\n        }\n    }\n    else {\n        MPI_Send(&temp[0], n*n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n    if(rank == 0) {\n        for(int i = 1; i < size; i++) {\n            MPI_Recv(&output[i*n], n*n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N;\n    int m = N;\n    int n_local = n/size;\n    int m_local = m/size;\n    int n_local_start = rank*n_local;\n    int m_local_start = rank*m_local;\n\n    std::vector<double> local_input(n_local*m_local);\n    std::vector<double> local_output(n_local*m_local);\n\n    for(int i = 0; i < n_local; i++){\n        for(int j = 0; j < m_local; j++){\n            local_input[i*m_local+j] = input[i*m+j];\n        }\n    }\n\n    for(int i = 0; i < n_local; i++){\n        for(int j = 0; j < m_local; j++){\n            if(i == 0){\n                if(j == 0){\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[i*m_local+j+1] + local_input[(i+1)*m_local+j] + local_input[(i+1)*m_local+j+1])/4;\n                }\n                else if(j == m_local-1){\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[i*m_local+j-1] + local_input[(i+1)*m_local+j] + local_input[(i+1)*m_local+j-1])/4;\n                }\n                else{\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[i*m_local+j-1] + local_input[i*m_local+j+1] + local_input[(i+1)*m_local+j] + local_input[(i+1)*m_local+j-1] + local_input[(i+1)*m_local+j+1])/6;\n                }\n            }\n            else if(i == n_local-1){\n                if(j == 0){\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[(i-1)*m_local+j] + local_input[(i-1)*m_local+j+1] + local_input[i*m_local+j+1])/4;\n                }\n                else if(j == m_local-1){\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[(i-1)*m_local+j] + local_input[(i-1)*m_local+j-1] + local_input[i*m_local+j-1])/4;\n                }\n                else{\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[(i-1)*m_local+j] + local_input[(i-1)*m_local+j-1] + local_input[i*m_local+j-1] + local_input[(i-1)*m_local+j+1] + local_input[i*m_local+j+1])/6;\n                }\n            }\n            else{\n                if(j == 0){\n                    local_output[i*m_local+j] = (local_input[i*m_local+j] + local_input[(i-1)*m_local+j] + local_input[(i-1)*m_local+j+1] + local_input[i*m_local+j+1] + local_input[(i+1)*m_local+j] + local_input[(i+1)*m_local+j+",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Implement this function\n    // You may use the following functions:\n    // - MPI_Send\n    // - MPI_Recv\n    // - MPI_Sendrecv\n    // - MPI_Bcast\n    // - MPI_Reduce\n    // - MPI_Allreduce\n    // - MPI_Scatter\n    // - MPI_Gather\n    // - MPI_Scatterv\n    // - MPI_Gatherv\n\n    // TODO: Im"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "cuda",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n            sum = input[i - 1 + j * N] + input[i + 1 + j * N] + input[i + (j - 1) * N] + input[i + (j + 1) * N] + input[i + j * N];\n            output[i + j * N] = sum / 5;\n        } else {\n            if (i > 0 && j > 0) {\n                sum += input[i - 1 + j * N] + input[i + j * N];\n            }\n            if (i > 0 && j < N - 1) {\n                sum += input[i - 1 + (j + 1) * N] + input[i + j * N];\n            }\n            if (i < N - 1 && j > 0) {\n                sum += input[i + 1 + (j - 1) * N] + input[i + j * N];\n            }\n            if (i < N - 1 && j < N - 1) {\n                sum += input[i + 1 + (j + 1) * N] + input[i + j * N];\n            }\n            output[i + j * N] = sum / 4;\n        }\n    }\n}",
            "// TODO: Implement this\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this kernel\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N)\n        return;\n    double sum = 0;\n    if (i > 0)\n        sum += input[(i - 1) * N + j];\n    if (i < N - 1)\n        sum += input[(i + 1) * N + j];\n    if (j > 0)\n        sum += input[i * N + j - 1];\n    if (j < N - 1)\n        sum += input[i * N + j + 1];\n    sum += input[i * N + j];\n    output[i * N + j] = sum / 5;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            sum += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            sum += input[i * N + j + 1];\n        }\n        sum += input[i * N + j];\n        output[i * N + j] = sum / 5;\n    }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  double sum = 0;\n  if (i > 0) sum += input[(i - 1) * N + j];\n  if (i < N - 1) sum += input[(i + 1) * N + j];\n  if (j > 0) sum += input[i * N + j - 1];\n  if (j < N - 1) sum += input[i * N + j + 1];\n  sum += input[i * N + j];\n  output[i * N + j] = sum / 5;\n}",
            "// TODO: Implement this function\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0 && j > 0 && i < N - 1 && j < N - 1) {\n            sum = input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j];\n            output[i * N + j] = sum / 5;\n        } else {\n            if (i > 0 && j > 0) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i > 0 && j < N - 1) {\n                sum = input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j] + input[(i + 1) * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i < N - 1 && j > 0) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j] + input[(i + 1) * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i < N - 1 && j < N - 1) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i > 0 && j == 0) {\n                sum = input[i * N + j + 1] + input[(i - 1) * N + j] + input[i * N + j] + input[(i + 1) * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i < N - 1 && j == 0) {\n                sum = input[i * N + j + 1] + input[i * N + j] + input[(i + 1) * N + j];\n                output[i * N + j] = sum / 3;\n            } else if (i == 0 && j > 0) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[(i + 1) * N + j] + input[i * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i == 0 && j < N - 1) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j] + input[(i + 1) * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i == N - 1 && j > 0) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j] + input[(i - 1) * N + j];\n                output[i * N + j] = sum / 4;\n            } else if (i == N - 1 && j < N - 1) {\n                sum = input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j];\n                output[i * N + j] = sum / 3;\n            } else if (i == 0 && j == 0) {\n                sum = input[i *",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Implement this\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N;\n    int n_local = n/size;\n    int n_local_start = rank*n_local;\n    int n_local_end = (rank+1)*n_local;\n\n    std::vector<double> local_input(n_local*n);\n    std::vector<double> local_output(n_local*n);\n\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n; j++) {\n            local_input[i*n + j] = input[i*n + j];\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_local; i++) {\n        for (int j = 0; j < n; j++) {\n            double sum = 0;\n            if (i > 0) {\n                sum += local_input[(i-1)*n + j];\n            }\n            if (i < n_local-1) {\n                sum += local_input[(i+1)*n + j];\n            }\n            if (j > 0) {\n                sum += local_input[i*n + j-1];\n            }\n            if (j < n-1) {\n                sum += local_input[i*n + j+1];\n            }\n            sum += local_input[i*n + j];\n            local_output[i*n + j] = sum/5;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n_local; i++) {\n            for (int j = 0; j < n; j++) {\n                output[i*n + j] = local_output[i*n + j];\n            }\n        }\n    }\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int N_local = N/size;\n  int N_local_rem = N%size;\n  int N_local_rem_rank = rank;\n\n  int N_local_rank = N_local + (rank < N_local_rem? 1 : 0);\n  int N_local_rank_rem = N_local + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank = rank + (rank < N_local_rem? 1 : 0);\n  int N_local_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem_rank_rem = rank + (rank < N_local_rem? 0 : -1);\n\n  int N_local",
            "// TODO: Your code here\n    // You may use the following functions:\n    // - std::min\n    // - std::max\n    // - std::abs\n    // - std::floor\n    // - std::ceil\n    // - std::sqrt\n    // - std::pow\n    // - std::round\n    // - std::fmod\n    // - std::sin\n    // - std::cos\n    // - std::tan\n    // - std::asin\n    // - std::acos\n    // - std::atan\n    // - std::atan2\n    // - std::sinh\n    // - std::cosh\n    // - std::tanh\n    // - std::asinh\n    // - std::acosh\n    // - std::atanh\n    // - std::exp\n    // - std::log\n    // - std::log10\n    // - std::cbrt\n    // - std::hypot\n    // - std::erf\n    // - std::erfc\n    // - std::tgamma\n    // - std::lgamma\n    // - std::ceil\n    // - std::floor\n    // - std::fmod\n    // - std::fmax\n    // - std::fmin\n    // - std::fdim\n    // - std::fma\n    // - std::fabs\n    // - std::abs\n    // - std::copysign\n    // - std::trunc\n    // - std::lround\n    // - std::llround\n    // - std::remainder\n    // - std::remquo\n    // - std::nearbyint\n    // - std::round\n    // - std::lrint\n    // - std::llrint\n    // - std::rint\n    // - std::lgamma\n    // - std::tgamma\n    // - std::logb\n    // - std::ilogb\n    // - std::log1p\n    // - std::expm1\n    // - std::cbrt\n    // - std::hypot\n    // - std::pow\n    // - std::sqrt\n    // - std::erf\n    // - std::erfc\n    // - std::ldexp\n    // - std::frexp\n    // - std::modf\n    // - std::fmod\n    // - std::fmax\n    // - std::fmin\n    // - std::fdim\n    // - std::fma\n    // - std::fabs\n    // - std::abs\n    // - std::copysign\n    // - std::isnan\n    // - std::isinf\n    // - std::isfinite\n    // - std::isnormal\n    // - std::signbit\n    // - std::fpclassify\n    // - std::isfinite\n    // - std::isnan\n    // - std::isinf\n    // - std::isnormal\n    // - std::signbit\n    // - std::fpclassify\n    // - std::isgreater\n    // - std::isgreaterequal\n    // - std::isless\n    // - std::islessequal\n    // - std::islessgreater\n    // - std::isunordered\n    // - std::isnan\n    // - std::isinf\n    // - std::isless\n    // - std::islessequal\n    // - std::islessgreater\n    // - std::isunordered\n    // - std::isgreater\n    // - std::isgreaterequal\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered\n    // - std::isunordered",
            "// TODO: Implement me\n    // You may use the following variables\n    // int rank, nproc;\n    // int nx, ny;\n    // int i, j;\n    // double sum;\n    // double *input_local, *output_local;\n    // double *input_local_row, *output_local_row;\n    // double *input_local_row_prev, *output_local_row_prev;\n    // double *input_local_row_next, *output_local_row_next;\n    // double *input_local_col, *output_local_col;\n    // double *input_local_col_prev, *output_local_col_prev;\n    // double *input_local_col_next, *output_local_col_next;\n    // double *input_local_col_prev_row, *output_local_col_prev_row;\n    // double *input_local_col_next_row, *output_local_col_next_row;\n    // double *input_local_col_prev_row_prev, *output_local_col_prev_row_prev;\n    // double *input_local_col_next_row_prev, *output_local_col_next_row_prev;\n    // double *input_local_col_prev_row_next, *output_local_col_prev_row_next;\n    // double *input_local_col_next_row_next, *output_local_col_next_row_next;\n\n    // TODO: Implement me\n    // You may use the following variables\n    // int rank, nproc;\n    // int nx, ny;\n    // int i, j;\n    // double sum;\n    // double *input_local, *output_local;\n    // double *input_local_row, *output_local_row;\n    // double *input_local_row_prev, *output_local_row_prev;\n    // double *input_local_row_next, *output_local_row_next;\n    // double *input_local_col, *output_local_col;\n    // double *input_local_col_prev, *output_local_col_prev;\n    // double *input_local_col_next, *output_local_col_next;\n    // double *input_local_col_prev_row, *output_local_col_prev_row;\n    // double *input_local_col_next_row, *output_local_col_next_row;\n    // double *input_local_col_prev_row_prev, *output_local_col_prev_row_prev;\n    // double *input_local_col_next_row_prev, *output_local_col_next_row_prev;\n    // double *input_local_col_prev_row_next, *output_local_col_prev_row_next;\n    // double *input_local_col_next_row_next, *output_local_col_next_row_next;\n\n    // TODO: Implement me\n    // You may use the following variables\n    // int rank, nproc;\n    // int nx, ny;\n    // int i, j;\n    // double sum;\n    // double *input_local, *output_local;\n    // double *input_local_row, *output_local_row;\n    // double *input_local_row_prev, *output_local_row_prev;\n    // double *input_local_row_next, *output_local_row_next;\n    // double *input_local_col, *output_local_col;\n    // double *input_local_col_prev, *output_local_col_prev;\n    // double *input_local_col_next, *output_local_col_next;\n    // double *input_local_col_prev_row, *output_local_col_prev_row;\n    // double *input_local_col_next_row, *output_local_col_next_row;\n    // double *input_local_col_prev_row_prev, *output_local_col_prev_row_prev;\n    // double *input_local_col_next_row_prev, *output_local_col_next_row_prev;\n    // double",
            "// TODO: Implement this function\n    // You can use the following variables to get the rank and size of the MPI communicator\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // You can use the following variables to get the number of threads in the OpenMP parallel region\n    int num_threads;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    // You can use the following variables to get the number of rows and columns of the input matrix\n    int rows = N;\n    int cols = N;\n\n    // You can use the following variables to get the number of rows and columns of the output matrix\n    int rows_out = N;\n    int cols_out = N;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local = rows / size;\n    int cols_local = cols;\n\n    // You can use the following variables to get the number of rows and columns of the local output matrix\n    int rows_local_out = rows_out / size;\n    int cols_local_out = cols_out;\n\n    // You can use the following variables to get the starting row and column of the local input matrix\n    int start_row = rank * rows_local;\n    int start_col = 0;\n\n    // You can use the following variables to get the starting row and column of the local output matrix\n    int start_row_out = rank * rows_local_out;\n    int start_col_out = 0;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local_out_thread = rows_local_out / num_threads;\n    int cols_local_out_thread = cols_local_out;\n\n    // You can use the following variables to get the number of rows and columns of the local output matrix\n    int start_row_out_thread = rank * rows_local_out_thread;\n    int start_col_out_thread = 0;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local_out_thread_rank = rows_local_out_thread / size;\n    int cols_local_out_thread_rank = cols_local_out_thread;\n\n    // You can use the following variables to get the number of rows and columns of the local output matrix\n    int start_row_out_thread_rank = rank * rows_local_out_thread_rank;\n    int start_col_out_thread_rank = 0;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local_out_thread_rank_thread = rows_local_out_thread_rank / num_threads;\n    int cols_local_out_thread_rank_thread = cols_local_out_thread_rank;\n\n    // You can use the following variables to get the number of rows and columns of the local output matrix\n    int start_row_out_thread_rank_thread = rank * rows_local_out_thread_rank_thread;\n    int start_col_out_thread_rank_thread = 0;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local_out_thread_rank_thread_thread = rows_local_out_thread_rank_thread / num_threads;\n    int cols_local_out_thread_rank_thread_thread = cols_local_out_thread_rank_thread;\n\n    // You can use the following variables to get the number of rows and columns of the local output matrix\n    int start_row_out_thread_rank_thread_thread = rank * rows_local_out_thread_rank_thread_thread;\n    int start_col_out_thread_rank_thread_thread = 0;\n\n    // You can use the following variables to get the number of rows and columns of the local input matrix\n    int rows_local_out_thread_rank_thread_thread_thread = rows_local_out_thread_rank_thread_thread / num_threads;\n    int cols",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Fill in the body of this function.\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function.\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N), [&](int i, int j) {\n    int count = 0;\n    if (i > 0 && input(i - 1, j)) count++;\n    if (i < N - 1 && input(i + 1, j)) count++;\n    if (j > 0 && input(i, j - 1)) count++;\n    if (j < N - 1 && input(i, j + 1)) count++;\n    if (count == 1) output(i, j) = 1;\n    else output(i, j) = 0;\n  });\n}",
            "// TODO\n}",
            "// TODO: Fill in the body of this function.\n  // You may need to use Kokkos::parallel_for to parallelize this loop.\n  // You may also need to use Kokkos::single to parallelize this loop.\n  // You may also need to use Kokkos::atomic to parallelize this loop.\n  // You may also need to use Kokkos::subview to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You may also need to use Kokkos::deep_copy to parallelize this loop.\n  // You",
            "// TODO: Implement this function\n}",
            "// TODO: Fill in the body of this function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, N, 0, N),\n                       KOKKOS_LAMBDA(const int i, const int j) {\n    int count = 0;\n    for (int di = -1; di <= 1; ++di) {\n      for (int dj = -1; dj <= 1; ++dj) {\n        if (di == 0 && dj == 0) {\n          continue;\n        }\n        int ii = i + di;\n        int jj = j + dj;\n        if (ii < 0 || ii >= N || jj < 0 || jj >= N) {\n          continue;\n        }\n        if (input(ii, jj) == 1) {\n          count++;\n        }\n      }\n    }\n    if (count == 1) {\n      output(i, j) = 1;\n    } else {\n      output(i, j) = 0;\n    }\n  });\n}",
            "// TODO: Fill in the body of this function.\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - N] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n  // Hint: Use OpenMP to parallelize this function\n  // Hint: You can use the following code to get the number of threads\n  // int num_threads = omp_get_num_threads();\n  // Hint: You can use the following code to get the thread number\n  // int thread_num = omp_get_thread_num();\n  // Hint: You can use the following code to get the number of cells in the grid\n  // size_t num_cells = N * N;\n  // Hint: You can use the following code to get the index of the cell in the grid\n  // size_t index = i * N + j;\n  // Hint: You can use the following code to get the row and column of the cell\n  // size_t row = index / N;\n  // size_t col = index % N;\n  // Hint: You can use the following code to get the index of the neighbor cell\n  // size_t neighbor_index = (row + dr) * N + (col + dc);\n  // Hint: You can use the following code to get the row and column of the neighbor cell\n  // size_t neighbor_row = neighbor_index / N;\n  // size_t neighbor_col = neighbor_index % N;\n  // Hint: You can use the following code to check if the neighbor cell is in the grid\n  // if (neighbor_row >= 0 && neighbor_row < N && neighbor_col >= 0 && neighbor_col < N) {\n  //  ...\n  // }\n  // Hint: You can use the following code to get the value of the cell\n  // int value = input[index];\n  // Hint: You can use the following code to get the value of the neighbor cell\n  // int neighbor_value = input[neighbor_index];\n  // Hint: You can use the following code to set the value of the cell\n  // output[index] = value;\n  // Hint: You can use the following code to set the value of the neighbor cell\n  // output[neighbor_index] = neighbor_value;\n  // Hint: You can use the following code to check if the cell is a 1\n  // if (value == 1) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the neighbor cell is a 1\n  // if (neighbor_value == 1) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 0\n  // if (value == 0) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the neighbor cell is a 0\n  // if (neighbor_value == 0) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 1 and the neighbor cell is a 1\n  // if (value == 1 && neighbor_value == 1) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 0 and the neighbor cell is a 0\n  // if (value == 0 && neighbor_value == 0) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 1 and the neighbor cell is a 0\n  // if (value == 1 && neighbor_value == 0) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 0 and the neighbor cell is a 1\n  // if (value == 0 && neighbor_value == 1) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 1 and the neighbor cell is a 1\n  // if (value == 1 && neighbor_value == 1) {\n  //  ...\n  // }\n  // Hint: You can use the following code to check if the cell is a 0 and the neighbor cell is a 0\n  // if (value == 0 && neighbor_value == 0) {\n  //  ...\n  // }\n  // Hint: You can use the following code",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - N] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Fill this in\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Fill in your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0) {\n                if (input[i * N + j - 1] == 1) {\n                    count++;\n                }\n            }\n            if (i > 0 && j < N - 1) {\n                if (input[i * N + j + 1] == 1) {\n                    count++;\n                }\n            }\n            if (i < N - 1 && j > 0) {\n                if (input[(i + 1) * N + j - 1] == 1) {\n                    count++;\n                }\n            }\n            if (i < N - 1 && j < N - 1) {\n                if (input[(i + 1) * N + j + 1] == 1) {\n                    count++;\n                }\n            }\n            if (i > 0 && j > 0) {\n                if (input[(i - 1) * N + j - 1] == 1) {\n                    count++;\n                }\n            }\n            if (i > 0 && j < N - 1) {\n                if (input[(i - 1) * N + j + 1] == 1) {\n                    count++;\n                }\n            }\n            if (i < N - 1 && j > 0) {\n                if (input[(i + 1) * N + j - 1] == 1) {\n                    count++;\n                }\n            }\n            if (i < N - 1 && j < N - 1) {\n                if (input[(i + 1) * N + j + 1] == 1) {\n                    count++;\n                }\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      int count = 0;\n      if (i > 0 && input[i * N + j - 1] == 1) {\n        count++;\n      }\n      if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n        count++;\n      }\n      if (j > 0 && input[(i - 1) * N + j] == 1) {\n        count++;\n      }\n      if (j > 0 && i < N - 1 && input[(i + 1) * N + j] == 1) {\n        count++;\n      }\n      if (count == 1) {\n        output[i * N + j] = 1;\n      } else {\n        output[i * N + j] = 0;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int neighbors = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                neighbors++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                neighbors++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                neighbors++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                neighbors++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j] == 1) {\n                neighbors++;\n            }\n            if (j > 0 && input[(i + 1) * N + j] == 1) {\n                neighbors++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                neighbors++;\n            }\n            if (neighbors == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                ++count;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                ++count;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "serial",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: implement\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n    std::vector<int> temp(N*N);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                        if (input[(i + k) * N + j + l] == 1) {\n                            count++;\n                        }\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (count == 1) {\n                    temp[i * N + j] = 1;\n                } else {\n                    temp[i * N + j] = 0;\n                }\n            } else {\n                if (count == 3) {\n                    temp[i * N + j] = 1;\n                } else {\n                    temp[i * N + j] = 0;\n                }\n            }\n        }\n    }\n    output = temp;\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    std::vector<int> temp(N*N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i*N + j - 1] == 1) {\n                ++count;\n            }\n            if (i > 0 && input[i*N + j - N] == 1) {\n                ++count;\n            }\n            if (i > 0 && j < N - 1 && input[i*N + j + 1] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[(i - 1)*N + j] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[(i + 1)*N + j] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[(i - 1)*N + j + 1] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[(i + 1)*N + j - 1] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[(i + 1)*N + j + 1] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                temp[i*N + j] = 1;\n            }\n        }\n    }\n    output = temp;\n}",
            "// TODO: Implement this function\n    std::vector<int> tmp(N*N, 0);\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i*N + j - 1] == 1) count++;\n            if (i < N - 1 && input[i*N + j + 1] == 1) count++;\n            if (j > 0 && input[(i-1)*N + j] == 1) count++;\n            if (j < N - 1 && input[(i+1)*N + j] == 1) count++;\n            if (count == 1) tmp[i*N + j] = 1;\n        }\n    }\n    output = tmp;\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && j > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && input[i * N + j - 1] == 1) {\n                count++;\n            }\n            if (i > 0 && j < N - 1 && input[i * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i - 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (j > 0 && input[(i + 1) * N + j - 1] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[(i + 1) * N + j + 1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "// TODO: implement this\n}",
            "// TODO: Implement this function\n    std::vector<int> temp(N*N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) {\n                count++;\n            }\n            if (i < N-1 && input[(i+1)*N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i*N + j-1] == 1) {\n                count++;\n            }\n            if (j < N-1 && input[i*N + j+1] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                temp[i*N + j] = 1;\n            }\n        }\n    }\n    output = temp;\n}",
            "std::vector<int> neighbors(N*N);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            size_t index = i*N + j;\n            neighbors[index] = 0;\n            if (i > 0) {\n                neighbors[index] += input[(i-1)*N + j];\n            }\n            if (i < N-1) {\n                neighbors[index] += input[(i+1)*N + j];\n            }\n            if (j > 0) {\n                neighbors[index] += input[i*N + j - 1];\n            }\n            if (j < N-1) {\n                neighbors[index] += input[i*N + j + 1];\n            }\n            output[index] = (neighbors[index] == 1)? 1 : 0;\n        }\n    }\n}",
            "// TODO: Implement this function\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int neighbors = 0;\n            for (size_t k = 0; k < N; ++k) {\n                for (size_t l = 0; l < N; ++l) {\n                    if (i == k && j == l) {\n                        continue;\n                    }\n                    if (input[k * N + l] == 1) {\n                        neighbors++;\n                    }\n                }\n            }\n            if (neighbors == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
            "std::vector<int> neighbors(N*N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            for (int di = -1; di <= 1; ++di) {\n                for (int dj = -1; dj <= 1; ++dj) {\n                    if (di == 0 && dj == 0) continue;\n                    int ii = i + di;\n                    int jj = j + dj;\n                    if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n                        count += input[ii*N + jj];\n                    }\n                }\n            }\n            neighbors[i*N + j] = count;\n        }\n    }\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            output[i*N + j] = (neighbors[i*N + j] == 1)? 1 : 0;\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1)\n                count++;\n            if (i < N - 1 && input[i + 1 + j * N] == 1)\n                count++;\n            if (j > 0 && input[i + (j - 1) * N] == 1)\n                count++;\n            if (j < N - 1 && input[i + (j + 1) * N] == 1)\n                count++;\n            if (count == 1)\n                output[i + j * N] = 1;\n            else\n                output[i + j * N] = 0;\n        }\n    }\n}",
            "// TODO: Implement this function\n    std::vector<int> temp(N*N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && j > 0) {\n                if (input[i*N + j - 1] == 1) {\n                    ++count;\n                }\n            }\n            if (i > 0 && j < N - 1) {\n                if (input[i*N + j + 1] == 1) {\n                    ++count;\n                }\n            }\n            if (i < N - 1 && j > 0) {\n                if (input[(i + 1)*N + j - 1] == 1) {\n                    ++count;\n                }\n            }\n            if (i < N - 1 && j < N - 1) {\n                if (input[(i + 1)*N + j + 1] == 1) {\n                    ++count;\n                }\n            }\n            if (i > 0) {\n                if (input[(i - 1)*N + j] == 1) {\n                    ++count;\n                }\n            }\n            if (i < N - 1) {\n                if (input[(i + 1)*N + j] == 1) {\n                    ++count;\n                }\n            }\n            if (j > 0) {\n                if (input[i*N + j - 1] == 1) {\n                    ++count;\n                }\n            }\n            if (j < N - 1) {\n                if (input[i*N + j + 1] == 1) {\n                    ++count;\n                }\n            }\n            if (count == 1) {\n                temp[i*N + j] = 1;\n            }\n        }\n    }\n    output = temp;\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int neighbors = 0;\n            if (i > 0) {\n                neighbors += input[i - 1 + N * j];\n            }\n            if (i < N - 1) {\n                neighbors += input[i + 1 + N * j];\n            }\n            if (j > 0) {\n                neighbors += input[i + N * (j - 1)];\n            }\n            if (j < N - 1) {\n                neighbors += input[i + N * (j + 1)];\n            }\n            output[i + N * j] = (neighbors == 1)? 1 : 0;\n        }\n    }\n}",
            "// TODO: Your code here\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1)\n                ++count;\n            if (i < N - 1 && input[i + 1 + j * N] == 1)\n                ++count;\n            if (j > 0 && input[i + (j - 1) * N] == 1)\n                ++count;\n            if (j < N - 1 && input[i + (j + 1) * N] == 1)\n                ++count;\n            if (count == 1)\n                output[i + j * N] = 1;\n            else\n                output[i + j * N] = 0;\n        }\n    }\n}",
            "std::vector<int> neighbors(N*N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int x_ = x + i;\n                    int y_ = y + j;\n                    if (x_ < 0) x_ = N - 1;\n                    if (x_ >= N) x_ = 0;\n                    if (y_ < 0) y_ = N - 1;\n                    if (y_ >= N) y_ = 0;\n                    if (input[x_ * N + y_] == 1) {\n                        count++;\n                    }\n                }\n            }\n            neighbors[i * N + j] = count;\n        }\n    }\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (neighbors[i * N + j] == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "hip",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int ii = i - 1; ii <= i + 1; ii++) {\n    for (int jj = j - 1; jj <= j + 1; jj++) {\n      if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n        if (input[ii * N + jj] == 1) {\n          count++;\n        }\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  int count = 0;\n  for (int k = 0; k < 8; ++k) {\n    int ii = i + dx[k];\n    int jj = j + dy[k];\n    if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n      count += input[ii * N + jj];\n    }\n  }\n  output[i * N + j] = count == 1? 1 : 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  int count = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n        count += input[(i + k) * N + (j + l)];\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int k = 0; k < N; k++) {\n      for (int l = 0; l < N; l++) {\n        if (input[i * N + k] == 1 && input[k * N + l] == 1 && input[l * N + j] == 1) {\n          count++;\n        }\n      }\n    }\n    if (count == 1) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[i - 1 + j * N] == 1)\n            count++;\n        if (i < N - 1 && input[i + 1 + j * N] == 1)\n            count++;\n        if (j > 0 && input[i + (j - 1) * N] == 1)\n            count++;\n        if (j < N - 1 && input[i + (j + 1) * N] == 1)\n            count++;\n\n        if (count == 1)\n            output[i + j * N] = 1;\n        else\n            output[i + j * N] = 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          count += input[(i + k) * N + j + l];\n        }\n      }\n    }\n    output[i * N + j] = count == 1? 1 : 0;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int count = 0;\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          count += input[(i + k) * N + j + l];\n        }\n      }\n    }\n    if (count == 1) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  int count = 0;\n  if (i > 0 && input[i - 1 + j * N] == 1) count++;\n  if (i < N - 1 && input[i + 1 + j * N] == 1) count++;\n  if (j > 0 && input[i + (j - 1) * N] == 1) count++;\n  if (j < N - 1 && input[i + (j + 1) * N] == 1) count++;\n  output[i + j * N] = count == 1? 1 : 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int sum = 0;\n  if (i < N && j < N) {\n    for (int k = 0; k < 8; k++) {\n      int x = i + dx[k];\n      int y = j + dy[k];\n      if (x >= 0 && x < N && y >= 0 && y < N) {\n        sum += input[x * N + y];\n      }\n    }\n    output[i * N + j] = (sum == 1)? 1 : 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          if (input[i + k + (j + l) * N] == 1) {\n            count++;\n          }\n        }\n      }\n    }\n    if (count == 1) {\n      output[i + j * N] = 1;\n    } else {\n      output[i + j * N] = 0;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int neighbors = 0;\n        if (i > 0) {\n            neighbors += input[(i - 1) * N + j];\n        }\n        if (i < N - 1) {\n            neighbors += input[(i + 1) * N + j];\n        }\n        if (j > 0) {\n            neighbors += input[i * N + j - 1];\n        }\n        if (j < N - 1) {\n            neighbors += input[i * N + j + 1];\n        }\n        output[i * N + j] = (input[i * N + j] == 1 && neighbors == 1)? 1 : 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          count += input[(i + k) * N + j + l];\n        }\n      }\n    }\n    output[i * N + j] = (count == 1)? 1 : 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int count = 0;\n    if (i > 0 && j > 0 && input[i - 1 + (j - 1) * N] == 1) {\n      count++;\n    }\n    if (i > 0 && input[i - 1 + j * N] == 1) {\n      count++;\n    }\n    if (i > 0 && j < N - 1 && input[i - 1 + (j + 1) * N] == 1) {\n      count++;\n    }\n    if (j > 0 && input[i + (j - 1) * N] == 1) {\n      count++;\n    }\n    if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n      count++;\n    }\n    if (i < N - 1 && j > 0 && input[i + 1 + (j - 1) * N] == 1) {\n      count++;\n    }\n    if (i < N - 1 && input[i + 1 + j * N] == 1) {\n      count++;\n    }\n    if (i < N - 1 && j < N - 1 && input[i + 1 + (j + 1) * N] == 1) {\n      count++;\n    }\n    if (count == 1) {\n      output[i + j * N] = 1;\n    } else {\n      output[i + j * N] = 0;\n    }\n  }\n}",
            "// TODO: Implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n  int count = 0;\n  for (int k = 0; k < N; k++) {\n    for (int l = 0; l < N; l++) {\n      if (input[k * N + l] == 1) {\n        count++;\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int count = 0;\n    if (i > 0 && j > 0) {\n      count += input[(i - 1) * N + j - 1];\n    }\n    if (i > 0 && j < N - 1) {\n      count += input[(i - 1) * N + j];\n    }\n    if (i < N - 1 && j > 0) {\n      count += input[i * N + j - 1];\n    }\n    if (i < N - 1 && j < N - 1) {\n      count += input[i * N + j];\n    }\n    if (i > 0) {\n      count += input[(i - 1) * N + j];\n    }\n    if (i < N - 1) {\n      count += input[i * N + j];\n    }\n    if (j > 0) {\n      count += input[i * N + j - 1];\n    }\n    if (j < N - 1) {\n      count += input[i * N + j];\n    }\n    output[i * N + j] = count == 1? 1 : 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) return;\n\n  int count = 0;\n  if (i > 0 && input[i - 1 + j * N] == 1) count++;\n  if (i < N - 1 && input[i + 1 + j * N] == 1) count++;\n  if (j > 0 && input[i + (j - 1) * N] == 1) count++;\n  if (j < N - 1 && input[i + (j + 1) * N] == 1) count++;\n\n  if (count == 1) {\n    output[i + j * N] = 1;\n  } else {\n    output[i + j * N] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n        count += input[i + k + (j + l) * N];\n      }\n    }\n  }\n  output[i + j * N] = count == 1? 1 : 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n        count += input[(i + k) * N + (j + l)];\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n\n  int count = 0;\n  if (i > 0 && input[i - 1 + j * N] == 1) {\n    count++;\n  }\n  if (i < N - 1 && input[i + 1 + j * N] == 1) {\n    count++;\n  }\n  if (j > 0 && input[i + (j - 1) * N] == 1) {\n    count++;\n  }\n  if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n    count++;\n  }\n\n  if (count == 1) {\n    output[i + j * N] = 1;\n  } else {\n    output[i + j * N] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int ii = i - 1; ii <= i + 1; ++ii) {\n    for (int jj = j - 1; jj <= j + 1; ++jj) {\n      if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n        count += input[ii * N + jj];\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO: Your code here\n    //\n    // Note:\n    // - You can assume that N is even.\n    // - You can assume that input and output have the same size.\n    // - You can assume that input and output have the same number of elements.\n    // - You can assume that input and output are not empty.\n    // - You can assume that input and output are NxN grids of ints in row-major.\n    // - You can assume that input and output are stored in row-major order.\n    // - You can assume that input and output are stored in contiguous memory.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.\n    // - You can assume that input and output are stored in the same order on all ranks.",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = N / size;\n    int m = N % size;\n    int start = rank * n + std::min(rank, m);\n    int end = (rank + 1) * n + std::min(rank + 1, m);\n    std::vector<int> local_input(input.begin() + start * N, input.begin() + end * N);\n    std::vector<int> local_output(local_input.size());\n    for (int i = 0; i < local_input.size(); i++) {\n        int x = i / N;\n        int y = i % N;\n        int count = 0;\n        if (x > 0 && local_input[i - N] == 1) {\n            count++;\n        }\n        if (x < N - 1 && local_input[i + N] == 1) {\n            count++;\n        }\n        if (y > 0 && local_input[i - 1] == 1) {\n            count++;\n        }\n        if (y < N - 1 && local_input[i + 1] == 1) {\n            count++;\n        }\n        if (count == 1) {\n            local_output[i] = 1;\n        } else {\n            local_output[i] = 0;\n        }\n    }\n    if (rank == 0) {\n        output.resize(N * N);\n    }\n    MPI_Gather(local_output.data(), local_output.size(), MPI_INT, output.data(), local_output.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Your code here\n    // Note: You may assume that N is even.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.\n    // Note: You may assume that input and output have the same size.",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    // Note that you can use MPI_Send, MPI_Recv, MPI_Bcast, etc.\n    // You can also use MPI_Sendrecv, MPI_Gather, MPI_Gatherv, etc.\n    // You can also use MPI_Allreduce, MPI_Reduce, MPI_Scan, etc.\n    // You can also use MPI_Scatter, MPI_Scatterv, etc.\n    // You can also use MPI_Sendrecv_replace, MPI_Reduce_scatter, etc.\n    // You can also use MPI_Reduce_scatter_block, etc.\n    // You can also use MPI_Alltoall, MPI_Alltoallv, etc.\n    // You can also use MPI_Alltoallw, etc.\n    // You can also use MPI_Exscan, etc.\n    // You can also use MPI_Ixxx, etc.\n    // You can also use MPI_Comm_split, etc.\n    // You can also use MPI_Comm_split_type, etc.\n    // You can also use MPI_Comm_create_group, etc.\n    // You can also use MPI_Comm_create_intercomm, etc.\n    // You can also use MPI_Comm_accept, etc.\n    // You can also use MPI_Comm_connect, etc.\n    // You can also use MPI_Comm_join, etc.\n    // You can also use MPI_Comm_spawn, etc.\n    // You can also use MPI_Comm_spawn_multiple, etc.\n    // You can also use MPI_Comm_disconnect, etc.\n    // You can also use MPI_Comm_get_parent, etc.\n    // You can also use MPI_Comm_spawn_multiple, etc.\n    // You can also use MPI_Comm_get_info, etc.\n    // You can also use MPI_Comm_set_info, etc.\n    // You can also use MPI_Comm_get_name, etc.\n    // You can also use MPI_Comm_set_name, etc.\n    // You can also use MPI_Comm_compare, etc.\n    // You can also use MPI_Comm_remote_size, etc.\n    // You can also use MPI_Comm_remote_group, etc.\n    // You can also use MPI_Comm_test_inter, etc.\n    // You can also use MPI_Comm_test_inter, etc.\n    // You can also use MPI_Comm_get_version, etc.\n    // You can also use MPI_Comm_set_errhandler, etc.\n    // You can also use MPI_Comm_get_errhandler, etc.\n    // You can also use MPI_Comm_call_errhandler, etc.\n    // You can also use MPI_Comm_errhandler_function, etc.\n    // You can also use MPI_Comm_errhandler_free, etc.\n    // You can also use MPI_Comm_create_errhandler, etc.\n    // You can also use MPI_Comm_set_name, etc.\n    // You can also use MPI_Comm_get_name, etc.\n    // You can also use MPI_Comm_set_info, etc.\n    // You can also use MPI_Comm_get_info, etc.\n    // You can also use MPI_Comm_get_attr, etc.\n    // You can also use MPI_Comm_set_attr, etc.\n    // You can also use MPI_Comm_delete_attr, etc.\n    // You can also use MPI_Comm_get_topology, etc.\n    // You can also use MPI_Comm_get_dims, etc.\n    // You can also use MPI_Comm_get_group, etc.\n    // You can also use MPI_Comm_group, etc.\n    // You can also use MPI_Comm_remote_group, etc.\n    // You can also use MPI_Comm_set_name, etc.\n    // You can also use MPI_Comm_get_name, etc.\n    // You can also use MPI_Comm_get_info, etc.\n    // You can also use MPI_Comm_set_info, etc.",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "cuda",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        if (x == 0 && y == 0) {\n          continue;\n        }\n        int x_ = i + x;\n        int y_ = j + y;\n        if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n          continue;\n        }\n        if (input[x_ * N + y_] == 1) {\n          count++;\n        }\n      }\n    }\n    if (count == 1) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    int count = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            if (x == 0 && y == 0) continue;\n            if (i + x < 0 || i + x >= N || j + y < 0 || j + y >= N) continue;\n            if (input[i + x + (j + y) * N] == 1) count++;\n        }\n    }\n    if (count == 1) output[i + j * N] = 1;\n    else output[i + j * N] = 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int k = -1; k <= 1; k++) {\n    for (int l = -1; l <= 1; l++) {\n      if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n        count += input[i + k + (j + l) * N];\n      }\n    }\n  }\n  if (count == 1) {\n    output[i + j * N] = 1;\n  } else {\n    output[i + j * N] = 0;\n  }\n}",
            "// TODO: implement this function\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int count = 0;\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          if (input[i + k + N * (j + l)] == 1) {\n            count++;\n          }\n        }\n      }\n    }\n    if (count == 1) {\n      output[i + N * j] = 1;\n    } else {\n      output[i + N * j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  int count = 0;\n  if (i < N && j < N) {\n    for (int k = -1; k <= 1; k++) {\n      for (int l = -1; l <= 1; l++) {\n        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n          if (input[i + k * N + j + l * N] == 1) {\n            count++;\n          }\n        }\n      }\n    }\n    if (count == 1) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int count = 0;\n    if (i < N && j < N) {\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                    if (input[i + k + (j + l) * N] == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n        if (count == 1) {\n            output[i + j * N] = 1;\n        } else {\n            output[i + j * N] = 0;\n        }\n    }\n}",
            "// TODO: Implement this function.\n  int i = blockIdx.x;\n  int j = blockIdx.y;\n  int count = 0;\n  for (int k = 0; k < N; k++) {\n    for (int l = 0; l < N; l++) {\n      if (input[i * N + k] == 1 && input[k * N + l] == 1) {\n        count++;\n      }\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int count = 0;\n    if (i < N && j < N) {\n        for (int k = 0; k < N; k++) {\n            for (int l = 0; l < N; l++) {\n                if (input[i * N + k] == 1 && input[k * N + l] == 1 && input[l * N + j] == 1) {\n                    count++;\n                }\n            }\n        }\n        if (count == 1) {\n            output[i * N + j] = 1;\n        } else {\n            output[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    int count = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            if (x == 0 && y == 0) continue;\n            int x_ = (i + x + N) % N;\n            int y_ = (j + y + N) % N;\n            count += input[x_ * N + y_];\n        }\n    }\n    output[i * N + j] = count == 1;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        int count = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                    count += input[(i + k) * N + j + l];\n                }\n            }\n        }\n        output[i * N + j] = count == 1? 1 : 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    int count = 0;\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                count += input[(i + k) * N + j + l];\n            }\n        }\n    }\n    if (count == 1) {\n        output[i * N + j] = 1;\n    } else {\n        output[i * N + j] = 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i >= N || j >= N) {\n    return;\n  }\n  int count = 0;\n  for (int k = 0; k < 4; k++) {\n    int ii = i + dx[k];\n    int jj = j + dy[k];\n    if (ii >= N || jj >= N) {\n      continue;\n    }\n    if (input[ii * N + jj] == 1) {\n      count++;\n    }\n  }\n  if (count == 1) {\n    output[i * N + j] = 1;\n  } else {\n    output[i * N + j] = 0;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n\n    int count = 0;\n    for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n            if (x == 0 && y == 0) continue;\n            int x_ = (i + x + N) % N;\n            int y_ = (j + y + N) % N;\n            count += input[x_ * N + y_];\n        }\n    }\n    output[i * N + j] = count == 1? 1 : 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int count = 0;\n    if (i < N && j < N) {\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                    if (input[i + k * N + j + l] == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n        if (count == 1) {\n            output[i * N + j] = 1;\n        } else {\n            output[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < N) {\n    int count = 0;\n    if (i > 0 && j > 0) {\n      count += input[(i - 1) * N + (j - 1)];\n    }\n    if (i > 0 && j < N - 1) {\n      count += input[(i - 1) * N + (j + 1)];\n    }\n    if (i < N - 1 && j > 0) {\n      count += input[(i + 1) * N + (j - 1)];\n    }\n    if (i < N - 1 && j < N - 1) {\n      count += input[(i + 1) * N + (j + 1)];\n    }\n    if (i > 0) {\n      count += input[(i - 1) * N + j];\n    }\n    if (i < N - 1) {\n      count += input[(i + 1) * N + j];\n    }\n    if (j > 0) {\n      count += input[i * N + (j - 1)];\n    }\n    if (j < N - 1) {\n      count += input[i * N + (j + 1)];\n    }\n    if (count == 1) {\n      output[i * N + j] = 1;\n    } else {\n      output[i * N + j] = 0;\n    }\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int count = 0;\n    if (i < N && j < N) {\n        for (int k = -1; k < 2; k++) {\n            for (int l = -1; l < 2; l++) {\n                if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                    if (input[(i + k) * N + j + l] == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n        if (count == 1) {\n            output[i * N + j] = 1;\n        } else {\n            output[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    int count = 0;\n    if (i < N && j < N) {\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) {\n                    continue;\n                }\n                int x_ = i + x;\n                int y_ = j + y;\n                if (x_ < 0 || x_ >= N || y_ < 0 || y_ >= N) {\n                    continue;\n                }\n                if (input[x_ * N + y_] == 1) {\n                    count++;\n                }\n            }\n        }\n        if (count == 1) {\n            output[i * N + j] = 1;\n        } else {\n            output[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && j > 0) {\n            if (input[(i - 1) * N + j - 1] == 1) {\n                count++;\n            }\n        }\n        if (i > 0 && j < N - 1) {\n            if (input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n        }\n        if (i < N - 1 && j > 0) {\n            if (input[i * N + j - 1] == 1) {\n                count++;\n            }\n        }\n        if (i < N - 1 && j < N - 1) {\n            if (input[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (i > 0 && j > 0) {\n            if (input[(i - 1) * N + j - 1] == 1) {\n                count++;\n            }\n        }\n        if (i > 0 && j < N - 1) {\n            if (input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n        }\n        if (i < N - 1 && j > 0) {\n            if (input[i * N + j - 1] == 1) {\n                count++;\n            }\n        }\n        if (i < N - 1 && j < N - 1) {\n            if (input[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count == 1) {\n            output[i * N + j] = 1;\n        } else {\n            output[i * N + j] = 0;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) return;\n    int count = 0;\n    for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n            if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                if (input[i + k + (j + l) * N] == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    if (count == 1) {\n        output[i + j * N] = 1;\n    } else {\n        output[i + j * N] = 0;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N && j < N) {\n        int neighbors = 0;\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) {\n                    continue;\n                }\n                int x_ = (i + x + N) % N;\n                int y_ = (j + y + N) % N;\n                neighbors += input[x_ * N + y_];\n            }\n        }\n        output[i * N + j] = (neighbors == 1)? 1 : 0;\n    }\n}"
        ]
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "47_stencil_xor_kernel",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "temperature": 0.2,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "outputs": [
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Implement this function\n    // Hint: Use MPI_Send and MPI_Recv to send and receive data between ranks\n    // Hint: Use OpenMP to parallelize the computation\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = N / size;\n    int m = N % size;\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                int count = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                            if (input[i + k + (j + l) * N] == 1) {\n                                count++;\n                            }\n                        }\n                    }\n                }\n                if (count == 1) {\n                    output[i + j * N] = 1;\n                } else {\n                    output[i + j * N] = 0;\n                }\n            }\n        }\n    } else {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                int count = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                            if (input[i + k + (j + l) * N] == 1) {\n                                count++;\n                            }\n                        }\n                    }\n                }\n                if (count == 1) {\n                    output[i + j * N] = 1;\n                } else {\n                    output[i + j * N] = 0;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < m; i++) {\n            for (int j = 0; j < m; j++) {\n                int count = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                            if (input[i + k + (j + l) * N] == 1) {\n                                count++;\n                            }\n                        }\n                    }\n                }\n                if (count == 1) {\n                    output[i + j * N] = 1;\n                } else {\n                    output[i + j * N] = 0;\n                }\n            }\n        }\n    } else {\n        for (int i = 0; i < m; i++) {\n            for (int j = 0; j < m; j++) {\n                int count = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1; l <= 1; l++) {\n                        if (i + k >= 0 && i + k < N && j + l >= 0 && j + l < N) {\n                            if (input[i + k + (j + l) * N] == 1) {\n                                count++;\n                            }\n                        }\n                    }\n                }\n                if (count == 1) {\n                    output[i + j * N] = 1;\n                } else {\n                    output[i + j * N] = 0;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < m; j++) {\n                int count = 0;\n                for (int k = -1; k <= 1; k++) {\n                    for (int l = -1;",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n  // Hint: You can use MPI_Send, MPI_Recv, MPI_Sendrecv, MPI_Bcast, MPI_Reduce, MPI_Allreduce, MPI_Scatter, MPI_Gather\n  // Hint: You can use OpenMP to parallelize the computation\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int *input_rank = new int[N*N];\n    int *output_rank = new int[N*N];\n    int *input_rank_temp = new int[N*N];\n    int *output_rank_temp = new int[N*N];\n    int *input_rank_temp_temp = new int[N*N];\n    int *output_rank_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *output_rank_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp = new int[N*N];\n    int *input_rank",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}"
        ]
    }
]